<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">






<link rel="stylesheet" href="/css/main.css?v=7.2.0">






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="作者: 唐进民出版社: 电子工业出版社出版年: 2018-6丛书: 博文视点AI系列ISBN: 9787121341441   内容简介计算机视觉、自然语言处理和语音识别是目前深度学习领域很热门的三大应用方向，本书旨在帮助零基础或基础较为薄弱的读者入门深度学习，达到能够独立使用深度学习知识处理计算机视觉问题的水平。通过阅读本书，读者将学到人工智能的基础概念及Python编程技能，掌握PyTorch">
<meta name="keywords" content="计算机,自评,books,计算机视觉,更毕,深度学习,PyTorch,python,计算科学,科普,豆瓣3">
<meta property="og:type" content="article">
<meta property="og:title" content="book_《深度学习之PyTorch实战计算机视觉》_唐进民">
<meta property="og:url" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/index.html">
<meta property="og:site_name" content="Hac_lang">
<meta property="og:description" content="作者: 唐进民出版社: 电子工业出版社出版年: 2018-6丛书: 博文视点AI系列ISBN: 9787121341441   内容简介计算机视觉、自然语言处理和语音识别是目前深度学习领域很热门的三大应用方向，本书旨在帮助零基础或基础较为薄弱的读者入门深度学习，达到能够独立使用深度学习知识处理计算机视觉问题的水平。通过阅读本书，读者将学到人工智能的基础概念及Python编程技能，掌握PyTorch">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00000.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00001.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00002.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00003.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00004.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00005.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00006.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00007.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00008.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00009.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00010.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00011.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00012.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00013.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00014.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00015.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00016.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00017.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00018.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00019.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00020.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00021.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00022.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00023.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00024.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00025.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00026.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00027.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00028.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00029.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00030.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00031.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00032.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00033.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00034.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00035.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00036.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00037.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00038.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00039.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00040.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00041.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00042.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00043.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00044.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00045.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00046.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00047.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00048.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00049.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00050.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00051.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00052.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00053.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00054.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00055.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00056.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00057.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00058.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00059.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00060.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00061.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00062.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00063.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00064.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00065.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00066.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00067.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00068.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00069.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00070.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00071.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00072.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00073.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00074.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00075.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00076.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00077.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00078.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00079.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00080.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00081.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00082.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00083.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00084.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00085.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00086.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00087.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00088.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00089.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00090.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00091.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00092.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00093.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00094.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00095.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00096.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00097.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00098.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00099.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00100.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00101.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00102.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00103.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00104.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00105.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00106.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00107.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00108.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00109.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00110.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00111.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00112.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00113.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00114.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00115.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00116.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00117.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00118.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00119.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00120.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00121.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00122.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00123.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00124.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00125.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00126.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00127.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00128.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00129.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00130.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00131.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00132.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00133.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00134.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00135.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00136.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00137.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00138.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00139.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00140.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00141.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00142.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00143.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00144.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00145.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00146.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00147.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00148.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00149.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00150.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00151.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00152.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00153.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00154.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00155.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00156.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00157.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00158.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00159.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00160.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00161.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00162.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00163.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00164.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00165.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00166.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00167.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00168.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00169.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00170.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00171.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00172.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00173.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00174.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00175.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00176.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00177.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00178.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00179.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00180.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00181.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00182.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00183.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00184.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00185.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00186.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00187.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00188.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00189.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00190.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00191.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00192.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00193.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00194.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00195.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00196.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00197.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00198.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00199.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00200.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00201.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00202.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00203.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00204.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00205.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00206.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00207.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00208.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00209.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00210.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00211.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00212.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00213.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00214.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00215.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00216.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00217.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00218.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00219.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00220.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00221.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00222.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00223.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00224.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00225.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00226.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00227.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00228.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00229.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00230.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00231.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00232.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00233.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00234.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00235.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00236.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00237.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00238.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00239.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00240.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00241.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00242.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00243.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00244.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00245.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00246.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00247.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00248.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00249.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00250.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00251.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00252.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00253.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00254.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00255.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00256.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00257.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00258.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00259.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00260.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00261.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00262.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00263.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00264.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00265.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00266.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00267.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00268.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00269.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00270.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00271.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00272.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00273.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00274.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00275.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00276.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00277.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00278.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00279.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00280.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00281.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00282.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00283.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00284.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00285.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00286.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00287.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00288.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00289.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00290.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00291.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00292.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00293.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00294.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00295.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00296.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00297.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00298.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00299.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00300.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00301.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00302.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00303.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00304.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00305.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00306.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00307.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00308.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00309.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00310.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00311.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00312.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00313.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00314.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00315.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00316.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00317.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00318.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00319.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00320.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00321.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00322.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00323.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00324.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00325.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00326.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00327.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00328.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00329.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00330.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00331.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00332.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00333.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00334.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00335.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00336.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00337.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00338.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00339.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00340.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00341.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00342.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00343.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00344.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00345.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00346.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00347.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00348.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00349.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00350.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00351.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00352.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00353.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00354.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00355.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00356.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00357.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00358.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00359.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00360.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00361.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00362.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00363.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00364.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00365.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00366.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00367.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00368.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00369.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00370.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00371.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00372.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00373.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00374.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00375.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00376.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00377.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00378.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00379.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00380.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00381.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00382.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00383.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00384.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00385.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00386.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00387.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00388.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00389.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00390.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00391.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00392.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00393.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00394.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00395.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00396.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00397.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00398.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00399.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00400.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00401.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00402.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00403.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00404.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00405.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00406.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00407.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00408.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00409.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00410.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00411.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00412.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00413.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00414.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00415.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00416.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00417.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00418.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00419.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00420.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00421.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00422.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00423.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00424.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00425.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00426.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00427.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00428.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00429.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00430.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00431.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00432.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00433.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00434.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00435.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00436.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00437.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00438.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00439.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00440.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00441.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00442.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00443.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00444.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00445.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00446.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00447.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00448.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00449.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00450.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00451.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00452.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00453.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00454.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00455.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00456.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00457.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00458.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00459.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00460.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00461.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00462.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00463.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00464.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00465.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00466.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00467.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00468.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00469.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00470.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00471.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00472.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00473.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00474.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00475.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00476.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00477.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00478.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00479.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00480.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00481.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00482.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00483.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00484.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00485.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00486.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00487.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00488.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00489.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00490.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00491.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00492.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00493.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00494.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00495.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00496.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00497.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00498.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00499.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00500.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00501.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00502.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00503.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00504.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00505.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00506.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00507.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00508.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00509.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00510.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00511.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00512.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00513.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00514.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00515.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00516.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00517.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00518.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00519.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00520.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00521.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00522.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00523.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00524.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00525.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00526.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00527.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00528.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00529.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00530.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00531.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00532.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00533.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00534.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00535.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00536.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00537.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00538.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00539.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00540.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00541.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00542.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00543.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00544.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00545.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00546.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00547.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00548.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00549.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00550.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00551.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00552.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00553.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00554.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00555.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00556.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00557.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00558.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00559.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00560.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00561.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00562.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00563.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00564.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00565.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00566.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00567.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00568.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00569.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00570.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00571.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00572.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00573.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00574.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00575.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00576.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00577.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00578.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00579.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00580.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00581.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00582.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00583.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00584.jpg">
<meta property="og:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00585.jpg">
<meta property="og:updated_time" content="2020-08-14T14:51:53.519Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="book_《深度学习之PyTorch实战计算机视觉》_唐进民">
<meta name="twitter:description" content="作者: 唐进民出版社: 电子工业出版社出版年: 2018-6丛书: 博文视点AI系列ISBN: 9787121341441   内容简介计算机视觉、自然语言处理和语音识别是目前深度学习领域很热门的三大应用方向，本书旨在帮助零基础或基础较为薄弱的读者入门深度学习，达到能够独立使用深度学习知识处理计算机视觉问题的水平。通过阅读本书，读者将学到人工智能的基础概念及Python编程技能，掌握PyTorch">
<meta name="twitter:image" content="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/Image00000.jpg">





  
  
  <link rel="canonical" href="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>book_《深度学习之PyTorch实战计算机视觉》_唐进民 | Hac_lang</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hac_lang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">小白hac_lang的笔记，涉及内容包含但不限于：人工智能   基因工程    信息安全   软件工程   嵌入式   天文物理</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-news">

    
    
      
    

    

    <a href="/news/" rel="section"><i class="menu-item-icon fa fa-fw fa-rss"></i> <br>news</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    
  
  

  

  <a href="https://github.com/HACLANG" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://haclang.github.io/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hac_lang">
      <meta itemprop="description" content="小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hac_lang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">book_《深度学习之PyTorch实战计算机视觉》_唐进民

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-12 21:05:14" itemprop="dateCreated datePublished" datetime="2018-09-12T21:05:14+08:00">2018-09-12</time>
            </span>
          

          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>作者: 唐进民<br>出版社: 电子工业出版社<br>出版年: 2018-6<br>丛书: 博文视点AI系列<br>ISBN: 9787121341441</p>
<p><img src="Image00000.jpg" alt="cover"></p>
<p><img src="Image00001.jpg" alt="img"></p>
<h1 id="内容简介"><a href="#内容简介" class="headerlink" title="内容简介"></a>内容简介</h1><p>计算机视觉、自然语言处理和语音识别是目前深度学习领域很热门的三大应用方向，本书旨在帮助零基础或基础较为薄弱的读者入门深度学习，达到能够独立使用深度学习知识处理计算机视觉问题的水平。通过阅读本书，读者将学到人工智能的基础概念及Python编程技能，掌握PyTorch的使用方法，学到深度学习相关的理论知识，比如卷积神经网络、循环神经网络、自动编码器，等等。在掌握深度学习理论和编程技能之后，读者还会学到如何基于PyTorch深度学习框架实战计算机视觉。本书中的大量实例可让读者在循序渐进地学习的同时，不断地获得成就感。</p>
<p>本书面向对深度学习技术感兴趣、但是相关基础知识较为薄弱或者零基础的读者。</p>
<h1 id="前-言"><a href="#前-言" class="headerlink" title="前 言"></a>前 言</h1><p>“人工智能”（Artificial Intelligence，简称AI）一词在很久以前就有了，被大众津津乐道却是近几年的事情，这和机器学习（Machine Learning）、深度学习（Deep Learning）等技术的崛起有着千丝万缕的联系，而这一切又得益于大数据的发展和计算机处理性能的不断提升。</p>
<p>本书将带领读者了解人工智能的相关技术和发展近况，通过一些实例来掌握必备的技能，并能够独立使用相关技术完成对计算机视觉问题的分析和处理。本书各个章节的知识要点如下。</p>
<p>第1章主要介绍人工智能、神经网络和计算机视觉的发展历史，让读者对这一领域有一个全面的认识。</p>
<p>第2章主要介绍在理解和掌握后面章节的内容时需要用到的数学知识，以及在实战操作的过程中进行环境搭建及安装相关软件的方法。本书中数学相关的大部分知识都集中在本章中，其主要目的是让读者先对这个领域的知识产生兴趣，这样才能更好地深入学习和研究。在本章中不会插入大量的数学公式，这样做会让初学者望而却步，在不断消化公式的过程中丧失学习兴趣和动力。通过不断实战来学习，可以累积成就感，这种自顶向下的方式不失为一种更好的学习方法。</p>
<p>第3章主要介绍在学习神经网络的过程中会经常遇到的一些概念和定义。比如后向传播（Back Propagation）、激活函数（Activation Function）、监督学习（Supervised Learning）、无监督学习（Unsupervised Learning），等等，这也是为之后学习深度神经网络做准备。在搭建一个完整的深度神经网络模型时，就需要不断地用到本章的内容了。</p>
<p>第4章主要介绍深度神经网络中的卷积神经网络（Convolutional Neural Network，简称CNN）。首先介绍卷积层、全连接层、池化层等相关内容，之后又列举了目前主流的一些卷积神经网络架构，并对比它们之间的相同点和不同点，以便于掌握不同的卷积神经网络的结构和技术细节。</p>
<p>第5章主要介绍Python编程语言的相关知识，目的是让读者掌握Python语言的语法定义和使用方式，并使用 Python语言进行功能代码的编写；还会介绍在处理计算机视觉问题时需要用到的两个重要的Python包：NumPy和Matplotlib。本章内容丰富，而且Python语言自身就很简单且易上手，读者很快就能掌握Python这门编程语言。</p>
<p>第6章主要介绍如何使用PyTorch深度学习框架。PyTorch非常简单易用，能够根据我们的需求快速搭建出我们想要的深度神经网络模型，这在很大程度上归功于PyTorch基于动态图计算的特性，它与基于静态图计算的深度学习框架相比，有更多的优势，比如PyTorch不仅速度快，还有许多功能强大的包可供调用。本章先介绍PyTorch中常用的包和类的使用方法；然后介绍如何使用PyTorch中的一些自动化方法来提升代码的执行效率和简洁度；最后会通过一个综合实例，使用本章的内容解决一个实际的计算机视觉问题。</p>
<p>第7章一开始就是一个关于计算机视觉问题的实战，介绍了一种非常实用的深度神经网络复用方法，即迁移学习（Transfer Learning）。在掌握迁移学习的原理之后，会基于PyTorch对迁移学习进行实战，并解决比之前更复杂的计算机视觉问题。对实战代码的解析会贯穿本章，让读者更深刻地理解代码。</p>
<p>第8章讲解如何基于PyTorch实战图像风格迁移（Neural Style）。通过对本章的学习，读者会发现，利用卷积神经网络不仅能处理图片分类问题，只要有想法和创意，还能做更多、更有趣的事情。</p>
<p>第9章介绍一种多模型融合方法，在现有的模型遭遇性能提升瓶颈时，可通过搭建一种经过科学融合的新模型达到超过预期的泛化能力。本章依然会基于PyTorch对多模型融合方法进行实战。</p>
<p>第10章介绍一种区别于卷积神经网络的新神经网络结构，即循环神经网络（Recurrent Neural Network，简称 RNN）。不同于卷积神经网络强大的图像特征提取能力，循环神经网络主要用于处理有序输入的数据。为了方便读者理解模型如何对有序数据进行处理，本章会基于PyTorch使用循环神经网络来处理一个计算机视觉问题。</p>
<p>第11章讲解自动编码器，它是一种使用非监督学习方法的神经网络。自编码器能够实现很多功能，本章会选取一个图像去噪问题来进行自动编码器实战。</p>
<p>本书前6章的内容可作为后5章的铺垫，前6章的知识偏向基础和理论，不过，只有掌握了这些内容，才能从容应对后5章的实战。这个循序渐进的过程会让读者对知识的理解更深刻，技能提升更迅速。</p>
<p>人工智能在近几年大热，网络上的相关资料良莠不齐且没有体系，即使有优秀的干货，对于基础薄弱的初学者来说起点也太高。本书也是出于对这一现状的考虑，通过从基础到实战、由浅入深的过程，让读者基于PyTorch来使用深度学习方法实际解决一些计算机视觉相关的问题，这样，读者在获取知识的过程中会更有成就感，学起来也会更积极、主动。</p>
<p>唐进民2018年5月</p>
<p>轻松注册成为博文视点社区用户（www.broadview.com.cn），扫码直达本书页面。</p>
<p>◎提交勘误： 您对书中内容的修改意见可在 提交勘误 处提交，若被采纳，将获赠博文视点社区积分（在您购买电子书时，积分可用来抵扣相应金额）。</p>
<p>◎交流互动： 在页面下方读者评论 处留下您的疑问或观点，与我们和其他读者一同学习交流。 页面入口：<a href="http://www.broadview.com.cn/34144" target="_blank" rel="noopener">http://www.broadview.com.cn/34144</a></p>
<p><img src="Image00002.jpg" alt="img"></p>
<h1 id="第1章-浅谈人工智能、神经网络和计算机视觉"><a href="#第1章-浅谈人工智能、神经网络和计算机视觉" class="headerlink" title="第1章 浅谈人工智能、神经网络和计算机视觉"></a>第1章 浅谈人工智能、神经网络和计算机视觉</h1><p>目前，关于人工智能，有不少值得我们期待的新技术涌现，比如自动驾驶、智能图像识别、智能医疗、智能金融等，这些技术正不断被应用到真实的场景中，而且真实的场景还在不断丰富，这势必在无形之中改变各行各业的现状。本章将对人工智能及其相关知识进行简单介绍，带领读者对人工智能及其相关知识进行初步了解。</p>
<h2 id="1-1-人工还是智能"><a href="#1-1-人工还是智能" class="headerlink" title="1.1 人工还是智能"></a>1.1 人工还是智能</h2><p>人工智能可以细分为强人工智能和弱人工智能，弱人工智能更注重“人工”的重要性，强人工智能更注重“智能”的重要性。</p>
<p>通俗地讲，在弱人工智能机器对某个问题进行决策时，人仍然需要积极参与其中，所以弱人工智能也被称作限制领域的人工智能或应用型人工智能。弱人工智能只能在特定的领域解决特定的问题，而且其中的一些问题已经有了明确的答案，比如作为人的智能助手，在某方面代替人的日常重复劳动。弱人工智能技术已经在某些特定领域落地，并对互联网、金融、制造业、医疗等各个领域产生了不小的冲击，而且会持续下去。</p>
<p>而我们在使用强人工智能机器对问题进行决策时，就不再需要人参与其中，因为强人工智能机器能够“思考”，进而不断优化和拓展自己解决问题的能力，甚至能够创造出全新的技能，所以强人工智能也被称作通用人工智能或完全人工智能，即已经具备了能够完全替代人在各领域工作的能力。</p>
<p>人工智能的舞台是巨大的，改变世界的机会无处不在，相信通过我们的不断努力，人工智能技术会发展得更好，应用场景会更丰富，人们的生活、工作方式也将因此发生翻天覆地的变化。</p>
<h2 id="1-2-人工智能的三起两落"><a href="#1-2-人工智能的三起两落" class="headerlink" title="1.2 人工智能的三起两落"></a>1.2 人工智能的三起两落</h2><p>人工智能技术发展至今，已经是第3次受到公众的高度关注了，这得益于计算机和大数据的发展，更重要的是人们看到了能够真正落地应用的产品。下面让我们回顾一下人工智能三起两落的历史。</p>
<h3 id="1-2-1-两起两落"><a href="#1-2-1-两起两落" class="headerlink" title="1.2.1 两起两落"></a>1.2.1 两起两落</h3><p>1956年夏季，John McCarthy、Marvin Minsky、Claude Shannon等人在美国举办的达特茅斯会议（Dartmouth Conference）上首次提出了“人工智能”的概念。这是人类历史上第1个有真正意义的关于人工智能的研讨会，也是人工智能学科诞生的标志，具有十分重要的意义。人工智能概念一经提出，便收获了空前的反响，人工智能历史上的第1股浪潮就这样顺理成章地形成了，该浪潮随即席卷全球。当时，普通大众和研究人工智能的科学家都极为乐观，相信人工智能技术在几年内必将取得重大突破和快速进展，甚至预言在20年内智能机器能完全取代人在各个领域的工作。这种乐观情绪持续高涨，直到1973年《莱特希尔报告》的出现将其终结，该报告用翔实的数据明确指出人工智能的任何部分都没有达到科学家一开始承诺达到的影响力水平，至此人工智能泡沫被无情地戳破，在人们幡然醒悟的同时，人工智能历史上的第1个寒冬到来，人们对人工智能的热情逐渐消退，社会各界的关注度和资金投入也逐年减少。</p>
<p>20世纪80年代，专家系统（Expert System）出现又让企业家和科学家看到了人工智能学科的新希望，继而形成人工智能历史上的第2股浪潮。专家系统是指解决特定领域问题的能力已达到该领域的专家能力水平，其核心是通过运用专家多年积累的丰富经验和专业知识，不断模拟专家解决问题的思维，处理只有专家才能处理的问题。专家系统的出现实现了人工智能学科从理论走向专业知识领域的应用，各种应用场景不断丰富，在人工智能历史上是一次重大突破和转折，具有深远的意义。真正意义上的计算机视觉、机器人、自然语言处理、语音识别等专业领域也诞生于这个阶段。但是随着时间的推移，专家系统的缺点也暴露无遗，最为致命的就是专家系统的应用领域相对狭窄，在很多方面缺乏常识性知识和专业理论的支撑，这直接将第2股人工智能浪潮推向了寒冬。</p>
<p>20世纪90年代后期，机器学习（Machine Learning）、深度学习（Deep Learning）等技术成为人工智能的主流，再加上大数据和计算机硬件的快速发展，使人工智能再次卷土重来，这一次，以语音识别、计算机视觉、自然语言处理为代表的专业领域均取得了巨大突破和进展。</p>
<h3 id="1-2-2-卷土重来"><a href="#1-2-2-卷土重来" class="headerlink" title="1.2.2 卷土重来"></a>1.2.2 卷土重来</h3><p>第3股人工智能浪潮的兴起与机器学习、深度学习技术被广泛应用和研究有着千丝万缕的联系。但是深度学习和机器学习不是什么新兴技术，并且深度学习还是机器学习的一个分支，人工智能、机器学习和深度学习之间的包含关系如图1-1所示。</p>
<p><img src="Image00003.jpg" alt="img"></p>
<p>图1-1</p>
<p>机器学习也被称为统计学习方法，顾名思义，机器学习中的大部分学习算法都是基于统计学原理的，所以机器学习和深度学习技术具备一个共同的特点：它们都需要使用尽可能多的数据来完成对自身模型的训练，这样才能让最终输出的模型拥有强大的泛化能力。</p>
<p>因为本书涉及的是计算机视觉相关的内容，所以我们重点看看计算机视觉领域因为第3股人工智能浪潮的冲击又发生了哪些变化。计算机视觉是人工智能学科中最能体现智能成分的技术，如果计算机视觉问题得到了完美解决，就可以说人类在人工智能领域又迈进了一大步。当前在计算机视觉领域应用得最好的技术是深度学习方法，也可以说深度学习之所以有如此大的影响力，和它在计算机视觉领域取得的突出成绩是分不开的，所以计算机视觉和深度学习成就了彼此。</p>
<p>使用深度学习方法处理计算机视觉问题的过程类似于人类的学习过程：我们搭建的深度学习模型通过对现有图片的不断学习总结出各类图片的特征，最后输出一个理想的模型，该模型能够准确预测新图片所属的类别。图1-2展示了两个不同的学习过程，上半部分是通过使用深度学习模型解决图片分类问题，下半部分是人通过学习总结的方式解决物体识别分类问题，它们之间的工作机理非常相似，这也让深度学习技术拥有了更浓郁的智能色彩。</p>
<p><img src="Image00004.jpg" alt="img"></p>
<p>图1-2</p>
<p>在使用深度学习方法解决计算机视觉问题的过程中，用得最多的网络架构是一种叫作卷积神经网络（Convolutional Neural Network）的模型。卷积神经网络是人工神经网络的变化和升级，是科学家通过模拟人类大脑的工作原理而发明的。人们发现人工神经网络模型能够很好地提取输入图像中的重要特征，而卷积神经网络在图像特征提取方面具有更明显的优势。就拿近几年举办的专业图像识别大赛来说，取得优异成绩的参赛队伍基本上都使用了卷积神经网络模型，这也证明了深度学习方法在图像识别、图像处理、图像特征的提取上要比目前的一些主流的传统机器学习方法效果更好。</p>
<p>人工智能技术是未来各行各业的生产力，国外的 Google、Facebook、Microsoft 等，以及国内的百度、腾讯和阿里巴巴等，都在大量招揽人工智能方向的人才，许多国家也已经将人工智能技术的发展提升到国家战略高度，人工智能相关技术领域的薪酬也是水涨船高，大家都在为通过人工智能学科改变相关领域和行业时刻准备着。</p>
<h2 id="1-3-神经网络简史"><a href="#1-3-神经网络简史" class="headerlink" title="1.3 神经网络简史"></a>1.3 神经网络简史</h2><p>神经网络的概念来自生物学科，人脑中错综复杂的生物神经网络承担着对人自身庞大的生物信息的处理工作。之后出现的人工神经网络其实是科学家根据人脑中生物神经网络的工作原理而抽象出的一种可以用数学进行定义的模型，但这个抽象的过程仅限于认知领域，因为在实际情况下生物神经网络的工作机理会比用数学定义的人工神经网络的表达式复杂许多。即便如此，人工神经网络在处理问题时效果却非常出众。如图1-3所示是我们对机械脑和人脑的一个臆想。</p>
<p><img src="Image00005.jpg" alt="img"></p>
<p>图1-3</p>
<h3 id="1-3-1-生物神经网络和人工神经网络"><a href="#1-3-1-生物神经网络和人工神经网络" class="headerlink" title="1.3.1 生物神经网络和人工神经网络"></a>1.3.1 生物神经网络和人工神经网络</h3><p>构成生物神经网络系统和功能的基本单位是生物神经细胞，也叫作生物神经元，一个生物神经元由细胞核、树突、轴突、突出等组织构成，而一个完整的生物神经网络系统由成千上万个生物神经元构造而成。如图1-4所示就是一个生物神经元的简单图例。</p>
<p><img src="Image00006.jpg" alt="img"></p>
<p>图1-4</p>
<p>在图1-4中左边类似于树枝的是树突，树突是神经元的信息输入端，一个神经元通过树突就可以接收来自外部神经元的信息传入；图中中间的长条是轴突，轴突是神经元的信息输出端，神经元通过轴突可以把神经元中已处理完成的信息传递到突触；图中右边同样类似于树枝但是带有小圆点的是突触，突触是本神经元和外部神经元之间的连接接口。大量的神经元通过树突和突触互相连接，最后构造出一个复杂的神经网络。生物神经元的信息处理流程简单来说是先通过本神经元的树突接收外部神经元传入本神经元的信息，这个信息会根据神经元内定义的激活阈值选择是否激活信息，如果输入的信息最终被神经元激活，那么会通过本神经元的轴突将信息输送到突触，最后通过突触传递至与本神经元连接的其他神经元。</p>
<h3 id="1-3-2-M-P模型"><a href="#1-3-2-M-P模型" class="headerlink" title="1.3.2 M-P模型"></a>1.3.2 M-P模型</h3><p>在知道生物神经元的工作机理后，我们来看一个经典的人工神经元模型。人工神经元的基础模型是由W.S.McCulloch和W.Pitts这两位科学家于1943年根据生物神经元的生物特性和运行机理发明的，这个经典的模型被命名为M- P模型，M和P分别是这两位科学家的名字的首字母。如图1-5所示是一个M-P模型的结构示意图。</p>
<p><img src="Image00007.jpg" alt="img"></p>
<p>图1-5</p>
<p>在图1-5中从左向右看，首先是一列从x 1 到xn 的参数，我们可以将从x 1 到xn 看作类似于生物神经元中的树突接收到的来自外部神经元的信息，不过还需要对这些输入的信息进行相应的处理，处理方法是对信息中的每个参数乘上一个对应的权重值，权重值的范围是从w 1 j 到wn j<br>；图中的圆圈等价于在生物神经元中判断是否对输入的信息进行激活、输出的部分，M-P模型在判断输入的信息能否被激活及输出前会对输入的信息使用∑来完成求和处理，∑就是数学中的累加函数，然后将求和的结果传送给函数f ，函数f 是一个定义了目标阈值的激活函数，这个激活函数只有在满足目标阈值时才能将信息激活及输出；Oj 类似于生物神经元中的轴突，用于承载输出的信息。</p>
<p>M-P模型的数学表达式如下：</p>
<p><img src="Image00008.jpg" alt="img"></p>
<p>为了更直观地理解M-P模型，我们通过一个简单的实例进行说明。假设某个神经元只有两个信息输入，分别是x 1 和x 2 ，其中x 1 =5，x 2 =2，x 1 和x 2 对应的权重值分别是w 1 =0.5，w 2 =2，并且定义激活函数的阈值为θ =5，且激活函数f 的激活条件为：x 1 ×w 1 +x 2 ×w 2 -θ 的计算结果在大于等于0时输出1，在小于0时输出0。通过计算，我们可以得到最后的输出结果Oj 为1。如果激活条件保持不变，则重新定义激活函数的阈值为θ =7，那么最终得到的输出结果就变成0了。</p>
<p>除了以上M-P模型的常规用法，我们还可以使用M- P模型轻易地构造出逻辑与门、逻辑或门和逻辑非门。首先看看如何构造逻辑与门，为了使效果更明显，我们设定三个输入参数x 0 、x 1 和x 2 ，其中x 0 =1。</p>
<p>逻辑与门的M-P模型的输入输出规则如下：当输入x 1 =0且x 2 =0时，输出的结果为0；当输入x 1 =0且x 2 =1时，输出的结果为0；当输入x 1 =1且x 2 =0时，输出的结果为0；当输入x 1 =1且x 2 =1时，输出的结果为1。逻辑与门的M-P模型结构如图1-6所示。</p>
<p><img src="Image00009.jpg" alt="img"></p>
<p>图1-6</p>
<p>为了实现逻辑与门的功能，我们需要对模型中的权重w 0 、w 1 、w 2 的值和激活函数f 进行定义。设定w 0 =-8、w 1 =5、w 2 =5，激活函数f 的激活条件为：1×w 0 +x 1 ×w 1 +x 2 ×w 2 的计算结果在小于0时输出0，在大于等于0时输出1，这样就构造出了一个逻辑与门。值得注意的是，我们当前使用的权重值和激活函数激活条件的组合并不是逻辑与门的唯一定义方式，使用不同的参数组合也能达到相同的效果。</p>
<p>然后，我们看看如何构造逻辑或门。逻辑或门的M-P模型的输入输出规则如下：当输入为x 1 =0且x 2 =0时，输出的结果为0；当输入为x 1 =0且x 2 =1时，输出的结果为1；当输入为x 1 =1且x 2 =0时，输出的结果为1；当输入为x 1 =1且x 2 =1时，输出的结果为1。逻辑或门的M- P模型结构如图1-7所示。</p>
<p><img src="Image00010.jpg" alt="img"></p>
<p>图1-7</p>
<p>同样，为了实现逻辑或门的功能，我们需要对模型中的权重w 0 、w 1 、w 2 的值和激活函数f 进行定义。设定w 0 =-5、w 1 =8、w 2 =8，激活函数f 的激活条件为：1×w 0 +x 1 ×w 1 +x 2 ×w 2 的计算结果在小于0时输出0，在大于等于0时输出1，这样就构造出了一个逻辑或门。我们发现激活函数的激活条件和之前的逻辑与门的激活条件一样，这里我们重点改变了模型的权重值。同样，搭建逻辑或门使用的权重值和激活函数激活条件的组合也不是唯一的。</p>
<p>最后，我们看看如何构造逻辑非门。在逻辑非门中输入的参数只需有两个，分别是x 0 和x 1 ，其中x 0 =1。逻辑非门的 M-P模型的输入输出规则如下：当输入为x 1 =0时，输出的结果为1；当输入为x 1 = 1时，输出的结果为0。逻辑非门的M-P模型结构如图1-8所示。</p>
<p><img src="Image00011.jpg" alt="img"></p>
<p>图1-8</p>
<p>为了实现逻辑非门的功能，在模型中定义的参数变成了权重w 0 、w 1 的值和激活函数f 。我们设定w 0 =10、w 1 =-20，激活函数f 的激活条件为：1×w 0 +x 1 ×w 1 的计算结果在小于0时输出0，在大于等于0时输出1，这样就构造了一个逻辑非门。搭建逻辑非门使用的权重值和激活函数激活条件的组合仍然不是唯一的。</p>
<p>通过对逻辑与门、逻辑或门和逻辑非门进行任意组合，我们可以构造更复杂的神经网络结构，所以M-P模型具备非常实用的特性，当然，它仍有不足之处。</p>
<h3 id="1-3-3-感知机的诞生"><a href="#1-3-3-感知机的诞生" class="headerlink" title="1.3.3 感知机的诞生"></a>1.3.3 感知机的诞生</h3><p>1957年，科学家Frank Rosenblatt提出了一种具有单层计算单元的神经网络模型，这种模型也叫作感知机（Perceptron），它在结构上和M- P模型极为相似，不同之处是感知机被使用的初衷是解决数据的分类问题，因为感知机本身就是一种能够进行二分类的线性模型。那么什么样的模型可被称作二分类线性模型呢？下面让我们通过图1-9来直观地感受一下。</p>
<p><img src="Image00012.jpg" alt="img"></p>
<p>图1-9</p>
<p>在图1-9中用到的数据都是二维的，一条直线将数据一分为二，这条直线就是一个二分类线性模型。在三维空间中同样可以对数据进行二分类，只不过在三维空间中划分数据的不是一条直线，而是一个平面。所以，只要被处理的数据线性可分，就能使用感知机模型不断地进行模型训练和参数优化，最后得到一个能够对数据进行二分类的模型。如果我们处理的数据是线性不可分的，在进行模型训练的过程中就会出现模型一直来回震荡的情况，也就得不到理想的结果了。感知机的数学表达式如下：</p>
<p><img src="Image00013.jpg" alt="img"></p>
<p>其中，参数x 为输入向量，w 为输入向量对应的权重值，b 为偏置，w ·x 是输入向量x 和权重向量w 的点积表示。sign 为符号函数，符号函数的定义如下：</p>
<p><img src="Image00014.jpg" alt="img"></p>
<p>也就是说，当符号函数的输入值大于0时输出正1，当符号函数的输入值小于0时输出负1。然后，我们使用w ·x +b 来代替以上公式中的x ，得到如下新的公式：</p>
<p><img src="Image00015.jpg" alt="img"></p>
<p>这样就更直观了。如果输入向量x 是我们要进行分类的数据，那么输出结果值负1和正1就可以被看作数据经过模型计算后输出的对应标签，这样我们就将输入向量x 划分成两类了。如果处于二维空间中，那么w ·x +b =0对应的就是对输入数据进行二分类的那条直线，在感知机中我们也把这条直线叫作分割超平面（Separating Hyperplane）。不过感知机也存在极为明显的优缺点，优点是很容易处理线性可分问题，缺点是不能处理异或问题，也就是说不能处理非线性问题。所以，之后出现了能够处理非线性问题的多层感知机模型，如图1-10所示就是一个多层感知机模型的结构图。</p>
<p><img src="Image00016.jpg" alt="img"></p>
<p>图1-10</p>
<p>多层感知机和单层感知机的最大区别是多层感知机在它的输入层（Input Layer）和输出层（Output Layer）之间加入了新的网络层次，这个新的网络层次叫作隐藏层（Hidden Layer），我们能够自定义隐藏层的层次数量，层数通常会是一层或者多层。同时，多层感知机具备了一种后向传播能力，我们可以暂时将后向传播理解为多层感知机模型进行自我学习和优化的一种方法。</p>
<h3 id="1-3-4-你好，深度学习"><a href="#1-3-4-你好，深度学习" class="headerlink" title="1.3.4 你好，深度学习"></a>1.3.4 你好，深度学习</h3><p>多层感知机的出现使神经网络模型在解决问题的能力上得到很大的提升，而且通过累加多层感知机的网络层次，模型有了能够解决现实世界的复杂问题的能力。因此，难免有人误以为只需对网络层次进行机械性累加，就可以得到一个有强泛化能力的多层感知机模型，最后得到的模型效果却差强人意，还引发了新的问题。模型的深度是一把双刃剑，随着模型深度的不断增加，模型本身会面临许多新的问题，最典型的就是通过机械性累加得到的深层次神经网络模型在进行后向传播的过程中会出现梯度消失的问题，梯度消失就意味着我们搭建的神经网络模型已经丧失了自我学习和优化的能力，所以在搭建神经网络模型时并不是网络的层次越深效果就越好。对于深层次神经网络模型，我们必须有特别的优化和控制手段。</p>
<p>对于在深层次神经网络模型训练中出现的梯度消失问题，科学家一直在探索解决方法，在2006年，由Geoffrey E.Hinton提出了一种有效的解决方案，就是通过无监督预训练对权值进行初始化和有监督训练微调模型，这也是本书着重使用的深度学习（Deep Learning）方法。不过随着历史的发展，深度学习方法中的有监督训练微调模型更受到人们的青睐，这种方法利用现有的样本数据，通过科学的方法不断微调模型参数，使模型的预测结果和真实结果之间的误差值不断减小。</p>
<p>在深度学习方法被提出后，科学家们通过不断改进和创新，开发出了基于深度学习方法的众多全新模型，这些模型在解决相关领域的问题的效果上比传统的机器学习方法要好出不少。</p>
<h2 id="1-4-计算机视觉"><a href="#1-4-计算机视觉" class="headerlink" title="1.4 计算机视觉"></a>1.4 计算机视觉</h2><p>视觉对于生物而言有着非常特殊的意义，远古时代的生物是没有视觉器官的，视觉器官的出现归功于生物长达数万年的进化过程。生物在拥有了视觉器官后也就拥有了一个强有力的图像信息捕获“工具”，通过这个“工具”完成对现实世界的图像分析和处理，这又促进了生物视觉和生物的其他能力的不断进化，所以视觉在某种程度上促进了生物的进化。</p>
<p>再来看看人类的视觉器官眼睛，眼睛是人类获取外部图像信息的重要渠道，通过眼睛和大脑的联动，我们能快速完成对物体的识别、定位等一系列复杂操作。在这个过程中眼睛的主要工作是帮助人类对外界的特定信息进行收集，然后将这些信息全部传递给大脑，并经过大脑的分析和处理，让相应的器官和肢体完成指定的动作。优秀的运动员要完成高难度的肢体动作，就需要不断对这个过程进行反复训练，以形成特定视觉下的肌肉记忆。人类通过视觉获得了了解世界的更好途径，视觉在人类不断探索世界的道路上是一个不可或缺的助推器。</p>
<p>那么，视觉对于机器而言又承载着什么特殊使命呢？其实在计算机被发明之初并没有计算机视觉的概念，我们知道，科学家们发明计算机的初衷是为了得到一个能够进行高精度、低耗时计算的工具，这个工具用于辅助人类更好地工作。在多年之后出现的计算机视觉概念其实和人工智能的发展密不可分，因为机器能否对视觉信息进行收集、处理和分析，是机器智能的一个重要体现途径，所以让机器拥有人类一样的视觉能力就是计算机视觉诞生的初衷。</p>
<p>传统的计算机视觉大致分为信息的收集、信息的分析和信息的处理三部分内容。</p>
<p>计算机获取外部信息主要通过硬件设备来完成，这些硬件设备可以是一些可以实时捕获高清信息的摄像头，当然，计算机还有其他渠道可以获取图像信息，比如将已经存在的视频或者图片作为图像信息提供给计算机进行处理和分析，这与人类进行信息收集的渠道相比是一个重大区别。</p>
<p>即便有了强大的硬件来捕获图像信息或者已经拥有海量的历史图像数据，但是没有进行图像信息的分析和处理的手段，则要想得到一个智能模型，是不切实际的。承担图像信息分析和处理这个艰巨任务的就是计算机视觉的核心算法，目前进行图像信息分析和处理的核心算法都采用了深度学习方法，通过这些核心算法能够处理很多计算机视觉上的问题，比如图片分类、对图像中目标的定位和语义分割，等等，所以在本书中深度学习方法就是在计算机视觉问题中负责对图像进行信息分析和处理的“大脑”。</p>
<h2 id="1-5-深度学习"><a href="#1-5-深度学习" class="headerlink" title="1.5 深度学习+"></a>1.5 深度学习+</h2><p>当深度学习方法开始融入对计算机视觉问题的分析和处理中时，传统的机器学习方法就逐渐被深度学习方法取代了。下面让我们大致了解一下哪些领域正在使用深度学习方法解决计算机视觉问题。</p>
<h3 id="1-5-1-图片分类"><a href="#1-5-1-图片分类" class="headerlink" title="1.5.1 图片分类"></a>1.5.1 图片分类</h3><p>图片分类具体指的是通过使用深度学习方法让计算机能够对输入图片的信息进行分析、处理并判定图片所属的类别。假设我们需要处理一个图片的二分类问题，现在有大量的苹果和梨的图片，其中一部分图片有类型标注，而另一部分没有，我们要完成的任务就是对没有类型标注的图片打上图片内容真实对应的类别标签，来完成对这部分图片的类型划分。其解决的思路就是将已经有标注的图片输入我们搭建好的计算机视觉模型中进行训练，然后使用训练好的模型对没有类型标注的图片进行类别预测，只要我们的预测结果足够准确，就解决了一个典型的图片二分类问题。当然，这个思路同样可以用于解决图片多分类问题。</p>
<h3 id="1-5-2-图像的目标识别和语义分割"><a href="#1-5-2-图像的目标识别和语义分割" class="headerlink" title="1.5.2 图像的目标识别和语义分割"></a>1.5.2 图像的目标识别和语义分割</h3><p>图像的目标识别（Object Recognition）和语义分割（Semantic Segmentation）可以说是图片分类的升级版本。图片的分类是指通过使用已经训练好的模型识别出输入图片的特征，然后才能将这些图片归属到具体的类别中。但是，在我们实际获取到的某张图片中不仅仅有一种类别的物品，有时我们还需要对一张图片中的多个物体进行分类和识别，这时就要用到目标识别和语义分割相关的算法了。</p>
<p>同样，在进行图像目标识别和语义分割前，我们首先需要通过训练让我们搭建的模型知道每个类别的重要特征，当在输入图像的信息中包含了我们的模型已经知道的类别特征时，就能很快将图像中的目标全部识别出来。而图像的目标识别和语义分割有一个很大的区别，就是它们对在图像中识别出的目标在结果呈现上有所不同：目标识别会对识别出的类别对象用长方形进行框选并在框上打上标签名，如图 1-11（a）所示；语义分割则会对识别出的类别使用同一种像素进行标识并打上标签，如图1-11（b）所示。</p>
<p><img src="Image00017.jpg" alt="img"></p>
<p>图 1-11（a）</p>
<p><img src="Image00018.jpg" alt="img"></p>
<p>图 1-11（b）</p>
<h3 id="1-5-3-自动驾驶"><a href="#1-5-3-自动驾驶" class="headerlink" title="1.5.3 自动驾驶"></a>1.5.3 自动驾驶</h3><p>目前，主流的自动驾驶技术也用到了计算机视觉相关的技术，在自动驾驶的汽车上会安置大量的高清摄像头和传感器，这些硬件设备会收集汽车附近的图像信息并将其输入汽车“大脑”中进行处理、分析，从而判断出汽车附近的实时路面情况。所以计算机视觉在自动驾驶中有着举足轻重的作用，而且自动驾驶对所分析路面状况的安全性和可靠性的要求非常高，因此自动驾驶中的计算机视觉技术比其在其他领域的应用要求更严苛，不能有半点马虎。</p>
<h3 id="1-5-4-图像风格迁移"><a href="#1-5-4-图像风格迁移" class="headerlink" title="1.5.4 图像风格迁移"></a>1.5.4 图像风格迁移</h3><p>除了图像目标识别和语义分割这类主要用于图像识别和分类问题的计算机视觉应用，人们还发现了一些比较有意思的应用场景，比如图像风格迁移（Neural Style）。我们知道，深度学习方法能够提取图像的重要特征，所以我们可以将提取的这些特征迁移到其他图片中进行融合，到达图像风格迁移的目的，这样，混合了其他图片风格的新图片就诞生了。如图1-12所示就是一个典型的图像风格迁移的应用。</p>
<p><img src="Image00019.jpg" alt="img"></p>
<p>图1-12</p>
<h1 id="第2章-相关的数学知识"><a href="#第2章-相关的数学知识" class="headerlink" title="第2章 相关的数学知识"></a>第2章 相关的数学知识</h1><p>在学习深度学习相关的内容之前，我们还需要掌握一些数学知识。本章将介绍一些基础的数学知识，比如如何在线性代数中进行矩阵运算，以及如何在微积分中对函数求导等。</p>
<h2 id="2-1-矩阵运算入门"><a href="#2-1-矩阵运算入门" class="headerlink" title="2.1 矩阵运算入门"></a>2.1 矩阵运算入门</h2><p>矩阵是线性代数中非常核心的内容，其优势就是能够进行大规模的并行计算，所以将矩阵的计算方式引入计算机中能够很大程度地提升计算机的计算效率。在深度学习方法中有很多的地方会涉及矩阵计算，所以掌握矩阵相关的计算方法和原理对我们理解深度学习的算法流程会有很大的帮助。</p>
<p>受限于篇幅，本章重点介绍在深度学习中涉及的矩阵相关的内容，以便于读者快速理解和上手。</p>
<h3 id="2-1-1-标量、向量、矩阵和张量"><a href="#2-1-1-标量、向量、矩阵和张量" class="headerlink" title="2.1.1 标量、向量、矩阵和张量"></a>2.1.1 标量、向量、矩阵和张量</h3><p>在线性代数中，我们必须掌握几个核心概念：标量、向量、矩阵和张量，它们是构成线性代数学科的基石。</p>
<p>（1）标量（Scalar）： 标量其实就是一个独立存在的数，比如在线性代数中一个实数5就可以被看作一个标量，所以标量的运算相对简单，与我们平常做的数字算术运算类似。</p>
<p>（2）向量（Vector）： 向量指一列按顺序排列的元素，我们通常习惯用括号将这一列元素括起来，其中的每个元素都由一个索引值来唯一地确定其在向量中的位置，假设这个向量中的第1个元素是x 1 ，它的索引值就是1，第2个元素是x 2 ，它的索引值就是2，以此类推。如下所示就是一个由三个元素组成的向量，这个向量的索引值从1到3分别对应了从x 1 到x 3 的这三个元素：</p>
<p><img src="Image00020.jpg" alt="img"></p>
<p>向量还有一个特性：向量中的不同数字还可以用于表示不同坐标轴上的坐标值。比如，我们可以把下面这个向量看作三个不同的坐标轴上的坐标值，可以假设2是x 轴上的坐标值，3是y 轴上的坐标值，8是z 轴上的坐标值：</p>
<p><img src="Image00021.jpg" alt="img"></p>
<p>（3）矩阵（Matrix）： 矩阵就是一个二维数组结构，我们会用括号将其中的全部元素括起来。向量的索引值是一维的，而矩阵的索引值是二维的，所以在确定矩阵中每个元素的位置时需要两个数字。举例来说，假设在一个矩阵的左上角存在一个元素x 11 ，那么确定这个元素的索引值就是由两个 1构成的二维索引值，即“11”，这个二维索引值代表矩阵中第1行和第1列交汇处的数字，所以前面的一个数字1可以被定义为当前矩阵的行号，后面的一个数字1可以被定义为当前矩阵的列号。如下就是一个三行两列的矩阵：</p>
<p><img src="Image00022.jpg" alt="img"></p>
<p>在本书中统一使用大写字母来表示一个矩阵的简写，这个矩阵由6个元素组成，其中x 11 的位置是第1行和第1列的交汇处，所以x 11 的索引值就是11；同理，x 21 的位置是第2行和第1列的交汇处，所以x 21 的索引值就是21；x 12 的位置是第1行和第2列的交汇处，所以x 12 的索引值是12；以此类推，最后得到x 2 2 的索引值是22，x 3 1 的索引值是31，x 32 的索引值是32。下面举一个具体的实例，有如下矩阵：</p>
<p><img src="Image00023.jpg" alt="img"></p>
<p>如果我们想要获得索引值是12和33的值，则根据之前的索引值定义规则，可以得到其对应的值分别是3和6。</p>
<p>（4）张量（Tensor）： 若数组的维度超过了二维，我们就可以用张量来表示，所以我们可以将张量理解为高维数组。同理，张量的索引值用两个维度的数字来表示已经不够了，其中的元素的索引值会随着张量维度的改变而改变。</p>
<h3 id="2-1-2-矩阵的转置"><a href="#2-1-2-矩阵的转置" class="headerlink" title="2.1.2 矩阵的转置"></a>2.1.2 矩阵的转置</h3><p>矩阵的转置是矩阵在进行相关运算时会采用的一种变换方法。在一般情况下，我们通过在矩阵的右上角加上符号“T”来表示其是一个转置矩阵，如下所示就是矩阵X 的转置矩阵表示：</p>
<p>XT</p>
<p>我们对原矩阵X 中的元素在经过变换后得到的相应的转置矩阵做如下定义：</p>
<p><img src="Image00024.jpg" alt="img"></p>
<p>等号左边括号内的内容是原矩阵的转置表示，括号外右下角的i 和j 分别是原矩阵中元素的行号索引值和列号索引值；等号右边是原矩阵X ，但是我们看到它的下标i 和j 调换了位置，这个位置的调换意义在于把原矩阵中所有位置为ij 的元素和位置为ji 的元素进行对调，在这个过程中完成的全部操作就是原矩阵的转置。</p>
<p>下面通过一个具体的实例来对比矩阵转置前后的元素的位置，这样就能够更清晰地理解转置的过程。如下所示是一个原矩阵X ：</p>
<p><img src="Image00025.jpg" alt="img"></p>
<p>按照之前的原矩阵的转置变换过程，通过将矩阵内元素的位置进行调换后得到如下结果：</p>
<p><img src="Image00026.jpg" alt="img"></p>
<p>在观察转置矩阵之后我们发现，如果原矩阵的维度是不对称的，那么在转置后的矩阵中不仅元素的位置会改变，维度也会有相应的改变，在如上所示的矩阵中原矩阵的三行两列在转置后变成了两行三列。其实，我们可以通过一种更简单的方法来记忆转置的变换过程，就是把原矩阵沿着对角线进行翻转，在翻转后得到的矩阵就是原矩阵的转置矩阵。</p>
<p>再来看两个实际的实例。首先是一个矩阵行号和列号相同的原矩阵X ：</p>
<p><img src="Image00027.jpg" alt="img"></p>
<p>我们沿着对角线对原始矩阵进行翻转，其中对角线的矩阵元素是1和4，在翻转后得到原矩阵的转置矩阵如下：</p>
<p><img src="Image00028.jpg" alt="img"></p>
<p>如果矩阵的行号和列号相同，那么对角线上的元素必定在翻转之后还是一样的，而且索引值不会发生变化。我们再来看一个矩阵的行号和列号不相同的实例，原始矩阵X 如下：</p>
<p><img src="Image00029.jpg" alt="img"></p>
<p>这时对角线上的矩阵元素变成了1和6，在翻转后得到的原矩阵的转置矩阵如下：</p>
<p><img src="Image00030.jpg" alt="img"></p>
<p>可以看到，转置矩阵的对角线上的元素仍然没有发生变化，但是转置矩阵的对角线上的元素索引值和原矩阵不同了：矩阵中元素1的索引值还是11，但是矩阵中元素6的索引值从原矩阵的32变成了现在的23。</p>
<h3 id="2-1-3-矩阵的基本运算"><a href="#2-1-3-矩阵的基本运算" class="headerlink" title="2.1.3 矩阵的基本运算"></a>2.1.3 矩阵的基本运算</h3><p>我们通常使用的数学算术运算包含加法运算、减法运算、乘法运算和除法运算，不过这些运算和矩阵中的算术运算稍微有些区别，因为在矩阵中是不能直接进行除法运算的，如果要在矩阵中进行除法运算，就要引入矩阵的逆来解决这个问题。本书不过多介绍矩阵的逆，而是重点讲解矩阵的加法运算、减法运算和乘法运算。</p>
<p>在进行矩阵算术运算前，我们先假设存在三个行号和列号都为2的矩阵，分别使用大写字母A 、B 和C 表示这三个矩阵，其中矩阵A 中的元素如下：</p>
<p><img src="Image00031.jpg" alt="img"></p>
<p>矩阵B 中的元素如下：</p>
<p><img src="Image00032.jpg" alt="img"></p>
<p>我们把矩阵A 和矩阵B 进行算术运算后的结果都存储在矩阵C 中，矩阵C 中的元素如下：</p>
<p><img src="Image00033.jpg" alt="img"></p>
<p>在定义好三个矩阵的结构之后，我们再来看看矩阵中算术运算的公式。</p>
<p>矩阵的加法运算公式如下：</p>
<p><img src="Image00034.jpg" alt="img"></p>
<p>矩阵的减法运算公式如下：</p>
<p><img src="Image00035.jpg" alt="img"></p>
<p>矩阵的乘法运算公式如下：</p>
<p><img src="Image00036.jpg" alt="img"></p>
<p>每个运算公式中的两个公式其实都是矩阵的相应的算术运算法则，只不过第1个公式使用的是矩阵的缩写，而第2个公式使用的是矩阵中的元素，但是为了便于理解，我们平时习惯使用第1种公式。在第2个公式中定义的i 和j 分别是矩阵的行索引值和列索引值。我们观察到这两个矩阵在进行加法运算和减法运算时，矩阵中对应的矩阵元素具有相同的行索引值i 和列索引值j ，这也是矩阵进行加法和减法运算的前提条件，即参与运算的矩阵必须具有相同的行数和列数。</p>
<p>不过，矩阵乘法运算的前提条件和加法、减法运算不同：在参与运算的元素索引值中多了一个k 值，而且这个k 值同时出现在两个参与矩阵乘法运算的元素索引值中，前一个元素的k 值代表的是列索引值，而后一个元素的k 值代表的是行索引值。</p>
<p>不仅如此，我们还能从矩阵的乘法运算中得到如下有用的信息：若两个矩阵能进行乘法运算，那么前一个矩阵的列数必须和后一个矩阵的行数相等，同时矩阵的乘法运算要满足乘法的分配率和乘法的结合律，即</p>
<p>A ×(B +C )=A ×B +A ×C</p>
<p>A ×(B ×C )=(A ×B )×C</p>
<p>如上所述是常用的算术运算公式，我们需要特别留意矩阵的加法、减法和乘法运算公式之间的区别和联系，下面来看一些实例。</p>
<p>例1： 假设存在两个行号和列号相同的矩阵，其中矩阵<img src="Image00037.jpg" alt="img"> ，矩阵<img src="Image00038.jpg" alt="img"> ，计算矩阵C =A +B 的运算结果。</p>
<p>解答： 根据矩阵的加法法则cij =aij +bij ，可以得到矩阵<img src="Image00039.jpg" alt="img"> ，然后将索引值对应的元素代入公式中进行计算，得到计算结果为矩阵<img src="Image00040.jpg" alt="img"> 。</p>
<p>例2： 假设存在两个行号和列号相同的矩阵，其中矩阵<img src="Image00041.jpg" alt="img"> ，矩阵<img src="Image00042.jpg" alt="img"> ，计算矩阵C =A -B 的运算结果。</p>
<p>解答： 根据矩阵的减法法则cij =aij -bij ，可以得到矩阵<img src="Image00043.jpg" alt="img"> ，然后将索引值对应的元素代入公式中进行计算，得到计算结果为矩阵<img src="Image00044.jpg" alt="img"> 。</p>
<p>例3： 假设存在两个行号和列号相同的矩阵，其中矩阵<img src="Image00045.jpg" alt="img"> ，矩阵<img src="Image00046.jpg" alt="img"> ，计算矩阵C =A ×B 的运算结果。</p>
<p>解答： 根据矩阵的乘法运算法则<img src="Image00047.jpg" alt="img"> ，可以得到矩阵<img src="Image00048.jpg" alt="img"> ，然后将索引值对应的元素代入公式中进行计算，得到计算结果为矩阵<img src="Image00049.jpg" alt="img"> 。</p>
<p>例4： 假设存在两个行号和列号不相同的矩阵，其中矩阵<img src="Image00050.jpg" alt="img"> ，矩阵<img src="Image00051.jpg" alt="img"> ，计算矩阵C =A ×B 的运算结果。</p>
<p>解答： 根据矩阵的乘法运算法则<img src="Image00052.jpg" alt="img"> ，可以得到矩阵<img src="Image00053.jpg" alt="img"> ，然后将索引值对应的元素代入公式中进行计算，计算结果为矩阵<img src="Image00054.jpg" alt="img"> 。</p>
<p>例5： 假设存在三个行号和列号相同的矩阵，其中矩阵<img src="Image00055.jpg" alt="img"> ，矩阵<img src="Image00056.jpg" alt="img"> ，矩阵<img src="Image00057.jpg" alt="img"> ，计算矩阵乘法运算A ×(B +C )的运算结果是否和矩阵乘法运算A ×B +A ×C 的运算结果相等，来证明矩阵乘法运算符合乘法的分配律。</p>
<p>解答： 根据矩阵的乘法法则<img src="Image00058.jpg" alt="img"> ，可以计算得到A ×(B +C )的矩阵乘法运算结果为</p>
<p><img src="Image00059.jpg" alt="img"></p>
<p>A ×B +A ×C 的矩阵乘法运算结果为</p>
<p><img src="Image00060.jpg" alt="img"></p>
<p>可以看到，矩阵乘法运算A ×(B +C )的结果和矩阵乘法运算A ×B +A ×C 的结果是相等的，由此可以证明矩阵乘法运算符合乘法的分配律。</p>
<p>例6： 假设存在三个行号和列号相同的矩阵，其中矩阵<img src="Image00061.jpg" alt="img"> ，矩阵<img src="Image00062.jpg" alt="img"> ，矩阵<img src="Image00063.jpg" alt="img"> ，计算矩阵乘法运算(A ×B )×C 的运算结果是否和矩阵乘法运算A ×(B ×C )的运算结果相等，来证明矩阵乘法运算符合乘法的结合律。</p>
<p>解答： 根据矩阵乘法的结合律公式<img src="Image00064.jpg" alt="img"> ，可以计算得到(A ×B )×C 的矩阵乘法运算结果为</p>
<p><img src="Image00065.jpg" alt="img"></p>
<p>A ×(B ×C )的矩阵乘法运算结果为</p>
<p><img src="Image00066.jpg" alt="img"></p>
<p>可以看到，矩阵乘法运算(A ×B )×C 的运算结果和矩阵乘法运算A ×(B ×C )的运算结果相等，由此可以证明矩阵乘法运算符合乘法的结合律。</p>
<h2 id="2-2-导数求解"><a href="#2-2-导数求解" class="headerlink" title="2.2 导数求解"></a>2.2 导数求解</h2><p>导数是微积分中非常核心的概念，它又包括一阶导数、二阶导数和高阶导数（或者说多阶导数），不过阶层不同的导数不仅在导数求解使用的运算方式和方法上存在诸多差异，其几何意义也完全不同，所以我们很难掌握各阶导数的求解方法和几何意义。</p>
<p>本节重点介绍一阶导数的求解方法和其对应的几何意义，因为一阶导数的求解相对于高阶导数会简单很多，而且几何意义比较直观，所以掌握这些内容会有助于我们理解后续要讲的深度学习中后向传播的内容。</p>
<h3 id="2-2-1-一阶导数的几何意义"><a href="#2-2-1-一阶导数的几何意义" class="headerlink" title="2.2.1 一阶导数的几何意义"></a>2.2.1 一阶导数的几何意义</h3><p>我们先来了解一阶导数的几何意义。假设存在函数y =f (x )，该函数在点x 0 处可导，则点x 0 处的导数的几何意义就是该函数曲线在点p (x 0 ,f<br>(x 0 ))处的切线的斜率，在几何图中显示的效果如图2-1所示。</p>
<p><img src="Image00067.jpg" alt="img"></p>
<p>图2-1</p>
<p>因此，如果想知道在函数曲线上某点的导数，则只需计算该函数在该点的切线的斜率。如图2-1所示的函数曲线特征还不是非常明显，下面我们来看看特征更明显的函数曲线，如图2-2所示。</p>
<p><img src="Image00068.jpg" alt="img"></p>
<p>图2-2</p>
<p>在图2-2中，左图中的函数曲线呈现一个很明显的凹型，而右图中的函数曲线呈现一个很明显的凸型，我们将左图中的函数曲线叫作凹函数，将右图中的函数曲线叫作凸函数。这两类曲线除这个特征外，还有一个重要的属性，就是在凹函数中一定存在一个该函数的最低点，相对应地，在凸函数中一定存在一个该函数的最高点。凸函数的最高点和凹函数的最低点的斜率均为0，即这两点的导数的求解结果为0，所以在凸函数中如果某点的导数求解结果为零，那么该点就是该函数曲线上的最大值点；在凹函数中如果某点的导数求解结果为零，那么该点就是该函数曲线上的最小值点。</p>
<p>在实际应用中一些函数曲线会同时存在多个点的斜率都是0的情况，这种类型的曲线不是完全的凸函数，也不是完全的凹函数，更像是由凹凸函数混合而成的，如图2-3所示。</p>
<p><img src="Image00069.jpg" alt="img"></p>
<p>图2-3</p>
<p>在该函数曲线中既有一部分凸曲线，又有一部分凹曲线，其中的凸曲线最高点的斜率为0，但是我们不把该点叫作最大值点，而把它叫作极大值点，同理，凹曲线的最低点是极小值点。我们需要明白的是，极大值点和极小值点只代表整个函数局部的最大和最小，在该类函数中极大值点不一定就是最大值点，极小值点也不一定就是最小值点，所以我们又可以将最大值点叫作函数全局最大值，将最小值点叫作函数全局最小值，除此之外，我们可以将其他极值点都叫作函数局部最大值或者函数局部最小值。</p>
<p>通过计算，我们可以得到图2-3中斜率为0的点有三个，有一个是极大值点，有两个是极小值点，很容易看出极大值点并不是最大值点，但是两个极小值点中的一个是最小值点。</p>
<h3 id="2-2-2-初等函数的求导公式"><a href="#2-2-2-初等函数的求导公式" class="headerlink" title="2.2.2 初等函数的求导公式"></a>2.2.2 初等函数的求导公式</h3><p>我们在理解了导数的几何意义之后，就可以进一步学习导数的求解方法了。这里不对导数的求解过程进行详细论证，而是直接讲解如何使用初等函数的导数求导公式，这样更便于理解导数计算的相关方法。</p>
<p>在学习导数的相关计算之前，我们需要知道几种常用的导数表示方法，比如存在函数y =f (x )，那么我们可以将函数f (x )的导数表示成y<br>′或者<img src="Image00070.jpg" alt="img"> 。多元函数导数的表示方法不太一样，比如对于一个二元函数h =f (x ,y )，对其中的x 求导时表示成<img src="Image00071.jpg" alt="img"> ，对其中的y 求导时表示成<img src="Image00072.jpg" alt="img"> ，这两个计算过程分别叫作对x 求偏导和对y 求偏导。</p>
<p>常用的初等函数求导公式如下。</p>
<p>（1）y =C ，y ′=0（C 表示实数）</p>
<p>（2）y =xn ，y ′=nxn -1 （n 表示整数）</p>
<p>（3）y =sinx ，y ′=cosx</p>
<p>（4）y =cosx ，y ′=-sinx</p>
<p>（5）y =tanx ，<img src="Image00073.jpg" alt="img"></p>
<p>（6）y =cotx ，<img src="Image00074.jpg" alt="img"></p>
<p>（7）y =lnx ，<img src="Image00075.jpg" alt="img"></p>
<p>（8）y =log a x ，<img src="Image00076.jpg" alt="img"></p>
<p>（9）y =ex ，y ′=ex</p>
<p>（10）y =ax ，y ′=ax lna (a &gt;0,a ≠1)</p>
<p>在掌握了初等函数求导的计算公式后，现在看几个具体的实例。</p>
<p>例1： 已知y =5，求解y 的导数y ′的值是多少。</p>
<p>解答： 使用y =C ，y ′=0初等函数求导公式进行求解，因为y 是实数，所以可以得到y 的导数y ′=0。</p>
<p>例2： 已知y =x 5 ，求解y 的导数y ′的值是多少。</p>
<p>解答： 使用y =xn ，y ′=nxn -1 初等函数求导公式进行求解，此时在函数中n 的值等于5，代入公式中得到y ′=5x 4 。</p>
<p>例3： 已知y =log10 x ，求解y 的导数y ′的值是多少。</p>
<p>解答： 使用y =log a x ，<img src="Image00077.jpg" alt="img"> 初等函数求导公式进行求解，此时在函数中a 的值等于10，代入公式中得到<img src="Image00078.jpg" alt="img"> 。</p>
<p>例4： 已知y =5 x ，求解y 的导数y ′的值是多少。</p>
<p>解答： 使用y =ax ，y ′=ax lna (a &gt;0,a ≠1)初等函数求导公式进行求解，此时在函数中a 的值等于5，代入公式中得到y ′=5 x ln5。</p>
<h3 id="2-2-3-初等函数的和、差、积、商求导"><a href="#2-2-3-初等函数的和、差、积、商求导" class="headerlink" title="2.2.3 初等函数的和、差、积、商求导"></a>2.2.3 初等函数的和、差、积、商求导</h3><p>只掌握初等函数的导数求解方法还远远不够，我们在函数计算的过程中经常会遇到在进行算术运算后需要求导的情况，在这种情况下，函数求导计算的内容较之前更复杂，不过可以直接套用初等函数的算数运算求导公式。假设存在函数u =u (x ),v =v (x )，其中函数u =u (x ),v =v (x )均可导，那么有如下算术运算求导公式：</p>
<p>（1）(u ±v )′=u ′±v ′</p>
<p>（2）(Cu )′=Cu ′（其中C 为实数）</p>
<p>（3）(uv )′=u ′v +uv ′</p>
<p>（4）<img src="Image00079.jpg" alt="img"></p>
<p>我们依旧通过几个实例来看看如何运用初等函数的算术运算求导公式。</p>
<p>例1： 已知u =2x ,v =x 2 ，求解(u ±v )的导数值(u ±v )′是多少。</p>
<p>解答： 使用(u ±v )′=u ′±v ′初等函数算术运算求导公式进行求解，代入公式中，得到(u ±v )′=u ′±v ′=(2x )′±(x 2 )′=2±2x 。</p>
<p>例2： 已知u =2x ，求解(5u )的导数值(5u )′是多少。</p>
<p>解答： 使用(Cu )′=Cu ′初等函数算术运算求导公式进行求解，代入公式中，得到(5u )′=5u ′=5×(2x )′=5×2=10。</p>
<p>例3： 已知u =2x ,v =x 2 ，求解(uv )的导数值(uv )′是多少。</p>
<p>解答： 使用(uv )′=u ′v +uv ′初等函数算术运算求导公式进行求解，代入公式中，得到(uv )′=u ′v +uv ′=(2x )′×x 2<br>+2x ×(x 2 )′=2x 2 +4x 2 =6x 2 。</p>
<p>例4： 已知u =2x ,v =x 2 ，求解<img src="Image00080.jpg" alt="img"> 的导数值<img src="Image00081.jpg" alt="img"> 是多少。</p>
<p>解答： 使用<img src="Image00082.jpg" alt="img"> 初等函数算术运算求导公式进行求解，代入公式中，得到<img src="Image00083.jpg" alt="img"> 。</p>
<h3 id="2-2-4-复合函数的链式法则"><a href="#2-2-4-复合函数的链式法则" class="headerlink" title="2.2.4 复合函数的链式法则"></a>2.2.4 复合函数的链式法则</h3><p>在介绍复合函数的链式法则之前，我们先回顾一下初等函数的算术运算求导过程。如果把初等函数的和、差、积、商运算改写成一个函数，那么上面的4个实例就变成了一个复合函数的求导过程。复合函数其实就是有限个函数使用不同的运算方法嵌套而成的，那么复合函数的导数就是有限个函数在相应的点的导数的乘积，就像锁链一样一环套一环，故将复合函数的求导方法称为链式法则。</p>
<p>在实际应用中，复合函数的求导更复杂：因为复合函数嵌套的函数数量会相对较多，而且在嵌套的函数中用到的不仅有简单的加减乘除运算，还有复杂的幂运算、正弦余弦运算等高级运算。</p>
<p>若在一个复合函数中只嵌套一个函数，则通用的公式可以写成：</p>
<p>f (g (x ))</p>
<p>而这个复合函数的相应求导结果如下：</p>
<p>(f (g (x )))′=f ′(g (x ))g ′(x )。</p>
<p>这个公式比较简单，针对的是只有两个函数复合的情况，如果复合函数嵌套两个及以上的函数，则计算方法和过程会变得复杂很多。下面通过几个实例来看看几种类型的复合函数求导的具体求解思路。</p>
<p>例1： 假设有函数y =sin(x 3 +1)，求函数的导数y ′。</p>
<p>解答： 首先观察原函数y =sin(x 3 +1)，可以知道它是由两个函数复合而成的，这两个函数分别是f (x )=sinx 及g (x )=x 3<br>+1，那么根据复合函数的链式法则(f (g (x )))′=f ′(g (x ))g ′(x )，可以得到y ′=(sin(x 3 +1))′=sin′(x 3 +1)×(x 3 +1)′=cos(x 3 +1)×3x 2 。</p>
<p>例2： 假设有函数<img src="Image00084.jpg" alt="img"> ，求函数的导数y ′。</p>
<p>解答： 首先观察原函数<img src="Image00085.jpg" alt="img"> ，可以知道它是由三个函数复合而成的，这三个函数分别是f (x )=ex ,g (x )=ln(x ),h (x )=x 3 +1，那么根据复合函数的链式法则(f (g (h (x ))))′=f ′(g (h (x )))g ′(h (x ))h ′(x )，可以得到<img src="Image00086.jpg" alt="img"> <img src="Image00087.jpg" alt="img"> 。</p>
<p>通过上面的两个实例，可以看到在例2中复合函数的求导要比在例1中的更复杂，因为例2的复合函数嵌套了三个函数。如果遇到复合程度更高的函数，就可以按照例2中的方法，将嵌套的函数层层剥离、分别求导，然后将各部分函数的求导结果相乘，就得到了最终结果。</p>
<h1 id="第3章-深度神经网络基础"><a href="#第3章-深度神经网络基础" class="headerlink" title="第3章 深度神经网络基础"></a>第3章 深度神经网络基础</h1><p>本章重点介绍机器学习中的一些基础知识和概念，掌握这些知识对于理解神经网络的架构和工作原理会有很大的帮助，只有掌握这些知识，我们才能对现有的深度神经网络模型进行解读，并按照自己的想法搭建出更好的网络模型。</p>
<h2 id="3-1-监督学习和无监督学习"><a href="#3-1-监督学习和无监督学习" class="headerlink" title="3.1 监督学习和无监督学习"></a>3.1 监督学习和无监督学习</h2><p>监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）是在机器学习中经常被提及的两个重要的学习方法，下面通过一个生活中的实例对这两个概率进行理解。</p>
<p>假如有一堆由苹果和梨混在一起组成的水果，需要设计一个机器对这堆水果按苹果和梨分类，但是这个机器现在并不知道苹果和梨是什么样的，所以我们首先要拿一堆苹果和梨的照片，告诉机器苹果和梨分别长什么样；经过多轮训练后，机器已经能够准确地对照片中的水果类别做出判断，并且对苹果和梨的特征形成自己的定义；之后我们让机器对这堆水果进行分类，看到这堆水果被准确地按类别分开。这就是一个监督学习的过程。</p>
<p>如果我们没有拿苹果和梨的照片对机器进行系统训练，机器也不知道苹果和梨长什么样，而是直接让机器对这一堆水果进行分类，则机器能够根据自己的“直觉”将这一堆水果准确地分成两类。这就是一个无监督学习的过程，说明机器自己总结出了苹果和梨的特征，该过程看起来更贴近我们所设想的人工智能技术。</p>
<h3 id="3-1-1-监督学习"><a href="#3-1-1-监督学习" class="headerlink" title="3.1.1 监督学习"></a>3.1.1 监督学习</h3><p>我们可以对监督学习做如下简单定义：提供一组输入数据和其对应的标签数据，然后搭建一个模型，让模型在通过训练后准确地找到输入数据和标签数据之间的最优映射关系，在输入新的数据后，模型能够通过之前学到的最优映射关系，快速地预测出这组新数据的标签。这就是一个监督学习的过程。</p>
<p>在实际应用中有两类问题使用监督学习的频次较高，这两类问题分别是回归问题和分类问题，如下所述。</p>
<p>1.回归问题</p>
<p>回归问题就是使用监督学习的方法，让我们搭建的模型在通过训练后建立起一个连续的线性映射关系，其重点如下：</p>
<p>◎ 通过提供数据训练模型，让模型得到映射关系并能对新的输入数据进行预测；</p>
<p>◎ 我们得到的映射模型是线性连续的对应关系。</p>
<p>下面通过图3-1来直观地看一个线性回归问题。</p>
<p><img src="Image00088.jpg" alt="img"></p>
<p>图3-1</p>
<p>在图3-1中提供的数据是两维的，其中X 轴表示房屋的面积，Y 轴表示房屋的价格，用叉号表示的单点是房价和面积相对应的数据。在该图中有一条弧形的曲线，这条曲线就是我们使用单点数据通过监督学习的方法最终拟合出来的线性映射模型。无论我们想要得到哪种房屋面积对应的价格，通过使用这个线性映射模型，都能很快地做出预测。这就是一个线性回归的完整过程。</p>
<p>线性回归的使用场景是我们已经获得一部分有对应关系的原始数据，并且问题的最终答案是得到一个连续的线性映射关系，其过程就是使用原始数据对建立的初始模型不断地进行训练，让模型不断拟合和修正，最后得到我们想要的线性模型，这个线性模型能够对我们之后输入的新数据准确地进行预测。</p>
<p>2.分类问题</p>
<p>分类问题就是让我们搭建的模型在通过监督学习之后建立起一个离散的映射关系。分类模型和回归问题在本质上有很大的不同，它依然需要使用提供的数据训练模型让模型得到映射关系，并能够对新的输入数据进行预测，不过最终得到的映射模型是一种离散的对应关系。如图3-2所示就是一个分类模型的实例。</p>
<p><img src="Image00089.jpg" alt="img"></p>
<p>图3-2</p>
<p>在图3-2中使用的依然是两个维度的数据，X 轴表示肿瘤的尺寸大小，Y 轴表示肿瘤的属性，即是良性肿瘤还是恶性肿瘤。因为Y 轴只有两个离散的输出结果，即0和1，所以用0表示良性肿瘤，用1表示恶性肿瘤。我们通过监督学习的方法对已有的数据进行训练，最后得到一个分类模型，这个分类模型能够对我们输入的新数据进行分类，预测它们最有可能归属的类别，因为这个分类模型最终输出的结果只有两个，所以我们通常也把这种类型的分类模型叫作二分类模型。</p>
<p>分类模型的输出结果有时不仅仅有两个，也可以有多个，多分类问题与二分类问题相比会更复杂。我们也可以将刚才的实例改造成一个四分类问题，比如将肿瘤大小对应的最终输出结果改成4个：0对应良性肿瘤；1对应第1类肿瘤；2对应第2类肿瘤；3对应第3类肿瘤，这样就构造出了四分类模型。当然，我们也需要相应地调整用于模型训练的输入数据，因为现在的标签数据变成了4个，不做调整会导致模型不能被正常训练。依照四分类模型的构造方法，我们还能够构造出五分类模型甚至五分类以上的多分类模型。</p>
<h3 id="3-1-2-无监督学习"><a href="#3-1-2-无监督学习" class="headerlink" title="3.1.2 无监督学习"></a>3.1.2 无监督学习</h3><p>我们可以对无监督学习做如下简单定义：提供一组没有任何标签的输入数据，将其在我们搭建好的模型中进行训练，对整个训练过程不做任何干涉，最后得到一个能够发现数据之间隐藏特征的映射模型，使用这个映射模型能够实现对新数据的分类，这就是一个无监督学习的过程。无监督学习主要依靠模型自己寻找数据中隐藏的规律和特征，人工参与的成分远远少于监督学习的过程。如图3-3所示为使用监督学习模型和使用无监督学习模型完成数据分类的效果。</p>
<p><img src="Image00090.jpg" alt="img"></p>
<p>图3-3</p>
<p>在图3-3中，左图显示的是监督学习中的一个二分类模型，因为每个数据都有自己唯一对应的标签，这个标签在图中体现为叉号或者圆点；右图显示的就是无监督学习的过程，虽然数据也被最终分成了两类，但没有相应的数据标签，统一使用圆点表示，这就像实现了将具有相似关系的数据聚集在一起，所以使用无监督学习实现分类的算法又叫作聚类。在无监督训练的整个过程中，我们需要做的仅仅是将训练数据提供给我们的无监督模型，让它自己挖掘数据中的特征和关系。</p>
<p>下面看一个离我们的实际生活很近的聚类的应用实例。如图3-4所示，假如我们在一个有大量深度学习相关文章的网站检索“深度学习”，就会显示很多带有“深度学习”关键字的相关网页，但是只要仔细观察就会发现，这些网页被大致分为几个主要的类别，比如关于深度学习的理论、算法、硬件、新闻等。其实这就是一个聚类应用，可以将这个网站中的检索工具看作一个已经训练好的无监督学习模型，在我们对检索工具输入指令后，它就会按照我们的要求将所有的页面搜索出来，但最后呈现在我们眼前的不会是乱糟糟的一堆链接，而是完成聚类后的几大类网址的主链接，这也极大提升了用户体验。</p>
<p><img src="Image00091.jpg" alt="img"></p>
<p>图3-4</p>
<p>这个检索工具在极短的时间内主要进行了如下三步。</p>
<p>（1）首先，提取网站中全部有关深度学习的网页。</p>
<p>（2）然后，按照检索的关键字或关键词完成这些网页的聚类并为每个类别设置一个主链接。</p>
<p>（3）最后，将主链接返回到用户的浏览器中进行显示。</p>
<h3 id="3-1-3-小结"><a href="#3-1-3-小结" class="headerlink" title="3.1.3 小结"></a>3.1.3 小结</h3><p>通过总结以上内容，我们发现监督学习和无监督学习的主要区别如下。</p>
<p>◎ 我们通过监督学习能够按照指定的训练数据搭建出想要的模型，但这个过程需要</p>
<p>我们投入大量的精力处理原始数据，也因为我们的紧密参与，所以最后得到的模</p>
<p>型更符合设计者的需求和初衷。</p>
<p>◎ 我们通过无监督学习过程搭建的训练模型能够自己寻找数据之间隐藏的特征和</p>
<p>关系，更具有创造性，有时还能够挖掘到数据之间让我们意想不到的映射关系，</p>
<p>不过最后的结果也可能会向不好的方向发展。</p>
<p>所以监督学习和无监督学习各有利弊，用好这两种方法对于我们挖掘数据的特征和搭建强泛化能力模型是必不可少的。</p>
<p>除了上面提到的监督学习和无监督学习方法，在实际应用中还有半监督学习和弱监督学习等更具创新性的方法出现，例如半监督学习结合了监督学习和无监督学习各自的优点，是一种更先进的方法。所以我们需要深刻理解各种学习方法的优缺点，只有这样才能知道在每个应用场景中具体使用哪种学习方法才能更好地解决问题。</p>
<h2 id="3-2-欠拟合和过拟合"><a href="#3-2-欠拟合和过拟合" class="headerlink" title="3.2 欠拟合和过拟合"></a>3.2 欠拟合和过拟合</h2><p>我们可以将搭建的模型是否发生欠拟合或者过拟合作为评价模型的拟合程度好坏的指标。欠拟合和过拟合的模型预测新数据的准确性都不理想，其最显著的区别就是拥有欠拟合特性的模型对已有数据的匹配性很差，不过对数据中的噪声不敏感；而拥有过拟合特性的模型对数据的匹配性太好，所以对数据中的噪声非常敏感。接下来介绍这两种拟合的具体细节。</p>
<h3 id="3-2-1-欠拟合"><a href="#3-2-1-欠拟合" class="headerlink" title="3.2.1 欠拟合"></a>3.2.1 欠拟合</h3><p>我们先通过之前在监督学习中讲到的线性回归的实例，来直观地感受一下模型在什么情况下才算欠拟合。</p>
<p>图3-5（a）所示的是已获得的房屋的大小和价格的关系数据；图3-5（b）所示的就是一个欠拟合模型，这个模型虽然捕获了数据的一部分特征，但是不能很好地对新数据进行准确预测，因为这个欠拟合模型的缺点非常明显，如果输入的新数据的真实价格在该模型的上下抖动，那么相同面积的房屋在模型中得到的预测价格会和真实价格存在较大的误差；图3-5（c）所示的是一个较好的拟合模型，从某种程度上来讲，该模型已经捕获了原始数据的大部分特征，与欠拟合模型相比，不会存在那么严重的问题。</p>
<p><img src="Image00092.jpg" alt="img"></p>
<p>图3-5</p>
<p>在解决欠拟合问题时，主要从以下三方面着手。</p>
<p>（1 ）增加特征项<br>：在大多数情况下出现欠拟合是因为我们没有准确地把握数据的主要特征，所以我们可以尝试在模型中加入更多的和原数据有重要相关性的特征来训练搭建的模型，这样得到的模型可能会有更好的泛化能力。</p>
<p>（2 ）构造复杂的多项式<br>：这种方法很容易理解，我们知道一次项函数就是一条直线，二次项函数是一条抛物线，一次项和二次项函数的特性决定了它们的泛化能力是有局限性的，如果数据不在直线或者抛物线附近，那么必然出现欠拟合的情形，所以我们可以通过增加函数中的次项来增强模型的变化能力，从而提升其泛化能力。</p>
<p>（3 ）减少正则化参数 ：正则化参数出现的目的其实是防止过拟合情形的出现，但是如果我们的模型已经出现了欠拟合的情形，就可以通过减少正则化参数来消除欠拟合。</p>
<h3 id="3-2-2-过拟合"><a href="#3-2-2-过拟合" class="headerlink" title="3.2.2 过拟合"></a>3.2.2 过拟合</h3><p>同样，我们通过之前在监督学习中讲到的线性回归的实例来直观地感受一下模型的过拟合。</p>
<p>图3-6（a）所示的仍然是之前已获得的房屋的大小和价格的关系数据，图3-6（b）所示的是一个过拟合的模型，可以看到这个模型过度捕获了原数据的特征。不仅同之前的欠拟合模型存在同样的问题，而且过拟合模型受原数据中的噪声数据影响非常严重。如图3-6（c）所示，如果噪声数据严重偏离既定的数据轨道，则拟合出来的模型会发生很大改变，这个影响是灾难性的。</p>
<p><img src="Image00093.jpg" alt="img"></p>
<p>图3-6</p>
<p>要想解决在实践中遇到的过拟合问题，则主要从以下三方面着手。</p>
<p>（1 ）增大训练的数据量<br>：在大多数情况下发生过拟合是因为我们用于模型训练的数据量太小，搭建的模型过度捕获了数据的有限特征，这时就会出现过拟合，在增加参与模型训练的数据量后，模型自然就能捕获数据的更多特征，模型就不会过于依赖数据的个别特征。</p>
<p>（2 ）采用正则化方法 ：正则化一般指在目标函数之后加上范数，用来防止模型过拟合的发生，在实践中最常用到的正则化方法有L0正则、L1正则和L2正则。</p>
<p>（3 ） Dropout方法<br>：Dropout方法在神经网络模型中使用的频率较高，简单来说就是在神经网络模型进行前向传播的过程中，随机选取和丢弃指定层次之间的部分神经连接，因为整个过程是随机的，所以能有效防止过拟合的发生。</p>
<h2 id="3-3-后向传播"><a href="#3-3-后向传播" class="headerlink" title="3.3 后向传播"></a>3.3 后向传播</h2><p>深度学习中的后向传播主要用于对我们搭建的模型中的参数进行微调，在通过多次后向传播后，就可以得到模型的最优参数组合。接下来介绍后向传播这一系列的优化过程具体是如何实现的。深度神经网络中的参数进行后向传播的过程其实就是一个复合函数求导的过程。</p>
<p>首先来看一个模型结构相对简单的实例，在这个实例中我们定义模型的前向传播的计算函数为f =(x +y )×z ，它的流程如图3-7所示。</p>
<p><img src="Image00094.jpg" alt="img"></p>
<p>图3-7</p>
<p>假设输入数据x =2、y =5、z =3，则可以得到前向传播的计算结果f =(x +y )× z =21，如果把原函数改写成复合函数的形式，令h =x +y =7，就可以得到f =h ×z =21。</p>
<p>接下来看看在后向传播中需要计算的内容，假设在后向传播的过程中需要微调的参数有三个，分别是x 、y 、z ，这三个参数每轮后向传播的微调值为<img src="Image00095.jpg" alt="img"> 和<img src="Image00096.jpg" alt="img"> 这三个，值计算的都是偏导数，我们把求偏导的步骤进行拆解，这样就更容易理解整个计算过程了。</p>
<p>首先，分别计算<img src="Image00097.jpg" alt="img"> ，然后计算x 、y 、z 的后向传播微调值，即它们的偏导数，如下所述。</p>
<p>◎z 的偏导数为<img src="Image00098.jpg" alt="img"> 。</p>
<p>◎y 的偏导数为<img src="Image00099.jpg" alt="img"> 。</p>
<p>◎x 的偏导数为<img src="Image00100.jpg" alt="img"> 。</p>
<p>在清楚后向传播的大致计算流程和思路后，我们再来看一个模型结构相对复杂的实例，其结构是一个初级神经网络，如图3-8所示。</p>
<p><img src="Image00101.jpg" alt="img"></p>
<p>图3-8</p>
<p>我们假设x 0 =1、x 1 =1、b =-1，同时存在相对应的权重值w 0 =0.5、w 1 =0.5，使用Sigmoid作为该神经网络的激活函数，就可以得到前向传播的计算函数为<img src="Image00102.jpg" alt="img"> ，将相应的参数代入函数中进行计算，得到<img src="Image00103.jpg" alt="img"> ，之后再对函数进行求导。同样，可以将原函数进行简化，改写成复合函数的形式求解，令h =w 0 x 0 +w 1 x 1 +b =0，简化后的函数为<img src="Image00104.jpg" alt="img"> ，在分别计算后得到<img src="Image00105.jpg" alt="img"> ，有了以上结果后，下面来看x 0 、x 1 的后向传播微调值。</p>
<p>◎x 0 的后向传播微调值为<img src="Image00106.jpg" alt="img"></p>
<p>◎x 1 的后向传播微调值为<img src="Image00107.jpg" alt="img"></p>
<h2 id="3-4-损失和优化"><a href="#3-4-损失和优化" class="headerlink" title="3.4 损失和优化"></a>3.4 损失和优化</h2><p>深度神经网络中的损失用来度量我们的模型得到的预测值和数据真实值之间的差距，也是一个用来衡量我们训练出来的模型泛化能力好坏的重要指标。模型预测值和真实值的差距越大，损失值就会越高，这时我们就需要通过不断地对模型中的参数进行优化来减少损失；同理，预测值和真实值的差距越小，则说明我们训练的模型预测越准确，具有更好的泛化能力。</p>
<p>对模型进行优化的最终目的是尽可能地在不过拟合的情况下降低损失值。在拥有一部分数据的真实值后，就可通过模型获得这部分数据的预测值，然后计算预测值与真实值之间的损失值，通过不断地优化模型参数来使这个损失值变得尽可能小。可见，优化在模型的整个过程中有举足轻重的作用。</p>
<p>下面看看损失和优化的具体应用过程。以之前讲到的二分类问题为例，在该二分类问题中我们的目的是让搭建的模型能够对一堆苹果和梨混合在一起的水果进行准确分类。首先，建立一个二分类模型，对这堆水果进行第1轮预测，得到预测值y p red ，同时把这堆水果中每个水果的真实类别记作真实值y true ，将y true 与y pred 之间的差值作为第1轮的损失值。第1轮计算得到的损失值极有可能会较大，这时我们就需要对模型中的参数进行优化，在优化过程中对参数做相应的更新，然后进行第2轮的预测和误差值计算，如此循环往复，最后得到理想模型，该模型的预测值和真实值的差异足够小。</p>
<p>在上面的二分类问题的解决过程中计算模型的真实值和预测值之间损失值的方法有很多，而进行损失值计算的函数叫作损失函数；同样，对模型参数进行优化的函数也有很多，这些函数叫作优化函数。下面对几种较为常用的损失函数和优化函数进行介绍。</p>
<h3 id="3-4-1-损失函数"><a href="#3-4-1-损失函数" class="headerlink" title="3.4.1 损失函数"></a>3.4.1 损失函数</h3><p>这里将会列举三种在深度学习实践中经常用到的损失函数，分别是均方误差函数、均方根误差函数和平方绝对误差函数。</p>
<p>1.均方误差函数</p>
<p>均方误差（Mean Square Error，简称MSE）函数计算的是预测值与真实值之差的平方的期望值，可用于评价数据的变化程度，其得到的值越小，则说明模型的预测值具有越好的精确度。均方误差函数的计算如下：</p>
<p><img src="Image00108.jpg" alt="img"></p>
<p>其中，y p red 表示模型的预测值，y true 表示真实值，它们的上标i 用于指明是哪个真实值和预测值在进行损失计算，下同。</p>
<p>2.均方根误差函数</p>
<p>均方根误差（Root Mean Square Error，简称RMSE）在均方误差函数的基础上进行了改良，计算的是均方误差的算术平方根值，其得到的值越小，则说明模型的预测值具有越好的精确度。均方根误差函数的计算如下：</p>
<p><img src="Image00109.jpg" alt="img"></p>
<p>3.平均绝对误差函数</p>
<p>平均绝对误差（Mean Absolute Error，MAE）计算的是绝对误差的平均值，绝对误差即模型预测值和真实值之间的差的绝对值，能更好地反映预测值误差的实际情况，其得到的值越小，则说明模型的预测值具有越好的精确度。平均绝对误差函数如下：</p>
<p><img src="Image00110.jpg" alt="img"></p>
<h3 id="3-4-2-优化函数"><a href="#3-4-2-优化函数" class="headerlink" title="3.4.2 优化函数"></a>3.4.2 优化函数</h3><p>在计算出模型的损失值之后，接下来需要利用损失值进行模型参数的优化。之前提到的后向传播只是模型参数优化中的一部分，在实际的优化过程中，我们还面临在优化过程中相关参数的初始化、参数以何种形式进行微调、如何选取合适的学习速率等问题。我们可以把优化函数看作上述问题的解决方案的集合。</p>
<p>在实践操作中最常用到的是一阶优化函数，典型的一阶优化函数包括 GD、SGD、Momentum、Adagrad、Adam，等等。一阶优化函数在优化过程中求解的是参数的一阶导数，这些一阶导数的值就是模型中参数的微调值。</p>
<p>这里引入了一个新的概念：梯度。梯度其实就是将多元函数的各个参数求得的偏导数以向量的形式展现出来，也叫作多元函数的梯度。举例来说，有一个二元函数f (x ,y )，分别对二元函数中的x 、y 求偏导数，然后把参数x 、y 求得的偏导数写成向量的形式，即<img src="Image00111.jpg" alt="img"> ，这就是二元函数f<br>(x ,y )的梯度，我们也可以将其记作gradf (x ,y )。同理，三元函数f (x ,y ,z )的梯度为<img src="Image00112.jpg" alt="img"> ，以此类推。</p>
<p>不难发现，梯度中的内容其实就是在后向传播中对每个参数求得的偏导数，所以我们在模型优化的过程中使用的参数微调值其实就是函数计算得到的梯度，这个过程又叫作参数的梯度更新。对于只有单个参数的函数，我们选择使用计算得到的导数来完成参数的更新，如果在一个函数中需要处理的是多个参数的问题，就选择使用计算得到的梯度来完成参数的更新。</p>
<p>下面来看几种常用的优化函数。</p>
<p>1.梯度下降</p>
<p>梯度下降（Gradient Descent，简称GD）是参数优化的基础方法。虽然梯度下降已被广泛应用，但是其自身纯在许多不足，所以在其基础上改进的优化函数也非常多。</p>
<p>全局梯度下降的参数更新公式如下：</p>
<p><img src="Image00113.jpg" alt="img"></p>
<p>其中，训练样本总数为n ，j =0…n 。可以将这里的等号看作编程中的赋值运算，θ 是我们优化的参数对象，η 是学习速率，J (θ )是损失函数，<img src="Image00114.jpg" alt="img"> 是根据损失函数来计算θ 的梯度。学习速率用于控制梯度更新的快慢，如果学习速率过快，参数的更新跨步就会变大，极易出现局部最优和抖动；如果学习速率过慢，梯度更新的迭代次数就会增加，参数更新、优化的时间也会变长，所以选择一个合理的学习速率是非常关键的。</p>
<p>全局的梯度下降在每次计算损失值时都是针对整个参与训练的数据集而言的，所以会出现一个令人困扰的问题：因为模型的训练依赖于整个数据集，所以增加了计算损失值的时间成本和模型训练过程中的复杂度，而参与训练的数据量越大，这两个问题越明显。</p>
<p>2.批量梯度下降</p>
<p>为了避免全局梯度下降问题带来的弊端，人们对全局梯度下降进行了改进，创造了批量梯度下降（Batch Gradient Descent，简称BGD）的优化算法。批量梯度下降就是将整个参与训练的数据集划分为若干个大小差不多的训练数据集，我们将其中的一个训练数据集叫作一个批量，每次用一个批量的数据来对模型进行训练，并以这个批量计算得到的损失值为基准来对模型中的全部参数进行梯度更新，默认这个批量只使用一次，然后使用下一个批量的数据来完成相同的工作，直到所有批量的数据全部使用完毕。</p>
<p>假设划分出来的批量个数为m ，其中的一个批量包含batch 个数据样本，那么一个批量的梯度下降的参数更新公式如下：</p>
<p><img src="Image00115.jpg" alt="img"></p>
<p>训练样本总数为batch ，j =0…batch 。从以上公式中我们可以知道，其批量梯度下降算法大体上和全局的梯度下降算法没有多大的区别，唯一的不同就是损失值的计算方式使用的是Jbatch (θj )，即这个损失值是基于我们的一个批量的数据来进行计算的。如果我们将批量划分得足够好，则计算损失函数的时间成本和模型训练的复杂度将会大大降低，不过仍然存在一些小问题，就是选择批量梯度下降很容易导致优化函数的最终结果是局部最优解。</p>
<p>3.随机梯度下降</p>
<p>还有一种方法能够很好地处理全局梯度下降中的问题，就是随机梯度下降（Stochastic Gradient Descent，简称SGD）。随机梯度下降是通过随机的方式从整个参与训练的数据集中选取一部分来参与模型的训练，所以只要我们随机选取的数据集大小合适，就不用担心计算损失函数的时间成本和模型训练的复杂度，而且与整个参与训练的数据集的大小没有关系。</p>
<p>假设我们随机选取的一部分数据集包含stochastic 个数据样本，那么随机梯度下降的参数更新公式如下：</p>
<p><img src="Image00116.jpg" alt="img"></p>
<p>训练样本的总数为stochastic ，j =0…stochastic 。从该公式中可以看出，随机梯度下降和批量梯度下降的计算过程非常相似，只不过计算随机梯度下降损失值时使用的是Jstochastic (θj )，即这个损失值基于我们随机抽取的stochastic 个训练数据集。随机梯度下降虽然很好地提升了训练速度，但是会在模型的参数优化过程中出现抖动的情况，原因就是我们选取的参与训练的数据集是随机的，所以模型会受到随机训练数据集中噪声数据的影响，又因为有随机的因素，所以也容易导致模型最终得到局部最优解。</p>
<p>4.Adam</p>
<p>最后来看一个比较“智能”的优化函数方法——自适应时刻估计方法（Adaptive Moment Estimation，简称Adam）。Adam在模型训练优化的过程中通过让每个参数获得自适应的学习率，来达到优化质量和速度的双重提升。举个简单的实例，假设我们在一开始进行模型参数的训练时损失值比较大，则这时需要使用较大的学习速率让模型参数进行较大的梯度更新，但是到了后期我们的损失值已经趋近于最小了，这时就需要使用较小的学习速率让模型参数进行较小的梯度更新，以防止在优化过程中出现局部最优解。</p>
<p>在实际应用中当然不止Adam这种类型的自适应优化函数，不过应用该方法在最后取得的效果都比较理想，这和Adam收敛速度快、学习效果好的优点脱不了干系，而且对于在优化过程中出现的学习速率消失、收敛过慢、高方差的参数更新等导致损失值波动等问题，Adam都有很好的解决方案。</p>
<h2 id="3-5-激活函数"><a href="#3-5-激活函数" class="headerlink" title="3.5 激活函数"></a>3.5 激活函数</h2><p>我们在了解感知机和多层感知机时，很容易得到一个没有激活函数的单层神经网络模型，其数学表示如下：</p>
<p>f (x )=W ·X</p>
<p>其中的大写字母代表矩阵或者张量。下面搭建一个二层的神经网络模型并在模型中加入激活函数。假设激活函数的激活条件是比较0和输入值中的最大值，如果小于0，则输出结果为0；如果大于 0，则输出结果是输入值本身。同时，在神经网络模型中加入偏置（Bias），偏置可以让我们搭建的神经网络模型偏离原点，而没有偏置的函数必定会经过原点，如图3-9所示。</p>
<p><img src="Image00117.jpg" alt="img"></p>
<p>图3-9</p>
<p>如图3-9所示，f (x )=2·x 是不带偏置的函数，而g (x )=2 ·x<br>+3是偏置为3的函数。模型偏离原点的好处就是能够使模型具有更强的变换能力，在面对不同的数据时拥有更好的泛化能力。在增加偏置后，我们之前的单层神经网络模型的数学表示如下：</p>
<p>f (x )=W ·X +b</p>
<p>如果搭建二层神经网络，那么加入激活函数的二层神经网络的数学表示如下：</p>
<p>f (x )=max(W 2 ·m ax(W 1 ·X +b 1 ,0)+b 2 ,0)</p>
<p>如果是更多层次的神经网络模型，比如一个三层神经网络模型，并且每层的神经输出都使用同样的激活函数，那么数学表示如下：</p>
<p>f (x )=max(W 3 ·m ax(W 2 ·m ax(W 1 ·X +b 1 ,0)+b 2 ,0)+b 3 ,0)</p>
<p>深度更深的神经网络模型按如上原则类推。就数学意义而言，在构建神经网络模型的过程中，激活函数发挥了重要的作用，比如就上面的三层神经网络模型而言，如果没有激活函数，而我们只是一味地加深模型层次，则搭建出来的神经网络数学表示如下：</p>
<p>f (x )=W 3 ·(W 2 ·(W 1 ·X +b 1 )+b 2 )+b 3</p>
<p>可以看出，上面的模型存在一个很大的问题，它仍然是一个线性模型，如果不引入激活函数，则无论我们加深多少层，其结果都一样，线性模型在应对非线性问题时会存在很大的局限性。激活函数的引入给我们搭建的模型带来了非线性因素，非线性的模型能够处理更复杂的问题，所以通过选取不同的激活函数便可以得到复杂多变的深度神经网络，从而应对诸如图片分类这类复杂的问题。</p>
<p>下面讲解我们在实际应用最常用到的三种非线性激活函数：Sigmoid、tanh和ReLU。</p>
<h3 id="3-5-1-Sigmoid"><a href="#3-5-1-Sigmoid" class="headerlink" title="3.5.1 Sigmoid"></a>3.5.1 Sigmoid</h3><p>Sigmoid的数学表达式如下：</p>
<p>f (x )=11+e - x</p>
<p>根据Sigmoid函数，我们可以得到Sigmoid的几何图形，如图3-10所示。</p>
<p><img src="Image00118.jpg" alt="img"></p>
<p>图3-10</p>
<p>从图 3-10中可以看到，输入 Sigmoid激活函数的数据经过激活后输出数据的区间为0～1，输入数据越大，输出数据越靠近1，反之越靠近0。Sigmoid在一开始被作为激活函数使用时就受到了大众的普遍认可，其主要原因是从输入到经过Sigmoid激活函数激活输出的一系列过程与生物神经网络的工作机理非常相似，不过Sigmoid作为激活函数的缺点也非常明显，其最大的缺点就是使用Sigmoid作为激活函数会导致模型的梯度消失，因为Sigmoid导数的取值区间为0～0.25，如图3-11所示。</p>
<p><img src="Image00119.jpg" alt="img"></p>
<p>图3-11</p>
<p>根据复合函数的链式法则可以知道，如果我们的每层神经网络的输出节点都使用Sigmoid作为激活函数，那么在后向传播的过程中每逆向经过一个节点，就要乘上一个Sigmoid的导数值，而Sigmoid的导数值的取值区间为0～0.25，所以即便每次乘上Sigmoid的导数值中的最大值0.25，也相当于在后向传播的过程中每逆向经过一个节点，梯度值的大小就会变成原来的四分之一，如果模型层次达到了一定深度，那么后向传播会导致梯度值越来越小，直到梯度消失。</p>
<p>其次是Sigmoid函数的输出值恒大于0，这会导致我们的模型在优化的过程中收敛速度变慢。因为深度神经网络模型的训练和参数优化往往需要消耗大量的时间，如果模型的收敛速度变慢，就又会增加我们的时间成本。考虑到这一点，在选取参与模型中相关计算的数据时，要尽量使用零中心（Zero- Centered）数据；而且要尽量保证计算得到的输出结果是零中心数据。</p>
<h3 id="3-5-2-tanh"><a href="#3-5-2-tanh" class="headerlink" title="3.5.2 tanh"></a>3.5.2 tanh</h3><p>激活函数tanh的数学表达式如下：</p>
<p><img src="Image00120.jpg" alt="img"></p>
<p>我们根据tanh函数可以得到tanh的几何图形，如图3-12所示。</p>
<p><img src="Image00121.jpg" alt="img"></p>
<p>图3-12</p>
<p>我们从图 3-12中可以知道，tanh函数的输出结果是零中心数据，所以解决了激活函数在模型优化过程中收敛速度变慢的问题。而 tanh函数的导数取值区间为0～1，仍然不够大，如图3-12所示。</p>
<p><img src="Image00122.jpg" alt="img"></p>
<p>图3-12</p>
<p>所以，因为导数取值范围的关系，在深度神经网络模型的后向传播过程中仍有可能出现梯度消失的情况。</p>
<h3 id="3-5-3-ReLU"><a href="#3-5-3-ReLU" class="headerlink" title="3.5.3 ReLU"></a>3.5.3 ReLU</h3><p>ReLU（Rectified Linear Unit，修正线性单元）是目前在深度神经网络模型中使用率最高的激活函数，其数学表达式如下：</p>
<p>f (x )=max(0,x )</p>
<p>ReLU函数通过判断0和输入数据x 中的最大值作为结果进行输出，即如果x 小于0，则输出结果0；如果x 大于0，则输出结果x 。其逻辑非常简单，使用该激活函数的模型在实际计算过程中非常高效。我们根据ReLU函数，可以得到ReLU的几何图形，如图3-13所示。</p>
<p><img src="Image00123.jpg" alt="img"></p>
<p>图3-13</p>
<p>ReLU函数的收敛速度非常快，其计算效率远远高于Sigmoid和tanh。不过ReLU也同样存在需要我们关注的问题：从ReLU的几何图形中可以看出，ReLU的输出并不是零中心数据，这可能会导致某些神经元永远不会被激活，并且这些神经元相对应的参数不能被更新。这一般是由于模型参数在初始化时使用了全正或者全负的值，或者在后向传播过程中设置的学习速率太快导致的。其解决方法是对模型参数使用更高级的初始化方法如Xavier，以及设置合理的后向传播学习速率，推荐使用自适应的算法如Adam。</p>
<p>ReLU尽管存在上述问题，但仍成为许多人搭建深度神经网络模型时使用的主流激活函数，它也在不断被改进，现在已经出现很多ReLU的改进版本如Leaky- ReLU、R-ReLU等。</p>
<h2 id="3-6-本地深度学习工作站"><a href="#3-6-本地深度学习工作站" class="headerlink" title="3.6 本地深度学习工作站"></a>3.6 本地深度学习工作站</h2><p>我们在掌握了搭建深度学习模型的方法后，接下来需要对我们的模型进行训练并优化模型的参数。在一个深度学习模型中需要对大量的参数进行计算和优化，这必然会耗费大量的计算机算力和时间。而一个好的计算机硬件可以大大减少降低训练的时间成本，特别是当我们需要对模型进行反复调参时，时间成本的增加将会是一个灾难性的体验。</p>
<p>一个好用的深度学习工作站对于想要从事深度学习领域相关工作的人员而言可谓如虎添翼，虽然现在已经存在很多提供云端深度学习虚拟主机租用服务的供应商，不过对于想要长期从事该领域的人而言，就成本和方便性来说不如自己搭建一个本地的简易工作站来得方便，本节会简单介绍组装本地深度学习工作站的知识，这里推荐以显卡为其计算核心。</p>
<h3 id="3-6-1-GPU和CPU"><a href="#3-6-1-GPU和CPU" class="headerlink" title="3.6.1 GPU和CPU"></a>3.6.1 GPU和CPU</h3><p>因为我们推荐的工作站会让显卡输出核心算力，所以在搭建一个深度学习本地工作站之前，我们先来了解一下计算机的GPU和CPU分别对深度学习的算力贡献有多大。CPU（Central Processing Unit）又被称作中央处理器，是一台计算机的计算处理核心，主要负责计算机的控制命令处理和核心运算输出。GPU（Graphics Processing Unit）又被称作图像处理器，是一台主机的显示处理核心，主要负责对计算机中的图形和图像的处理与运算。它们之间的具体差异如下。</p>
<p>（1）核心数： GPU相较于CPU的硬件构成，在结构上拥有更多的核心数量，虽然GPU的核心数量更多，但是CPU中的单个核心相对于GPU中的单个核心，拥有更快速、高效的算力。</p>
<p>（2）应用场景： 在应用场景上GPU和CPU也各有侧重，GPU适用于需要并行计算能力的场景，比如图像处理；CPU更适用于需要串行计算能力的场景，比如计算机的指令处理。</p>
<p>从各个方面来看，CPU都是为了计算而生的，那么我们为什么还会强力推荐将 GPU作为核心算力的输出硬件呢？这是因为在深度学习模型中生成的参数结构都是张量（Tensor）形式的，之前我们已经了解了矩阵和张量是如何进行算术运算的，我们发现，其实矩阵和张量的算术运算的模式就是一种并行运算。所以张量的算术运算在 GPU的加持下会获得比CPU更快速、高效的计算能力，因此在对深度学习参数的训练和优化过程中，GPU能够为我们提供更多的帮助。</p>
<p>下面我们通过一个简单的实验，来看看GPU和CPU在计算效率上的差距，这个实验会对不同维度的张量进行计算并统计每次计算的耗时，读者暂时不用理解该段代码的含义，只关注最后的输出结果。</p>
<p>首先，我们使用CPU来进行10次张量运算，并打印最后的计算耗时结果，张量的维度从（100,1000,1000）到（1000,1000,1000）逐次累加。代码如下：</p>
<p><img src="Image00124.jpg" alt="img"></p>
<p>使用CPU进行计算的10次不同维度的张量计算耗时如下：</p>
<p><img src="Image00125.jpg" alt="img"></p>
<p>然后，我们使用GPU进行10次张量运算，并打印最后的计算耗时结果，同样，张量的维度从（100,1000,1000）到（1000,1000,1000）逐次累加。代码如下：</p>
<p><img src="Image00126.jpg" alt="img"></p>
<p><img src="Image00127.jpg" alt="img"></p>
<p>使用GPU进行计算的10次不同维度的张量计算耗时如下：</p>
<p><img src="Image00128.jpg" alt="img"></p>
<p>这里使用的硬件分别是Intel-i7-8700K型号的CPU和GTX-1080Ti型号的GPU，我们通过该实验可以看出，GPU在处理张量计算的效率上要远远高于 CPU，如果计算参数更多，那么GPU和CPU之间的差距会更明显。</p>
<h3 id="3-6-2-配置建议"><a href="#3-6-2-配置建议" class="headerlink" title="3.6.2 配置建议"></a>3.6.2 配置建议</h3><p>我们在了解了GPU和CPU在深度学习参数训练过程中各自的侧重点后，必然需要一款功能强大的显卡作为我们组建深度学习工作站的重要部件。这里推荐两款NVDIA系列中的性价比较高的显卡：GTX 1080Ti及GTX 1070Ti。</p>
<p>这两款显卡比普通版的显卡型号多出了一个Ti，这个Ti在NVIDIA中主要用于区别普通版显卡，表示Ti版显卡具备更强劲的性能，比如，Ti版的显卡GTX 1080Ti就比普通版本的GTX 1080在性能上提升了至少35%；而且，Ti版的显卡的核心数量比我们熟知的X80系列还要多，在显卡的规格上向更高端的泰坦（TITAN）系列显卡看齐，并且Ti版的显卡比泰坦系列拥有更实惠的性价比。如表2-1所示是官方给出的GTX 1080 Ti显卡的核心参数。</p>
<p>表2-1</p>
<p><img src="Image00129.jpg" alt="img"></p>
<p>续表</p>
<p><img src="Image00130.jpg" alt="img"></p>
<p>GTX 1080 Ti的显存高达11GB，显卡的Boost频率高达1.6GHz，超频则高达2GHz，CUDA核心数已经多达3584个，这些都是显卡的高性能表现。对于其他电脑硬件，我们建议再增加一块能够高速存取数据的固态硬盘 SSD和一个能够和显卡高效协同工作的性能强劲的CPU，再加上一块能够实现多块显卡拓展的主板。这样，我们的深度学习工作站的核心部件就凑齐了，该工作站能够满足我们对深度神经网络模型进行训练和对参数进行优化的绝大多数需求。</p>
<h1 id="第4章-卷积神经网络"><a href="#第4章-卷积神经网络" class="headerlink" title="第4章 卷积神经网络"></a>第4章 卷积神经网络</h1><p>卷积神经网络（Convolutional Neural Networks，简称CNN）可以说是深度神经网络模型中的“明星”网络架构，在计算机视觉方面贡献颇丰。一个标准的卷积神经网络架构主要由卷积层、池化层和全连接层等核心层次构成，卷积层、池化层和全连接层不仅是搭建卷积神经网络的基础，也是我们需要重点掌握和理解的内容。本章会先对卷积层、池化层和全连接层进行详细介绍，再介绍如何使用这些基本的层次结构，并配合一些调整和改进，来搭建形态各异的卷积神经网络模型。</p>
<h2 id="4-1-卷积神经网络基础"><a href="#4-1-卷积神经网络基础" class="headerlink" title="4.1 卷积神经网络基础"></a>4.1 卷积神经网络基础</h2><p>下面讲解卷积神经网络中的核心基础，涉及卷积层、池化层、全连接层在卷积神经网络中扮演的角色、实现的具体功能和工作原理。</p>
<h3 id="4-1-1-卷积层"><a href="#4-1-1-卷积层" class="headerlink" title="4.1.1 卷积层"></a>4.1.1 卷积层</h3><p>卷积层（Convolution Layer）的主要作用是对输入的数据进行特征提取，而完成该功能的是卷积层中的卷积核（Filter）。我们可以将卷积核看作一个指定窗口大小的扫描器，扫描器通过一次又一次地扫描输入的数据，来提取数据中的特征。如果我们输入的是图像数据，那么在通过卷积核的处理后，就可以识别出图像中的重要特征了。</p>
<p>那么，在卷积层中是如何定义这个卷积核的呢？卷积层又是怎样工作的呢？下面通过一个实例进行说明。假设有一张32×32×3的输入图像，其中32×32指图像的高度×宽度，3指图像具有 R、G、B三个色彩通道，即红色（Red）、绿色（Green）和蓝色（Blue），我们定义一个窗口大小为5×5×3的卷积核，其中 5×5指卷积核的高度×宽度，3指卷积核的深度，对应之前输入图像的 R、G、B三个色彩通道，这样做的目的是当卷积核窗口在输入图像上滑动时，能够一次在其三个色彩通道上同时进行卷积操作。注意，如果我们的原始输入数据都是图像，那么我们定义的卷积核窗口的宽度和高度要比输入图像的宽度和高度小，较常用的卷积核窗口的宽度和高度大小是3×3和5×5。在定义卷积核的深度时，只要保证与输入图像的色彩通道一致就可以了，如果输入图像是3个色彩通道的，那么卷积核的深度就是3；如果输入图像是单色彩通道的，那么卷积核的深度就是1，以此类推。如图4-1所示为单色彩通道的输入图像的卷积过程。</p>
<p><img src="Image00131.jpg" alt="img"></p>
<p>图4-1</p>
<p>如图4-1所示，输入的是一张原始图像，中间的是卷积核，图中显示的是卷积核的一次工作过程，通过卷积核的计算输出了一个结果，其计算方式就是将对应位置的数据相乘然后相加，如下所示：</p>
<p>-8=0×4+0×0+0×0+0×0+1×0+1×0+0×0+1×0+2×(-4)</p>
<p>下面，根据我们定义的卷积核步长对卷积核窗口进行滑动。卷积核的步长其实就是卷积核窗口每次滑动经过的图像上的像素点数量，如图4-2所示是一个步长为2的卷积核经过一次滑动后窗口位置发生的变化。</p>
<p><img src="Image00132.jpg" alt="img"></p>
<p>图4-2</p>
<p>如果我们仔细观察，则还会发现在图4-2中输入图像的最外层多了一圈全为0的像素，这其实是一种用于提升卷积效果的边界像素填充方式。我们在对输入图像进行卷积之前，有两种边界像素填充方式可以选择，分别是Same和Valid。Valid方式就是直接对输入图像进行卷积，不对输入图像进行任何前期处理和像素填充，这种方式的缺点是可能会导致图像中的部分像素点不能被滑动窗口捕捉；Same 方式是在输入图像的最外层加上指定层数的值全为0的像素边界，这样做是为了让输入图像的全部像素都能被滑动窗口捕捉。</p>
<p>通过对卷积过程的计算，我们可以总结出一个通用公式，在本书中我们统一把它叫作卷积通用公式，用于计算输入图像经过一轮卷积操作后的输出图像的宽度和高度的参数，公式如下：</p>
<p><img src="Image00133.jpg" alt="img"></p>
<p>其中，通用公式中的W 和H 分别表示图像的宽度（Weight）和高度（Height）的值；下标input 表示输入图像的相关参数；下标output 表示输出的图像的相关参数；下标filter 表示卷积核的相关参数；S 表示卷积核的步长；P<br>（是Padding的缩写）表示在图像边缘增加的边界像素层数，如果图像边界像素填充方式选择的是Same模式，那么P 的值就等于图像增加的边界层数，如果选择的是Valid模式，那么P =0。</p>
<p>下面看一个具体的实例。输入一个7×7×1的图像数据，卷积核窗口为3×3×1，输入图像的最外层使用了一层边界像素填充，卷积核的步长stride为1，这样可以得到Wi nput =7、Hinput =7、Wfilter =3、P =1、S =1，然后根据公式就能够计算出最后输出特征图的宽度和高度都是7，即<img src="Image00134.jpg" alt="img"> 。</p>
<p>我们已经了解了单通道的卷积操作过程，但是在实际应用中一般很少处理色彩通道只有一个的输入图像，所以接下来看看如何对三个色彩通道的输入图像进行卷积操作，三个色彩通道的输入图像的卷积过程如图4-3所示。</p>
<p><img src="Image00135.jpg" alt="img"></p>
<p>图4-3</p>
<p>在卷积过程中我们还加入了一个值为1的偏置，其实整个计算过程和之前的单通道的卷积过程大同小异，我们可以将三通道的卷积过程看作三个独立的单通道卷积过程，最后将三个独立的单通道卷积过程的结果进行相加，就得到了最后的输出结果。</p>
<h3 id="4-1-2-池化层"><a href="#4-1-2-池化层" class="headerlink" title="4.1.2 池化层"></a>4.1.2 池化层</h3><p>卷积神经网络中的池化层可以被看作卷积神经网络中的一种提取输入数据的核心特征的方式，不仅实现了对原始数据的压缩，还大量减少了参与模型计算的参数，从某种意义上提升了计算效率。其中，最常被用到的池化层方法是平均池化层和最大池化层，池化层处理的输入数据在一般情况下是经过卷积操作之后生成的特征图。如图4-4所示是一个最大池化层的操作过程。</p>
<p><img src="Image00136.jpg" alt="img"></p>
<p>图4-4</p>
<p>如图4-4所示，池化层也需要定义一个类似卷积层中卷积核的滑动窗口，但是这个滑动窗口仅用来提取特征图中的重要特征，本身并没有参数。这里使用的滑动窗口的高度×宽度是2×2，滑动窗口的深度和特征图的深度保持一致。如图4-4所示是对单层特征图进行的操作，并且滑动窗口的步长为2。</p>
<p>下面来看看这个滑动窗口的计算细节。首先通过滑动窗口框选出特征图中的数据，然后将其中的最大值作为最后的输出结果。图4-4中左边的方框就是输入的特征图像，即原特征图，如果滑动窗口是步长为2的2×2窗口，则刚好可以将输入图像划分成4部分，取每部分中数字的最大值作为该部分的输出结果，便可以得到图4-4中右边的输出图像，即目标特征图。第1个滑动窗口框选的4个数字分别是1、1、5、6，所以最后选出的最大的数字是6；第2个滑动窗口框选的4个数字分别是2、4、7、8，所以最后选出的最大的数字是8，以此类推，最后得到的结果就是6、8、3、4。</p>
<p>在了解最大池化层的工作方法后，我们再来看另一种常用的池化层方法，如图4-5所示是一个平均池化层的操作过程。</p>
<p><img src="Image00137.jpg" alt="img"></p>
<p>图4-5</p>
<p>平均池化层的窗口、步长和最大池化层没有区别，但平均池化层最后对窗口框选的数据使用的计算方法与最大池化层不同。平均池化层在得到窗口中的数字后，将它们全部相加再求平均值，将该值作为最后的输出结果。如果滑动窗口依旧是步长为2的2×2窗口，则同样刚好将输入图像划分成4部分，将每部分的数据相加然后求平均值，并将该值作为该部分的输出结果，最后得到图4-5中右边的输出图像，即目标特征图。第1个滑动窗口框选的4个数字分别是1、1、5、6，那么最后求得平均值为3.25并将其作为输出结果；第2个滑动窗口框选的4个数字分别是2、4、7、8，那么最后求得平均值为5.25并将其作为输出结果，以此类推，最后得到的结果就是3.25、5.25、2、2。</p>
<p>通过池化层的计算，我们也能总结出一个通用公式，在本书中我们统一把它叫作池化通用公式，用于计算输入的特征图经过一轮池化操作后输出的特征图的宽度和高度：</p>
<p><img src="Image00138.jpg" alt="img"></p>
<p>其中，W 和H 分别表示特征图的宽度和高度值，下标input 表示输入的特征图的相关参数，下标output 表示输出的特征图的相关参数，下标filter 表示滑动窗口的相关参数，S 表示滑动窗口的步长，并且输入的特征图的深度和滑动窗口的深度保持一致。</p>
<p>下面通过一个实例来看看如何计算输入的特征图经过池化层后输出的特征图的高度和宽度，定义一个16×16×6的输入图像，池化层的滑动窗口为2×2×6，滑动窗口的步长stride为2。这样可以得到Wi nput =16、Hinput =16、Wfilter =2、S =2，然后根据总结得到的公式，最后输出特征图的宽度和高度都是8，即<img src="Image00139.jpg" alt="img"> 。从结果可以看出，在使用2×2×6的滑动窗口对输入图像进行池化操作后，得到的输出特征图的高度和宽度变成了原来的一半，这也印证了我们之前提到的池化层的作用：池化层不仅能够最大限度地提取输入的特征图的核心特征，还能够对输入的特征图进行压缩。</p>
<h3 id="4-1-3-全连接层"><a href="#4-1-3-全连接层" class="headerlink" title="4.1.3 全连接层"></a>4.1.3 全连接层</h3><p>全连接层的主要作用是将输入图像在经过卷积和池化操作后提取的特征进行压缩，并且根据压缩的特征完成模型的分类功能。如图4-6所示是一个全连接层的简化流程。</p>
<p><img src="Image00140.jpg" alt="img"></p>
<p>图4-6</p>
<p>其实全连接层的计算比卷积层和池化层更简单，如图4-6所示的输入就是我们通过卷积层和池化层提取的输入图像的核心特征，与全连接层中定义的权重参数相乘，最后被压缩成仅有的10个输出参数，这10个输出参数其实已经是一个分类的结果，再经过激活函数的进一步处理，就能让我们的分类预测结果更明显。将 10个参数输入到 Softmax激活函数中，激活函数的输出结果就是模型预测的输入图像对应各个类别的可能性值。</p>
<p>在介绍完卷积层、池化层、全连接层后，接下来讲解一些经典的卷积神经网络模型的架构和工作原理。</p>
<h2 id="4-2-LeNet模型"><a href="#4-2-LeNet模型" class="headerlink" title="4.2 LeNet模型"></a>4.2 LeNet模型</h2><p>LeNet是由LeCun在1989年提出的历史上第1个真正意义上的卷积神经网络模型。不过最初的 LeNet模型已经不再被人们使用了，被使用最多的是在 1998年出现的 LeNet的改进版本LeNet-5。LeNet-5作为卷积神经网络模型的先驱，最先被用于处理计算机视觉问题，在识别手写字体的准确性上取得了非常好的成绩。如图4-7所示是LeNet-5卷积神经网络的网络架构。</p>
<p><img src="Image00141.jpg" alt="img"></p>
<p>图4-7</p>
<p>在图4-7中，从上往下分别是INPUT层、C1层、S2层、C3层、S4层、C5层、F6层和OUTPUT层，下面对这些层一一进行介绍。</p>
<p>（1 ） INPUT层 ：为输入层，LeNet-5卷积神经网络的默认输入数据必须是维度为32×32×1的图像，即输入的是高度和宽度均为32的单通道图像。</p>
<p>（2 ） C1层<br>：为LeNet-5的第1个卷积层，使用的卷积核滑动窗口为5×5×1，步长为1，不使用Padding，如果输入数据的高度和宽度均为32，那么通过套用卷积通用公式，可以得出最后输出的特征图的高度和宽度均为28，即<img src="Image00142.jpg" alt="img"> 。同时，我们看到这个卷积层要求最后输出深度为6的特征图，所以需要进行6次同样的卷积操作，最后得到输出的特征图的维度为28×28×6。</p>
<p>（3）S2层<br>：为LeNet-5中的下采样层，下采样要完成的功能是缩减输入的特征图的大小，这里我们使用最大池化层来进行下采样。选择最大池化层的滑动窗口为2×2×6，步长为2，因为输入的特征图的高度和宽度均为28，所以通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为14，即<img src="Image00143.jpg" alt="img"> ，所以本层输出的特征图的维度为14×14×6。</p>
<p>（4）C3层 ：为LeNet-5的第 2个卷积层，使用的卷积核滑动窗口发生了变化，变成了5×5×6，因为输入的特征图维度是14×14×6，所以卷积核滑动窗口的深度必须要和输入特征图的深度一致，步长依旧为1，不使用 Padding。套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为10，即<img src="Image00144.jpg" alt="img"> ，同时，这个卷积层要求最后输出深度为16的特征图，所以需要进行16次卷积，最后得到输出的特征图维度为10×10×16。</p>
<p>（5）S4层<br>：为第2个下采样层，同样使用最大池化层，这时的输入特征图是C3层输出的维度为10×10×16的特征图，我们对最大池化层的滑动窗口选择2×2×16，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度为5，即<img src="Image00145.jpg" alt="img"> ，最后得到输出的特征图维度为5×5×16。</p>
<p>（6）C5层 ：这一层可以看作LeNet-5的第 3个卷积层，是之前的下采样层和之后的全连接层的一个中间层。该层使用的卷积核滑动窗口为5×5×16，步长为1，不使用Padding。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度为1，即<img src="Image00146.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为120的特征图，所以需要进行120次卷积，最后得到输出的特征图维度为1×1×120。</p>
<p>（7）F6层 ：为LeNet-5的第 1个全连接层，该层的输入数据是维度为1×1×120的特征图，要求最后输出深度为84的特征图，所以本层要完成的任务就是对输入的特征图进行压缩，最后得到输出维度为1×84的特征图。要完成这个过程，就需要让输入的特征图乘上一个维度为120×84的权重参数，根据矩阵的乘法运算法则，一个维度为1×120的矩阵乘上一个维度为120×84的矩阵最后输出的是维度为1×84的矩阵，这个维度为1×84的矩阵就是全连接层最后输出的特征图。</p>
<p>（8）OUTPUT层 ：为输出层，因为LeNet-5是用来解决分类问题的，所以需要根据输入图像判断图像中手写字体的类别，输出的结果是输入图像对应 10个类别的可能性值，在此之前我们需要先将F6层输入的维度为1×84的数据压缩成维度为1×10的数据，同样依靠一个84×10的矩阵来完成。将最终得到10个数据全部输入Softmax激活函数中，得到的就是模型预测的输入图像所对应10个类别的可能性值了。</p>
<h2 id="4-3-AlexNet模型"><a href="#4-3-AlexNet模型" class="headerlink" title="4.3 AlexNet模型"></a>4.3 AlexNet模型</h2><p>Hinton课题组为了证明深度学习的潜力，在2012年的ILSVRC（ImageNet Large Scale Visual Recognition Competition，简称ILSVRC）比赛中使用了AlexNet搭建卷积神经网络模型，并通过AlexNet模型在这次比赛中一举获得冠军，而且在识别准确率上比使用支持向量机（Support Vector Machines，简称SVM）这种传统的机器学习方法的第2名有一定的优势。由于在这个比赛上取得的显著成绩，卷积神经网络模型受到众多科学家的关注和重视。下面让我们来看看这个卷积神经网络模型的网络架构，如图4-8所示。</p>
<p><img src="Image00147.jpg" alt="img"></p>
<p>图4-8</p>
<p>在图4-8中，从上往下分别是INPUT层、Conv1层、MaxPool1层、Conv2层、MaxPool2层、Conv3层、Conv4层、Conv5层、MaxPool3层、FC6层、FC7层、FC8层和OUTPUT层，可见AlexNet的卷积神经网络架构比LeNet-5的卷积神经网络架构的层次更深，也更复杂，下面一一进行介绍。</p>
<p>（1）INPUT 层<br>：为输入层，AlexNet卷积神经网络默认的输入数据必须是维度为224×224×3的图像，即输入图像的高度和宽度均为224，色彩通道是R、G、B三个。</p>
<p>（2）Conv1层<br>：为AlexNet的第1个卷积层，使用的卷积核滑动窗口为11×11×3，步长为4，Padding为2。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为55，即<img src="Image00148.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为96的特征图，所以需要进行96次卷积，最后得到输出的特征图的维度为55×55×96。</p>
<p>（3）MaxPool1层<br>：为AlexNet的第1个最大池化层，最大池化层的滑动窗口为3×3×96，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为27，即<img src="Image00149.jpg" alt="img"> ，最后得到的输出的特征图的维度为27×27×96。</p>
<p>（4）Conv2层 ：为AlexNet的第 2个卷积层，使用的卷积核滑动窗口为5×5×96，步长为1，Padding为2。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为27，即<img src="Image00150.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为256的特征图，所以需要进行256次卷积，最后得到输出的特征图的维度为27×27×256。</p>
<p>（5）MaxPool2层<br>：为AlexNet的第2个最大池化层。最大池化层的滑动窗口为3×3×256，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为13，即<img src="Image00151.jpg" alt="img"> ，最后得到输出的特征图的维度为13×13×256。</p>
<p>（6）Conv3层<br>：为AlexNet的第3个卷积层，使用的卷积核维度为3×3×256，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为13，即<img src="Image00152.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为384的特征图，所以需要进行384次卷积，最后得到特征图的维度为13×13×384。</p>
<p>（7）Conv4层<br>：为AlexNet的第4个卷积层，使用的卷积核滑动窗口为3×3×384，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为13，即<img src="Image00153.jpg" alt="img"> ，同时这个卷积层要求最后输出深度依旧为384的特征图，所以需要进行384次卷积，最后得到输出的特征图的维度为13×13×384。</p>
<p>（8）Conv5层<br>：为AlexNet的第5个卷积层，使用的卷积核滑动窗口为3×3×384，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为13，即<img src="Image00154.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为256的特征图，所以需要进行256次卷积，最后得到输出的特征图的维度为13×13×256。</p>
<p>（9）MaxPool3层<br>：为AlexNet的第3个最大池化层，最大池化层的滑动窗口为3×3×256，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均6，即<img src="Image00155.jpg" alt="img"> ，最后得到输出的特征图的维度为6×6×256。</p>
<p>（10）FC6层 ：为AlexNet的第 1个全连接层，输入的特征图的维度为6×6×256，首先要对输入的特征图进行扁平化处理，将其变成维度为1×9216的输入特征图，因为本层要求输出数据的维度是1×4096，所以需要一个维度为9216×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。</p>
<p>（11）FC7层<br>：为AlexNet的第2个全连接层，输入数据的维度为1×4096，输出数据的维度仍然是1×4096，所以需要一个维度为4096×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度依旧为1×4096。</p>
<p>（12）FC8层<br>：为AlexNet的第3个全连接层，输入数据的维度为1×4096，输出数据的维度要求是1×1000，所以需要一个维度为4096×1000的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×1000。</p>
<p>（13）OUTPUT层<br>：为输出层，要求最后得到输入图像对应1000个类别的可能性值，因为AlexNet用来解决图像分类问题，即要求通过输入图像判断该图像所属的类别，所以我们要将全连接层最后输出的维度为1×1000的数据传递到 Softmax激活函数中，就能得到1000个全新的输出值，这1000个输出值就是模型预测的输入图像对应1000个类别的可能性值。</p>
<h2 id="4-4-VGGNet模型"><a href="#4-4-VGGNet模型" class="headerlink" title="4.4 VGGNet模型"></a>4.4 VGGNet模型</h2><p>VGGNet由牛津大学的视觉几何组（Visual Geometry Group）提出，并在2014年举办的ILSVRC中获得了定位任务第1名和分类任务第2名的好成绩，相对于2012年的ILSVRC冠军模型AlexNet而言，在VGGNet模型中统一了卷积中使用的参数，比如卷积核滑动窗口的高度和宽度统一为3×3，卷积核步长统一为1，Padding统一为1，等等；而且增加了卷积神经网络模型架构的深度，分别定义了16层的VGG16模型和19层的VGG19模型，与AlexNet的8层结构相比，深度更深。这两个重要的改变对于人们重新定义卷积神经网络模型架构也有不小的帮助，至少证明使用更小的卷积核并且增加卷积神经网络的深度，可以更有效地提升模型的性能。下面看一个16层结构的VGGNet模型，如图4-9所示。</p>
<p><img src="Image00156.jpg" alt="img"></p>
<p>图4-9</p>
<p>在图4-9中，从上往下分别是INPUT层、Conv1层、Conv2层、MaxPool1层、Conv3层、Conv4层、MaxPool2层、Conv5层、Conv6层、MaxPool3层、Conv7层、Conv8层、Conv9层、MaxPool4层、Conv10层、Conv11层、Conv12层、MaxPool5层、FC13层、FC14层、FC15层和OUTPUT层，一共有16层，所以我们将这个模型叫作VGG16，下面我们重点对模型中的前7层和后8层进行介绍。</p>
<p>（1）INPUT层 ：为输入层，VGG16卷积神经网络默认的输入数据必须是维度为224×224×3的图像，和 AlexNet一样，其输入图像的高度和宽度均为224，而且拥有的色彩通道是R、G、B这三个。</p>
<p>（2）Conv1层 ：为VGG16的第 1个卷积层，使用的卷积核滑动窗口为3×3×3，步长为1，Padding为1。套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为224，即<img src="Image00157.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为64的特征图，所以需要进行64次卷积，最后得到输出的特征图的维度为224×224×64。</p>
<p>（3）Conv2层<br>：为VGG16的第2个卷积层，使用的卷积核滑动窗口为3×3×64，步长为1，Padding为1。套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为224，即<img src="Image00158.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为64的特征图，所以需要进行64次卷积，最后得到输出的特征图的维度为224×224×64。</p>
<p>（4）MaxPool1层<br>：为VGG16的第1个最大池化层，最大池化层的滑动窗口为2×2×64，步长为2。套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为112，即<img src="Image00159.jpg" alt="img"> ，最后得到输出的特征图的维度为112×112×64。</p>
<p>（5）Conv3层<br>：为VGG16的第3个卷积层，使用的卷积核滑动窗口为3×3×64，步长为1，Padding为1。套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为112，即<img src="Image00160.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为128的特征图，所以需要进行128次卷积，最后得到输出的特征图的维度为112×112×128。</p>
<p>（6）Conv4层 ：为VGG16的第 4个卷积层，使用的卷积核滑动窗口为3×3×128，步长为1，Padding为1。在套用卷积通用公式后，可以得到最后输出的特征图的高度和宽度均为112，即<img src="Image00161.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为128的特征图，所以需要进行128次卷积，最后得到输出的特征图的维度为112×112×128。</p>
<p>（7）MaxPool2层<br>：为VGG16的第2个最大池化层，最大池化层的滑动窗口为2×2×128，步长为2。在套用池化通用公式后，可以得到最后输出的特征图的高度和宽度均为56，即<img src="Image00162.jpg" alt="img"> ，最后得到本层输出的特征图的维度为56×56×128。</p>
<p>（8）Conv10层<br>：为VGG16的第10个卷积层，使用的卷积核滑动窗口为3×3×512，步长为1，Padding为1。通过套用卷积通用公式，我们可以得到最后输出的特征图的高度和宽度均为14，即<img src="Image00163.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为512的特征图，所以需要进行512次卷积，最后得到输出的特征图的维度为14×14×512。</p>
<p>（9）Conv11层 ：为VGG16的第 11个卷积层，使用的卷积核滑动窗口为3×3×512，步长为1，Padding为1。通过套用卷积通用公式，我们可以得到最后输出的特征图的高度和宽度均为14，即<img src="Image00164.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为512的特征图，所以需要进行512次卷积，最后得到输出的特征图的维度为14×14×512。</p>
<p>（10）Conv12层<br>：为VGG16的第12个卷积层，使用的卷积核滑动窗口为3×3×512，步长为1，Padding为1。通过套用卷积通用公式，我们可以得到最后输出的特征图的高度和宽度均为14，即<img src="Image00165.jpg" alt="img"> ，同时这个卷积层要求最后输出深度为512的特征图，所以需要进行512次卷积，最后得到输出的特征图的维度为14×14×512。</p>
<p>（11）MaxPool5层<br>：为VGG16的第5个最大池化层。最大池化层的滑动窗口为2×2，步长为2。通过套用池化通用公式，我们可以得到最后输出的特征图的高度和宽度均为7，即<img src="Image00166.jpg" alt="img"> ，最后得到输出的特征图的维度为7×7×512。</p>
<p>（12）FC13层<br>：为VGG16的第1个全连接层。输入特征图的维度为7×7×512，和AlexNet模型一样，都需要对输入特征图进行扁平化处理以得到1×25088的数据，输出数据的维度要求是1×4096，所以需要一个维度为25088×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。</p>
<p>（13）FC14层<br>：为VGG16的第2个全连接层，输入数据的维度为1×4096，输出数据的维度要求是1×4096，所以需要一个维度为4096×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。</p>
<p>（14）FC15层<br>：为VGG16的第3个全连接层，输入数据的维度为1×4096，输出数据的维度要求是1×1000，所以需要一个维度为4096×1000的矩阵完成输入数据和输出数据的全连接，最后得到输入数据的维度为1×1000。</p>
<p>（15）OUTPUT层<br>：为输出层，VGG16同样用于解决图像的分类问题，我们将全连接层最后输出的维度为1×1000的数据传递到Softmax函数中，就能得到1000个预测值，这1000个预测值就是模型预测的输入图像所对应每个类别的可能性。</p>
<h2 id="4-5-GoogleNet"><a href="#4-5-GoogleNet" class="headerlink" title="4.5 GoogleNet"></a>4.5 GoogleNet</h2><p>在 2014年还有一个引起人们高度关注的模型出现，这个模型就是在 2014年举办的ILSVRC大赛中取得分类任务第1名的GoogleNet模型。与在2014年的分类任务中取得亚军的VGGNet模型相比较，GoogleNet模型的网络深度已经达到了22层，而且在网络架构中引入了Inception单元。这两个重要的改变证明，通过使用Inception单元构造的深层次卷积神经网络模型，能进一步提升模型整体的性能。先通过图4-10大致预览一下GoogleNet模型的22层网络架构。</p>
<p><img src="Image00167.jpg" alt="img"></p>
<p>图4-10</p>
<p>因为在GoogleNet模型中重复性比较大，所以我们就不进行逐层介绍了。下面重点看一下模型中的Inception单元结构，在此之前先来了解一下Naive Inception单元的结构，如图4-11所示。</p>
<p><img src="Image00168.jpg" alt="img"></p>
<p>图4-11</p>
<p>从图4-11中可以看出，前一层（Previous Layer）是Naive Inception单元的数据输入层，之后被分成了4个部分，这4个部分分别对应滑动窗口的高度和宽度为1×1的卷积层、3×3的卷积层、5×5的卷积层和3×3的最大池化层，然后将各层计算的结果汇聚至合并层（Filter Concatenation），在完成合并后将结果输出。</p>
<p>下面通过一个具体的实例来看看整个 Naive Inception单元的详细工作过程，假设在图4-11中Naive Inception单元的前一层输入的数据是一个32×32×256的特征图，该特征图先被复制成4份并分别被传至接下来的4个部分。我们假设这4个部分对应的滑动窗口的步长均为1，其中，1×1卷积层的Padding为0，滑动窗口维度为1×1×256，要求输出的特征图深度为128；3×3卷积层的Padding为1，滑动窗口维度为3×3×256，要求输出的特征图深度为192；5×5卷积层的Padding为2，滑动窗口维度为5×5×256，要求输出的特征图深度为96；3×3最大池化层的 Padding为1，滑动窗口维度为3×3×256。这里对每个卷积层要求输出的特征图深度没有特殊意义，之后通过计算，分别得到这4部分输出的特征图为32×32×128、32×32×192、32×32×96 和 32×32×256，最后在合并层进行合并，得到32×32×672的特征图，合并的方法是将各个部分输出的特征图相加，最后这个 Naive Inception单元输出的特征图维度就是32×32×672。</p>
<p>但是Naive Inception单元有两个非常严重的问题：首先，所有卷积层直接和前一层输入的数据对接，所以卷积层中的计算量会很大；其次，在这个单元中使用的最大池化层保留了输入数据的特征图的深度，所以在最后进行合并时，总的输出的特征图的深度只会增加，这样增加了该单元之后的网络结构的计算量。所以，人们为了解决这些问题，在Naive Inception单元的基础上对单元结构进行了改进，开发出了在 GoogleNet模型中使用的Inception单元。</p>
<p>下面看看GoogleNet模型中的Inception单元结构，如图4-12所示。</p>
<p><img src="Image00169.jpg" alt="img"></p>
<p>图4-12</p>
<p>在对 GoogleNet中的 Inception单元的内容进行详细介绍之前，我们先了解一下 NIN（Network in Network）中1×1卷积层的意义和作用。我们都知道卷积是用来做数据特征提取的，而且最常使用的卷积核滑动窗的高度和宽度一般是3×3或者5×5，那么在卷积核中使用高度和宽度为1×1的滑动窗又有什么作用呢？答案是能够完成特征图通道的聚合或发散。</p>
<p>举个例子来说，假设我们现在有一个维度为50×50×100的特征图，三个参数分别代表特征图的宽度、高度和深度，接下来将这个特征图输入1×1的卷积层中，定义该卷积层使用的卷积核的滑动窗口维度为1×1×100。如果我们想要输出一个深度为90的特征图，则在通过90次卷积操作之后，刚才的维度为50×50×100的特征图就变成了维度为50×50×90的特征图，这就是特征图通道的聚合。反过来，如果我们想要输出的特征图的深度是110，那么在通过 110次卷积操作之后，刚才的维度为50×50×100的特征图就变成了维度为50×50×110的特征图，这个过程实现了特征图通道的发散，通过1×1卷积层来控制特征图最后输出的深度，从而间接影响了与其相关联的层的卷积参数数量。比如将一个32×32×10的特征图输入 3×3的卷积层中，要求最后输出的特征图深度为20，那么在这个过程中需要用到的卷积参数为1800个，即1800=10×20× 3×3，如果将32×32×10的特征图先输入1×1的卷积层中，使其变成32×32×5的特征图，再将其输入3x3的卷积层中，那么在这个过程中需要用到的卷积参数减少至950个，即950=1×1×10×5+5×20×3×3。使用1×1的卷积层使卷积参数几乎减少了一半，极大提升了模型的性能。</p>
<p>GoogleNet中的 Inception单元与 Naive Inception单元的结构相比，就是在如图 4-12所示的相应位置增加了 1×1的卷积层。假设新增加的 1×1的卷积的输出深度为64，步长为1，Padding为0，其他卷积和池化的输出深度、步长都和之前在Naive Inception单元中定义的一样，前一层输入的数据仍然使用同之前一样的维度为32×32×256的特征图，通过计算，分别得到这 4 部分输出的特征图维度为32×32×128、32×32×192、32×32×96 和32×32×64，将其合并后得到维度为32×32×480的特征图，将这4部分输出的特征图进行相加，最后Inception单元输出的特征图维度是32×32×480。</p>
<p>在输出的结果中，32×32×128、32×32×192、32×32×96 和之前的 Naive Inception 单元是一样的，但其实这三部分因为1×1卷积层的加入，总的卷积参数数量已经大大低于之前的Naive Inception单元，而且因为在最大池化层之前也加入了1×1的卷积层，所以最终输出的特征图的深度也降低了。</p>
<p>GoogleNet的网络架构从上往下一共有22层，层次类型主要包括输入层、卷积层、最大池化层、平均池化层、全连接层、Inception单元和输出层。虽然GoogleNet在结构上和我们之前列举的几个模型有所差异，不过可以把整个GoogleNet模型看作由三大块组成，分别是模型的起始部分、Inception单元堆叠部分和模型最后的分类输出部分。</p>
<p>GoogleNet模型中的起始部分的结构如图4-13所示。</p>
<p><img src="Image00170.jpg" alt="img"></p>
<p>图4-13</p>
<p>在图4-13中，INPUT是整个GoogleNet模型最开始的数据输入层；Conv层对应在模型中使用的卷积层；MaxPooL层对应在模型中使用的最大池化层；Local Response Normalization是在模型中使用的局部响应归一化层。每个层后面的数字表示滑动窗口的高度和宽度及步长，比如第1个卷积层中的数字是7×7+1(S)，7×7就是滑动窗口的高度和宽度，1就是滑动窗口的步长。大写的 S是Stride的缩写，这个起始部分的输出结果作为Inception单元堆叠部分的输入。</p>
<p>在GoogleNet模型中最后分类输出部分的结构如图4-14所示。</p>
<p><img src="Image00171.jpg" alt="img"></p>
<p>图4-14</p>
<p>在图 4-14中，最后分类输出部分的输入数据来自 Inception单元堆叠部分最后一个Inception单元的合并输出，AveragePool层对应模型中的平均池化层（Average pooling），FC层对应模型中的全连接层，Softmax对应模型最后进行分类使用的Softmax激活函数。</p>
<p>总而言之，在GoogLeNet模型中使用Inception单元，使卷积神经网络模型的搭建实现了模块化，如果我们想要增加或者减少GoogLeNet模型的深度，则只需增添或者减少相应的 Inception单元就可以了，非常方便。另外，为了避免出现深层次模型中的梯度消失问题，在GoogLeNet模型结构中还增加了两个额外的辅助Softmax激活函数，用于向前传导梯度。</p>
<h2 id="4-6-ResNet"><a href="#4-6-ResNet" class="headerlink" title="4.6 ResNet"></a>4.6 ResNet</h2><p>ResNet是更深的卷积神经网络模型，在2015年的ILSVRC大赛中获得分类任务的第1名。在ResNet模型中引入了一种残差网络（Residual Network）结构，通过使用残差网络结构，深层次的卷积神经网络模型不仅避免了出现模型性能退化的问题，反而取得了更好的性能。下面是一个具有34层网络结构的ResNet模型，如图4-15所示。</p>
<p><img src="Image00172.jpg" alt="img"></p>
<p>图4-15</p>
<p>在图4-15中，虽然ResNet模型的深度达到了34层，但是我们发现其实在ResNet模型中大部分结构都是残差网络结构，所以同样具备了模块化的性质。之前讲到过，在搭建卷积神经网络模型时如果只是一味地对模型的深度进行机械式累加，则最后得到的模型会出现梯度消失、极易过拟合等模型性能退化的问题。在 ResNet模型中大量使用了一些相同的模块来搭建深度更深的网络，最后得到的模型在性能上却有不俗的表现，其中一个非常关键的因素就是模型累加的模块并非简单的单输入单输出的结构，而是一种设置了附加关系的新结构，这个附加关系就是恒等映射（Identity Mapping），这个新结构也是我们要重点介绍的残差网络结构。如图4-16所示是没有设置附加关系的单输入单输出模块。</p>
<p><img src="Image00173.jpg" alt="img"></p>
<p>图4-16</p>
<p>图 4-16显示了该模块的工作流程，输入数据 X 在通过两个卷积层后得到输出结果H(X)。在ResNet模型中设置附加的恒等映射关系的残差网络结构如图4-17所示。</p>
<p><img src="Image00174.jpg" alt="img"></p>
<p>图4-17</p>
<p>如图 4-17所示的结构和之前的单输入单输出结构相比并没有太大的变化，唯一的不同是残差模块的最终输出结果等于输入数据 X经过两个卷积之后的输出 F(X)加上输入数据的恒等映射。在残差模块中使用的这个附加的恒等映射关系能起到什么作用呢？事实证明，残差模块进行的这个简单的加法并不会给整个ResNet模型增加额外的参数和计算量，却能加快模型的训练速度，提升模型的训练效果；另外，在我们搭建的 ResNet模型的深度加深时，使用残差模块的网络结构不仅不会出现模型退化问题，性能反而有所提升。</p>
<p>这里需要注意附加的恒等映射关系的两种不同的使用情况，残差模块的输入数据若和输出结果的维度一致，则直接相加；若维度不一致，则先进行线性投影，在得到一致的维度后，再进行相加或者对维度不一致的部分使用0填充。</p>
<p>近几年，ResNet的研究者还提出了能够让网络结构更深的残差模块，如图4-18所示。</p>
<p><img src="Image00175.jpg" alt="img"></p>
<p>图4-18</p>
<p>其在之前的残差模块的基础上引入了NIN，使用1×1的卷积层来减少模型训练的参数量，同时减少整个模型的计算量，使得拓展更深的模型结构成为可能。于是出现了拥有50层、101层、152层的ResNet模型，这不仅没有出现模型性能退化的问题，而且错误率和计算复杂度都保持在很低的程度。</p>
<h1 id="第5章-Python基础"><a href="#第5章-Python基础" class="headerlink" title="第5章 Python基础"></a>第5章 Python基础</h1><p>对于想要从事深度学习相关工作的人而言，掌握一门编程语言是必不可少的；对于既没有编程经验又想要从事编程开发的人而言，把 Python作为第 1个编程语言再合适不过了，因为Python不仅简单易学，而且是一个集解释性、编译性、互动性和面向对象等优点于一身的高级脚本语言。Python的设计者曾说：“Life is short，You need Python”。</p>
<h2 id="5-1-Python简介"><a href="#5-1-Python简介" class="headerlink" title="5.1 Python简介"></a>5.1 Python简介</h2><p>Python之所以受到大家的关注，是因为它有如下特性。</p>
<p>（1）易读性 ：Python 的语法结构层次分明，语法逻辑简单易懂，按照 Python语法构造出的程序代码简单易读，就整体而言已经很贴合自然语言的使用习惯了。</p>
<p>（2）解释性 ：在 Python 中去除了编译环节和链接环节，这些修改提升了语言的解释性，提升了程序的开发效率，所以使用Python进行程序开发能够相对缩短开发周期。</p>
<p>（3）可移植性 ：因为Python 在设计之初就是一门面向开源的编程语言，而开源的一大特性就是兼容性，所以 Python 能够被众多平台移植和使用。</p>
<p>（4）可扩展性<br>：Python汲取了其他编程语言的精华，自然也能够使用其他程序语言实现自身的扩展，比如在我们想要提升关键代码的运行效率，或者想编写一些不愿开源的算法时，就可以使用其他编程语言如 C或者 C++来完成这部分工作，然后扩展到用 Python设计的程序中。</p>
<p>（5）交互性 ：Python提供了很多实时交互的软件，而且本身自带交互功能，我们可以使用 Python自带的交互提示，以互动的方式执行我们的程序，这样还能方便程序的调试和维护。</p>
<p>（6）面向对象 ：Python还是一门面向对象的编程语言，在 Python中提供了支持面向对象的程序设计和对象封装的编程技术。</p>
<p>（7）初学者的语言<br>：Python对于初学者而言，是一种易于理解和掌握的编程语言，有相对较少的关键字，而且结构简单，有明确定义的语法，更容易学习；Python对代码的定义更清晰，还有完备的开源社区和大量的开源库可供学习和使用，对主流的操作系统如UNIX、Windows、iOS等也有极强的兼容性。</p>
<h2 id="5-2-Jupyter-Notebook"><a href="#5-2-Jupyter-Notebook" class="headerlink" title="5.2 Jupyter Notebook"></a>5.2 Jupyter Notebook</h2><p>本书的后续代码实践部分都会基于 Jupyter Notebook。Jupyter Notebook 也被称作IPython Notebook，是一款基于Web的开源应用软件，不仅具备强大的文本和代码编辑功能，还具备很棒的交互式体验，这使我们在使用Jupyter Notebook进行数据分析和逻辑整理时事半功倍；而且Jupyter Notebook是一款兼容性很强的应用软件，具备了IPython的所有优点，除了支持运行Python，还支持运行其他40多种编程语言。正是这些特性，使Jupyter Notebook在机器学习、数据分析、数据挖掘等领域受到众多用户的青睐。</p>
<h3 id="5-2-1-Anaconda的安装与使用"><a href="#5-2-1-Anaconda的安装与使用" class="headerlink" title="5.2.1 Anaconda的安装与使用"></a>5.2.1 Anaconda的安装与使用</h3><p>在学习使用 Jupyter Notebook之前，我们有必要先了解一下Anaconda。Anaconda是一款基于Python的软件平台，集成了环境管理、Python包的安装、Python包的检索等非常实用的功能，并集成了大约1000多种可供用户调用和安装的Python包，同时兼容目前的主流操作系统，所以安装起来十分方便。</p>
<p>因为Anaconda 是免费的，所以我们可以轻松地完成该软件的下载和安装。登录Anaconda官网并进入软件的安装界面，就可以看到有多个操作系统的Anaconda版本可供选择安装，这里通过安装Windows系统的Anaconda版本进行演示，如图5-1所示。</p>
<p><img src="Image00176.jpg" alt="img"></p>
<p>图5-1</p>
<p>从图5-1中可以看到，Windows系统的Anaconda安装软件支持两个Python版本，这两个Python版本指的是我们安装完Anaconda软件后root环境默认使用的Python版本，而root环境是完成Anaconda的首次安装后软件默认生成的一个根环境。如果之后我们想使用不同的Python版本，则可以按自己的意愿在root环境的基础上新建自定义环境，然后进行其他版本的Python安装，所以安装Anaconda时选择的Python的版本在我们后期的使用中不一定会用到。在顺利完成安装后，就可以使用自动生成的root环境了，在这个root环境里包含了我们安装Anaconda时选择的Python和一些默认的Python包。</p>
<p>下面来看如何对环境进行操作，因为我们现在只有root环境，所以可以查看一下root环境的基本信息，在CMD命令行窗口中输入指令“conda info”并回车，就会显示该环境的基本信息，如图5-2所示。</p>
<p><img src="Image00177.jpg" alt="img"></p>
<p>图5-2</p>
<p>下面对在图5-2中显示的信息进行简单解读。</p>
<p>（1）platform:win-64 ：用于说明我们安装Anaconda软件时所使用的操作系统的信息，win-64指Windows的64位操作系统。</p>
<p>（2）conda version:4.3.14 ：用于说明我们安装的 Anaconda软件版本，“4.3.14”就是Anaconda的软件版本号。</p>
<p>（3）Python version:2.7.13.final.0<br>：用于说明在当前环境下安装和使用的Python版本，“2.7.13.final.0”就是Python的版本号。</p>
<p>（4）requests version:2.12.4 ：用于说明在当前环境下安装和使用的 requests 版本，“2.12.4”就是requests的版本号。</p>
<p>在实际使用时我们并不需要安装和使用一些 Python包，而是更愿意创建一个专属于自己的新环境，然后根据自己的项目需求在环境下安装相应的包，这样更有利于管理和维护环境。下面看看如何搭建一个自定义的新环境。</p>
<p>进入CMD命令行窗口并在窗口中输入命令“conda create –n test python=3.5”，然后回车，界面如图5-3所示。</p>
<p><img src="Image00178.jpg" alt="img"></p>
<p>图5-3</p>
<p>该命令的各部分含义如下。</p>
<p>（1）conda ：用于指定接下来使用的命令集来自Anaconda。</p>
<p>（2）create ：表示我们的具体操作是要创建一个新环境。</p>
<p>（3）-n ：用于指定新环境的名称，新环境的名称必须紧跟在空格之后。</p>
<p>（4）test ：用于指定新环境的名称为test。</p>
<p>（5）python=3.5 ：用于指定在新环境下需要预先安装的 Python版本，这里使用的Python版本为3.5。</p>
<p>在回车之后在CMD命令行窗口中返回的信息如图5-4所示。</p>
<p><img src="Image00179.jpg" alt="img"></p>
<p>图5-4</p>
<p>在图5-4中提供了一些在创建新环境后建议安装的默认包，包括我们指定安装的版本为Python 3.5；最后一行是一个是否选择安装的确认提示信息，如果输入“y”，则表示确认新环境的创建和包的安装，之后系统会自动对这些包进行下载和安装，无须过多干涉，整个过程非常方便。</p>
<h3 id="5-2-2-环境管理"><a href="#5-2-2-环境管理" class="headerlink" title="5.2.2 环境管理"></a>5.2.2 环境管理</h3><p>接下来简单介绍如何对创建的新环境进行基本维护和管理。在环境维护和管理中常用的命令如下。</p>
<p>（1）activate test ：在命令行窗口中输入“activate test”并回车，命令行窗口将返回如图5-5所示的界面。</p>
<p><img src="Image00180.jpg" alt="img"></p>
<p>图5-5</p>
<p>其实这个命令用于激活并进入我们指定名称的环境下。在回车后，可以看到在命令行的最前面出现了一个带有括号的环境名，这说明我们已经进入指定的环境下。</p>
<p>在开启一个命令行窗口后，所有操作都默认在 root环境下进行，我们必须通过“activate”加上新环境的名称来激活并进入我们创建的新环境下。</p>
<p>（2）deactivate test ：这个命令用于退出当前环境并进入 root环境下。在命令行窗口中输入“deactivate test”并回车，命令行窗口将返回如图5-6所示的界面。</p>
<p><img src="Image00181.jpg" alt="img"></p>
<p>图5-6</p>
<p>从图5-6中可以看到，命令行最前面带有括号的环境名消失了，这表明我们已经成功退出当前环境并进入root环境下。在退出当前环境时需要使用命令“deactivate”来执行，不过在命令行中输入“deactivate”加当前环境的名称和直接输入“deactivate”，都能达到同样的效果。</p>
<p>（3）conda remove-ntest-all ：在命令行窗口中输入“conda remove-ntest-all”并回车，将返回如图5-7所示的界面。</p>
<p><img src="Image00182.jpg" alt="img"></p>
<p>图5-7</p>
<p>这个命令用于删除指定名称的环境。若不想再使用已创建的环境了，则可以将它完全删除，不过这个命令只能在root环境下执行。“remove”是Anaconda的环境删除命令；“-n”表示环境名选项，在空格后紧跟环境的名字；“test”表示需要删除的环境的名称；“-all”表示删除指定环境下所有已经安装的包。在回车后，命令行窗口会返回确认提示信息，在输入“y”后回车，就会删除指定环境及该环境下的所有包。</p>
<h3 id="5-2-3-环境包管理"><a href="#5-2-3-环境包管理" class="headerlink" title="5.2.3 环境包管理"></a>5.2.3 环境包管理</h3><p>对环境下的包进行维护和管理时的常用命令如下。</p>
<p>（1）conda search numpy ：在命令行窗口中输入“conda search numpy”并回车，命令行窗口将返回如图5-8所示的界面。</p>
<p><img src="Image00183.jpg" alt="img"></p>
<p>图5-8</p>
<p>这个命令用于搜索平台中指定名称的Python包。不过在搜索之前我们必须知道Python包的大致名称，如果使用了错误的名称进行搜索，则很有可能找不到或者找到的并不是我们想要的 Python包。在图 5-8中搜索的名称是numpy，numpy是一个非常通用的Python包，所以在名称上并不会有太大的出入，通过命令进行搜索后在返回的信息中会看到很多搜索结果。“search”就是Anaconda中用于搜索指定包的命令，在空格后面紧跟我们需要查找的包的名称，这里的numpy就是我们想要查找的包的名称。</p>
<p>（2）conda install numpy ：在命令行窗口中输入“conda install numpy”并回车，命令行窗口将返回如图5-9所示的界面。</p>
<p><img src="Image00184.jpg" alt="img"></p>
<p>图5-9</p>
<p>这个命令用于将指定名称的包安装到当前环境下，前提条件是我们已经通过包搜索命令在Anaconda软件平台中找到了其对应的名称。“install”是Anaconda中用于执行包安装的命令，在空格后面紧跟需要安装的包的名称，这里numpy就是我们想要安装的包的名称。</p>
<p>使用 Anaconda执行包的安装还有一个好处，就是我们在执行某个包的安装命令后，命令行窗口会返回一些建议安装的关联包，我们只要对最后返回的确认提示信息进行确认，就能让所有这些关联包被自动下载和安装，避免了我们在安装指定包的过程中因遗漏关联包而导致程序运行出错，或者因缺少相应的关联包而导致程序不能正常使用。</p>
<p>（3）conda install anaconda ：在命令行窗口中输入“conda install anaconda”并回车，命令行窗口将返回如图5-10所示的界面。</p>
<p><img src="Image00185.jpg" alt="img"></p>
<p>图5-10</p>
<p>这个命令用于在当前环境下安装所有 Anaconda软件平台已经集成的包，当我们不知道自己需要安装什么或者需求不太明确时，就可以通过该方法对全部的包进行安装，尤其在不考虑安装效率且只想确保包安装的完备性时。与对单个包进行安装的区别是，“anaconda”并不是某个包的名称。</p>
<p>（4）conda list ：在命令行窗口中输入“conda list”并回车，命令行窗口将返回如图5-11所示的界面。</p>
<p><img src="Image00186.jpg" alt="img"></p>
<p>图5-11</p>
<p>这个命令用于查看当前环境下已经安装的包，这些信息在我们对环境下的包进行管理和维护时非常有帮助。从图5-11可以看到罗列的部分已安装的包信息，这些信息主要包含了所安装包的版本号和包所基于的Python版本。通过这些信息，我们可以移除一些不再使用的包，下载、安装一些之前没有安装的包。“list”就是在Anaconda中执行查看的命令。</p>
<h3 id="5-2-4-Jupyter-Notebook的安装"><a href="#5-2-4-Jupyter-Notebook的安装" class="headerlink" title="5.2.4 Jupyter Notebook的安装"></a>5.2.4 Jupyter Notebook的安装</h3><p>下面讲解Jupyter Notebook的安装过程。首先进入我们创建的新环境下，在CMD命令行窗口中输入“conda install jupyter”并回车，命令行窗口会返回如图5-12所示的界面。</p>
<p><img src="Image00187.jpg" alt="img"></p>
<p>图5-12</p>
<p>使用Anaconda中的“install”命令进行软件或包的安装，在安装过程中程序中会自动罗列所有需要安装的关联包，在Jupyter Notebook的安装过程中也不例外，所以对于命令行窗口中返回的确认提示信息，我们只需输入“y”进行确认，就完成 Jupyter Notebook的安装了。</p>
<p>在安装完成后，要想验证Jupyter Notebook能否正常使用，则只需在命令行窗口中输入“jupyter notebook”并回车，这个命令用于开启Jupyter Notebook，命令行窗口随后会返回如图5-13所示的界面。</p>
<p><img src="Image00188.jpg" alt="img"></p>
<p>图5-13</p>
<p>命令行窗口返回的信息大部分是与Web交互的信息，要想确认我们的Jupyter Notebook能否正常使用，最简单的方法就是看此时操作系统是否使用默认的浏览器打开了一个Web应用程序。</p>
<h3 id="5-2-5-Jupyter-Notebook的使用"><a href="#5-2-5-Jupyter-Notebook的使用" class="headerlink" title="5.2.5 Jupyter Notebook的使用"></a>5.2.5 Jupyter Notebook的使用</h3><p>在成功运行Jupyter Notebook的Web应用程序后，我们首先在浏览器中看到的是该应用程序的主界面，如图5-14所示。在Jupyter Notebook中保存或者创建的文件默认使用的后缀名是“ipynb”，在本书中我们把带有该后缀名的文件称作Notebook文件。</p>
<p><img src="Image00189.jpg" alt="img"></p>
<p>图5-14</p>
<p>在Jupyter Notebook的Web应用程序主界面中包含以下操作元素。</p>
<p>（1）Files ：用于显示当前主目录下所有文件的信息，还可以通过不同的筛选方式找到主目录下相应的文件并进行选择和操作。在单击下三角符号后会显示如图 5-15所示的界面，其中包含所有可供选择的筛选选项。其中，Folders用于选取当前目录下的所有子文件；All Notebooks用于选取当前目录下的所有Notebook文件；Running用于选取当前目录下所有正在运行的Notebook文件；Files用于选取当前目录下所有文件夹类型的文件。</p>
<p><img src="Image00190.jpg" alt="img"></p>
<p>图5-15</p>
<p>在通过筛选方式选出相应的文件后，相关的操作信息也会被显示出来，而且不同类别的文件对应不同的操作方法。以All Notebooks选项为例，显示的操作信息如图5-16所示，可以看到，操作选项包括Duplicate、Rename、Move、Download、View、Edit和最后的删除图标。</p>
<p><img src="Image00191.jpg" alt="img"></p>
<p>图5-16</p>
<p>（2）Running ：用于显示正在运行的 Notebook文件，如图 5-17所示，Untitled.ipynb就是当前正在运行的Notebook文件。在该图的右下角还有一个shutdown的操作选项，这个选项用于强制关闭正在运行的Notebook文件。</p>
<p><img src="Image00192.jpg" alt="img"></p>
<p>图5-17</p>
<p>（3）New<br>：用于创建新的文件，如图5-18所示。可以看到，新建的文件主要有两个类别可供选择，分别是Notebook文件和Other文件。如果我们选择的是Notebook文件中的Python 3，那么系统会新建一个支持Python 3.5版本的Notebook文件，这个文件具体支持哪个 Python版本，是根据我们在启动 Jupyter Notebook应用程序时所在的环境下安装的Python版本而定的。如果我们选择的是Other文件中的Text File或Folder，那么系统会新建一个空的文本文件或文件夹。</p>
<p><img src="Image00193.jpg" alt="img"></p>
<p>图5-18</p>
<p>下面重点看看对新建的Notebook文件的相关操作，Notebook文件的主界面如图5-19所示，图中以In [ ]开始的是Notebook文件的一个输入单元。在该主界面的第1行显示的是这个Notebook文件的名称，这里的Untitled就是我们当前打开的Notebook文件的名称，它是一个默认的文件名，所有被新建且没被重命名的文件都是这个名称，若我们重复新建了Notebook文件但没有进行重命名，那么新的Notebook文件的名称还是Untitled，只不过会在名称的最后加上一个数字以做区分。</p>
<p><img src="Image00194.jpg" alt="img"></p>
<p>图5-19</p>
<p>对当前的默认文件进行重命名的方法有很多，最快捷的方式就是直接双击Untitled这个名称，之后会弹出如图5-20所示的进行重命名操作的窗口，在输入框中输入我们想要更改的名字，然后单击Rename，重命名的操作就完成了。</p>
<p>Notebook主界面的第2行显示的是一列菜单工具栏，在这一列菜单工具栏中我们最常用的是File和Kernel这两个菜单选项，单击File后会显示如图5-21所示的界面，下面对其中常用的子菜单选项进行介绍。</p>
<p><img src="Image00195.jpg" alt="img"></p>
<p>图5-20</p>
<p>（1）New Notebook ：用于创建新的 Notebook 文件，在创建完成后会直接进入新的Notebook文件的主界面。</p>
<p>（2）Open ：用于选取我们需要打开的文件，如果文件不在当前默认的目录下，则需要通过目录切换找到文件并载入。</p>
<p>（3）Rename ：重命名当前打开的Notebook文件。</p>
<p>（4）Download as ：将当前打开的Notebook文件保存为指定的文件格式，可选择后缀名为ipynb的 Notebook文件，也可选择后缀名为html的 HTML文件和后缀名为md的Markdown文件。</p>
<p><img src="Image00196.jpg" alt="img"></p>
<p>图5-21</p>
<p>在单击 Kernel菜单后会显示如图 5-22所示的界面，其实我们可以将 Kernel看作Notebook文件的核心进程。在Kernel菜单下常用的子菜单选项如下。</p>
<p>（1）Restart ：重启当前Notebook文件的Kernel。</p>
<p>（2）Restart &amp; Clear Output ：重启当前Notebook文件的Kernel，并清空因运行代码而出现在Notebook文件中的内容。</p>
<p>（3）Restart &amp; Run All ：重启当前Notebook文件的Kernel，在重启完成后重新运行Notebook文件的输入单元中的全部代码。</p>
<p>（4）Shutdown ：强制关闭当前Notebook文件的Kernel，在关闭后如需重新使用，则要手动激活当前文件的Kernel。</p>
<p><img src="Image00197.jpg" alt="img"></p>
<p>图5-22</p>
<p>Notebook文件主界面的第3行是一系列快捷操作图标，如图5-23所示。</p>
<p><img src="Image00198.jpg" alt="img"></p>
<p>图5-23</p>
<p>在图5-23中从左至右代表的快捷操作分别是保存当前Notebook文件、增加Notebook文件中新的输入单元、剪切被选中的输入单元、复制被选中的输入单元、粘贴被选中的输入单元、上移被选中的输入单元、下移被选中的输入单元、运行被选中的输入单元、暂停和刷新当前 Notebook文件的 Kernel，最后一个选项指定被选中的输入单元的内容编辑模式。而我们最常用的是前面两种内容编辑模式：Code模式和Markdown模式。</p>
<p>（1）Code模式<br>：在Code模式下，我们在输入单元中输入的内容必须是代码才能运行，如图5-24所示。我们在运行输入单元中的内容后会马上得到Notebook文件反馈的信息，在我们写的 Python代码中使用了打印函数，所以在输入单元被运行后，“Hello World！”被打印输出在Notebook文件中。同时，我们可以看到输入单元最前面的内容由In [ ]变成了In [1]，这表明这个输入单元目前已经被运行了一次，如果同一个输入单元被运行了多次，那么括号中的数字还会累加。</p>
<p><img src="Image00199.jpg" alt="img"></p>
<p>图5-24</p>
<p>（2）Markdown模式<br>：在Markdown模式下，Notebook文件的输入单元变成了一种文本编辑器，我们可以通过不同的设置让输入单元中的内容被运行后显示不同的字体、大小、格式等。如图5-25所示就是一个使用了Markdown模式的输入单元中的内容。</p>
<p><img src="Image00200.jpg" alt="img"></p>
<p>图5-25</p>
<p>在输入单元中的每行文本内容前有数量不等的井号（#），在运行后会得到如图 5-26所示的输出信息。</p>
<p><img src="Image00201.jpg" alt="img"></p>
<p>图5-26</p>
<p>我们通过该输出信息可以知道，其实在输入单元的文本前使用井号是为了实现类似标题的效果，通过使用数量不等的井号可以定义不同级别的标题。还有个细节需要注意：在井号后面必须紧跟一个空格，然后才是文本的内容。</p>
<p>井号和标题级别的对应关系为：一个井号对应一级标题；两个井号对应二级标题；三个井号对应三级标题，以此类推。</p>
<p>另外，在Markdown模式下在输入单元格中还能使用LaTex语法编辑公式，如图5-27所示，在运行该输入单元中的 LaTex语法内容后，输出的信息是一个标准的公式：f (x )=3x +7。当然，我们也可以构造更复杂的公式，不过这些属于使用 LaTex语法构造相关公式的范畴，这里不再展开讲解。</p>
<p><img src="Image00202.jpg" alt="img"></p>
<p>图5-27</p>
<h3 id="5-2-6-Jupyter-Notebook常用的快捷键"><a href="#5-2-6-Jupyter-Notebook常用的快捷键" class="headerlink" title="5.2.6 Jupyter Notebook常用的快捷键"></a>5.2.6 Jupyter Notebook常用的快捷键</h3><p>我们对于创建的Notebook文件有两种操作模式，分别是命令模式（Command Mode）和编辑模式（Edit Mode）。在命令模式下进行的操作是针对整个Notebook文件而言的，而在编辑模式下进行的操作是针对输入单元中的内容而言的。</p>
<p>一种简单判断当前所处模式的方法是观察输入单元最左边的条纹颜色：如果为蓝色，则说明当前处于命令模式下；如果为绿色，则说明当前处于编辑模式下。</p>
<p>当处于命令模式时，我们常用的快捷键如下。</p>
<p>（1）Enter ：进入编辑模式。</p>
<p>（2）Esc ：退出编辑模式，进入命令模式。</p>
<p>（3）Shift-Enter ：运行当前选中的输入单元中的内容，并选中下一个输入单元。</p>
<p>（4）Ctrl-Enter ：仅运行当前选中的输入单元中的内容。</p>
<p>（5）Alt-Enter ：运行当前选中的输入单元中的内容，并在选中的输入单元之后插入新的输入单元。</p>
<p>（6）1 ：将输入单元的内容编辑模式设置为Markdown模式，并在输入单元中的内容的开始处添加一级标题对应的井号个数和一个空格字符。</p>
<p>（7）2 ：将输入单元的内容编辑模式设置为Markdown模式，并在输入单元中的内容的开始处添加二级标题对应的井号个数和一个空格字符。</p>
<p>（8）3 ：将输入单元的内容编辑模式设置为Markdown模式，并在输入单元中的内容的开始处添加三级标题对应的井号个数和一个空格字符。</p>
<p>（9）4 ：将输入单元的内容编辑模式设置为Markdown模式，并在输入单元中的内容的开始处添加四级标题对应的井号个数和一个空格字符。</p>
<p>（10）5 ：将输入单元的内容编辑模式设置为Markdown模式，并在输入单元中的内容的开始处添加五级标题对应的井号个数和一个空格字符。</p>
<p>（11）6 ：将输入单元的内容编辑模式设置为Markdown模式，并在输入单元中的内容的开始处添加六级标题对应的井号个数和一个空格字符。</p>
<p>（12）Y ：将输入单元的内容编辑模式设置为Code模式。</p>
<p>（13）M ：将输入单元的内容编辑模式设置为Markdown模式。</p>
<p>（14）A ：在当前的输入单元的上方插入新的输入单元。</p>
<p>（15）B ：在当前的输入单元的下方插入新的输入单元。</p>
<p>（16）D ：删除当前选中的输入单元。</p>
<p>（17）X ：剪切当前选中的输入单元。</p>
<p>（18）C ：复制当前选中的输入单元。</p>
<p>（19）Shift-V ：将复制或者剪切的输入单元粘贴到选定的输入单元上方。</p>
<p>（20）V ：将复制或者剪切的输入单元粘贴到选定的输入单元下方。</p>
<p>（21）Z ：恢复删除的最后一个输入单元。</p>
<p>（22）S ：保存当前正在编辑的Notebook文件。</p>
<p>（23）L ：在Notebook的所有输入单元前显示行号。</p>
<p>当处于编辑模式时，我们常用的快捷键如下。</p>
<p>（1）Enter ：进入编辑模式。</p>
<p>（2）Esc ：退出编辑模式，进入命令模式。</p>
<p>（3）Tab ：如果输入单元的内容编辑模式为Code模式，则可以通过该快捷键对不完整的代码进行补全或缩进。</p>
<p>（4）Shift-Tab ：如果输入单元的内容编辑模式为Code模式，则可以通过该快捷键显示被选取的代码的相关提示信息。</p>
<p>（5）Shift-Enter ：运行当前选中的输入单元中的内容并选中下一个输入单元，在运行后会退出编辑模式并进入命令模式。</p>
<p>（6）Ctrl-Enter ：仅运行当前选中的输入单元中的内容，在运行后会退出编辑模式并进入命令模式。</p>
<p>（7）Alt-Enter ：运行当前选中的输入单元中的内容，并在选中的输入单元之后插入新的输入单元，在运行后会退出编辑模式并进入命令模式。</p>
<p>（8）PageUp ：将光标上移到输入单元的内容前面。</p>
<p>（9）PageDown ：将光标下移到输入单元的内容后面。</p>
<h2 id="5-3-Python入门"><a href="#5-3-Python入门" class="headerlink" title="5.3 Python入门"></a>5.3 Python入门</h2><p>学习编程语言最快的方式就是实践，本节会提供一些实例供大家学习和参考，所有实例都在Notebook文件中运行，并且实例代码都基于Python 3.5。</p>
<p>下面，让我们开始编写真正意义上的第1行Python代码。首先打开Jupyter Notebook的Web应用程序，创建一个新的Notebook文件，在该Notebook文件的输入单元中输入如下代码：</p>
<p>print(“Hello,World.”)</p>
<p>该代码实现的功能是打印输出一段“Hello,World.”的字符串，运行输入单元中的内容，得到如图5-28所示的输出结果。</p>
<p><img src="Image00203.jpg" alt="img"></p>
<p>图5-28</p>
<p>通过上面的简单实例可以看出，Python语言非常简洁、高效，不用定义很多规则，仅仅通过一串简单的脚本代码就实现了程序的打印输出功能。</p>
<p>接下来让我们深入学习Python编程语言，学习Python中常用的函数和语法。</p>
<h3 id="5-3-1-Python的基本语法"><a href="#5-3-1-Python的基本语法" class="headerlink" title="5.3.1 Python的基本语法"></a>5.3.1 Python的基本语法</h3><p>这里首先介绍Python中的打印输出、代码缩进、语句分割等基本语法。</p>
<p>1.打印输出</p>
<p>在Python中使用print语句来完成代码的打印输出，而且默认对打印输出的结果进行自动换行，所以如果我们写了多个print语句，则会发现每个print语句打印输出的内容自成一行。如果我们想在一行中对多个独立的内容进行打印输出，则需要在代码中使用逗号将这些独立的内容分开；如果我们想对某个内容自定义重复打印输出的次数，则可以在代码中通过乘以一个数字来实现。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00204.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00205.jpg" alt="img"></p>
<p>2.代码中的注释</p>
<p>在Python中可以通过在开头添加井号（#）来标注我们想要注释的内容。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00206.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>Hello,World</p>
<p>注意，一个井号只能完成对一行内容进行注释。如果需要对多行内容进行注释，则可以使用三个单引号（’’’）或三个双引号（”””），使用的位置是被注释内容的第1行和最后一行。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00207.jpg" alt="img"></p>
<p><img src="Image00208.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>Hello,World</p>
<p>3.代码的缩进 在某些编程语言中，代码的缩进对代码本身并没有特别的影响，但是在Python中代 码的缩进有着特别的意义。Python 通过对每行代码使用不同的缩进来控制语法的判断逻辑，让程序知道有相同缩进的代码构成的是同一个逻辑代码块，而大部分编程语言使用大括号（{}）来帮助程序识别代码的逻辑代码块，从而完成逻辑的判断。</p>
<p>在 Python 中代码的缩进使用的是空格，所以必须严格控制空格的使用数量，对同一层次的逻辑代码块必须使用相同的缩进，如果缩进使用的空格数量不同，则会导致程序的逻辑出现错误。下面来看一个实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00209.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00210.jpg" alt="img"></p>
<p>在代码中，因为第2个打印输出的语句使用了不合法的缩进，即多了一个空格，所以程序在运行时发生错误。</p>
<p>我们建议在写 Python代码时按如下原则来控制缩进字符的使用数量：在第 1 个逻辑代码块中不使用缩进，而在第2个逻辑代码块中使用一个空格来缩进，在第3个逻辑代码块中使用两个空格来缩进，以此类推，通过逐层增加缩进字符的使用数量来表示不同的逻辑代码块。</p>
<p>缩进相同的一组语句可以构成同一个逻辑代码块，不过对于复合语句而言会有一些小小的差异。复合语句有个很大的特点，就是在第 1 行语句结束后会使用冒号（：）作为结尾，在冒号之后新起的一行或者多行代码会被默认为归属于复合语句下的一个逻辑代码块，所以在这个逻辑代码块中使用的缩进字符的数量必须保持一致，循环语句、判断语句等常用的复合语句都要遵守这个约定。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00211.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00212.jpg" alt="img"></p>
<p>如上所示是一个循环复合语句，在循环语句冒号之后的就是该循环的具体操作内容，这些内容同属于一个逻辑代码块，所以两个 print语句必须具有相同的缩进，这样的代码才是合法的。</p>
<p>4.多行语句的分割</p>
<p>有时我们将某行代码写得过长，导致代码既不美观，又不易读，所以我们需要对这种冗长的代码进行分割。</p>
<p>不同的编程语言对代码分割使用的语法也不一样，在Python中可以使用斜杠（\）来将原本完整的一行代码分割成多行，虽然代码被分割成了多行，但是这些代码仍然是一个完整的整体，所以在 Python中使用的分割方法比较简单且容易操作。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p>print(“Hello,World.Hello,World.\</p>
<p>Hello,World.Hello,World.”)</p>
<p>在运行后，输出的内容如下：</p>
<p>Hello,World.Hello,World.Hello,World.Hello,World.</p>
<h3 id="5-3-2-Python变量"><a href="#5-3-2-Python变量" class="headerlink" title="5.3.2 Python变量"></a>5.3.2 Python变量</h3><p>在编程语言中，变量主要用于存储目标数据，Python 中的变量也不例外。在 Python中，我们通过定义变量来存储不同类型的数据，所以对变量的命名也是非常关键的，最好使用与需要存储的数据相对应的名字。下面来看一些常用的变量操作和运算。</p>
<p>我们可以将对变量赋值的过程理解为往盒子里装东西的过程，定义的变量是一个可以存储数据的盒子，一开始这个盒子没有任何参数和属性，只有一个名字，当我们把不同类型的数据装入这个盒子中后，这个盒子就有了和被装入的数据相同的参数和属性，比如装入的是一个参数为10的整型数据，这个盒子的参数就变成了10，数据类型就是整型数据。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00213.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00214.jpg" alt="img"></p>
<p>在代码中使用了三种不同类型的数据对变量进行赋值，int_num、float_num、string是我们定义的三个初始变量，int_num、float_num、string也是这三个变量的名字，如需使用这些变量，则可以通过直接调用这些变量名进行相关操作，所以变量名是不能重复的；等号右边的是准备赋值给变量的数据，第1个是参数为10的整型数据，第2个是参数为10.00的浮点型数据，第3个是参数为“Hello，World”的字符串型数据，在赋值完成后变量就获得了与右边数据相同的参数和属性，所以最后打印输出的结果就是相应类型的数据。</p>
<p>以上只是变量的点对点的赋值操作过程，在实际应用中还可以有点对多点和多点对多点的变量赋值操作。点对多点的变量赋值操作其实就是将一个数据在一次赋值操作中同时传递给已经定义好的多个变量。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00215.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00216.jpg" alt="img"></p>
<p>另外，多点对多点其实就是在一条语句中将多个数据赋值给多个变量，不过赋值的数据个数和定义的变量个数要相等，并且位置要一一对应，在改变位置后会得到截然不同的结果。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00217.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00218.jpg" alt="img"></p>
<p>如果我们不小心打乱了赋值操作的顺序，将最后的两个字符串对调了位置，那么最后打印输出的结果会发生变化。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00219.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00220.jpg" alt="img"></p>
<p>如果正确的打印输出顺序是“Hello”、“World”和“Hello,World”，那么在赋值顺序被颠倒后得到了完全错误的结果。</p>
<h3 id="5-3-3-常用的数据类型"><a href="#5-3-3-常用的数据类型" class="headerlink" title="5.3.3 常用的数据类型"></a>5.3.3 常用的数据类型</h3><p>在 Python中定义了很多通用的数据类型，常用的数据类型有数字、字符串、列表、元组和字典。</p>
<p>1.数字</p>
<p>常用的数字数据类型有整型（int）和浮点型（float）。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00221.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00222.jpg" alt="img"></p>
<p>在以上代码中分别对变量int_num和float_num赋值整型数据和浮点型数据，然后进行打印输出。我们看到整型数据和浮点型数据的最大区别是浮点型数据带有精度而整型数据没有。在打印输出时浮点型数据默认保留到小数点后一位，和我们赋值时使用的精度不同，其实，我们可以自定义浮点数打印输出的精度。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00223.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00224.jpg" alt="img"></p>
<p>如上所示，一开始被赋值给float_num变量的浮点数精度保留到了小数点后3位，第1个打印输出使用了默认的输出精度；第 2个打印输出使用了“%f”来定义输出的浮点数精度，让输出的结果保留到小数点后6位；第3个打印输出将之前的“%f”替换成了“%.2f”，“%.2f”中的“.2”指将输出的结果保留到小数点后2位，如果数字写在点号（.）之前，则表示保留到小数点前的位数，如果写在点号之后，则表示保留到小数点后的位数；第4个打印输出使用了“%.4f”来定义输出的浮点数精度，让输出的结果保留到小数点后4位。</p>
<p>2.字符串</p>
<p>字符串数据类型是由字母、数字和下画线等特殊符号组成的一连串字符表示，使用单引号（’）或者双引号（”）来标识赋值给变量的数据。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00225.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00226.jpg" alt="img"></p>
<p>代码中的string1和string2变量均包含了14个字符。对于这类长字符串变量，我们可以使用索引值对变量中的字符进行操作，通过使用变量的索引值，我们不仅可以随意提取变量中的字符，还可以通过相应的操作生成新的字符串。</p>
<p>下面看看在字符串变量中对索引值的定义。以 string1字符串变量为例，如果从左往右看，变量最左边的字符“H”的索引值是0，并且索引值以 1为步长向右依次递增，那么变量最右边的字符“1”的索引值是变量的总长度减去 1，即 13。如果从右往左看，变量最右边的字符“1”的索引值是-1，并且索引值以 1为步长向左依次递减，那么变量最左边的字符“H”的索引值是-14。根据这个索引值的定义规则，我们可以得到 string1和string2的索引值从左到右的范围是0～13，从右到左的范围是-1～-14。</p>
<p>下面介绍具体的索引值操作方法，如果想选取特定位置的字符，则只需在变量后加上[索引值]；如果想选取特定位置的字符串，则需要在变量后加上[前索引值:后索引值]，其中后索引值对应的字符不会被提取，结束字符是后索引值的前一个字符，若前索引值或后索引值为空，则表示取前索引以前的全部字符或后索引以后的全部字符。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00227.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>H</p>
<p><img src="Image00228.jpg" alt="img"></p>
<p>3.列表</p>
<p>列表是一种容器型数据类型，容器型数据类型的最大特点就是可以实现多种数据类型的嵌套，所以我们可以在列表中将数字、字符串等类型的数据嵌套到列表中，甚至能够在列表中嵌套列表。我们之前把变量比作一个盒子，那么可以将具备容器特性的变量比作一个更大的盒子，在这个大盒子里还装了许多不同的小盒子，这些不同的小盒子就是不同数据类型的变量。列表用方括号（[]）进行标识，列表的索引值的使用规则和字符串一样，这里不再赘述。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00229.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00230.jpg" alt="img"></p>
<p>正因为列表具备了容器的特性，所以我们还可以对列表中的元素进行重新赋值。重新赋值相当于对旧数据进行了一次覆盖操作，可将其理解为使用新的小盒子替换了大盒子里面的旧的小盒子。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00231.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00232.jpg" alt="img"></p>
<p>4.元组</p>
<p>元组是另一种容器型数据类型，用圆括号（()）进行标识，它的基本性质、索引值操作和列表是一样的，其最大的区别就是元组内的元素不能重新赋值，如果定义好了一个元组，那么它内部的元素就固定了，所以元组也被称作只读型列表。这个只读的特性也非常有用，可以应用于不需要重新赋值的场景下。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00233.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00234.jpg" alt="img"></p>
<p>5.字典</p>
<p>字典虽然也是一种容器型数据类型，但是相较于列表和元组，具有更灵活的操作和复杂的性质，相应地，对字典数据类型的操作也更有难度。其中一个区别就是列表和元组是有序的元素集合，字典却是一组无序的元素集合，虽然是无序的，但是为了达到对字典内元素的可操控性，在字典的每个元素中都会加入相应的键值。若我们需要对字典中元素的值进行赋值或者重新赋值等，则只能通过元素对应的键值来进行，而不能使用在列表和元组中操作索引值的方法。字典用大括号（{}）进行标识。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00235.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00236.jpg" alt="img"></p>
<h3 id="5-3-4-Python运算"><a href="#5-3-4-Python运算" class="headerlink" title="5.3.4 Python运算"></a>5.3.4 Python运算</h3><p>代码的运算是程序设计中很重要的内容，通过各种运算，我们能够实现更复杂的代码功能。这里介绍在Python中常用的运算操作和运算符，先来看算术运算、逻辑运算在Python中是如何实现的。</p>
<p>1.算术运算符</p>
<p>算术运算符是我们对变量进行算术运算时用到的符号，不同的符号代表使用了不同的算术运算方式，常用的算术运算符如下。</p>
<p>（1）加法算术运算符 ：使用符号“+”表示，对符号前后的变量进行相加运算。</p>
<p>（2）减法运算符 ：使用符号“-”表示，对符号前后的变量进行相减运算。</p>
<p>（3）乘法运算符 ：使用符号“*”表示，对符号前后的变量进行相乘运算。</p>
<p>（4）除法运算符 ：使用符号“/”表示，对符号前后的变量进行相除运算。</p>
<p>（5）取模运算符 ：使用符号“%”表示，符号前的变量以符号后的变量为模进行取模运算。</p>
<p>（6）求幂运算符 ：使用符号“**”表示，符号前的变量以符号后的变量为幂进行求幂运算。</p>
<p>（7）取整运算符 ：使用符号“//”表示，符号前的变量以符号后的变量为底进行取整运算。</p>
<p>在明白了 Python中各种算术运算符在代码中的具体表示方法和功能后，下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00237.jpg" alt="img"></p>
<p>print(“a ** b =”, c)</p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00238.jpg" alt="img"></p>
<p>如果不需要对算术运算后的结果进行存储，则还可以对算术运算代码进行简化。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00239.jpg" alt="img"></p>
<p><img src="Image00240.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00241.jpg" alt="img"></p>
<p>2.比较运算符</p>
<p>比较运算符主要用于对比较运算符前后的变量进行比较，然后返回一个布尔值。布尔值也是一种数据类型，不过布尔型数据的值只有两个，分别是真值（True）和假值（False）。下面以运算符号前后均为变量为例，来看看比较运算符具体是如何工作的。</p>
<p>（1）相等比较运算符 ：使用符号“==”表示，比较运算符前后的变量的值，如果两个变量的值相等，那么返回True，否则返回False。</p>
<p>（2）不等比较运算符 ：使用符号“！=”表示，比较运算符前后的变量的值，如果两个变量的值不相等，那么返回True，否则返回False。</p>
<p>（3）大于比较运算符 ：使用符号“&gt;”表示，比较运算符前后的变量的值，如果前一个变量的值大于后一个变量的值，那么返回True，否则返回False。</p>
<p>（4）小于比较运算符 ：使用符号“&lt;”表示，比较运算符前后的变量的值，如果前一个变量的值小于后一个变量的值，那么返回True，否则返回False。</p>
<p>（5）大于等于比较运算符 ：使用符号“&gt;=”表示，比较运算符前后的变量的值，如果前一个变量的值大于等于后一个变量的值，那么返回True，否则返回False。</p>
<p>（6）小于等于比较运算符 ：使用符号“&lt;=”表示，比较运算符前后的变量的值，如果前一个变量的值小于等于后一个变量的值，那么返回True，否则返回False。</p>
<p>在实际应用中比较运算还分为单层比较和多层比较，单层比较指参与比较运算的变量只是前后两个变量，多层比较指参与比较运算的变量可以有3个及以上。下面来看一个单层比较的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00242.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00243.jpg" alt="img"></p>
<p>接着来看一个多层比较的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00244.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00245.jpg" alt="img"></p>
<p>可以看到多层比较需要考虑的情况要比单层比较更复杂。</p>
<p>3.布尔运算符</p>
<p>我们最常用的布尔运算符是与、或、非这三个，并且这三个布尔运算符在进行运算后返回的运算结果值也是布尔型的。接下来我们以运算符号前后均为变量为例，来看看布尔运算符具体是如何工作的。</p>
<p>（1）与布尔运算符 ：使用字母“and”来表示，在字母前后参与运算的变量均为True时返回True，否则返回False。</p>
<p>（2）或布尔运算符 ：使用字母“or”来表示，在字母前后参与运算的变量均为False时返回False，否则返回True。</p>
<p>（3）非布尔运算符 ：使用字母“not”来表示，在字母后参与运算的变量为True时返回False，为假时返回True。</p>
<p>首先来看一个简单的布尔运算的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00246.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00247.jpg" alt="img"></p>
<p>我们通过将比较运算和布尔运算进行融合来构造一个相对复杂的布尔运算，实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00248.jpg" alt="img"></p>
<p><img src="Image00249.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00250.jpg" alt="img"></p>
<p>4.成员运算符</p>
<p>什么是成员运算符呢？若我们已经拥有一个目标列表，则当我们想判断某个元素是否是目标列表中的元素时，就可以使用成员运算符进行操作，使用成员运算符进行运算后返回的值是布尔型的。最典型的成员运算符是“in”，下面来看一个实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00251.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>True</p>
<p>False</p>
<p>如上所示，我们首先定义了一个目标列表和两个变量a、b，然后通过成员运算符判断变量a的值和变量b的值是否在目标列表中，如果在，则返回布尔值True，否则返回布尔值 False。</p>
<p>5.身份运算符</p>
<p>以变量为例，身份运算符用于判断我们比较的变量是否是同一个对象，或者定义的这些变量是否指向相同的内存地址。身份运算符在进行运算后返回的值同样是布尔型的。常用的身份运算符是“is”和“is not”。下面来看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00252.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00253.jpg" alt="img"></p>
<p>代码中的 id函数用于返回我们所定义的变量的内存地址。可以看到，只要变量指向的内存地址相同，那么使用身份运算符运算后返回的结果就是True，否则返回的结果是False。不过上面有个很奇怪的地方，就是在赋给变量a和变量b的值为10时，它们的内存地址是一样的，但是在赋给变量a和b的值为500时，它们的内存地址就不一样了，这其实是Python的解释器引起的问题。在代码中我们还使用了等于运算符来比较变量 a和变量 b，可见“is”和“==”运算有着本质的区别，后者仅仅比较变量值是否相等，而前者比较变量是否属于同一个对象。</p>
<h3 id="5-3-5-Python条件判断语句"><a href="#5-3-5-Python条件判断语句" class="headerlink" title="5.3.5 Python条件判断语句"></a>5.3.5 Python条件判断语句</h3><p>条件判断语句用于通过对给定的条件进行判断来确定接下来是否执行指定的代码块，最简单的条件判断语句的设计方式就是：如果满足已经给定的条件，就执行指定的代码块，否则进行其他操作，定义的其他操作也可以是另一个代码块或者不做任何处理。</p>
<p>在条件判断语句中有4种常用的条件判断形式，但其使用的关键词只有三个，分别是if、elif和else。如下所示的第1种形式是只有一个代码块的条件判断语句，如果满足在if后紧跟的条件判断，那么执行if下的代码块；如果不满足在if后紧跟的条件判断，那么不做任何操作，伪代码如下：</p>
<p><img src="Image00254.jpg" alt="img"></p>
<p>第2种形式是只有两个代码块的条件判断语句，如果满足在if后紧跟的条件判断语句，那么执行if下的代码块；如果不满足在if后紧跟的条件判断语句，那么执行else下的代码块，伪代码如下：</p>
<p><img src="Image00255.jpg" alt="img"></p>
<p>第3种形式是有三个代码块的条件判断语句，如果满足在if后紧跟的条件判断语句，那么执行if下的代码块；如果不满足在if后紧跟的条件判断语句，就再看看在elif后紧跟的条件判断语句，如果满足在elif后紧跟的条件判断语句，就执行elif下的代码块，否则执行else下的代码块，伪代码如下：</p>
<p><img src="Image00256.jpg" alt="img"></p>
<p>第4种形式是有多个代码块的条件判断语句，和第3种形式类似，唯一的区别是在开始的if条件判断语句和结束的else条件判断语句之间有两个或两个以上的elif条件判断语句，伪代码如下：</p>
<p><img src="Image00257.jpg" alt="img"></p>
<p>下面通过实例来看看第2种和第4种条件判断形式在使用上的不同。</p>
<p>首先是使用第2种条件判断形式的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00258.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>The number is equal 10</p>
<p>然后是使用第4种条件判断形式的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00259.jpg" alt="img"></p>
<p><img src="Image00260.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>The number is greater than 10.</p>
<h3 id="5-3-6-Python循环语句"><a href="#5-3-6-Python循环语句" class="headerlink" title="5.3.6 Python循环语句"></a>5.3.6 Python循环语句</h3><p>循环语句要做的是先定义一个循环条件，当满足循环条件的情况发生时，就会执行我们事先定义好的内容代码块，如果一直满足循环条件，就会一直执行下去。循环语句和之前介绍的条件判断语句不同，循环语句只要满足条件就能够反复执行，如果使用得当，就可以减少很多重复的工作。在Python中最常用的循环语句有两种，分别是while循环和for循环。除了循环语句，还有三种常用的循环控制语句，分别是break、continue和pass，下面简单介绍这三种循环控制语句。</p>
<p>（1）break： 出现在循环代码块中，用于中断当次循环并结束整个循环语句。</p>
<p>（2）continue： 出现在循环代码块中，用于中断当次循环并直接开始下次循环。</p>
<p>（3）pass： 出现在循环代码块中，不做任何操作，继续执行当次循环中的后续代码。该循环控制语句主要用于保持代码块的完整性。</p>
<p>下面看看具体的循环语句的使用方法，首先通过while循环语句来构建一个循环，具体的实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00261.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00262.jpg" alt="img"></p>
<p><img src="Image00263.jpg" alt="img"></p>
<p>再看看如何使用for循环语句来构建一个循环，具体的实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00264.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00265.jpg" alt="img"></p>
<p>通过以上内容，我们知道了 while和 for循环语句之间的区别和联系，接下来通过实例看看如何使用之前提到的循环控制语句，首先看看break循环控制语句的具体使用方法。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00266.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00267.jpg" alt="img"></p>
<p><img src="Image00268.jpg" alt="img"></p>
<p>在以上代码中定义了触发 break循环控制语句的条件是变量 i的值等于 5，不过，在break循环控制语句没有被触发前，循环一直在打印输出变量i的值，而在break循环控制语句之后，整个循环语句就直接结束了，所以我们看到最后的输出结果是0、1、2、3、4，没有 5、6、7、8、9。</p>
<p>下面看看continue循环控制语句的使用方法，实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00269.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00270.jpg" alt="img"></p>
<p>在最后的打印输出结果中我们发现少了数字5。在以上代码中我们定义的continue循环控制语句的触发条件和break循环控制语句的是一样的，但是在continue循环控制语句被触发后，受影响的仅仅是当次循环，所以导致当次循环没有进行打印输出，但是之后的循环过程没有受到任何影响。</p>
<p>最后来看看pass循环控制语句的使用方法，实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p>number = 10</p>
<p><img src="Image00271.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00272.jpg" alt="img"></p>
<p>代码中的pass语句不执行任何处理和操作，但是循环并没有卡壳，而是继续执行了下去，整个循环代码的运行没有任何异常。</p>
<p>除了以上内容，还有个非常实用的操作，就是通过使用循环完成列表的迭代，通过对列表的迭代可以遍历列表中的每个元素。来看一个具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00273.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>1</p>
<p>2</p>
<p>3</p>
<p>4</p>
<p>5</p>
<p>6</p>
<h3 id="5-3-7-Python中的函数"><a href="#5-3-7-Python中的函数" class="headerlink" title="5.3.7 Python中的函数"></a>5.3.7 Python中的函数</h3><p>编程语言中的函数就是一组按照相应的规则编写的能够实现相关功能的代码块，Python中的函数也是这样的。通过对函数的使用，代码具备了模块化的性质，降低了代码编写的重复性。在 Python中已经存在许多优秀的函数库，可供我们设计程序时使用，不过要想满足一些个性化的需求，使用自定义的函数会更方便。</p>
<p>1.定义函数</p>
<p>在 Python中创建自定义函数必须遵守函数定义的语法，如下所述是定义函数需要遵守的通用规则。</p>
<p>（1）在定义的函数代码块开头要使用def关键词，而且在关键词后需要紧跟函数名称和括号（()），在括号内定义在函数被调用时需要传入的参数，在括号后以冒号（：）结尾。</p>
<p>（2）在函数内同一个逻辑代码块需要使用相同的空格缩进。</p>
<p>（3）在函数代码块的最后，我们可以通过return关键词返回一个值给调用该函数的方法，如果在return后没有接任何内容或者在代码段中根本没有使用return关键词，那么函数默认返回一个空值（None）给调用该函数的方法。</p>
<p>接下来通过一些实例来掌握对函数进行定义和使用的方法。</p>
<p>不需要进行参数传递的函数的调用实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00274.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00275.jpg" alt="img"></p>
<p>因为我们定义的函数不需要进行参数传递，所以在调用该函数时括号内的内容必须留空，不然会报错。</p>
<p>在函数被调用的过程中发生参数传递的实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00276.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00277.jpg" alt="img"></p>
<p>虽然函数被定义成了可传递参数的函数，但是在实际使用中我们可以传递参数给这个函数，也可以不传递参数给这个函数。在不传递参数给这个函数时，函数本身也不会报错，而是使用函数定义的默认参数，这和无参数传递的函数是有明显区别的。如果在调用函数时传递了合法的参数，在该函数中就会使用我们传递的参数来进行相应的操作，而不再使用默认的参数。</p>
<p>2.函数的参数</p>
<p>在定义附带参数的函数时，会涉及参数的传递方式，但是在函数中参数的传递方式又与定义的参数类别有非常大的联系。下面简要说明在参数传递的过程中经常会用到的几种参数类别。</p>
<p>（1）必备参数 ：如果函数定义的参数是必备参数，那么在调用该函数时必须将相应的参数传递给函数，否则程序会报错。</p>
<p>（2）关键字参数<br>：关键字参数和函数的调用关系很紧密，在函数调用时使用关键字参数来确定传入的参数值，在传递时调换关键字的位置不会对最终的参数传递顺序产生影响。</p>
<p>（3）默认参数 ：使用默认参数的函数，在调用函数时如果我们没有对该函数进行参数传递，那么该函数使用的参数就是其已经定义的默认参数。</p>
<p>（4）不定长参数 ：当我们需要传递给函数的参数比函数声明时的参数要多很多时，我们就可以使用不定长参数来完成。</p>
<p>下面通过具体的实例来看看各种不同类别的参数间的具体使用方式。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00278.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00279.jpg" alt="img"></p>
<h3 id="5-3-8-Python中的类"><a href="#5-3-8-Python中的类" class="headerlink" title="5.3.8 Python中的类"></a>5.3.8 Python中的类</h3><p>之前我们说过，Python也是面向对象的程序语言，所以在 Python中也有面向对象的方法，所以在 Python中创建一个类或对象并不是一件困难的事情。类是用来描述具有相同属性和方法的对象的集合，定义了该集合中每个对象所共有的属性和方法，对象则是类的实例。</p>
<p>1.类的创建</p>
<p>在Python中使用class关键词来创建一个类，在 class关键词之后紧接着的是类的名称，以冒号（：）结尾。在类的创建过程中需要注意的事项如下。</p>
<p>（1）类变量<br>：在创建的类中会定义一些变量，我们把这些变量叫作类变量，类变量的值在这个类的所有实例之间是共享的，同时内部类或者外部类也能对这个变量的值进行访问。</p>
<p>（2）<strong>init</strong>() ：是类的初始化方法，我们在创建一个类的实例时就会调用一次这个方法。</p>
<p>（3）self ：代表类的实例，在定义类的方法时是必须要有的，但是在调用时不必传入参数。</p>
<p>下面通过具体的实例来看看如何创建和使用类。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00280.jpg" alt="img"></p>
<p><img src="Image00281.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00282.jpg" alt="img"></p>
<p>2.类的继承</p>
<p>我们可以将继承理解为：定义一个类，通过继承获得另一个类的所有方法，被继承的类叫作父类，进行继承的类叫作子类。这样可以有效地解决代码的重用问题，在提升了代码的效率和利用率的基础上还增加了可扩展性。</p>
<p>不过需要注意的是，当一个类被继承时，这个类中的类初始化方法是不会被自动调用的，所以我们需要在子类中重新定义类的初始化方法；另外，我们在使用 Python代码去调用某个方法时，默认会先在所在的类中进行查找，如果没有找到，则判断所在的类是否为子类，如果为子类，就继续到父类中查找。下面通过一个具体的实例来看看如何创建和使用子类。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00283.jpg" alt="img"></p>
<p><img src="Image00284.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00285.jpg" alt="img"></p>
<p>3.类的重写</p>
<p>在继承一个类后，父类中的很多方法也许就不能满足我们现有的需求了，这时我们就要对类进行重写。下面通过一个实例来看看如何对类中的内容进行重写。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00286.jpg" alt="img"></p>
<p><img src="Image00287.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>This is Child.</p>
<h2 id="5-4-Python中的NumPy"><a href="#5-4-Python中的NumPy" class="headerlink" title="5.4 Python中的NumPy"></a>5.4 Python中的NumPy</h2><p>NumPy是一个高性能的科学计算和数据分析基础包，在现在的数据分析领域有很多应用，这得益于 NumPy的多维数组对象、线性代数、傅里叶变换和随机数等强大功能，下面看看如何对NumPy进行安装和使用。</p>
<h3 id="5-4-1-NumPy的安装"><a href="#5-4-1-NumPy的安装" class="headerlink" title="5.4.1 NumPy的安装"></a>5.4.1 NumPy的安装</h3><p>NumPy的安装相对简单，我们可以通过Anaconda中的命令进行安装，也可以通过“pip install numpy”语句对NumPy进行安装。如果需要验证NumPy是否安装成功，则可以在NumPy安装完成后通过输入“import numpy”后运行，看看是否输出报错提示。如图5-29所示，在“import numpy”运行后并没有输出报错，说明NumPy已经正确安装并可使用了。在平时的使用中，我们习惯将“import numpy”写成“import numpy as np”。</p>
<p><img src="Image00288.jpg" alt="img"></p>
<p>图5-29</p>
<h3 id="5-4-2-多维数组"><a href="#5-4-2-多维数组" class="headerlink" title="5.4.2 多维数组"></a>5.4.2 多维数组</h3><p>在 NumPy中非常重要的一个应用就是其多维数组对象。为了更好地理解多维数组对象，我们可以将它想象成一个用于存放数据的表格，在这个表格内存放的都是同一种类型的数据，当我们想要提取表格中的某个数据时，就可通过索引值来完成，索引值使用整数来表示。</p>
<p>1.创建多维数组</p>
<p>这里介绍如何创建一个多维数组，常用的方法是使用NumPy中的array。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00289.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>array([1, 2, 3])</p>
<p>在以上代码中创建的是一个一维数组，其方法是直接在 array中传入一个带有参数的列表，这个列表会被转换成数组。</p>
<p>接下来看看如何创建一个更高维度的数组。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00290.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00291.jpg" alt="img"></p>
<p>创建更高维数组的方法和一维数组差不多，只不过变成了把一个嵌套的列表作为参数传递给 array。当然，创建高维数组还有很多其他方式，并不局限于在以上实例中使用的方法。但是，直接将一串数字作为参数传递给 array来创建一个数组是不合法的，如果我们传递给array的参数是列表，则列表会被array当作一个不定长参数进行处理，但若传递给array的参数是一串数字，则会被array当作多个参数而不能处理，实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00292.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00293.jpg" alt="img"></p>
<p>有时我们需要创建一个临时的数组，以方便之后对数据进行存储。对于这种情况，在NumPy中有多种方法可以创建维度指定的临时数组，如下所示。</p>
<p>（1）使用NumPy中的onse可以创建维度指定且元素全为1的数组。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00294.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00295.jpg" alt="img"></p>
<p>在以上代码中使用onse生成了一个元素全为1且维度为（2,3）的数组，传递给onse的参数是一个列表，如果使用元组，则也能够达到一样的效果。这样，一个可用来进行临时数据存放的数组就创建完成了，之后若要更新这个数组中的元素，则直接进行覆盖就可以了。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00296.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00297.jpg" alt="img"></p>
<p>（2）使用NumPy中的zeros可以创建维度指定且元素全为0的数组。在Notebook的输入单元中输入：</p>
<p>import numpy as np</p>
<p>np.zeros([2,3]) #创建全0的数组</p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00298.jpg" alt="img"></p>
<p>（3）使用NumPy中的empty可以创建维度指定且元素全为随机数的数组。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00299.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00300.jpg" alt="img"></p>
<p>2.多维数组的常用属性</p>
<p>下面介绍多维数组中的常用属性。</p>
<p>（1）ndim ：返回统计的数组维数，即维度的数量。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00301.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>2</p>
<p>（2）shape ：返回数组的维度值，对返回的结果使用一个数据类型为整型的元组来表示，比如一个二维数组返回的结果为（n ,m ），那么n 和m 表示数组中对应维度的数据的长度。如果使用shape输出的是矩阵的维度，那么在输出的（n ,m ）中，n 表示矩阵的行，m 表示矩阵的列。查看维度的方法如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00302.jpg" alt="img"></p>
<p>print(a.shape)</p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00303.jpg" alt="img"></p>
<p>在以上代码中输出的数组维度为（2,3），其中2表示第1个维度的数据长度为2，只要我们把以上代码的数组中的[1.1.1.]看作一个整体，那么在数组中就只有两个数据。然后我们把[1.1.1.]看作第2个维度的数据，那么第2个维度的数据长度是3，所以数组的维度是（2,3）。查看矩阵维度的方法如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00304.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00305.jpg" alt="img"></p>
<p>在以上代码中使用了np.matrix来搭建矩阵，搭建矩阵需要传递的参数和使用array搭建数组时传递参数的方法一样。</p>
<p>（3）size ：返回要统计的数组中的元素的总数量。具体的实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00306.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>6</p>
<p>（4）dtype ：返回数组中的元素的数据类型。不过其显示的数据类型和我们之前定义的变量的数据类型名有所区别，因为这些数据类型都是使用 NumPy进行定义的，而在NumPy中表示数据类型使用的是numpy.int32、numpy.int16和numpy.float64这类格式的名字。具体的实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00307.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>dtype(‘float64’)</p>
<p>若想要重新改写现有元素的数据类型，则可以通过如下方法进行，不过只能使用在NumPy中定义的数据类型的名字。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00308.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>dtype(‘int32’)</p>
<p>（5）itemsize ：返回数组中每个元素的字节大小。比如元素的 dtype是float64，那么其itemsize是8，计算方法为<img src="Image00309.jpg" alt="img"><br>；如果元素的dtype是complex32，那么其itemsize是4，计算方法为<img src="Image00310.jpg" alt="img"> ，其他dtype的计算方法以此类推。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00311.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>4</p>
<p>3.数组的打印</p>
<p>数组可以通过print进行打印输出，打印出的数组和嵌套的列表很相似。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00312.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00313.jpg" alt="img"></p>
<p>还有一种情况，在数组中的元素太多时，若全部进行打印输出，则会占用大面积的显示空间，而且不易查看，所以在打印输出元素过多的数组时，输出显示的内容会自动跳过中间的部分，只打印首尾的一小部分，对中间的部分用省略号（…）来代替。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00314.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>[ 0 1 2 …, 1997 1998 1999]</p>
<h3 id="5-4-3-多维数组的基本操作"><a href="#5-4-3-多维数组的基本操作" class="headerlink" title="5.4.3 多维数组的基本操作"></a>5.4.3 多维数组的基本操作</h3><p>在讲解完如何构建数组后，接下来讲解对数组的一些基本操作，这些操作包括数组的算术运算、索引、切片、迭代等。</p>
<p>1.数组的算术运算</p>
<p>数组能够直接进行加法、减法、乘法和除法算术运算，实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00315.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00316.jpg" alt="img"></p>
<p>从上面的实例可以看出，虽然数组在构造上类似于矩阵，但是其运算和之前介绍的矩阵运算存在诸多不同：首先，矩阵是不存在除法运算的，但是数组能够进行除法运算；其次，数组的乘法运算机制是通过将位置对应的元素相乘来完成的，和矩阵的乘法运算机制不同。下面来看看如何通过数组实现矩阵乘法运算。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00317.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00318.jpg" alt="img"></p>
<p>在以上代码中使用了两种方法来实现矩阵的乘法运算，其计算结果是一样的。数组和矩阵的算术运算还有一个较大的不同点，就是数组可以直接和标量进行算术运算，但是在矩阵运算中是不可以的。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00319.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00320.jpg" alt="img"></p>
<p>2.数组的自身运算</p>
<p>除了数组和数组、数组和标量之间的算术运算，我们还可以通过自定义一些方法来对数组本身进行操作。一些常用的操作方法如下。</p>
<p>（1）min ：默认找出数组的所有元素中值最小的元素，可以通过设置axis的值来按行或者列查找元素中的最小值。</p>
<p>（2）max ：默认找出数组的所有元素中值最大的元素，可以通过设置axis的值来按行或者列查找元素中的最大值。</p>
<p>（3）sum ：默认对数组中的所有元素进行求和运算，并返回运算结果，同样可以通过设置axis的值来按行或者列对元素进行求和运算。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00321.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00322.jpg" alt="img"></p>
<p>以上代码只是针对一维数组的情况编写的，如果是多维数组的情况，则操作又将如何进行呢？下面来看看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00323.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00324.jpg" alt="img"></p>
<p><img src="Image00325.jpg" alt="img"></p>
<p>在复杂的多维数组中，我们通过对axis参数进行不同的设置，来得到不同的运算结果：当axis为0时，计算方向是针对数组的列的；当axis为1时，计算方向是针对数组的行的。</p>
<p>（4）exp ：对数组中的所有元素进行指数运算。</p>
<p>（5）sqrt ：对数组中的所有元素进行平方根运算。</p>
<p>（6）square ：对数组中的所有元素进行平方运算。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00326.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00327.jpg" alt="img"></p>
<p>3.随机数组</p>
<p>生成随机数在我们平时的应用中是很有用的，在 NumPy中有许多方法可以生成不同属性的随机数，以满足在计算中使用随机数字的需求。</p>
<p>（1）seed<br>：随机因子，在随机数生成器的随机因子被确定后，无论我们运行多少次随机程序，最后生成的数字都是一样的，随机因子更像把随机的过程变成一种伪随机的机制，不过这有利于结果的复现。</p>
<p>（2）rand ：生成一个在[0,1)范围内满足均匀分布的随机样本数。</p>
<p>（3）randn ：生成一个满足平均值为0且方差为1的正太分布随机样本数。</p>
<p>（4）randint ：在给定的范围内生成类型为整数的随机样本数。</p>
<p>（5）binomial ：生成一个维度指定且满足二项分布的随机样本数。</p>
<p>（6）beta ：生成一个指定维度且满足beta分布的随机样本数。</p>
<p>（7）normal ：生成一个指定维度且满足高斯正太分布的随机样本数。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00328.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00329.jpg" alt="img"></p>
<p>我们在以上代码中确定了随机因子，所以不论运行几次，最后得到的随机结果都是一样的。</p>
<p>4.索引、切片和迭代</p>
<p>我们已经掌握了如何在列表中对索引、切片和迭代进行操作，而在数组中也有索引、切片和迭代，其操作过程和列表类似，不过多维数组相较于一维数组，在索引、切片和迭代等操作上会更复杂。来看一个一维数组的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00330.jpg" alt="img"></p>
<p><img src="Image00331.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00332.jpg" alt="img"></p>
<p>下面重点介绍多维数组的索引、切片和迭代操作。对于多维数组而言，在每个维度上都可以通过索引值进行切片，这些索引值构成的是一个用逗号分隔的元组。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00333.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00334.jpg" alt="img"></p>
<p>我们从以上实例的输出结果中可以看到，多维数组的切片可以分别针对多个维度进行操作，而且多维数组可以针对不同的维度进行迭代，具体的实例如下。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00335.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00336.jpg" alt="img"></p>
<p>8</p>
<p>9</p>
<p>将代码中的第2种迭代方法写成如下形式，也可以达到同样的效果，这相当于将多维数组进行了扁平化处理，将其转变成了一维数组：</p>
<p><img src="Image00337.jpg" alt="img"></p>
<h2 id="5-5-Python中的Matplotlib"><a href="#5-5-Python中的Matplotlib" class="headerlink" title="5.5 Python中的Matplotlib"></a>5.5 Python中的Matplotlib</h2><p>Matplotlib是Python的绘图库，不仅具备强大的绘图功能，还能够在很多平台上使用，和Jupyter Notebook有极强的兼容性。</p>
<h3 id="5-5-1-Matplotlib的安装"><a href="#5-5-1-Matplotlib的安装" class="headerlink" title="5.5.1 Matplotlib的安装"></a>5.5.1 Matplotlib的安装</h3><p>我们可以通过Anaconda中的命令或者“pip install matplotlib”语句来安装Matplotlib，在安装完成后通过运行“import matplotlib”语句，来检验安装是否成功。如果没有输出报错，则说明安装没有问题，可以正常使用。在实际应用中，我们同样习惯于将“import matplotlib”写成“import matplotlib.pyplot as plt”。如果是在Jupyter Notebook的Notebook文件中使用的，则要想直接显示Matplotlib绘制的图像，就需要添加“%matplotlib inline”语句，如图5-30所示。</p>
<p><img src="Image00338.jpg" alt="img"></p>
<p>图5-30</p>
<h3 id="5-5-2-创建图"><a href="#5-5-2-创建图" class="headerlink" title="5.5.2 创建图"></a>5.5.2 创建图</h3><p>Matplotlib本身定位于数据的可视化展现，所以集成了很多数据可视化方法。下面通过实例来了解在Matplotlib中进行数据可视化的常用方法。</p>
<p>1.线型图</p>
<p>线型图通过线条的形式对数据进行展示，可以通过它很方便地看出数据的趋势和波动性，下面来看一个简单的线型图绘制实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00339.jpg" alt="img"></p>
<p>在运行后，输出的内容如图5-31所示。</p>
<p><img src="Image00340.jpg" alt="img"></p>
<p>图5-31</p>
<p>以上代码的前三行用于导入我们需要的包，并让通过 Matplotlib绘制的图像直接在Notebook文件中显示；然后通过 np.random.seed(42)设置了随机种子，以方便我们之后的结果复现；接着通过np.random.randn(30)生成30个随机参数并赋值给变量x；最后，绘图的核心代码通过plt.plot(x,<br>“r—o”)将这30个随机参数以点的方式绘制出来并用线条进行连接，传递给plot的参数r— o用于在线型图中标记每个参数点使用的形状、连接参数点使用的线条颜色和线型，而且线型图的横轴和纵轴也是有区别的，纵轴生成的是30个随机数的值，横轴生成的是这30个点的索引值，同样是30个。</p>
<p>2.线条颜色、标记形状和线型</p>
<p>在绘制线型图时，我们通过标记每个参数点使用的形状、连接参数点使用的线条颜色和线型，可以很好地区分不同的数据，这样做可以使我们想要显示的数据更清晰，还能突出重要的数据。</p>
<p>用于设置线型图中线条颜色的常用参数如下。</p>
<p>（1）“b” ：指定绘制的线条颜色为蓝色。</p>
<p>（2）“g” ：指定绘制的线条颜色为绿色。</p>
<p>（3）“r” ：指定绘制的线条颜色为红色。</p>
<p>（4）“c” ：指定绘制的线条颜色为蓝绿色。</p>
<p>（5）“m” ：指定绘制的线条颜色为洋红色。</p>
<p>（6）“y” ：指定绘制的线条颜色为黄色。</p>
<p>（7）“k” ：指定绘制的线条颜色为黑色。</p>
<p>（8）“w” ：指定绘制的线条颜色为白色。</p>
<p>用于设置线型图中标记参数点形状的常用参数如下。</p>
<p>（1）“o” ：指定标记实际点使用的形状为圆形。</p>
<p>（2）“<em>” ：指定标记实际点使用“</em> ”符号。</p>
<p>（3）“+” ：指定标记实际点使用“+ ”符号。</p>
<p>（4）“x” ：指定标记实际点使用“x”符号。</p>
<p>用于设置线型图中连接参数点线条形状的常用参数如下。</p>
<p>（1）“-” ：指定线条形状为实线。</p>
<p>（2）“—” ：指定线条形状为虚线。</p>
<p>（3）“-.” ：指定线条形状为点实线。</p>
<p>（4）“:” ：指定线条形状为点线。</p>
<p>下面来看一个使用不同的线条颜色、形状和标记参数点形状的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00341.jpg" alt="img"></p>
<p><img src="Image00342.jpg" alt="img"></p>
<p>在运行后，输出的内容如图5-32所示。</p>
<p><img src="Image00343.jpg" alt="img"></p>
<p>图5-32</p>
<p>以上代码和之前代码的不同是没有使用随机种子，这样做是为了让最后得到的结果有更大的差异性，在绘制的图中对比更明显。</p>
<p>3.标签和图例</p>
<p>为了让我们绘制的图像更易理解，我们可以增加一些绘制图像的说明，一般是添加图像的轴标签和图例，如下所示就是一个添加图例和轴标签的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00344.jpg" alt="img"></p>
<p><img src="Image00345.jpg" alt="img"></p>
<p>在运行后，输出的内容如图5-33所示。</p>
<p><img src="Image00346.jpg" alt="img"></p>
<p>图5-33</p>
<p>我们在图 5-33中看到了图标签和图例，这是因为在以上代码中增加了标签的显示代码plt.xlabel(“Y”)、plt.ylabel(“Y”)和图例的显示代码plt.legend([X, Y], [“X”, “Y”])，传递给plt.legend的是两个列表参数，第1个列表参数是在图中实际使用的标记和线形，第2个列表参数是对应图例的文字描述。</p>
<p>4.子图</p>
<p>若我们需要将多个图像同时在不同的位置显示，则需要用到子图（Subplot）的功能。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00347.jpg" alt="img"></p>
<p><img src="Image00348.jpg" alt="img"></p>
<p>在运行后，输出的内容如图5-34所示。</p>
<p><img src="Image00349.jpg" alt="img"></p>
<p>图5-34</p>
<p>在绘制子图时，我们首先需要通过 fig = plt.figure()定义一个实例，然后通过fig.add_subplot方法向fig实例中添加我们需要的子图。在代码中传递给fig.add_subplot方法的参数是1组数字，拿第1组数字（2,2,1）来说，前两个数字表示把整块图划分成了两行两列，一共4张子图，最后1个数字表示具体使用哪一张子图进行绘制。除了绘制线型图，利用Matplotlib强大的绘图库还能绘制散点图、直方图、饼图等常用的图形。</p>
<p>5.散点图</p>
<p>如果我们获取的是一些散点数据，则可以通过绘制散点图（Scatter）更清晰地展示所有数据的分布和布局。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00350.jpg" alt="img"></p>
<p><img src="Image00351.jpg" alt="img"></p>
<p>在运行后，输出的内容如图5-35所示。</p>
<p><img src="Image00352.jpg" alt="img"></p>
<p>图5-35</p>
<p>绘制散点图的核心代码是plt.scatter(x,y, c=”g”, marker=”o”, label=”(X,Y)”)，其中有三个我们需要特别留意的参数，如下所述。</p>
<p>（1）“c” ：指定散点图中绘制的参数点使用哪种颜色，可设置的颜色参数可参考之前绘制线型图时对线条颜色选择的参数范围，这里使用“g”表示设置为绿色。</p>
<p>（2）“marker” ：指定散点图中绘制的参数点使用哪种形状，和之前线型图中的设置一样，这里使用“o”表示设置为圆形。</p>
<p>（3）“label” ：指定在散点图中绘制的参数点使用的图例，与线型图中的图例不同。</p>
<p>我们还可以通过plt.legend(loc=1)对图例的位置进行强制设定，对图例位置的参数设置一般有以下几种。</p>
<p>（1）“loc=0” ：图例使用最好的位置。</p>
<p>（2）“loc=1” ：强制图例使用图中右上角的位置。</p>
<p>（3）“loc=2” ：强制图例使用图中左上角的位置。</p>
<p>（4）“loc=3” ：强制图例使用图中左下角的位置。</p>
<p>（5）“loc=4” ：强制图例使用图中右上角的位置。</p>
<p>当然还有其他位置可供选择，以上几个位置已经是最常用的了。</p>
<p>6.直方图</p>
<p>直方图（Histogram）又称质量分布图，是一种统计报告图，通过使用一系列高度不等的纵向条纹或直方表示数据分布的情况，一般用横轴表示数据类型，用纵轴表示分布情况，下面看看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00353.jpg" alt="img"></p>
<p>在运行后，输出的内容如图5-36所示。</p>
<p><img src="Image00354.jpg" alt="img"></p>
<p>图5-36</p>
<p>绘制直方图的核心代码是plt.hist(x,bins=20,color=”g”)，其中color的功能和散点图中的c是一样的，bins用于指定我们绘制的直方图条纹的数量。</p>
<p>7.饼图</p>
<p>饼图用于显示一个数据系列，我们可以将一个数据系列理解为一类数据，而每个数据系列都应当拥有自己唯一的颜色。在同一个饼图中可以绘制多个系列的数据，并根据每个系列的数据量的不同来分配它们在饼图中的占比。下面看看具体的实例。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00355.jpg" alt="img"></p>
<p>在运行后，输出的内容如图5-37所示。</p>
<p><img src="Image00356.jpg" alt="img"></p>
<p>图5-37</p>
<p>绘制饼图的核心代码为 plt.pie(sizes, explode=(0, 0, 0.1), labels=labels, autopct=’%1.1f%%’, startangle=60)，其中sizes= [15, 50, 35]的三个数字确定了每部分数据系列在整个圆形中的占比；explode定义每部分数据系列之间的间隔，如果设置两个0和一个0.1，就能突出第 3部分；autopct其实就是将 sizes中的数据以所定义的浮点精度进行显示；startangle是绘制第1块饼图时该饼图与X轴正方向的夹角度数，这里设置为90，默认为0；plt.axis(‘equal’)是必不可少的，用于使X轴和Y轴的刻度保持一致，只有这样，最后得到饼图才是圆形的。</p>
<h1 id="第6章-PyTorch基础"><a href="#第6章-PyTorch基础" class="headerlink" title="第6章 PyTorch基础"></a>第6章 PyTorch基础</h1><p>PyTorch是美国互联网巨头Facebook在深度学习框架Torch的基础上使用Python重写的一个全新的深度学习框架，它更像NumPy的替代产物，不仅继承了NumPy的众多优点，还支持GPUs计算，在计算效率上要比NumPy有更明显的优势；不仅如此，PyTorch还有许多高级功能，比如拥有丰富的API，可以快速完成深度神经网络模型的搭建和训练。所以 PyTorch一经发布，便受到了众多开发人员和科研人员的追捧和喜爱，成为AI从业者的重要工具之一。</p>
<h2 id="6-1-PyTorch中的Tensor"><a href="#6-1-PyTorch中的Tensor" class="headerlink" title="6.1 PyTorch中的Tensor"></a>6.1 PyTorch中的Tensor</h2><p>安装 PyTorch比较便捷的方法是直接登录它的官网 <a href="http://pytorch.org/，通过如图" target="_blank" rel="noopener">http://pytorch.org/，通过如图</a> 6-1所示的界面生成相应的安装命令。</p>
<p>在顺利安装PyTorch后便可以开始我们的PyTorch之旅了。首先，我们需要学会使用PyTorch中的 Tensor。Tensor在 PyTorch中负责存储基本数据，PyTorch针对 Tensor也提供了丰富的函数和方法，所以PyTorch中的Tensor与NumPy的数组具有极高的相似性。Tensor是一种高级的API，我们在使用Tensor时并不用了解PyTorch中的高层次架构，也不用明白什么是深度学习、什么是后向传播、如何对模型进行优化、什么是计算图等技术细节。更重要的是，在PyTorch中定义的Tensor数据类型的变量还可以在GPUs上进行运算，而且只需对变量做一些简单的类型转换就能够轻易实现。</p>
<p><img src="Image00357.jpg" alt="img"></p>
<p>图6-1</p>
<h3 id="6-1-1-Tensor的数据类型"><a href="#6-1-1-Tensor的数据类型" class="headerlink" title="6.1.1 Tensor的数据类型"></a>6.1.1 Tensor的数据类型</h3><p>在使用Tensor时，我们首先要掌握如何使用Tensor来定义不同数据类型的变量。和NumPy差不多，PyTorch中的Tensor也有自己的数据类型定义方式，常用的如下。</p>
<p>（1）torch.FloatTensor<br>：用于生成数据类型为浮点型的Tensor，传递给torch.FloatTensor的参数可以是一个列表，也可以是一个维度值。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00358.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00359.jpg" alt="img"></p>
<p><img src="Image00360.jpg" alt="img"></p>
<p>可以看到，打印输出的两组变量数据类型都显示为浮点型，不同的是，前面的一组是按照我们指定的维度随机生成的浮点型Tensor，而另外一组是按我们给定的列表生成的浮点型Tensor。</p>
<p>（2）torch.IntTensor ：用于生成数据类型为整型的 Tensor，传递给 torch.IntTensor的参数可以是一个列表，也可以是一个维度值。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00361.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00362.jpg" alt="img"></p>
<p>可以看到，以上生成的两组Tensor最后显示的数据类型都为整型。</p>
<p>（3）torch.rand<br>：用于生成数据类型为浮点型且维度指定的随机Tensor，和在NumPy中使用numpy.rand生成随机数的方法类似，随机生成的浮点数据在0～1区间均匀分布。</p>
<p>在Notebook的输入单元中输入：</p>
<p>import torch</p>
<p><img src="Image00363.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00364.jpg" alt="img"></p>
<p>（4）torch.randn<br>：用于生成数据类型为浮点型且维度指定的随机Tensor，和在NumPy中使用numpy.randn生成随机数的方法类似，随机生成的浮点数的取值满足均值为0、方差为1的正太分布。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00365.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00366.jpg" alt="img"></p>
<p>（5）torch.range<br>：用于生成数据类型为浮点型且自定义起始范围和结束范围的Tensor，所以传递给torch.range的参数有三个，分别是范围的起始值、范围的结束值和步长，其中，步长用于指定从起始值到结束值的每步的数据间隔。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00367.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>1</p>
<p>2</p>
<p>3</p>
<p>4</p>
<p>5</p>
<p><img src="Image00368.jpg" alt="img"></p>
<p>（6）torch.zeros ：用于生成数据类型为浮点型且维度指定的Tensor，不过这个浮点型的Tensor中的元素值全部为0。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00369.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00370.jpg" alt="img"></p>
<h3 id="6-1-2-Tensor的运算"><a href="#6-1-2-Tensor的运算" class="headerlink" title="6.1.2 Tensor的运算"></a>6.1.2 Tensor的运算</h3><p>这里通过对Tensor数据类型的变量进行运算，来组合一些简单或者复杂的算法，常用的Tensor运算如下。</p>
<p>（1）torch.abs ：将参数传递到torch.abs后返回输入参数的绝对值作为输出，输入参数必须是一个Tensor数据类型的变量。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00371.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00372.jpg" alt="img"></p>
<p>（2）torch.add ：将参数传递到 torch.add后返回输入参数的求和结果作为输出，输入参数既可以全部是Tensor数据类型的变量，也可以一个是Tensor数据类型的变量，另一个是标量。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00373.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>-0.9922 0.1921 -0.9051</p>
<p><img src="Image00374.jpg" alt="img"></p>
<p>如上所示，无论是调用torch.add对两个Tensor数据类型的变量进行计算，还是完成Tensor数据类型的变量和标量的计算，计算方式都和NumPy中的数组的加法运算如出一辙。</p>
<p>（3）torch.clamp<br>：对输入参数按照自定义的范围进行裁剪，最后将参数裁剪的结果作为输出。所以输入参数一共有三个，分别是需要进行裁剪的Tensor数据类型的变量、裁剪的上边界和裁剪的下边界，具体的裁剪过程是：使用变量中的每个元素分别和裁剪的上边界及裁剪的下边界的值进行比较，如果元素的值小于裁剪的下边界的值，该元素就被重写成裁剪的下边界的值；同理，如果元素的值大于裁剪的上边界的值，该元素就被重写成裁剪的上边界的值。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00375.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p>-0.4914 -0.1085 0.4345</p>
<p><img src="Image00376.jpg" alt="img"></p>
<p>（4）torch.div<br>：将参数传递到torch.div后返回输入参数的求商结果作为输出，同样，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00377.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00378.jpg" alt="img"></p>
<p><img src="Image00379.jpg" alt="img"></p>
<p>（5）torch.mul ：将参数传递到 torch.mul后返回输入参数求积的结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00380.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00381.jpg" alt="img"></p>
<p><img src="Image00382.jpg" alt="img"></p>
<p>（6）torch.pow<br>：将参数传递到torch.pow后返回输入参数的求幂结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00383.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00384.jpg" alt="img"></p>
<p>（7）torch.mm ：将参数传递到 torch.mm后返回输入参数的求积结果作为输出，不过这个求积的方式和之前的torch.mul运算方式不太一样，torch.mm运用矩阵之间的乘法规则进行计算，所以被传入的参数会被当作矩阵进行处理，参数的维度自然也要满足矩阵乘法的前提条件，即前一个矩阵的行数必须和后一个矩阵的列数相等，否则不能进行计算。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00385.jpg" alt="img"></p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00386.jpg" alt="img"></p>
<p>（8）torch.mv<br>：将参数传递到torch.mv后返回输入参数的求积结果作为输出，torch.mv运用矩阵与向量之间的乘法规则进行计算，被传入的参数中的第1个参数代表矩阵，第2个参数代表向量，顺序不能颠倒。</p>
<p>在Notebook的输入单元中输入：</p>
<p><img src="Image00387.jpg" alt="img"></p>
<p>print(c)</p>
<p>在运行后，输出的内容如下：</p>
<p><img src="Image00388.jpg" alt="img"></p>
<h3 id="6-1-3-搭建一个简易神经网络"><a href="#6-1-3-搭建一个简易神经网络" class="headerlink" title="6.1.3 搭建一个简易神经网络"></a>6.1.3 搭建一个简易神经网络</h3><p>下面通过一个示例来看看如何使用已经掌握的知识，搭建出一个基于PyTorch架构的简易神经网络模型。</p>
<p>搭建神经网络模型的具体代码如下，这里会将完整的代码分成几部分进行详细介绍，以便于读者理解。</p>
<p>代码的开始处是相关包的导入：</p>
<p><img src="Image00389.jpg" alt="img"></p>
<p>我们先通过import torch导入必要的包，然后定义4个整型变量，其中：batch_n是在一个批次中输入数据的数量，值是100，这意味着我们在一个批次中输入100个数据，同时，每个数据包含的数据特征有input_data个，因为input_data的值是1000，所以每个数据的数据特征就是1000个；hidden_layer用于定义经过隐藏层后保留的数据特征的个数，这里有100个，因为我们的模型只考虑一层隐藏层，所以在代码中仅定义了一个隐藏层的参数；output_data是输出的数据，值是10，我们可以将输出的数据看作一个分类结果值的数量，个数10表示我们最后要得到10个分类结果值。</p>
<p>一个批次的数据从输入到输出的完整过程是：先输入100个具有1000个特征的数据，经过隐藏层后变成100个具有100个特征的数据，再经过输出层后输出100个具有10个分类结果值的数据，在得到输出结果之后计算损失并进行后向传播，这样一次模型的训练就完成了，然后循环这个流程就可以完成指定次数的训练，并达到优化模型参数的目的。下面看看如何完成从输入层到隐藏层、从隐藏层到输出层的权重初始化定义工作，代码如下：</p>
<p><img src="Image00390.jpg" alt="img"></p>
<p>在以上代码中定义的从输入层到隐藏层、从隐藏层到输出层对应的权重参数，同在之前说到的过程中使用的参数维度是一致的，由于我们现在并没有好的权重参数的初始化方法，所以选择通过torch.randn来生成指定维度的随机参数作为其初始化参数，尽管这并不是一个好主意。可以看到，在代码中定义的输入层维度为（100,1000），输出层维度为（100,10），同时，从输入层到隐藏层的权重参数维度为（1000,100），从隐藏层到输出层的权重参数维度为（100,10），这里我们可能会好奇权重参数的维度是怎么定义下来的，其实，只要我们把整个过程看作矩阵连续的乘法运算，就自然能够很快明白了。在代码中我们的真实值y也是通过随机的方式生成的，所以一开始在使用损失函数计算损失值时得到的结果会较大。</p>
<p>在定义好输入、输出和权重参数之后，就可以开始训练模型和优化权重参数了，在此之前，我们还需要明确训练的总次数和学习速率，代码如下：</p>
<p><img src="Image00391.jpg" alt="img"></p>
<p>因为接下来会使用梯度下降的方法来优化神经网络的参数，所以必须定义后向传播的次数和梯度下降使用的学习速率。在以上代码中使用了epoch_n定义训练的次数，epoch_n的值为20，所以我们需要通过循环的方式让程序进行20次训练，来完成对初始化权重参数的优化和调整。在优化的过程中使用的学习速率learning_rate的值为1e-6，表示6 110- ×，即0.000001。接下来对模型进行正式训练并对参数进行优化，代码如下：</p>
<p><img src="Image00392.jpg" alt="img"></p>
<p><img src="Image00393.jpg" alt="img"></p>
<p>以上代码通过最外层的一个大循环来保证我们的模型可以进行 20次训练，循环内的是神经网络模型具体的前向传播和后向传播代码，参数的优化和更新使用梯度下降来完成。在这个神经网络的前向传播中，通过两个连续的矩阵乘法计算出预测结果，在计算的过程中还对矩阵乘积的结果使用clamp方法进行裁剪，将小于零的值全部重新赋值为0，这就像加上了一个ReLU激活函数的功能。</p>
<p>前向传播得到的预测结果通过 y_pred来表示，在得到了预测值后就可以使用预测值和真实值来计算误差值了。我们用loss来表示误差值，对误差值的计算使用了均方误差函数。之后的代码部分就是通过实现后向传播来对权重参数进行优化了，为了计算方便，我们的代码实现使用的是每个节点的链式求导结果，在通过计算之后，就能够得到每个权重参数对应的梯度分别是grad_w1和grad_w2。在得到参数的梯度值之后，按照之前定义好的学习速率对w1和w2的权重参数进行更新，在代码中每次训练时，我们都会对loss的值进行打印输出，以方便看到整个优化过程的效果，所以最后会有20个loss值被打印显示，打印输出的结果如下：</p>
<p><img src="Image00394.jpg" alt="img"></p>
<p><img src="Image00395.jpg" alt="img"></p>
<p>可以看到，loss值从之前的巨大误差逐渐缩减，这说明我们的模型经过 20次训练和权重参数优化之后，得到的预测的值和真实值之间的差距越来越小了。</p>
<h2 id="6-2-自动梯度"><a href="#6-2-自动梯度" class="headerlink" title="6.2 自动梯度"></a>6.2 自动梯度</h2><p>我们在 6.1.3节中基于PyTorch深度学习框架搭建了一个简易神经网络模型，并通过在代码中使用前向传播和后向传播实现了对这个模型的训练和对权重参数的优化，不过该模型在结构上很简单，而且神经网络的代码也不复杂。我们在实践中搭建的网络模型都是层次更深的神经网络模型，即深度神经网络模型，结构会有所变化，代码也会更复杂。对于深度的神经网络模型的前向传播使用简单的代码就能实现，但是很难实现涉及该模型中后向传播梯度计算部分的代码，其中最困难的就是对模型计算逻辑的梳理。</p>
<p>在PyTorch中提供了一种非常方便的方法，可以帮助我们实现对模型中后向传播梯度的自动计算，避免了“重复造轮子”，这就是接下来要重点介绍的torch.autograd包。通过使用 torch.autograd 包，可以使模型参数自动计算在优化过程中需要用到的梯度值，在很大程度上帮助降低了实现后向传播代码的复杂度。</p>
<h3 id="6-2-1-torch-autograd和Variable"><a href="#6-2-1-torch-autograd和Variable" class="headerlink" title="6.2.1 torch.autograd和Variable"></a>6.2.1 torch.autograd和Variable</h3><p>torch.autograd包的主要功能是完成神经网络后向传播中的链式求导，手动实现链式求导的代码会给我们带来很大的困扰，而 torch.autograd 包中丰富的类减少了这些不必要的麻烦。实现自动梯度功能的过程大致为：先通过输入的Tensor数据类型的变量在神经网络的前向传播过程中生成一张计算图，然后根据这个计算图和输出结果准确计算出每个参数需要更新的梯度，并通过完成后向传播完成对参数的梯度更新。</p>
<p>在实践中完成自动梯度需要用到torch.autograd包中的Variable类对我们定义的Tensor数据类型变量进行封装，在封装后，计算图中的各个节点就是一个Variable对象，这样才能应用自动梯度的功能。</p>
<p>如果已经按照如上方式完成了相关操作，则在选中了计算图中的某个节点时，这个节点必定会是一个Variable对象，用X 来代表我们选中的节点，那么X .data代表Tensor数据类型的变量，X .grad也是一个Variable对象，不过它表示的是X 的梯度，在想访问梯度值时需要使用X .grad.data。</p>
<p>下面通过一个自动梯度的示例来看看如何使用 torch.autograd.Variable类和torch.autograd包。我们同样搭建一个二层结构的神经网络模型，这有利于和我们之前搭建的简易神经网络模型的训练和优化过程进行对比，重新实现的代码如下：</p>
<p><img src="Image00396.jpg" alt="img"></p>
<p>同样，一开始导入必要的包和类，但是在代码中增加了“from torch.autograd import Variable”。定义的4个变量和之前的代码是一样的，其中batch_n是一个批次输入的数据量，input_data是输入数据的特征个数，hidden_layer是通过隐藏层后输出的特征数，output_data是最后输出的分类结果数，然后来看如下代码：</p>
<p><img src="Image00397.jpg" alt="img"></p>
<p>“Variable(torch.randn(batch_n, input_data), requires_grad = False)”这段代码就是之前讲到的用 Variable类对 Tensor数据类型变量进行封装的操作。在以上代码中还使用了一个requires_grad参数，这个参数的赋值类型是布尔型，如果requires_grad的值是False，那么表示该变量在进行自动梯度计算的过程中不会保留梯度值。我们将输入的数据x和输出的数据y的requires_grad参数均设置为False，这是因为这两个变量并不是我们的模型需要优化的参数，而两个权重w1和w2的requires_grad参数的值为True。</p>
<p>之后的代码用于定义模型的训练次数和学习速率，代码如下：</p>
<p><img src="Image00398.jpg" alt="img"></p>
<p>和之前一样，将训练次数 epoch_n设为20次，将学习速率 learning_rate设置为0.000001。</p>
<p>新的模型训练和参数优化的代码如下：</p>
<p><img src="Image00399.jpg" alt="img"></p>
<p>和之前的代码相比，当前的代码更简洁了，之前代码中的后向传播计算部分变成了新代码中的 loss.backward()，这个函数的功能在于让模型根据计算图自动计算每个节点的梯度值并根据需求进行保留，有了这一步，我们的权重参数 w1.data和 w2.data就可以直接使用在自动梯度过程中求得的梯度值w1.data.grad和w2.data.grad，并结合学习速率来对现有的参数进行更新、优化了。在代码的最后还要将本次计算得到的各个参数节点的梯度值通过grad.data.zero_()全部置零，如果不置零，则计算的梯度值会被一直累加，这样就会影响到后续的计算。同样，在整个模型的训练和优化过程中，每个循环都加入了打印loss值的操作，所以最后会得到20个loss值的打印输出，输出的结果如下：</p>
<p><img src="Image00400.jpg" alt="img"></p>
<p><img src="Image00401.jpg" alt="img"></p>
<p>从结果来看，对参数的优化在顺利进行，因为loss值也越来越低了。</p>
<h3 id="6-2-2-自定义传播函数"><a href="#6-2-2-自定义传播函数" class="headerlink" title="6.2.2 自定义传播函数"></a>6.2.2 自定义传播函数</h3><p>其实除了可以采用自动梯度方法，我们还可以通过构建一个继承了 torch.nn.Module的新类，来完成对前向传播函数和后向传播函数的重写。在这个新类中，我们使用forward作为前向传播函数的关键字，使用 backward作为后向传播函数的关键字。下面介绍如何使用自定义传播函数的方法，来调整之前具备自动梯度功能的简易神经网络模型。整个代码的开始部分如下：</p>
<p><img src="Image00402.jpg" alt="img"></p>
<p>和之前的代码一样，在代码的开始部分同样是导入必要的包、类，并定义需要用到的4 个变量。下面看看新的代码部分是如何定义我们的前向传播 forward 函数和后向传播backward函数的：</p>
<p><img src="Image00403.jpg" alt="img"></p>
<p><img src="Image00404.jpg" alt="img"></p>
<p>以上代码展示了一个比较常用的Python类的构造方式：首先通过class Model(torch.nn.Module)完成了类继承的操作，之后分别是类的初始化，以及forward函数和backward函数。forward函数实现了模型的前向传播中的矩阵运算，backward实现了模型的后向传播中的自动梯度计算，后向传播如果没有特别的需求，则在一般情况下不用进行调整。在定义好类之后，我们就可以对其进行调用了，代码如下：</p>
<p>model = Model()</p>
<p>这一系列操作相当于完成了对简易神经网络的搭建，然后就只剩下对模型进行训练和对参数进行优化的部分了，代码如下：</p>
<p><img src="Image00405.jpg" alt="img"></p>
<p><img src="Image00406.jpg" alt="img"></p>
<p>这里，变量的赋值、训练次数和学习速率的定义，以及模型训练和参数优化使用的代码，和在 6.2.1节中使用的代码没有太大的差异，不同的是，我们的模型通过“y_pred =model(x, w1, w2)”来完成对模型预测值的输出，并且整个训练部分的代码被简化了。在20次训练后，20个loss值的打印输出如下：</p>
<p><img src="Image00407.jpg" alt="img"></p>
<p>从结果来看，对参数的优化同样在顺利进行，每次输出的loss值也在逐渐减小。</p>
<h2 id="6-3-模型搭建和参数优化"><a href="#6-3-模型搭建和参数优化" class="headerlink" title="6.3 模型搭建和参数优化"></a>6.3 模型搭建和参数优化</h2><p>接下来看看如何基于PyTorch深度学习框架用简单快捷的方式搭建出复杂的神经网络模型，同时让模型参数的优化方法趋于高效。如同使用PyTorch中的自动梯度方法一样，在搭建复杂的神经网络模型的时候，我们也可以使用PyTorch中已定义的类和方法，这些类和方法覆盖了神经网络中的线性变换、激活函数、卷积层、全连接层、池化层等常用神经网络结构的实现。在完成模型的搭建之后，我们还可以使用PyTorch提供的类型丰富的优化函数来完成对模型参数的优化，除此之外，还有很多防止模型在模型训练过程中发生过拟合的类。</p>
<h3 id="6-3-1-PyTorch之torch-nn"><a href="#6-3-1-PyTorch之torch-nn" class="headerlink" title="6.3.1 PyTorch之torch.nn"></a>6.3.1 PyTorch之torch.nn</h3><p>PyTorch中的 torch.nn包提供了很多与实现神经网络中的具体功能相关的类，这些类涵盖了深度神经网络模型在搭建和参数优化过程中的常用内容，比如神经网络中的卷积层、池化层、全连接层这类层次构造的方法、防止过拟合的参数归一化方法、Dropout 方法，还有激活函数部分的线性激活函数、非线性激活函数相关的方法，等等。在学会使用PyTorch的 torch.nn进行神经网络模型的搭建和参数优化后，我们就会发现实现一个神经网络应用并没有我们想象中那么难。</p>
<p>下面使用PyTorch的torch.nn包来简化我们之前的代码，开始部分的代码变化不大，如下所示：</p>
<p><img src="Image00408.jpg" alt="img"></p>
<p>和之前一样，这里首先导入必要的包、类并定义了4个变量，不过这里仅定义了输入和输出的变量，之前定义神经网络模型中的权重参数的代码被删减了，这和我们之后在代码中使用的torch.nn包中的类有关，因为这个类能够帮助我们自动生成和初始化对应维度的权重参数。模型搭建的代码如下：</p>
<p><img src="Image00409.jpg" alt="img"></p>
<p>torch.nn.Sequential括号内的内容就是我们搭建的神经网络模型的具体结构，这里首先通过torch.nn.Linear(input_data, hidden_layer)完成从输入层到隐藏层的线性变换，然后经过激活函数及torch.nn.Linear(hidden_layer, output_data)完成从隐藏层到输出层的线性变换。下面分别对在以上代码中使用的torch.nn.Sequential、torch.nn.Linear和torch.nn.RelU这三个类进行详细介绍。</p>
<p>（1）torch.nn.Sequential<br>：torch.nn.Sequential类是torch.nn中的一种序列容器，通过在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神经网络模型的搭建，最主要的是，参数会按照我们定义好的序列自动传递下去。我们可以将嵌套在容器中的各个部分看作各种不同的模块，这些模块可以自由组合。模块的加入一般有两种方式，一种是在以上代码中使用的直接嵌套，另一种是以orderdict有序字典的方式进行传入，这两种方式的唯一区别是，使用后者搭建的模型的每个模块都有我们自定义的名字，而前者默认使用从零开始的数字序列作为每个模块的名字。下面通过示例来直观地看一下使用这两种方式搭建的模型之间的区别。</p>
<p>首先，使用直接嵌套搭建的模型代码如下：</p>
<p><img src="Image00410.jpg" alt="img"></p>
<p>这里对该模型的结构进行打印输出，结果如下：</p>
<p><img src="Image00411.jpg" alt="img"></p>
<p>使用orderdict有序字典进行传入来搭建的模型代码如下：</p>
<p><img src="Image00412.jpg" alt="img"></p>
<p>这里对该模型的结构进行打印输出，结果如下：</p>
<p><img src="Image00413.jpg" alt="img"></p>
<p>通过对这两种方式进行比较，我们会发现，对模块使用自定义的名称可让我们更便捷地找到模型中相应的模块并进行操作。</p>
<p>（2）torch.nn.Linear<br>：torch.nn.Linear类用于定义模型的线性层，即完成前面提到的不同的层之间的线性变换。torch.nn.Linear类接收的参数有三个，分别是输入特征数、输出特征数和是否使用偏置，设置是否使用偏置的参数是一个布尔值，默认为True，即使用偏置。在实际使用的过程中，我们只需将输入的特征数和输出的特征数传递给torch.nn.Linear类，就会自动生成对应维度的权重参数和偏置，对于生成的权重参数和偏置，我们的模型默认使用了一种比之前的简单随机方式更好的参数初始化方法。</p>
<p>根据我们搭建模型的输入、输出和层次结构需求，它的输入是在一个批次中包含100个特征数为1000的数据，最后得到100个特征数为10的输出数据，中间需要经过两次线性变换，所以要使用两个线性层，两个线性层的代码分别是torch.nn.Linear(input_data,hidden_layer)和 torch.nn.Linear(hidden_layer, output_data)。可看到，其代替了之前使用矩阵乘法方式的实现，代码更精炼、简洁。</p>
<p>（3）torch.nn.ReLU ：torch.nn.ReLU类属于非线性激活分类，在定义时默认不需要传入参数。当然，在 torch.nn包中还有许多非线性激活函数类可供选择，比如之前讲到的PReLU、LeakyReLU、Tanh、Sigmoid、Softmax等。</p>
<p>在掌握torch.nn.Sequential、torch.nn.Linear和torch.nn.RelU的使用方法后，快速搭建更复杂的多层神经网络模型变为可能，而且在整个模型的搭建过程中不需要对在模型中使用到的权重参数和偏置进行任何定义和初始化说明，因为参数已经完成了自动生成。</p>
<p>接下来对已经搭建好的模型进行训练并对参数进行优化，代码如下：</p>
<p><img src="Image00414.jpg" alt="img"></p>
<p>前两句代码和之前的代码没有多大区别，只是单纯地增加了学习速率和训练次数，学习速率现在是0.0001，训练次数增加到了10000次，这样做是为了让最终得到的结果更好。不过计算损失函数的代码发生了改变，现在使用的是在torch.nn包中已经定义好的均方误差函数类torch.nn.MSELoss来计算损失值，而之前的代码是根据损失函数的计算公式来编写的。</p>
<p>下面简单介绍在torch.nn包中常用的损失函数的具体用法，如下所述。</p>
<p>（1）torch.nn.MSELoss<br>：torch.nn.MSELoss类使用均方误差函数对损失值进行计算，在定义类的对象时不用传入任何参数，但在使用实例时需要输入两个维度一样的参数方可进行计算。示例如下：</p>
<p><img src="Image00415.jpg" alt="img"></p>
<p>以上代码首先通过随机方式生成了两个维度都是（100,100）的参数，然后使用均方误差函数来计算两组参数的损失值，打印输出的结果如下：</p>
<p><img src="Image00416.jpg" alt="img"></p>
<p>（2）torch.nn.L1Loss<br>：torch.nn.L1Loss类使用平均绝对误差函数对损失值进行计算，同样，在定义类的对象时不用传入任何参数，但在使用实例时需要输入两个维度一样的参数进行计算。示例如下：</p>
<p><img src="Image00417.jpg" alt="img"></p>
<p>以上代码也是通过随机方式生成了两个维度都是（100,100）的参数，然后使用平均绝对误差函数来计算两组参数的损失值，打印输出的结果如下：</p>
<p><img src="Image00418.jpg" alt="img"></p>
<p>（3）torch.nn.CrossEntropyLoss<br>：torch.nn.CrossEntropyLoss类用于计算交叉熵，在定义类的对象时不用传入任何参数，在使用实例时需要输入两个满足交叉熵的计算条件的参数，代码如下：</p>
<p><img src="Image00419.jpg" alt="img"></p>
<p>这里生成的第1组参数是一个随机参数，维度为（3,5）；第2组参数是3个范围为0～4的随机数字。计算这两组参数的损失值，打印输出的结果如下：</p>
<p><img src="Image00420.jpg" alt="img"></p>
<p>在学会使用PyTorch中的优化函数之后，我们就可以对自己建立的神经网络模型进行训练并对参数进行优化了，代码如下：</p>
<p><img src="Image00421.jpg" alt="img"></p>
<p>以上代码中的绝大部分和之前训练和优化部分的代码是一样的，但是参数梯度更新的方式发生了改变。因为使用了不同的模型搭建方法，所以访问模型中的全部参数是通过对“models.parameters()”进行遍历完成的，然后才对每个遍历的参数进行梯度更新。其打印输入结果的方式是每完成1000次训练，就打印输出当前的loss值，最后输出的结果如下：</p>
<p><img src="Image00422.jpg" alt="img"></p>
<p>从该结果可以看出，参数的优化效果比较理想，loss值被控制在相对较小的范围之内，这和我们增加了训练次数有很大关系。</p>
<h3 id="6-3-2-PyTorch之torch-optim"><a href="#6-3-2-PyTorch之torch-optim" class="headerlink" title="6.3.2 PyTorch之torch.optim"></a>6.3.2 PyTorch之torch.optim</h3><p>到目前为止，代码中的神经网络权重的参数优化和更新还没有实现自动化，并且目前使用的优化方法都有固定的学习速率，所以优化函数相对简单，如果我们自己实现一些高级的参数优化算法，则优化函数部分的代码会变得较为复杂。在PyTorch的torch.optim包中提供了非常多的可实现参数自动优化的类，比如SGD、AdaGrad、RMSProp、Adam等，这些类都可以被直接调用，使用起来也非常方便。我们使用自动化的优化函数实现方法对之前的代码进行替换，新的代码如下：</p>
<p><img src="Image00423.jpg" alt="img"></p>
<p>这里使用了torch.optim包中的torch.optim.Adam类作为我们的模型参数的优化函数，在torch.optim.Adam类中输入的是被优化的参数和学习速率的初始值，如果没有输入学习速率的初始值，那么默认使用0.001这个值。因为我们需要优化的是模型中的全部参数，所以传递给torch.optim.Adam类的参数是models.parameters。另外，Adam优化函数还有一个强大的功能，就是可以对梯度更新使用到的学习速率进行自适应调节，所以最后得到的结果自然会比之前的代码更理想。进行模型训练的代码如下：</p>
<p><img src="Image00424.jpg" alt="img"></p>
<p>optimzer.step()</p>
<p>在以上代码中有几处代码和之前的训练代码不同，这是因为我们引入了优化算法，所以通过直接调用optimzer.zero_grad来完成对模型参数梯度的归零；并且在以上代码中增加了optimzer.step，它的主要功能是使用计算得到的梯度值对各个节点的参数进行梯度更新。这里只进行20次训练并打印每轮训练的loss值，结果如下：</p>
<p><img src="Image00425.jpg" alt="img"></p>
<p>在看到这个结果后我们会很惊讶，因为使用torch.optim.Adam类进行参数优化后仅仅进行了20次训练，得到的loss值就已经远远低于之前进行10000次优化训练的结果。所以，如果对torch.optim中的优化算法类使用得当，就更能帮助我们优化好模型中的参数。</p>
<h2 id="6-4-实战手写数字识别"><a href="#6-4-实战手写数字识别" class="headerlink" title="6.4 实战手写数字识别"></a>6.4 实战手写数字识别</h2><p>我们现在已经学会了基于PyTorch深度学习框架高效、快捷地搭建一个神经网络，并对模型进行训练和对参数进行优化的方法，接下来让我们小试牛刀，基于PyTorch框架使用神经网络来解决一个关于手写数字识别的计算机视觉问题，评价我们搭建的模型的标准是它能否准确地对手写数字图片进行识别。</p>
<p>其具体过程是：先使用已经提供的训练数据对搭建好的神经网络模型进行训练并完成参数优化；然后使用优化好的模型对测试数据进行预测，对比预测值和真实值之间的损失值，同时计算出结果预测的准确率。在将要搭建的模型中会用到卷积神经网络模型，下面让我们开始吧。</p>
<h3 id="6-4-1-torch和torchvision"><a href="#6-4-1-torch和torchvision" class="headerlink" title="6.4.1 torch和torchvision"></a>6.4.1 torch和torchvision</h3><p>在PyTorch中有两个核心的包，分别是torch和torchvision。我们之前已经接触了torch包的一部分内容，比如使用了torch.nn中的线性层加激活函数配合torch.optim完成了神经网络模型的搭建和模型参数的优化，并使用了 torch.autograd实现自动梯度的功能，接下来会介绍如何使用torch.nn中的类来搭建卷积神经网络。</p>
<p>torchvision包的主要功能是实现数据的处理、导入和预览等，所以如果需要对计算机视觉的相关问题进行处理，就可以借用在torchvision包中提供的大量的类来完成相应的工作。</p>
<p>代码中的开始部分如下：</p>
<p><img src="Image00426.jpg" alt="img"></p>
<p>首先，导入必要的包。对这个手写数字识别问题的解决只用到了torchvision中的部分功能，所以这里通过 from torchvision import方法导入其中的两个子包 datasets和transforms，我们将会用到这两个包。</p>
<p>之后，我们就要想办法获取手写数字的训练集和测试集。使用 torchvision.datasets可以轻易实现对这些数据集的训练集和测试集的下载，只需要使用 torchvision.datasets再加上需要下载的数据集的名称就可以了，比如在这个问题中我们要用到手写数字数据集，它的名称是MNIST，那么实现下载的代码就是torchvision.datasets.MNIST。其他常用的数据集如COCO、ImageNet、CIFCAR等都可以通过这个方法快速下载和载入。实现数据集下载的代码如下：</p>
<p><img src="Image00427.jpg" alt="img"></p>
<p><img src="Image00428.jpg" alt="img"></p>
<p>其中，root用于指定数据集在下载之后的存放路径，这里存放在根目录下的data文件夹中；transform用于指定导入数据集时需要对数据进行哪种变换操作，在后面会介绍详细的变换操作类型，注意，要提前定义这些变换操作；train用于指定在数据集下载完成后需要载入哪部分数据，如果设置为True，则说明载入的是该数据集的训练集部分；如果设置为False，则说明载入的是该数据集的测试集部分。</p>
<h3 id="6-4-2-PyTorch之torch-transforms"><a href="#6-4-2-PyTorch之torch-transforms" class="headerlink" title="6.4.2 PyTorch之torch.transforms"></a>6.4.2 PyTorch之torch.transforms</h3><p>在前面讲到过，在torch.transforms中提供了丰富的类对载入的数据进行变换，现在让我们看看如何进行变换。我们知道，在计算机视觉中处理的数据集有很大一部分是图片类型的，而在PyTorch中实际进行计算的是Tensor数据类型的变量，所以我们首先需要解决的是数据类型转换的问题，如果获取的数据是格式或者大小不一的图片，则还需要进行归一化和大小缩放等操作，庆幸的是，这些方法在torch.transforms中都能找到。</p>
<p>在torch.transforms中有大量的数据变换类，其中有很大一部分可以用于实现数据增强（Data Argumentation）。若在我们需要解决的问题上能够参与到模型训练中的图片数据非常有限，则这时就要通过对有限的图片数据进行各种变换，来生成新的训练集了，这些变换可以是缩小或者放大图片的大小、对图片进行水平或者垂直翻转等，都是数据增强的方法。不过在手写数字识别的问题上可以不使用数据增强的方法，因为可用于模型训练的数据已经足够了。对数据进行载入及有相应变化的代码如下：</p>
<p><img src="Image00429.jpg" alt="img"></p>
<p>我们可以将以上代码中的torchvision.transforms.Compose类看作一种容器，它能够同时对多种数据变换进行组合。传入的参数是一个列表，列表中的元素就是对载入的数据进行的各种变换操作。</p>
<p>在以上代码中，在 torchvision.transforms.Compose中只使用了一个类型的转换变换transforms.ToTensor和一个数据标准化变换transforms.Normalize。这里使用的标准化变换也叫作标准差变换法，这种方法需要使用原始数据的均值（Mean）和标准差（Standard Deviation）来进行数据的标准化，在经过标准化变换之后，数据全部符合均值为0、标准差为1的标准正态分布。计算公式如下：</p>
<p><img src="Image00430.jpg" alt="img"></p>
<p>不过我们在这里偷了一个懒，均值和标准差的值并非来自原始数据的，而是自行定义了一个，不过仍然能够达到我们的目的。</p>
<p>下面看看在torchvision.transforms中常用的数据变换操作。</p>
<p>（1）torchvision.transforms.Resize<br>：用于对载入的图片数据按我们需求的大小进行缩放。传递给这个类的参数可以是一个整型数据，也可以是一个类似于（h ,w ）的序列，其中，h 代表高度，w 代表宽度，但是如果使用的是一个整型数据，那么表示缩放的宽度和高度都是这个整型数据的值。</p>
<p>（2）torchvision.transforms.Scale<br>：用于对载入的图片数据按我们需求的大小进行缩放，用法和torchvision.transforms.Resize类似。</p>
<p>（3）torchvision.transforms.CenterCrop<br>：用于对载入的图片以图片中心为参考点，按我们需要的大小进行裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于（h ,w ）的序列。</p>
<p>（4）torchvision.transforms.RandomCrop<br>：用于对载入的图片按我们需要的大小进行随机裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于（h ,w ）的序列。</p>
<p>（5）torchvision.transforms.RandomHorizontalFlip<br>：用于对载入的图片按随机概率进行水平翻转。我们可以通过传递给这个类的参数自定义随机概率，如果没有定义，则使用默认的概率值0.5。</p>
<p>（6）torchvision.transforms.RandomVerticalFlip<br>：用于对载入的图片按随机概率进行垂直翻转。我们可以通过传递给这个类的参数自定义随机概率，如果没有定义，则使用默认的概率值0.5。</p>
<p>（7）torchvision.transforms.ToTensor<br>：用于对载入的图片数据进行类型转换，将之前构成PIL图片的数据转换成Tensor数据类型的变量，让PyTorch能够对其进行计算和处理。</p>
<p>（8）torchvision.transforms.ToPILImage ：用于将Tensor变量的数据转换成PIL图片数据，主要是为了方便图片内容的显示。</p>
<h3 id="6-4-3-数据预览和数据装载"><a href="#6-4-3-数据预览和数据装载" class="headerlink" title="6.4.3 数据预览和数据装载"></a>6.4.3 数据预览和数据装载</h3><p>在数据下载完成并且载入后，我们还需要对数据进行装载。我们可以将数据的载入理解为对图片的处理，在处理完成后，我们就需要将这些图片打包好送给我们的模型进行训练了，而装载就是这个打包的过程。在装载时通过batch_size的值来确认每个包的大小，通过shuffle的值来确认是否在装载的过程中打乱图片的顺序。装载图片的代码如下：</p>
<p><img src="Image00431.jpg" alt="img"></p>
<p>对数据的装载使用的是torch.utils.data.DataLoader类，类中的dataset参数用于指定我们载入的数据集名称，batch_size参数设置了每个包中的图片数据个数，代码中的值是64，所以在每个包中会包含64张图片。将shuffle参数设置为True，在装载的过程会将数据随机打乱顺序并进行打包。</p>
<p>在装载完成后，我们可以选取其中一个批次的数据进行预览。进行数据预览的代码如下：</p>
<p><img src="Image00432.jpg" alt="img"></p>
<p>在以上代码中使用了iter和next来获取一个批次的图片数据和其对应的图片标签，然后使用torchvision.utils中的make_grid类方法将一个批次的图片构造成网格模式。需要传递给torchvision.utils.make_grid的参数就是一个批次的装载数据，每个批次的装载数据都是4维的，维度的构成从前往后分别为batch_size、channel、height和weight，分别对应一个批次中的数据个数、每张图片的色彩通道数、每张图片的高度和宽度。在通过torchvision.utils.make_grid之后，图片的维度变成了（channel,height,weight），这个批次的图片全部被整合到了一起，所以在这个维度中对应的值也和之前不一样了，但是色彩通道数保持不变。</p>
<p>若我们想使用Matplotlib将数据显示成正常的图片形式，则使用的数据首先必须是数组，其次这个数组的维度必须是（height,weight,channel），即色彩通道数在最后面。所以我们要通过numpy和transpose完成原始数据类型的转换和数据维度的交换，这样才能够使用Matplotlib绘制出正确的图像。</p>
<p>在完成数据预览的代码中，我们先打印输出了这个批次中的数据的全部标签，然后才对这个批次中的所有图片数据进行显示，代码如下：</p>
<p><img src="Image00433.jpg" alt="img"></p>
<p>效果如图6-2所示，可以看到，打印输出的首先是64张图片对应的标签，然后是64张图片的预览结果。</p>
<p><img src="Image00434.jpg" alt="img"></p>
<p>图6-2</p>
<h3 id="6-4-4-模型搭建和参数优化"><a href="#6-4-4-模型搭建和参数优化" class="headerlink" title="6.4.4 模型搭建和参数优化"></a>6.4.4 模型搭建和参数优化</h3><p>在顺利完成数据装载后，我们就可以开始编写卷积神经网络模型的搭建和参数优化的代码了。因为我们想要搭建一个包含了卷积层、激活函数、池化层、全连接层的卷积神经网络来解决这个问题，所以模型在结构上会和之前简单的神经网络有所区别，当然，各个部分的功能实现依然是通过torch.nn中的类来完成的，比如卷积层使用torch.nn.Conv2d类方法来搭建；激活层使用 torch.nn.ReLU类方法来搭建；池化层使用 torch.nn.MaxPool2d类方法来搭建；全连接层使用torch.nn.Linear类方法来搭建。</p>
<p>实现卷积神经网络模型搭建的代码如下：</p>
<p><img src="Image00435.jpg" alt="img"></p>
<p>因为这个问题并不复杂，所以我们选择搭建一个在结构层次上有所简化的卷积神经网络模型，在结构上使用了两个卷积层：一个最大池化层和两个全连接层，这里对其具体的使用方法进行补充说明。</p>
<p>（1）torch.nn.Conv2d<br>：用于搭建卷积神经网络的卷积层，主要的输入参数有输入通道数、输出通道数、卷积核大小、卷积核移动步长和Paddingde值。其中，输入通道数的数据类型是整型，用于确定输入数据的层数；输出通道数的数据类型也是整型，用于确定输出数据的层数；卷积核大小的数据类型是整型，用于确定卷积核的大小；卷积核移动步长的数据类型是整型，用于确定卷积核每次滑动的步长；Paddingde 的数据类型是整型，值为0时表示不进行边界像素的填充，如果值大于0，那么增加数字所对应的边界像素层数。</p>
<p>（2）torch.nn.MaxPool2d<br>：用于实现卷积神经网络中的最大池化层，主要的输入参数是池化窗口大小、池化窗口移动步长和Paddingde值。同样，池化窗口大小的数据类型是整型，用于确定池化窗口的大小。池化窗口步长的数据类型也是整型，用于确定池化窗口每次移动的步长。Paddingde值和在torch.nn.Conv2d中定义的Paddingde值的用法和意义是一样的。</p>
<p>（3）torch.nn.Dropout<br>：torch.nn.Dropout类用于防止卷积神经网络在训练的过程中发生过拟合，其工作原理简单来说就是在模型训练的过程中，以一定的随机概率将卷积神经网络模型的部分参数归零，以达到减少相邻两层神经连接的目的。图 6-3显示了 Dropout方法的效果。</p>
<p><img src="Image00436.jpg" alt="img"></p>
<p>图6-3</p>
<p>在图6-3中打叉的神经节点就是被随机抽中并丢弃的神经连接，正是因为选取方式的随机性，所以在模型的每轮训练中选择丢弃的神经连接也是不同的，这样做是为了让我们最后训练出来的模型对各部分的权重参数不产生过度依赖，从而防止过拟合。对于torch.nn.Dropout类，我们可以对随机概率值的大小进行设置，如果不做任何设置，就使用默认的概率值0.5。</p>
<p>最后看看代码中前向传播 forward函数中的内容。首先，经过 self.conv1进行卷积处理；然后进行x.view(-1, 14<em>14</em>128)，对参数实现扁平化，因为之后紧接着的就是全连接层，所以如果不进行扁平化，则全连接层的实际输出的参数维度和其定义输入的维度将不匹配，程序会报错；最后，通过self.dense定义的全连接进行最后的分类。</p>
<p>在编写完搭建卷积神经网络模型的代码后，我们就可以开始对模型进行训练和对参数进行优化了。首先，定义在训练之前使用哪种损失函数和优化函数：</p>
<p><img src="Image00437.jpg" alt="img"></p>
<p>在以上代码中定义了计算损失值的损失函数使用的是交叉熵，也确定了优化函数使用的是Adam自适应优化算法，需要优化的参数是在Model中生成的全部参数，因为没有定义学习速率的值，所以使用默认值；然后，通过打印输出的方式查看搭建好的模型的完整结构，只需使用print(model)就可以了，输出结果如下：</p>
<p><img src="Image00438.jpg" alt="img"></p>
<p>最后，卷积神经网络模型进行模型训练和参数优化的代码如下：</p>
<p><img src="Image00439.jpg" alt="img"></p>
<p><img src="Image00440.jpg" alt="img"></p>
<p>总的训练次数是5次，训练中的大部分代码和之前相比没有大的改动，增加的内容都在原来的基础上加入了更多的打印输出，其目的是更好地显示模型训练过程中的细节，同时，在每轮训练完成后，会使用测试集验证模型的泛化能力并计算准确率。在模型训练过程中打印输出的结果如下：</p>
<p><img src="Image00441.jpg" alt="img"></p>
<p>可以看到，结果表现非常不错，训练集达到的最高准确率为99.73%，而测试集达到的最高准确率为98.96%。如果我们使用功能更强大的卷积神经网络模型，则会取得比现在更好的结果。</p>
<p>为了验证我们训练的模型是不是真的已如结果显示的一样准确，则最好的方法就是随机选取一部分测试集中的图片，用训练好的模型进行预测，看看和真实值有多大的偏差，并对结果进行可视化。测试过程的代码如下：</p>
<p><img src="Image00442.jpg" alt="img"></p>
<p><img src="Image00443.jpg" alt="img"></p>
<p>用于测试的数据标签结果输出的结果如下：</p>
<p><img src="Image00444.jpg" alt="img"></p>
<p>在输出结果中，第1个结果是我们训练好的模型的预测值，第2个结果是这4个测试数据的真实值。对测试数据进行可视化，如图6-4所示。</p>
<p><img src="Image00445.jpg" alt="img"></p>
<p>图6-4</p>
<p>可以看到，在图6-4中可视化的这部分测试集图片，模型的预测结果和真实的结果是完全一致的。当然，如果想选取更多的测试集进行可视化，则只需将batch_size的值设置得更大。</p>
<h1 id="第7章-迁移学习"><a href="#第7章-迁移学习" class="headerlink" title="第7章 迁移学习"></a>第7章 迁移学习</h1><p>我们在之前的章节中学会了使用自己搭建的卷积神经网络模型解决手写图片识别问题，因为卷积神经网络在解决计算机视觉问题上有着独特的优势，所以，采用简单的神经网络模型就能使手写图片识别的准确率达到很高的水平。不过，用来训练和测试模型的手写图片数据的特征非常明显，所以也很容易被卷积神经网络模型捕获到。</p>
<p>本章将通过搭建卷积神经网络模型对生活中的普通图片进行分类，并引入迁移学习（Transfer Learning）方法。为了验证迁移学习方法的方便性和高效性，我们先使用自定义结构的卷积神经网络模型解决图片的分类问题，然后通过使用迁移学习方法得到的模型来解决同样的问题，以此来看看在效果上是传统的方法更出色，还是迁移学习方法更出色。</p>
<h2 id="7-1-迁移学习入门"><a href="#7-1-迁移学习入门" class="headerlink" title="7.1 迁移学习入门"></a>7.1 迁移学习入门</h2><p>在开始之前，我们先来了解一下什么是迁移学习。在深度神经网络算法的应用过程中，如果我们面对的是数据规模较大的问题，那么在搭建好深度神经网络模型后，我们势必要花费大量的算力和时间去训练模型和优化参数，最后耗费了这么多资源得到的模型只能解决这一个问题，性价比非常低。如果我们用这么多资源训练的模型能够解决同一类问题，那么模型的性价比会提高很多，这就促使使用迁移模型解决同一类问题的方法出现。因为该方法的出现，我们通过对一个训练好的模型进行细微调整，就能将其应用到相似的问题中，最后还能取得很好的效果；另外，对于原始数据较少的问题，我们也能够通过采用迁移模型进行有效解决，所以，如果能够选取合适的迁移学习方法，则会对解决我们所面临的问题有很大的帮助。</p>
<p>假如我们现在需要解决一个计算机视觉的图片分类问题，需要通过搭建一个模型对猫和狗的图片进行分类，并且提供了大量的猫和狗的图片数据集。假如我们选择使用卷积神经网络模型来解决这个图片分类问题，则首先要搭建模型，然后不断对模型进行训练，使其预测猫和狗的图片的准确性达到要求的阈值，在这个过程中会消耗大量的时间在参数优化和模型训练上。不久之后我们又面临另一个图片分类问题，这次需要搭建模型对猫和狗的图片进行分类，同样提供了大量的图片数据集，如果已经掌握了迁移学习方法，就不必再重新搭建一套全新的模型，然后耗费大量的时间进行训练了，可以直接使用之前已经得到的模型和模型的参数并稍加改动来满足新的需求。不过，对迁移的模型需要进行重新训练，这是因为最后分类的对象发生了变化，但是重新训练的时间和搭建全新的模型进行训练的时间相对很少，如果调整的仅仅是迁移模型的一小部分，那么重新训练所耗费的时间会更少。通过迁移学习可以节省大量的时间和精力，而且最终得到的结果不会太差，这就是迁移学习的优势和特点。</p>
<p>需要注意的是，在使用迁移学习的过程中有时会导致迁移模型出现负迁移，我们可以将其理解为模型的泛化能力恶化。假如我们将迁移学习用于解决两个毫不相关的问题，则极有可能使最后迁移得到的模型出现负迁移。</p>
<h2 id="7-2-数据集处理"><a href="#7-2-数据集处理" class="headerlink" title="7.2 数据集处理"></a>7.2 数据集处理</h2><p>本章使用的数据集来自Kaggle网站上的“Dogs vs.Cats”竞赛项目，可以通过网络免费下载这些数据集。在这个数据集的训练数据集中一共有25000张猫和狗的图片，其中包含12500张猫的图片和12500张狗的图片。在测试数据集中有12500张图片，不过其中的猫狗图片是无序混杂的，而且没有对应的标签。这些数据集将被用于对模型进行训练和对参数进行优化，以及在最后对模型的泛化能力进行验证。</p>
<h3 id="7-2-1-验证数据集和测试数据集"><a href="#7-2-1-验证数据集和测试数据集" class="headerlink" title="7.2.1 验证数据集和测试数据集"></a>7.2.1 验证数据集和测试数据集</h3><p>在实践中，我们不会直接使用测试数据集对搭建的模型进行训练和优化，而是在训练数据集中划出一部分作为验证集，来评估在每个批次的训练后模型的泛化能力。这样做的原因是如果我们使用测试数据集进行模型训练和优化，那么模型最终会对测试数据集产生拟合倾向，换而言之，我们的模型只有在对测试数据集中图片的类别进行预测时才有极强的准确率，而在对测试数据集以外的图片类别进行预测时会出现非常多的错误，这样的模型缺少泛化能力。所以，为了防止这种情况的出现，我们会把测试数据集从模型的训练和优化过程中隔离出来，只在每轮训练结束后使用。如果模型对验证数据集和测试数据集的预测同时具备高准确率和低损失值，就基本说明模型的参数优化是成功的，模型将具备极强的泛化能力。在本章的实践中我们分别从训练数据集的猫和狗的图片中各抽出 2500 张图片组成一个具有5000张图片的验证数据集。</p>
<p>我们也可以将验证数据集看作考试中的模拟训练测试，将测试数据集看作考试中的最终测试，通过两个结果看测试的整体能力，但是测试数据集最后会有绝对的主导作用。不过本章使用的测试数据集是没有标签的，而且本章旨在证明迁移学习比传统的训练高效，所以暂时不使用在数据集中提供的测试数据集，我们进行的只是模型对验证数据集的准确性的横向比较。</p>
<h3 id="7-2-2-数据预览"><a href="#7-2-2-数据预览" class="headerlink" title="7.2.2 数据预览"></a>7.2.2 数据预览</h3><p>在划分好数据集之后，就可以先进行数据预览了。我们通过数据预览可以掌握数据的基本信息，从而更好地决定如何使用这些数据。</p>
<p>开始部分的代码如下：</p>
<p><img src="Image00446.jpg" alt="img"></p>
<p>在以上代码中先导入了必要的包，和之前不同的是新增加了os包和time包，os包集成了一些对文件路径和目录进行操作的类，time包主要是一些和时间相关的方法。</p>
<p>在获取全部的数据集之后，我们就可以对这些数据进行简单分类了。新建一个名为DogsVSCats的文件夹，在该文件夹下面新建一个名为train和一个名为valid的子文件夹，在子文件夹下面再分别新建一个名为cat的文件夹和一个名为dog的文件夹，最后将数据集中对应部分的数据放到对应名字的文件夹中，之后就可以进行数据的载入了。对数据进行载入的代码如下：</p>
<p><img src="Image00447.jpg" alt="img"></p>
<p>在进行数据的载入时我们使用torch.transforms中的Scale类将原始图片的大小统一缩放至64×64。在以上代码中对数据的变换和导入都使用了字典的形式，因为我们需要分别对训练数据集和验证数据集的数据载入方法进行简单定义，所以使用字典可以简化代码，也方便之后进行相应的调用和操作。</p>
<p>os.path.join就是来自之前提到的 os包的方法，它的作用是将输入参数中的两个名字拼接成一个完整的文件路径。其他常用的os.path类方法如下。</p>
<p>（1）os.path.dirname ：用于返回一个目录的目录名，输入参数为文件的目录。</p>
<p>（2）os.path.exists ：用于测试输入参数指定的文件是否存在。</p>
<p>（3）os.path.isdir ：用于测试输入参数是否是目录名。</p>
<p>（4）os.path.isfile ：用于测试输入参数是否是一个文件。</p>
<p>（5）os.path.samefile ：用于测试两个输入的路径参数是否指向同一个文件。</p>
<p>（6）os.path.split ：用于对输入参数中的目录名进行分割，返回一个元组，该元组由目录名和文件名组成。</p>
<p>下面获取一个批次的数据并进行数据预览和分析，代码如下：</p>
<p>X_example, y_example = next(iter(dataloader[“train”]))</p>
<p>以上代码通过next和iter迭代操作获取一个批次的装载数据，不过因为受到我们之前定义的batch_size值的影响，这一批次的数据只有16张图片，所以X_example和y_example的长度也全部是16，可以通过打印这两个变量来确认。打印输出的代码如下：</p>
<p><img src="Image00448.jpg" alt="img"></p>
<p>输出结果如下：</p>
<p><img src="Image00449.jpg" alt="img"></p>
<p>其中，X_example是Tensor数据类型的变量，因为做了图片大小的缩放变换，所以现在图片的大小全部是64×64了，那么X_example的维度就是（16, 3, 64, 64），16代表在这个批次中有16张图片；3代表色彩通道数，因为原始图片是彩色的，所以使用了R、G、B这三个通道；64代表图片的宽度值和高度值。</p>
<p>y_example也是Tensor数据类型的变量，不过其中的元素全部是0和1。为什么会出现0和1？这是因为在进行数据装载时已经对dog文件夹和cat文件夹下的内容进行了独热编码（One- Hot Encoding），所以这时的0和1不仅是每张图片的标签，还分别对应猫的图片和狗的图片。我们可以做一个简单的打印输出，来验证这个独热编码的对应关系，代码如下：</p>
<p><img src="Image00450.jpg" alt="img"></p>
<p>输出的结果如下：</p>
<p>{‘cat’: 0, ‘dog’: 1}</p>
<p>这样就很明显了，猫的图片标签和狗的图片标签被独热编码后分别被数字化了，相较于使用文字作为图片的标签而言，使用0和1也可以让之后的计算方便很多。不过，为了增加之后绘制的图像标签的可识别性，我们还需要通过image_datasets[“train”].classes将原始标签的结果存储在名为example_clasees的变量中。代码如下：</p>
<p>example_clasees = image_datasets[“train”].classes</p>
<p>print(example_clasees)</p>
<p>输出的内容如下：</p>
<p>[‘cat’, ‘dog’]</p>
<p>example_clasees变量其实是一个列表，而且在这个列表中只有两个元素，分别是dog和cat。我们使用Matplotlib对一个批次的图片进行绘制，具体的代码如下：</p>
<p><img src="Image00451.jpg" alt="img"></p>
<p>打印输出的该批次的所有图片的标签结果如下：</p>
<p><img src="Image00452.jpg" alt="img"></p>
<p>标签对应的图片如图7-1所示。</p>
<p><img src="Image00453.jpg" alt="img"></p>
<p>图7-1</p>
<h2 id="7-3-模型搭建和参数优化"><a href="#7-3-模型搭建和参数优化" class="headerlink" title="7.3 模型搭建和参数优化"></a>7.3 模型搭建和参数优化</h2><p>本节会先基于一个简化的VGGNet架构搭建卷积神经网络模型并进行模型训练和参数优化，然后迁移一个完整的VGG16架构的卷积神经网络模型，最后迁移一个ResNet50架构的卷积神经网络模型，并对比这三个模型在预测结果上的准确性和在泛化能力上的差异。</p>
<h3 id="7-3-1-自定义VGGNet"><a href="#7-3-1-自定义VGGNet" class="headerlink" title="7.3.1 自定义VGGNet"></a>7.3.1 自定义VGGNet</h3><p>我们首先需要搭建一个卷积神经网络模型，考虑到训练时间的成本，我们基于VGG16架构来搭建一个简化版的VGGNet模型，这个简化版模型要求输入的图片大小全部缩放到64×64，而在标准的VGG16架构模型中输入的图片大小应当是224×224的；同时简化版模型删除了VGG16最后的三个卷积层和池化层，也改变了全连接层中的连接参数，这一系列的改变都是为了减少整个模型参与训练的参数数量。简化版模型的搭建代码如下：</p>
<p><img src="Image00454.jpg" alt="img"></p>
<p><img src="Image00455.jpg" alt="img"></p>
<p>在搭建好模型后，通过 print 对搭建的模型进行打印输出来显示模型中的细节，打印输出的代码如下：</p>
<p><img src="Image00456.jpg" alt="img"></p>
<p>输出的内容如下：</p>
<p><img src="Image00457.jpg" alt="img"></p>
<p><img src="Image00458.jpg" alt="img"></p>
<p>然后，定义好模型的损失函数和对参数进行优化的优化函数，代码如下：</p>
<p><img src="Image00459.jpg" alt="img"></p>
<p><img src="Image00460.jpg" alt="img"></p>
<p>在代码中优化函数使用的是Adam，损失函数使用的是交叉熵，训练次数总共是10 次，最后的输出结果如下：</p>
<p><img src="Image00461.jpg" alt="img"></p>
<p><img src="Image00462.jpg" alt="img"></p>
<p><img src="Image00463.jpg" alt="img"></p>
<p>虽然准确率不错，但因为全程使用了计算机的 CPU进行计算，所以整个过程非常耗时，约为492分钟（492=29520/60）。下面我们对原始代码进行适当调整，将在模型训练的过程中需要计算的参数全部迁移至GPUs上，这个过程非常简单和方便，只需重新对这部分参数进行类型转换就可以了，当然，在此之前，我们需要先确认GPUs硬件是否可用，具体的代码如下：</p>
<p><img src="Image00464.jpg" alt="img"></p>
<p>打印输出的结果如下：</p>
<p>True</p>
<p>返回的值是True，这说明我们的GPUs已经具备了被使用的全部条件，如果遇到False，则说明显卡暂时不支持，如果是驱动存在问题，则最简单的办法是将显卡驱动升级到最新版本。</p>
<p>在完成对模型训练过程中参数的迁移之后，新的训练代码如下：</p>
<p><img src="Image00465.jpg" alt="img"></p>
<p><img src="Image00466.jpg" alt="img"></p>
<p>在以上代码中，model = model.cuda()和X, y = Variable(X.cuda()), Variable(y.cuda())就是参与迁移至GPUs的具体代码，在进行10次训练后，输出的结果如下：</p>
<p><img src="Image00467.jpg" alt="img"></p>
<p><img src="Image00468.jpg" alt="img"></p>
<p><img src="Image00469.jpg" alt="img"></p>
<p>从结果可以看出，不仅验证测试集的准确率提升了近10%，而且最后输出的训练耗时缩短到了大约14分钟（14=855/60），与之前的训练相比，耗时大幅下降，明显比使用CPU进行参数计算在效率上高出不少。</p>
<p>到目前为止，我们构建的卷积神经网络模型已经具备了较高的预测准确率了，下面引入迁移学习来看看预测的准确性还能提升多少，看看计算耗时能否进一步缩短。在使用迁移学习时，我们只需对原模型的结构进行很小一部分重新调整和训练，所以预计最后的结果能够有所突破。</p>
<h3 id="7-3-2-迁移VGG16"><a href="#7-3-2-迁移VGG16" class="headerlink" title="7.3.2 迁移VGG16"></a>7.3.2 迁移VGG16</h3><p>下面看看迁移学习的具体实施过程，首先需要下载已经具备最优参数的模型，这需要对我们之前使用的model = Models()代码部分进行替换，因为我们不需要再自己搭建和定义训练的模型了，而是通过代码自动下载模型并直接调用，具体代码如下：</p>
<p>model = models.vgg16(prepare=True)</p>
<p>在以上代码中，我们指定进行下载的模型是VGG16，并通过设置 prepare=True 中的值为True，来实现下载的模型附带了已经优化好的模型参数。这样，迁移学习的第一步就完成了，如果想要查看迁移模型的细节，就可以通过print将其打印输出，输出的结果如下：</p>
<p><img src="Image00470.jpg" alt="img"></p>
<p><img src="Image00471.jpg" alt="img"></p>
<p>下面开始进行迁移学习的第2步，对当前迁移过来的模型进行调整，尽管迁移学习要求我们需要解决的问题之间最好具有很强的相似性，但是每个问题对最后输出的结果会有不一样的要求，而承担整个模型输出分类工作的是卷积神经网络模型中的全连接层，所以在迁移学习的过程中调整最多的也是全连接层部分。其基本思路是冻结卷积神经网络中全连接层之前的全部网络层次，让这些被冻结的网络层次中的参数在模型的训练过程中不进行梯度更新，能够被优化的参数仅仅是没有被冻结的全连接层的全部参数。</p>
<p>下面看看具体的代码。首先，迁移过来的VGG16架构模型在最后输出的结果是1000个，在我们的问题中只需两个输出结果，所以全连接层必须进行调整。模型调整的具体代码如下：</p>
<p><img src="Image00472.jpg" alt="img"></p>
<p>首先，对原模型中的参数进行遍历操作，将参数中的parma.requires_grad全部设置为False，这样对应的参数将不计算梯度，当然也不会进行梯度更新了，这就是之前说到的冻结操作；然后，定义新的全连接层结构并重新赋值给model.classifier。在完成了新的全连接层定义后，全连接层中的parma.requires_grad参数会被默认重置为True，所以不需要再次遍历参数来进行解冻操作。损失函数的loss值依然使用交叉熵进行计算，但是在优化函数中负责优化的参数变成了全连接层中的所有参数，即对 model.classifier.parameters这部分参数进行优化。在调整完模型的结构之后，我们通过打印输出对比其与模型没有进行调整前有什么不同，结果如下：</p>
<p><img src="Image00473.jpg" alt="img"></p>
<p><img src="Image00474.jpg" alt="img"></p>
<p>可以看出，其最大的不同就是模型的最后一部分全连接层发生了变化。下面进行新模型的训练和参数优化，通过5次训练来看看最终的结果表现。最后的输出结果如下：</p>
<p><img src="Image00475.jpg" alt="img"></p>
<p><img src="Image00476.jpg" alt="img"></p>
<p>通过应用迁移学习，最后的结果在准确率上提升非常多，而且仅仅通过5次训练就达到了这个效果，所以迁移学习是一种提升模型泛化能力的非常有效的方法。</p>
<p>下面是对VGG16结构的卷积神经网络模型进行迁移学习的完整代码实现：</p>
<p><img src="Image00477.jpg" alt="img"></p>
<p><img src="Image00478.jpg" alt="img"></p>
<p><img src="Image00479.jpg" alt="img"></p>
<h3 id="7-3-3-迁移ResNet50"><a href="#7-3-3-迁移ResNet50" class="headerlink" title="7.3.3 迁移ResNet50"></a>7.3.3 迁移ResNet50</h3><p>在掌握了迁移学习方法之后，我们就可以进行更多的尝试了，下面来看强大的ResNet架构的卷积神经网络模型的迁移学习。在下面的实例中会将ResNet架构中的ResNet50模型进行迁移，进行模型迁移的代码为model = models.resnet50(pretrained=True)。和迁移VGG16模型类似，在代码中使用resnet50对 vgg16进行替换就完成了对应模型的迁移。对迁移得到的模型进行打印输出，结果显示如下：</p>
<p><img src="Image00480.jpg" alt="img"></p>
<p><img src="Image00481.jpg" alt="img"></p>
<p><img src="Image00482.jpg" alt="img"></p>
<p><img src="Image00483.jpg" alt="img"></p>
<p><img src="Image00484.jpg" alt="img"></p>
<p><img src="Image00485.jpg" alt="img"></p>
<p>同之前迁移VGG16模型一样，我们需要对ResNet50的全连接层部分进行调整，代码调整如下：</p>
<p><img src="Image00486.jpg" alt="img"></p>
<p>因为ResNet50中的全连接层只有一层，所以对代码的调整非常简单，在调整完成后再次将模型结构进行打印输出，来对比与模型调整之前的差异，结果如下：</p>
<p><img src="Image00487.jpg" alt="img"></p>
<p><img src="Image00488.jpg" alt="img"></p>
<p><img src="Image00489.jpg" alt="img"></p>
<p><img src="Image00490.jpg" alt="img"></p>
<p><img src="Image00491.jpg" alt="img"></p>
<p><img src="Image00492.jpg" alt="img"></p>
<p><img src="Image00493.jpg" alt="img"></p>
<p>同样，仅仅是最后一部分全连接层有差异，对迁移得到的模型进行5次训练，最终的结果如下：</p>
<p><img src="Image00494.jpg" alt="img"></p>
<p><img src="Image00495.jpg" alt="img"></p>
<p>可以看到，准确率同样非常理想，和我们之前迁移学习得到的VGG16模型相比在模型预测的准确率上相差不大。</p>
<p>如下是对ResNet50进行迁移学习的完整代码实现：</p>
<p><img src="Image00496.jpg" alt="img"></p>
<p><img src="Image00497.jpg" alt="img"></p>
<p><img src="Image00498.jpg" alt="img"></p>
<p><img src="Image00499.jpg" alt="img"></p>
<h2 id="7-4-小结"><a href="#7-4-小结" class="headerlink" title="7.4 小结"></a>7.4 小结</h2><p>我们通过学习本章发现，GPUs在深度学习的计算优化过程中效率明显高于CPU；迁移学习非常强大，能快速解决同类问题，对于类似的问题不用再从头到尾对模型的全部参数进行优化。我们对于复杂模型的参数优化可能需要数周，采用迁移学习的思路能大大节约时间成本。当然，如果模型的训练结果不很理想，则还可以训练更多的模型层次，优化更多的模型参数，而不是盲目地从头训练。也许正是这些优点使迁移学习得到了广泛应用。</p>
<h1 id="第8章-图像风格迁移实战"><a href="#第8章-图像风格迁移实战" class="headerlink" title="第8章 图像风格迁移实战"></a>第8章 图像风格迁移实战</h1><p>本章将完成一个有趣的应用，基于卷积神经网络实现图像风格迁移（Style Transfer）。和之前基于卷积神经网络的图像分类有所不同，这次是神经网络与艺术的碰撞，再一次证明卷积神经网络对图像特征的提取是如此给力。神经网络和艺术的结合不仅是技术领域的创新，还在艺术领域引起了相关人员的高度关注。基于神经网络图像风格迁移的技术也被集成到了相关的应用软件中，吸引了大量的用户参与和体验。下面先来了解一下图像风格迁移技术的原理和实现方法。</p>
<h2 id="8-1-风格迁移入门"><a href="#8-1-风格迁移入门" class="headerlink" title="8.1 风格迁移入门"></a>8.1 风格迁移入门</h2><p>其实在现实生活中有很多人都在使用与图像风格迁移技术相关的 App，比如在一些App中，我们可以选择一张自己喜欢的照片，然后与一些其他风格的图片进行融合，将我们原始照片的风格进行转变，如图8-1所示。</p>
<p>总的来说，图像风格迁移算法的实现逻辑并不复杂，我们首先选取一幅图像作为基准图像，也可将其叫作内容图像，然后选取另一幅或多幅图像作为我们希望获取相应风格的图像，也可将其叫作风格图像。图像风格迁移的算法就是在保证内容图像的内容完整性的前提下，将风格图像的风格融入内容图像中，使得内容图像的原始风格最后发生了转变，最终的输出图像呈现的将是输入的内容图像的内容和风格图像风格之间的理想融合。当然，如果我们选取的风格图像的风格非常突出，那么最后得到的合成图像的风格和原始图像相比，会有明显的差异。</p>
<p><img src="Image00500.jpg" alt="img"></p>
<p>图8-1</p>
<p>所以，图像风格迁移实现的难点就是如何有效地提取一张图像的风格。和传统的图像风格提取方法不同，我们在基于神经网络的图像风格迁移算法中使用卷积神经网络来完成对图像风格的提取。我们已经在之前的实践中领教过卷积神经网络的强大：我们通过卷积神经网络中的卷积方法获取输入图像的重要特征，最后对提取到的特征进行组合，实现了对图片的分类。</p>
<p>其实，图像风格迁移成功与否对于不同的人而言评判标准也存在很大的差异，所以在数学上也并没有对怎样才算完成了图像风格迁移做出严格的定义。图像的风格包含了丰富的内容，比如图像的颜色、图像的纹理、图像的线条、图像本身想要表达的内在含义，等等。对于普通人而言，若他们觉得两种图像在某些特征上看起来很相似，就会认为它们属于同一个风格体系；但是对于专业人士而言，他们更关注图像深层次的境界是否相同。所以图像风格是否完成了迁移也和每个人的认知相关，我们在实例中更注重图像在视觉的展现上是否完成了风格迁移。</p>
<p>其实早在 20世纪初就有很多学者开始研究图像风格迁移了，当时更多的是通过获取风格图像的纹理、颜色、边角之类的特征来完成风格迁移，更高级一些的通过结合数学中各种图像变换的统计方法来完成风格迁移，不过最后的效果都不理想。直到2015年以后，受到深度神经网络在计算机视觉领域的优异表现的启发，人们借助卷积神经网络中强大的图像特征提取功能，让图像风格迁移的问题得到了看似更好的解决。</p>
<h2 id="8-2-PyTorch图像风格迁移实战"><a href="#8-2-PyTorch图像风格迁移实战" class="headerlink" title="8.2 PyTorch图像风格迁移实战"></a>8.2 PyTorch图像风格迁移实战</h2><p>首先，我们需要获取一张内容图片和一张风格图片；然后定义两个度量值，一个度量叫作内容度量值，另一个度量叫作风格度量值，其中的内容度量值用于衡量图片之间的内容差异程度，风格度量值用于衡量图片之间的风格差异程度；最后，建立神经网络模型，对内容图片中的内容和风格图片的风格进行提取，以内容图片为基准将其输入建立的模型中，并不断调整内容度量值和风格度量值，让它们趋近于最小，最后输出的图片就是内容与风格融合的图片。</p>
<h3 id="8-2-1-图像的内容损失"><a href="#8-2-1-图像的内容损失" class="headerlink" title="8.2.1 图像的内容损失"></a>8.2.1 图像的内容损失</h3><p>内容度量值可以使用均方误差作为损失函数，在代码中定义的图像内容损失如下：</p>
<p><img src="Image00501.jpg" alt="img"></p>
<p>以上代码中的target是通过卷积获取到的输入图像中的内容；weight是我们设置的一个权重参数，用来控制内容和风格对最后合成图像的影响程度；input代表输入图像，target.detach()用于对提取到的内容进行锁定，不需要进行梯度；forward函数用于计算输入图像和内容图像之间的损失值；backward函数根据计算得到的损失值进行后向传播，并返回损失值。</p>
<h3 id="8-2-2-图像的风格损失"><a href="#8-2-2-图像的风格损失" class="headerlink" title="8.2.2 图像的风格损失"></a>8.2.2 图像的风格损失</h3><p>风格度量同样使用均方误差作为损失函数，代码如下：</p>
<p><img src="Image00502.jpg" alt="img"></p>
<p>风格损失计算的代码基本和内容损失计算的代码相似，不同之处是在代码中引入了一个Gram_matrix类定义的实例参与风格损失的计算，这个类的代码如下：</p>
<p><img src="Image00503.jpg" alt="img"></p>
<p>以上代码实现的是格拉姆矩阵（Gram matrix）的功能。我们通过卷积神经网络提取了风格图片的风格，这些风格其实是由数字组成的，数字的大小代表了图片中风格的突出程度，而Gram矩阵是矩阵的内积运算，在运算过后输入到该矩阵的特征图中的大的数字会变得更大，这就相当于图片的风格被放大了，放大的风格再参与损失计算，便能够对最后的合成图片产生更大的影响。</p>
<h3 id="8-2-3-模型搭建和参数优化"><a href="#8-2-3-模型搭建和参数优化" class="headerlink" title="8.2.3 模型搭建和参数优化"></a>8.2.3 模型搭建和参数优化</h3><p>在定义好内容损失和风格损失的计算方法之后，我们还需要搭建一个自定义的模型，并将这两部分内容融入模型中。我们首先要做的是迁移一个卷积神经网络的特征提取部分，即卷积相关的部分，代码如下：</p>
<p><img src="Image00504.jpg" alt="img"></p>
<p>在以上代码中首先迁移了一个VGG16架构的卷积神经网络模型的特征提取部分，然后定义了content_layer和style_layer，分别指定了我们需要在整个卷积过程中的哪一层提取内容，以及在哪一层提取风格。content_losses和 style_losses是两个用于保存内容损失和风格损失的列表；conten_weight和style_weight指定了内容损失和风格损失对我们最后得到的融合图片的影响权重。</p>
<p>搭建图像风格迁移模型的代码如下：</p>
<p><img src="Image00505.jpg" alt="img"></p>
<p><img src="Image00506.jpg" alt="img"></p>
<p>在以上代码中，for layer in list(model)[:8]指明了我们仅仅用到迁移模型特征提取部分的前8层，因为我们的内容提取和风格提取在前8层就已经完成了。然后建立一个空的模型，使用torch.nn.Module类的add_module方法向空的模型中加入指定的层次模块，最后得到我们自定义的图像风格迁移模型。add_module方法传递的参数分别是层次的名字和模块，该模块是使用 isinstance实例检测函数得到的，而名字是对应的层次。在定义好模型之后对其进行打印输出，输出的结果如下：</p>
<p><img src="Image00507.jpg" alt="img"></p>
<p><img src="Image00508.jpg" alt="img"></p>
<p>接下来就是参数优化部分的代码：</p>
<p><img src="Image00509.jpg" alt="img"></p>
<p>在以上代码中使用的优化函数是torch.optim.LBFGS，原因是在这个模型中需要优化的损失值有多个并且规模较大，使用该优化函数可以取得更好的效果。</p>
<h3 id="8-2-4-训练新定义的卷积神经网络"><a href="#8-2-4-训练新定义的卷积神经网络" class="headerlink" title="8.2.4 训练新定义的卷积神经网络"></a>8.2.4 训练新定义的卷积神经网络</h3><p>在完成模型的搭建和优化函数的定义后，就可以开始进行模型的训练和参数的优化了，代码如下：</p>
<p><img src="Image00510.jpg" alt="img"></p>
<p>我们定义了训练次数为300次，使用sl.backward和cl.backward实现了前向传播和后向传播算法。每进行50次训练，便对损失值进行一次打印输出，最后的输出结果如下：</p>
<p><img src="Image00511.jpg" alt="img"></p>
<p>可以看到，风格和内容损失已经降到了一个比较低的值了，这时对风格迁移的图片进行输出，如图8-2所示。</p>
<p><img src="Image00512.jpg" alt="img"></p>
<p>图8-2</p>
<p>输出的图片无论是在颜色的基调上还是在图像的轮廓上，都和风格图片极为相似，但是整个图像的内容仍然没有发生太大的变化。</p>
<p>实现图像风格迁移的完整代码如下：</p>
<p><img src="Image00513.jpg" alt="img"></p>
<p><img src="Image00514.jpg" alt="img"></p>
<p><img src="Image00515.jpg" alt="img"></p>
<p><img src="Image00516.jpg" alt="img"></p>
<h2 id="8-3-小结"><a href="#8-3-小结" class="headerlink" title="8.3 小结"></a>8.3 小结</h2><p>本章展示的是比较基础的图像风格迁移算法，所以这个图像风格迁移过程的实现存在一个比较明显的缺点，就是每次训练只能对其中的一种风格进行迁移，如果需要进行其他风格的迁移，则还需再重新对模型进行训练，而且需要通过对内容和风格设置不同的权重来控制风格调节的方式，这种方式在实际应用中不太理想，在现实中我们需要更高效、智能的实现方式。若有兴趣，则可以深度了解这方面的内容。</p>
<h1 id="第9章-多模型融合"><a href="#第9章-多模型融合" class="headerlink" title="第9章 多模型融合"></a>第9章 多模型融合</h1><p>多模型融合是一种“集百家之所长”的方法。我们在使用单一的模型处理某个问题时很容易遇到模型泛化瓶颈，模型的泛化能力因为一些客观因素受到了限制；另外，在建立好一个模型后，这个模型可能在解决某个问题的能力上表现比较出色，在解决其他问题时效果却不尽如人意。所以，人们开始通过一些科学的方法对优秀的模型进行融合，以突破单个模型对未知问题的泛化能力的瓶颈，并且综合各个模型的优点得到同一个问题的最优解决方法，这就是多模型融合。多模型融合的宗旨就是通过科学的方法融合各个模型的优势，以获得对未知问题的更强的解决能力。</p>
<h2 id="9-1-多模型融合入门"><a href="#9-1-多模型融合入门" class="headerlink" title="9.1 多模型融合入门"></a>9.1 多模型融合入门</h2><p>我们先来看看在使用多模型融合方法融合神经网络模型的过程中会遇到哪些问题。</p>
<p>首先，在使用融合神经网络模型的过程中遇到的第1个问题就是训练复杂神经网络非常耗时，因为优秀的模型一般都是深度神经网络模型，这些网络模型的特点是层次较深、参数较多，所以对融合了多个深度神经网络的模型进行参数训练，会比我们使用单一的深度神经网络模型进行参数训练所耗费的时间要多上好几倍。对于这种情况，我们一般使用两种方法进行解决：挑选一些结构比较简单、网络层次较少的神经网络参与到多模型融合中；如果还想继续使用深度神经网络模型进行多模型融合，就需要使用迁移学习方法来辅助模型的融合，以减少训练耗时。</p>
<p>其次，在对各个模型进行融合时，在融合方法的类型选择上也很让人头痛，因为在选择不同的模型融合方法解决某些问题时其结果的表现不同，而且可以选择是针对模型的过程进行融合，还是仅针对各个模型输出的结果进行融合，这都是值得我们思考的。本章为了方便大家理解和掌握最基本的多模型融合方法，在实践模型融合的实例中选取了相对简单的结果融合法。</p>
<p>结果融合法是针对各个模型的输出结果进行的融合，主要包括结果多数表决、结果直接平均和结果加权平均这三种主要的类型。在结果融合法中有一个比较通用的理论，就是若我们想通过多模型融合来提高输出结果的预测准确率，则各个模型的相关度越低，融合的效果会更好，也就是说各个模型的输出结果的差异性越高，多模型融合的效果就会越好。</p>
<h3 id="9-1-1-结果多数表决"><a href="#9-1-1-结果多数表决" class="headerlink" title="9.1.1 结果多数表决"></a>9.1.1 结果多数表决</h3><p>结果多数表决有点类似于我们现实生活中的多人投票表决，假设有三个人需要对一个问题进行公开表决，每个人手中有且仅有一票而且不能弃权，只能投赞成票或者否决票，最后统计投票的结果；如果其中任意两个人投了赞成票，就算三个人通过了对这个问题的表决；如果其中任意两个人投了否决票，就算三个人没有通过对这个问题的表决。这就是典型的结果多数表决方法，多模型融合使用的结果多数表决也是如此，但是需要注意：在使用这个方法的过程中最好保证我们融合的模型个数为基数，如果为偶数，则极有可能会出现结果无法判断的情况。</p>
<p>下面来看一个使用结果多数表决的方法进行多模型融合的实例，假设我们现在已经拥有三个优化好的模型，而且它们能够独立完成对新输入数据的预测，现在我们向这三个模型分别输入 10个同样的新数据，然后统计模型的预测结果。如果模型预测的结果和真实的结果是一样的，那么我们将该次预测结果记录为True，否则将其记录为False，这三个模型的最终预测结果如表9-1所示。</p>
<p>表9-1</p>
<p><img src="Image00517.jpg" alt="img"></p>
<p>续表</p>
<p><img src="Image00518.jpg" alt="img"></p>
<p>可以看出，这三个模型的预测结果的准确率分别是80%、80%和60%，现在我们使用结果多数表决的方法对统计得到的结果进行融合。以三个模型中的第1个新数据的预测结果为例，模型一对新数据的预测结果为True，模型二对新数据集的预测结果为False，模型三对新数据集的预测结果为True，通过多数表决得到的融合模型对新数据的预测结果为True，其他新数据的预测结果以此类推，最后得到多模型的预测结果如表9-2所示。</p>
<p>表9-2</p>
<p><img src="Image00519.jpg" alt="img"></p>
<p>通过统计结果的准确率可以发现，使用多模型融合后的预测准确率也是80%，虽然在准确率的表现上比最差的模型三要好，但是和模型一和模型二的准确率处于同一水平，没有体现出模型融合的优势，所以这也是我们需要注意的。进行多模型融合并不一定能取得理想的效果，需要使用不同的方法不断地尝试。下面对之前的实例稍微进行改变，如表9-3所示。</p>
<p>表9-3</p>
<p><img src="Image00520.jpg" alt="img"></p>
<p>在进行调整之后，三个模型的预测结果的准确率依然分别是80%、80%和60%，我们再计算一遍多模型融合的预测结果，得到如表9-4所示的预测结果。</p>
<p>表9-4</p>
<p><img src="Image00521.jpg" alt="img"></p>
<p>这时多模型融合的预测结果对10个新数据的预测准确率已经提升到了90%，在预测结果的准确率上超过了被融合的三个模型中的任意一个。为什么预测结果最后会发生这样的改变？这是因为我们扩大了模型三在预测结果上和模型一及模型二的差异性，这也印证了我们之前提到过的通用理论，参与融合的各个模型在输出结果的表现上差异性越高，则最终的融合模型的预测结果越好。</p>
<h3 id="9-1-2-结果直接平均"><a href="#9-1-2-结果直接平均" class="headerlink" title="9.1.2 结果直接平均"></a>9.1.2 结果直接平均</h3><p>结果直接平均追求的是融合各个模型的平均预测水平，以提升模型整体的预测能力，但是与结果多数表决相比，结果直接平均不强调个别模型的突出优势，却可以弥补个别模型的明显劣势，比如在参与融合的模型中有一个模型已经发生了过拟合的问题，另一个模型却发生了欠拟合的问题，但是通过结果直接平均的方法能够很好地综合这两个模型的劣势，最后可预防融合模型过拟合和欠拟合的发生，如图9-1和图9-2所示。</p>
<p><img src="Image00522.jpg" alt="img"></p>
<p>图9-1</p>
<p><img src="Image00523.jpg" alt="img"></p>
<p>图9-2</p>
<p>假设在图9-1中两个模型处理的是同一个分类问题，圆圈和叉号代表不同的类别，则两个模型在泛化能力上的表现都不尽如人意，一个模型出现了严重的过拟合现象，另一个模型出现了严重的欠拟合现象；再看图9-2中通过结果直接平均的融合模型，它在泛化能力上表现不错，受噪声值的影响不大。所以，如果我们对两个模型进行融合并且使用结果直接平均的方法，那么最后得到的结果在一定程度上弥补了各个模型的不足，不仅如此，融合的模型还有可能取得比两个模型更好的泛化能力。</p>
<p>虽然结果直接平均的方法追求的是“平均水平”，但是使用结果直接平均的多模型融合在处理很多问题时取得了优于平均水平的成绩。</p>
<p>下面我们来看一个使用结果直接平均进行多模型融合的实例。依旧假设我们现在已经拥有三个优化好的模型，而且它们能够独立完成对新输入数据的预测，现在我们向这三个模型分别输入 10个新数据，然后统计模型的预测结果。不过，我们对结果的记录使用的不是“True”和“False”，而是直接记录每个模型对新数据预测的可能性值，如果预测正确的可能性值大于50%，那么在计算准确率时就把这个预测结果看作正确的，三个模型的预测结果如表9-5所示。</p>
<p>表9-5</p>
<p><img src="Image00524.jpg" alt="img"></p>
<p>可以看出，这三个模型的预测结果的准确率分别是80%、80%和60%，现在使用多模型融合的方法对这三个模型的输出结果进行直接平均。以三个模型中的第1个预测结果为例，模型一对新数据的预测结果为80%，模型二对新数据的预测结果为30%，模型三对新数据的预测结果为70%，通过直接平均得到的融合模型对新数据的预测结果为60%，其他新数据的预测结果以此类推，最后得到多模型的预测结果如表9-6所示。</p>
<p>表9-6</p>
<p><img src="Image00525.jpg" alt="img"></p>
<p>通过对结果进行简单计算，我们便可以知道使用直接平均方法得到的融合模型的最终预测准确率是90%，在融合模型得到的对新数据的所有预测结果中，预测可能性值低于50%的只有一个，所以结果直接平均在总体的准确率上都好于被融合的三个模型，不过我们在经仔细观察后发现，融合的模型在单个数据的预测能力上并没有完胜其他三个模型，所以这也是结果直接平均的最大不足。</p>
<h3 id="9-1-3-结果加权平均"><a href="#9-1-3-结果加权平均" class="headerlink" title="9.1.3 结果加权平均"></a>9.1.3 结果加权平均</h3><p>我们可以将结果加权平均看作结果直接平均的改进版本，在结果加权平均的方法中会新增一个权重参数，这个权重参数用于控制各个模型对融合模型结果的影响程度。简单来说，我们之前使用结果直接平均融合的模型，其实可以被看作由三个使用了一样的权重参数的模型按照结果加权平均融合而成的。所以结果加权平均的关键是对权重参数的控制，通过对不同模型的权重参数的控制，可以得到不同的模型融合方法，最后影响融合模型的预测结果。</p>
<p>下面再来看一个使用结果加权平均进行多模型融合的实例。假设我们现在已经拥有了两个优化好的模型，不是之前的三个，而且它们能够独立预测新的输入数据，现在，我们向这两个模型分别输入 10个同样的新数据，然后统计模型的预测结果，并直接记录每个模型对新数据预测的可能性值，同样，如果预测正确的可能性值大于50%，那么在计算准确率时把这个预测结果看作正确的，两个模型的预测结果如表9-7所示。</p>
<p>表9-7</p>
<p><img src="Image00526.jpg" alt="img"></p>
<p>可以看出，这两个模型的预测结果的准确率均是80%，如果要对两个模型进行结果加权平均，那么首先需要设定各个模型的权重参数，假设模型一的权重值是0.8，模型二的权重值是0.2，则接下来看看如何使用结果加权平均对输出的结果进行融合。我们在计算过程中首先看到模型一对第1个新数据的预测结果为80%，模型二对第1个新数据的预测结果为30%，通过结果加权平均得到的融合模型对新数据的预测结果为70%，计算方法如下：</p>
<p>70%=0.8× 80%+0.2×30%</p>
<p>其他新数据的预测结果以此类推，最后得到多模型的预测结果如表9-8所示。</p>
<p>表9-8</p>
<p><img src="Image00527.jpg" alt="img"></p>
<p>通过简单计算，我们可以知道使用结果加权平均的方法融合的模型的预测准确率是90%，在我们得到的对新数据的所有预测结果中，预测可能性值低于50%的预测结果只有一个，融合的模型在预测结果准确率的表现上优于被融合的两个模型，而且融合模型对新数据的单个预测值也不低。下面再做一个实验，把模型一的权重值和模型二的权重值进行对调，即模型一的权重值变成了0.2，模型二的权重值变成了0.8，那么我们融合的模型的预测结果如表9-9所示。</p>
<p>表9-9</p>
<p><img src="Image00528.jpg" alt="img"></p>
<p>这次结果的准确率降低到了80%，而且融合模型对新数据的单个预测值明显下降，可见调节各个模型的权重参数对最后的融合模型的结果影响较大。所以在使用权重平均的过程中，我们需要不断尝试使用不同的权重值组合，以达到多模型融合的最优解决方案。</p>
<h2 id="9-2-PyTorch之多模型融合实战"><a href="#9-2-PyTorch之多模型融合实战" class="headerlink" title="9.2 PyTorch之多模型融合实战"></a>9.2 PyTorch之多模型融合实战</h2><p>下面基于PyTorch来实现一个多模型的融合，这里使用的是多模型融合方法中的结果加权平均，其基本思路是首先构建两个卷积神经网络模型，然后使用我们的训练数据集分别对这两个模型进行训练和对参数进行优化，使用优化后的模型对验证集进行预测，并将各模型的预测结果进行加权平均以作为最后的输出结果，通过对输出结果和真实结果的对比，来完成对融合模型准确率的计算。</p>
<p>这里，在训练和优化模型的数据集及验证数据集时依然使用了在第7章中划分好的猫狗数据集。数据导入部分的代码如下：</p>
<p><img src="Image00529.jpg" alt="img"></p>
<p>在完成数据导入之后，就可以开始搭建我们需要使用的两个卷积神经网络模型了。使用VGG16架构和ResNet50架构的卷积神经网络模型参与本次模型的融合，然后按照结果加权平均的方法分别对这两个模型提前拟定好会使用到的权重值，对VGG16模型预测结果给予权重值 0.6，对 ResNet50的预测结果给予权重值0.4。考虑到这两个模型如果从头开始搭建、训练、优化会耗费大量的时间成本，所以我们使用迁移学习的方法来获取模型，具体的代码如下：</p>
<p><img src="Image00530.jpg" alt="img"></p>
<p><img src="Image00531.jpg" alt="img"></p>
<p>在以上代码中，首先通过代码models.vgg16和models.resnet50得到我们想要迁移的两个卷积神经网络模型，并分别将其赋值到model_1和model_2上。因为使用的是之前的猫狗数据，所以最后的输出结果仍然是两个，对模型也需要进行相应的调整，同时我们搭建的模型使用GPUs来计算参数。</p>
<p>接下来开始训练模型。我们通过5次训练来看看最终的输出结果，代码如下：</p>
<p><img src="Image00532.jpg" alt="img"></p>
<p><img src="Image00533.jpg" alt="img"></p>
<p><img src="Image00534.jpg" alt="img"></p>
<p>在输出结果中，Model1代表VGG16模型的输出结果，Model2代表ResNet50模型的输出结果，Blending_Model代表融合模型的输出结果。我们通过观察其输出结果的准确率会发现，通过结果加权平均得到的融合模型在预测结果的准确率上优于 VGG16和ResNet50这两个模型，所以我们进行多模型融合的方法是成功的。</p>
<p>实现多模型融合的完整代码如下：</p>
<p><img src="Image00535.jpg" alt="img"></p>
<p><img src="Image00536.jpg" alt="img"></p>
<p><img src="Image00537.jpg" alt="img"></p>
<p><img src="Image00538.jpg" alt="img"></p>
<h2 id="9-3-小结"><a href="#9-3-小结" class="headerlink" title="9.3 小结"></a>9.3 小结</h2><p>多模型融合的方法其实还是非常受大众喜爱的，比如在 Kaggle比赛中就经常会用到各种各样的多模型融合实例。其实多模型融合的内容不仅仅局限于本章所介绍的内容，因为本章讲到的只是用于模型输出结果的融合方法，而且这些方法还在不断创新，所以我们最主要的还是要发挥自己的想象力和创造力，这样才有可能发现更多、更优秀的模型融合方法。</p>
<h1 id="第10章-循环神经网络"><a href="#第10章-循环神经网络" class="headerlink" title="第10章 循环神经网络"></a>第10章 循环神经网络</h1><p>循环神经网络（Recurrent Neural Network，简称RNN）是深度学习中的重要的内容，和我们之前使用的卷积神经网络有着同等重要的地位。循环神经网络主要被用于处理序列（Sequences）相关的问题，比如在自然语言领域应用循环神经网络的情况就较多；当然，循环神经网络也可以用于解决分类问题，虽然在图片特征的提取上没有卷积神经网络那样强大，但是本章仍然会使用循环神经网络来解决图片分类的问题，并主要讲解循环神经网络的工作机制和原理。</p>
<h2 id="10-1-循环神经网络入门"><a href="#10-1-循环神经网络入门" class="headerlink" title="10.1 循环神经网络入门"></a>10.1 循环神经网络入门</h2><p>之前讲到，卷积神经网络有几个特点：首先，对于一个已搭建好的卷积神经网络模型，它的输入数据的维度是固定的，比如在处理图片分类问题时输入的图片大小是固定的；其次，卷积神经网络模型最后输出的数据的维度也是固定的，比如在图片分类问题中我们最后得到模型的输出结果数量；最后，卷积神经网络模型的层次结构也是固定不变的。但是循环神经网络与之不同，因为在循环神经网络中循环单元可以随意控制输入数据及输出数据的数量，具有非常大的灵活性。如图10-1所示就是这两种模式之间的一个简单对比。</p>
<p><img src="Image00539.jpg" alt="img"></p>
<p>图10-1</p>
<p>在图10-1中一共绘制了4种类型的网络结构，分别是一对一、一对多和两种多对一。可以将一对一的网络结构看作一个简单的卷积神经网络模型，固定维度的输入和固定维度的输出。在一对多的网络结构中引入了循环单元，通过一个输入得到数量不等的输出。多对多的网络结构同样是一种循环模式，通过数量不等的输入得到数量不等的输出。</p>
<p>下面我们进一步对循环神经网络进行了解，如图 10-2 所示是循环神经网络的网络简化模型。</p>
<p><img src="Image00540.jpg" alt="img"></p>
<p>图10-2</p>
<p>图10-2中的X是整个模型的输入层，RNN代表循环神经网络中的循环层（Recurrent Layers），Y是整个模型的输出层。图10-2用最简单的方式诠释了循环神经网络中的循环过程，通过不断地对自身的网络结构进行复制来构造不同的循环神经网络模型。图 10-3是对图10-2的展开，这样我们能够更明白它的工作流程。</p>
<p><img src="Image00541.jpg" alt="img"></p>
<p>图10-3</p>
<p>图10-3的结构其实是循环神经网络中的多对多类型，其中，从X1 到X n 代表模型的输入层，从Y1 到Y n 代表模型的输出层，H0 是最初输入的隐藏层，在一般情况下该隐藏层使用的是零初始化，即它的全部参数都是零。</p>
<p>图10-4展示了图10-3中RNN所代表的循环层内部的运算细节。</p>
<p><img src="Image00542.jpg" alt="img"></p>
<p>图10-4</p>
<p>图 10-4 中的虚线部分是循环层中的内容，假设我们截取的是一个循环神经网络中的第t<br>+1个循环单元，W表示权重参数，而tanh是使用的激活函数，则根据图10-4的运算流程可以得到一个计算公式，公式如下：</p>
<p>Ht + 1 =tanh(Ht ×WHH +Xt +1 ×WXH )</p>
<p>如果在计算过程中还使用了偏置，那么计算公式变为：</p>
<p>Ht + 1 =tanh(Ht ×WHH +bHH +Xt +1 ×WXH +bXH )</p>
<p>在得到了Ht +1 后，可以通过如下公式计算输出结果：</p>
<p>Yt +1 =Ht +1 ×WHY</p>
<p>虽然循环神经网络已经能够很好地对输入的序列数据进行处理，但它有一个弊端，就是不能进行长期记忆，其带来的影响就是如果近期输入的数据发生了变化，则会对当前的输出结果产生重大影响。为了避免这种情况的出现，研究者开发了LSTM（Long Short Term Memory）类型的循环神经网络模型。</p>
<p>下面使用循环神经网络解决一个计算机视觉问题，这就是之前的手写数字识别问题。</p>
<h2 id="10-2-PyTorch之循环神经网络实战"><a href="#10-2-PyTorch之循环神经网络实战" class="headerlink" title="10.2 PyTorch之循环神经网络实战"></a>10.2 PyTorch之循环神经网络实战</h2><p>下面来看几段比较重要的代码，首先是循环神经网络模型搭建的相关代码：</p>
<p><img src="Image00543.jpg" alt="img"></p>
<p>在代码中构建循环层使用的是torch.nn.RNN类，在这个类中使用的几个比较重要的参数如下。</p>
<p>（1）input_size ：用于指定输入数据的特征数。</p>
<p>（2）hidden_size ：用于指定最后隐藏层的输出特征数。</p>
<p>（3）num_layers ：用于指定循环层堆叠的数量，默认会使用1。</p>
<p>（4）bias ：这个值默认是True，如果我们将其指定为False，就代表我们在循环层中不再使用偏置参与计算。</p>
<p>（5）batch_first ：在我们的循环神经网络模型中输入层和输出层用到的数据的默认维度是(seq, batch, feature)，其中，seq为序列的长度，batch为数据批次的数量，feature为输入或输出的特征数。如果我们将该参数指定为True，那么输入层和输出层的数据维度将重新对应为(batch, seq, feature)。</p>
<p>在以上代码中，我们定义的input_size为28，这是因为输入的手写数据的宽高为28×28，所以可以将每一张图片看作序列长度为28且每个序列中包含28个数据的组合。模型最后输出的结果是用作分类的，所以仍然需要输出10个数据，在代码中的体现就是self.output= torch.nn.Linear(128,10)。再来看前向传播函数forward中的两行代码，首先是output,_ =self.rnn(input, None)，其中包含两个输入参数，分别是input输出数据和H0 的参数。在循环神经网络模型中，对H0 的初始化我们一般采用0初始化，所以这里传入的参数就是None。再看代码output = self.output(output[:,-1,:])，因为我们的模型需要处理的是分类问题，所以需要提取最后一个序列的输出结果作为当前循环神经网络模型的输出。</p>
<p>在搭建好模型后，就可以对模型进行打印输出了，打印输出的代码如下：</p>
<p><img src="Image00544.jpg" alt="img"></p>
<p>打印输出的结果为：</p>
<p><img src="Image00545.jpg" alt="img"></p>
<p>然后对我们的模型进行训练，这里进行 10次训练，来查看最后的输出结果，训练代码如下：</p>
<p><img src="Image00546.jpg" alt="img"></p>
<p><img src="Image00547.jpg" alt="img"></p>
<p>需要注意的是，我们在进行数据输入时，首先需要对输入特征数进行维度变更，代码为X_train = X_train.view(-1,28,28)，因为这样才能够对应我们之前定义的输入数据的维度(batch, seq, feature)。同样，在每轮训练中都将损失值进行打印输出，在经过10轮训练后得到的输出结果如下：</p>
<p><img src="Image00548.jpg" alt="img"></p>
<p><img src="Image00549.jpg" alt="img"></p>
<p>可以看出，输出的准确率比较高而且有较低的损失值，这说明我们的模型已经非常不错了。然后对结果进行测试，测试代码如下：</p>
<p><img src="Image00550.jpg" alt="img"></p>
<p>打印输出测试图片对应的标签，结果如下：</p>
<p><img src="Image00551.jpg" alt="img"></p>
<p><img src="Image00552.jpg" alt="img"></p>
<p>通过Matplotlib对测试用到的图片进行绘制，效果如图10-5所示。</p>
<p><img src="Image00553.jpg" alt="img"></p>
<p>图10-5</p>
<p>从最后的输出结果和图片可以看出，错误率已经非常低了，这说明我们搭建的循环神经网络模型已经能够很好地解决图片分类的问题了。</p>
<p>使用循环神经网络解决手写数字识别问题的完整代码如下：</p>
<p><img src="Image00554.jpg" alt="img"></p>
<p><img src="Image00555.jpg" alt="img"></p>
<p><img src="Image00556.jpg" alt="img"></p>
<h2 id="10-3-小结"><a href="#10-3-小结" class="headerlink" title="10.3 小结"></a>10.3 小结</h2><p>循环神经网络模型目前主要应用于自然语言处理领域，不过在计算式视觉的相关问题上也能够看到循环神经网络的身影。比如，我们在使用卷积神经网络识别出一张图片中的多个对象后，就可以通过循环神经网络依据识别的目标对象生成一个图片摘要。又比如，我们可以应用循环神经网络处理连续的视频数据，因为一个完整的视频画面是由它的最小单位帧构成的，每一帧画面都可以作为一个输入数据进行处理，这就变成了一个序列问题。这样的例子还有很多，我们可以不断地发现和发掘，让循环神经网络和卷积神经网络有效结合起来，这必然能够开拓计算机视觉领域的新思路。</p>
<h1 id="第11章-自动编码器"><a href="#第11章-自动编码器" class="headerlink" title="第11章 自动编码器"></a>第11章 自动编码器</h1><p>自动编码器（AutoEncoder）是一种可以进行无监督学习的神经网络模型。一般而言，一个完整的自动编码器主要由两部分组成，分别是用于核心特征提取的编码部分和可以实现数据重构的解码部分。下面介绍自动编码器中编码部分和解码部分的具体内容。</p>
<h2 id="11-1-自动编码器入门"><a href="#11-1-自动编码器入门" class="headerlink" title="11.1 自动编码器入门"></a>11.1 自动编码器入门</h2><p>在自动编码器中负责编码的部分也叫作编码器（Encoder），而负责解码的部分也叫作解码器（Decoder）。编码器主要负责对原始的输入数据进行压缩并提取数据中的核心特征，而解码器主要是对在编码器中提取的核心特征进行展开并重新构造出之前的输入数据。我们通过图11-1来看看数据在自动编码器模型中的流程。</p>
<p>如图11-1所示就是一个简化的自动编码器模型，它的主要结构是神经网络，该模型的最左边是用于数据输入的输入层，在输入数据通过神经网络的层层传递之后得到了中间输入数据的核心特征，这就完成了在自编码器中输入数据的编码过程。然后，将输入数据的核心特征再传递到一个逆向的神经网络中，核心特征会被解压并重构，最后得到了一个和输入数据相近的输出数据，这就是自动编码器中的解码过程。输入数据通过自动编码器模型的处理后又被重新还原了。</p>
<p><img src="Image00557.jpg" alt="img"></p>
<p>图11-1</p>
<p>我们会好奇自动编码器模型这种先编码后解码的神经网络模型到底有什么作用，下面进行讲解。自动编码器模型的最大用途就是实现输入数据的清洗，比如去除输入数据中的噪声数据、对输入数据的某些关键特征进行增强和放大，等等。举一个比较简单的例子，假设我们现在有一些被打上了马赛克的图片需要进行除码处理，这时就可以通过自动编码器模型来解决这个问题。其实可以将这个除码的过程看作对数据进行除噪的过程，这也是我们接下来会实现的实践案例。下面看看具体如何实现基于PyTorch的自动编码器。</p>
<h2 id="11-2-PyTorch之自动编码实战"><a href="#11-2-PyTorch之自动编码实战" class="headerlink" title="11.2 PyTorch之自动编码实战"></a>11.2 PyTorch之自动编码实战</h2><p>本节的自动编码器模型解决的是一个去除图片马赛克的问题。要训练出这个模型，我们首先需要生成一部分有马赛克的图片，实现图片打码操作的代码如下：</p>
<p>noisy_images = images+ 0.5<em>np.random.randn(</em>images.shape)</p>
<p>noisy_images = np.clip(noisy_images, 0., 1.)</p>
<p>以上代码中的 images是我们现有的正常图片，我们知道图片是由像素点构成的，而像素点其实就是一个个的数字，我们使用的 MNIST数据集中的手写图片的像素点数字的范围是0到1，所以处理马赛克的一种简单方式就是对原始图片中的像素点进行扰乱，我们在这里通过对输入的原始图片加上一个维度相同的随机数字来达到了处理马赛克的目的。假设我们原始的输入图片如图11-2所示。</p>
<p><img src="Image00558.jpg" alt="img"></p>
<p>图11-2</p>
<p><img src="Image00559.jpg" alt="img"></p>
<p>图11-3</p>
<p>则经过我们的打码处理后，图片如图11-3所示。我们现在已经有了获取大量马赛克图片的方法，下面就可以搭建自动编码器模型了。搭建自动编码器模型最常用的两种方式分别是使用线性变换来构建模型中的神经网络和使用卷积变换来构建模型中的神经网络，下面我们先来看看如何使用线性变换的方式来实现。</p>
<h3 id="11-2-1-通过线性变换实现自动编码器模型"><a href="#11-2-1-通过线性变换实现自动编码器模型" class="headerlink" title="11.2.1 通过线性变换实现自动编码器模型"></a>11.2.1 通过线性变换实现自动编码器模型</h3><p>线性变换的方式仅使用线性映射和激活函数作为神经网络结构的主要组成部分，代码如下：</p>
<p><img src="Image00560.jpg" alt="img"></p>
<p><img src="Image00561.jpg" alt="img"></p>
<p>以上代码中的self.encoder对应的是自动编码器中的编码部分，在这个过程中实现了输入数据的数据量从224个到128个再到64个最后到32个的压缩过程，这32个数据就是我们提取到的核心特征。self.decoder对应的是自动编码器中的解码部分，这个过程实现了从32个到64个再到128个最后到224个的逆向解压过程。在模型搭建完成后对其进行打印输出，结果如下：</p>
<p><img src="Image00562.jpg" alt="img"></p>
<p>然后对我们定义好的模型进行训练，训练代码如下：</p>
<p><img src="Image00563.jpg" alt="img"></p>
<p><img src="Image00564.jpg" alt="img"></p>
<p>在以上代码中损失函数使用的是torch.nn.MSELoss，即计算的是均方误差，我们在之前处理的都是图片分类相关的问题，所以在这里使用交叉熵来计算损失值。而在这个问题中我们需要衡量的是图片在去码后和原始图片之间的误差，所以选择均方误差这类损失函数作为度量。总体的训练流程是我们首先获取一个批次的图片，然后对这个批次的图片进行打码处理并裁剪到指定的像素值范围内，因为之前说过，在MNIST数据集使用的图片中每个像素点的数字值在0到1之间。在得到了经过打码处理的图片后，将其输入搭建好的自动编码器模型中，经过模型处理后输出一个预测图片，用这个预测图片和原始图片进行损失值计算，通过这个损失值对模型进行后向传播，最后就能得到去除图片马赛克效果的模型了。</p>
<p>在每轮训练中，我们都对预测图片和原始图片计算得到的损失值进行输出，在训练10轮之后，输出的结果如下：</p>
<p><img src="Image00565.jpg" alt="img"></p>
<p>从以上结果可以看出，我们得到的损失值在逐渐减小，而且损失值已经在一个足够小的范围内了。最后，我们通过使用一部分测试数据集中的图片来验证我们的模型能否正常工作，代码如下：</p>
<p><img src="Image00566.jpg" alt="img"></p>
<p><img src="Image00567.jpg" alt="img"></p>
<p>在运行代码后，我们得到的第1张输出图片绘制了我们使用的测试集中的图片经过打码后的效果，如图11-4所示。</p>
<p><img src="Image00568.jpg" alt="img"></p>
<p>图11-4</p>
<p>输出的第2张图片绘制了打码图片经过我们训练好的自动编码器模型处理后的效果，如图11-5所示。</p>
<p><img src="Image00569.jpg" alt="img"></p>
<p>图11-5</p>
<p>可以看到，最后的输出结果虽然有些模糊，但是已经基本达到了和原始图片同等水平的可辨度，而且图片中的乱码基本被清除了。</p>
<p>这部分的完整代码如下：</p>
<p><img src="Image00570.jpg" alt="img"></p>
<p><img src="Image00571.jpg" alt="img"></p>
<p><img src="Image00572.jpg" alt="img"></p>
<h3 id="11-2-2-通过卷积变换实现自动编码器模型"><a href="#11-2-2-通过卷积变换实现自动编码器模型" class="headerlink" title="11.2.2 通过卷积变换实现自动编码器模型"></a>11.2.2 通过卷积变换实现自动编码器模型</h3><p>以卷积变换的方式和以线性变换方式构建的自动编码器模型会有较大的区别，而且相对复杂一些，卷积变换的方式仅使用卷积层、最大池化层、上采样层和激活函数作为神经网络结构的主要组成部分，代码如下：</p>
<p><img src="Image00573.jpg" alt="img"></p>
<p><img src="Image00574.jpg" alt="img"></p>
<p>在以上代码中出现了一个我们之前从来没有接触过的上采样层，即torch.nn.Upsample类。这个类的作用就是对我们提取到的核心特征进行解压，实现图片的重写构建，传递给它的参数一共有两个，分别是scale_factor和mode：前者用于确定解压的倍数；后者用于定义图片重构的模式，可选择的模式有nearest、linear、bilinear和trilinear，其中nearest是最邻近法，linear是线性插值法，bilinear是双线性插值法，trilinear是三线性插值法。因为在我们的代码中使用的是最邻近法，所以这里通过一张图片来看一下最邻近法的具体工作方式，如图11-6所示。</p>
<p><img src="Image00575.jpg" alt="img"></p>
<p>图11-6</p>
<p>在模型搭建完成后对其进行打印输出，结果如下：</p>
<p><img src="Image00576.jpg" alt="img"></p>
<p><img src="Image00577.jpg" alt="img"></p>
<p>然后对我们定义好的模型进行训练，训练代码如下：</p>
<p><img src="Image00578.jpg" alt="img"></p>
<p><img src="Image00579.jpg" alt="img"></p>
<p>我们在每轮训练中都对预测图片和原始图片计算得到的损失值进行输出，在训练5轮之后，输出结果如下：</p>
<p><img src="Image00580.jpg" alt="img"></p>
<p>可以看出，这比之前使用的以线性变换方式构建的自动编码器模型好很多。</p>
<p>最后，我们通过使用一部分测试数据集中的图片来验证我们的模型能否正常工作，输出的第1张图片绘制了我们使用的测试集中的图片经过打码后的效果，如图11-7所示。</p>
<p><img src="Image00581.jpg" alt="img"></p>
<p>图11-7</p>
<p>输出的第2张图片绘制了已打码的图片经过我们训练好的自动编码器模型处理后的效果，如图11-8所示。</p>
<p><img src="Image00582.jpg" alt="img"></p>
<p>图11-8</p>
<p>首先，在结果的可视性上没有问题；其次，去码的效果更好，还原出来的图片内容更清晰。</p>
<p>这部分的完整代码如下：</p>
<p><img src="Image00583.jpg" alt="img"></p>
<p><img src="Image00584.jpg" alt="img"></p>
<p><img src="Image00585.jpg" alt="img"></p>
<h2 id="11-3-小结"><a href="#11-3-小结" class="headerlink" title="11.3 小结"></a>11.3 小结</h2><p>本章处理的问题比较简单，所使用的自动编码器模型的网络结构也不复杂，如果输入数据具有更高的维度，那么可以尝试增加我们的模型层次进行应对。</p>
<p>不过本章介绍的主要是一种方法，使用自动编码器这种非监督学习的神经网络模型同样可以解决很多和计算机视觉相关的问题，虽然监督学习方法在目前仍然是主流，但是通过无监督学习和监督学习可以处理更多、更复杂的问题。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      <div>    
       
       
      <ul class="post-copyright">
        <li class="post-copyright-author">
            <strong>本文作者：</strong>hac_lang
        </li>
        <li class="post-copyright-link">
          <strong>本文链接：</strong>
          <a href="/2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/" title="book_《深度学习之PyTorch实战计算机视觉》_唐进民">2018/09/12/book-《深度学习之PyTorch实战计算机视觉》-唐进民/</a>
        </li>
        <li class="post-copyright-license">
          <strong>版权声明： </strong>
          许可协议，请勿用于商业，转载注明出处！
        </li>
      </ul>
      
      </div>
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/计算机/" rel="tag"># 计算机</a>
          
            <a href="/tags/自评/" rel="tag"># 自评</a>
          
            <a href="/tags/books/" rel="tag"># books</a>
          
            <a href="/tags/计算机视觉/" rel="tag"># 计算机视觉</a>
          
            <a href="/tags/更毕/" rel="tag"># 更毕</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/计算科学/" rel="tag"># 计算科学</a>
          
            <a href="/tags/科普/" rel="tag"># 科普</a>
          
            <a href="/tags/豆瓣3/" rel="tag"># 豆瓣3</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/15/book-《scikit-learn机器学习：常用算法原理及编程实战》-黄永昌/" rel="next" title="book_《scikit-learn机器学习：常用算法原理及编程实战》_黄永昌">
                <i class="fa fa-chevron-left"></i> book_《scikit-learn机器学习：常用算法原理及编程实战》_黄永昌
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/25/book-《机器学习实战：基于Scikit-Learn和TensorFlow》-王静源等译/" rel="prev" title="book_《机器学习实战：基于Scikit-Learn和TensorFlow》_王静源等译">
                book_《机器学习实战：基于Scikit-Learn和TensorFlow》_王静源等译 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
    <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="hac_lang">
  
  <p class="site-author-name" itemprop="name">hac_lang</p>
  <div class="site-description motion-element" itemprop="description">小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理</div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    

    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>







  <div class="links-of-author motion-element">
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://github.com/HACLANG" title="GitHub &rarr; https://github.com/HACLANG" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://stackoverflow.com/yourname" title="StackOverflow &rarr; https://stackoverflow.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://gitter.im" title="Gitter &rarr; https://gitter.im" rel="noopener" target="_blank"><i class="fa fa-fw fa-github-alt"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.jianshu.com/u/442ddccf3f32" title="简书 &rarr; https://www.jianshu.com/u/442ddccf3f32" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="Quora &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://plus.google.com/yourname" title="Google &rarr; https://plus.google.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="mailto:haclang.org@gmail.com" title="E-Mail &rarr; mailto:haclang.org@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="skype:haclang?call|chat" title="Skype &rarr; skype:haclang?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://twitter.com/haclang2" title="Twitter &rarr; https://twitter.com/haclang2" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="FaceBook &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
    
  </div>








          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#内容简介"><span class="nav-text">内容简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#前-言"><span class="nav-text">前 言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-浅谈人工智能、神经网络和计算机视觉"><span class="nav-text">第1章 浅谈人工智能、神经网络和计算机视觉</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-人工还是智能"><span class="nav-text">1.1 人工还是智能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-人工智能的三起两落"><span class="nav-text">1.2 人工智能的三起两落</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-两起两落"><span class="nav-text">1.2.1 两起两落</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-卷土重来"><span class="nav-text">1.2.2 卷土重来</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-神经网络简史"><span class="nav-text">1.3 神经网络简史</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-生物神经网络和人工神经网络"><span class="nav-text">1.3.1 生物神经网络和人工神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-M-P模型"><span class="nav-text">1.3.2 M-P模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-感知机的诞生"><span class="nav-text">1.3.3 感知机的诞生</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-你好，深度学习"><span class="nav-text">1.3.4 你好，深度学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-计算机视觉"><span class="nav-text">1.4 计算机视觉</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-深度学习"><span class="nav-text">1.5 深度学习+</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-1-图片分类"><span class="nav-text">1.5.1 图片分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-2-图像的目标识别和语义分割"><span class="nav-text">1.5.2 图像的目标识别和语义分割</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-3-自动驾驶"><span class="nav-text">1.5.3 自动驾驶</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-4-图像风格迁移"><span class="nav-text">1.5.4 图像风格迁移</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第2章-相关的数学知识"><span class="nav-text">第2章 相关的数学知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-矩阵运算入门"><span class="nav-text">2.1 矩阵运算入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-标量、向量、矩阵和张量"><span class="nav-text">2.1.1 标量、向量、矩阵和张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-矩阵的转置"><span class="nav-text">2.1.2 矩阵的转置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-矩阵的基本运算"><span class="nav-text">2.1.3 矩阵的基本运算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-导数求解"><span class="nav-text">2.2 导数求解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-一阶导数的几何意义"><span class="nav-text">2.2.1 一阶导数的几何意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-初等函数的求导公式"><span class="nav-text">2.2.2 初等函数的求导公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-初等函数的和、差、积、商求导"><span class="nav-text">2.2.3 初等函数的和、差、积、商求导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-复合函数的链式法则"><span class="nav-text">2.2.4 复合函数的链式法则</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第3章-深度神经网络基础"><span class="nav-text">第3章 深度神经网络基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-监督学习和无监督学习"><span class="nav-text">3.1 监督学习和无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-监督学习"><span class="nav-text">3.1.1 监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-无监督学习"><span class="nav-text">3.1.2 无监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-小结"><span class="nav-text">3.1.3 小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-欠拟合和过拟合"><span class="nav-text">3.2 欠拟合和过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-欠拟合"><span class="nav-text">3.2.1 欠拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-过拟合"><span class="nav-text">3.2.2 过拟合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-后向传播"><span class="nav-text">3.3 后向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-损失和优化"><span class="nav-text">3.4 损失和优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-损失函数"><span class="nav-text">3.4.1 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-2-优化函数"><span class="nav-text">3.4.2 优化函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-激活函数"><span class="nav-text">3.5 激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-1-Sigmoid"><span class="nav-text">3.5.1 Sigmoid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-2-tanh"><span class="nav-text">3.5.2 tanh</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-3-ReLU"><span class="nav-text">3.5.3 ReLU</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-本地深度学习工作站"><span class="nav-text">3.6 本地深度学习工作站</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-1-GPU和CPU"><span class="nav-text">3.6.1 GPU和CPU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-2-配置建议"><span class="nav-text">3.6.2 配置建议</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第4章-卷积神经网络"><span class="nav-text">第4章 卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-卷积神经网络基础"><span class="nav-text">4.1 卷积神经网络基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-卷积层"><span class="nav-text">4.1.1 卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-池化层"><span class="nav-text">4.1.2 池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-全连接层"><span class="nav-text">4.1.3 全连接层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-LeNet模型"><span class="nav-text">4.2 LeNet模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-AlexNet模型"><span class="nav-text">4.3 AlexNet模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-VGGNet模型"><span class="nav-text">4.4 VGGNet模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-GoogleNet"><span class="nav-text">4.5 GoogleNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-ResNet"><span class="nav-text">4.6 ResNet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第5章-Python基础"><span class="nav-text">第5章 Python基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-Python简介"><span class="nav-text">5.1 Python简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Jupyter-Notebook"><span class="nav-text">5.2 Jupyter Notebook</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-1-Anaconda的安装与使用"><span class="nav-text">5.2.1 Anaconda的安装与使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-环境管理"><span class="nav-text">5.2.2 环境管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-3-环境包管理"><span class="nav-text">5.2.3 环境包管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-4-Jupyter-Notebook的安装"><span class="nav-text">5.2.4 Jupyter Notebook的安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-5-Jupyter-Notebook的使用"><span class="nav-text">5.2.5 Jupyter Notebook的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-6-Jupyter-Notebook常用的快捷键"><span class="nav-text">5.2.6 Jupyter Notebook常用的快捷键</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-Python入门"><span class="nav-text">5.3 Python入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-1-Python的基本语法"><span class="nav-text">5.3.1 Python的基本语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-2-Python变量"><span class="nav-text">5.3.2 Python变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-3-常用的数据类型"><span class="nav-text">5.3.3 常用的数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-4-Python运算"><span class="nav-text">5.3.4 Python运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-5-Python条件判断语句"><span class="nav-text">5.3.5 Python条件判断语句</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-6-Python循环语句"><span class="nav-text">5.3.6 Python循环语句</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-7-Python中的函数"><span class="nav-text">5.3.7 Python中的函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-8-Python中的类"><span class="nav-text">5.3.8 Python中的类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-Python中的NumPy"><span class="nav-text">5.4 Python中的NumPy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-1-NumPy的安装"><span class="nav-text">5.4.1 NumPy的安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-2-多维数组"><span class="nav-text">5.4.2 多维数组</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-3-多维数组的基本操作"><span class="nav-text">5.4.3 多维数组的基本操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-Python中的Matplotlib"><span class="nav-text">5.5 Python中的Matplotlib</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-1-Matplotlib的安装"><span class="nav-text">5.5.1 Matplotlib的安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-2-创建图"><span class="nav-text">5.5.2 创建图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第6章-PyTorch基础"><span class="nav-text">第6章 PyTorch基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-PyTorch中的Tensor"><span class="nav-text">6.1 PyTorch中的Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-1-Tensor的数据类型"><span class="nav-text">6.1.1 Tensor的数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-2-Tensor的运算"><span class="nav-text">6.1.2 Tensor的运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-3-搭建一个简易神经网络"><span class="nav-text">6.1.3 搭建一个简易神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-自动梯度"><span class="nav-text">6.2 自动梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-1-torch-autograd和Variable"><span class="nav-text">6.2.1 torch.autograd和Variable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-2-自定义传播函数"><span class="nav-text">6.2.2 自定义传播函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-模型搭建和参数优化"><span class="nav-text">6.3 模型搭建和参数优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-1-PyTorch之torch-nn"><span class="nav-text">6.3.1 PyTorch之torch.nn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-2-PyTorch之torch-optim"><span class="nav-text">6.3.2 PyTorch之torch.optim</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-实战手写数字识别"><span class="nav-text">6.4 实战手写数字识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-1-torch和torchvision"><span class="nav-text">6.4.1 torch和torchvision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-2-PyTorch之torch-transforms"><span class="nav-text">6.4.2 PyTorch之torch.transforms</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-3-数据预览和数据装载"><span class="nav-text">6.4.3 数据预览和数据装载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-4-模型搭建和参数优化"><span class="nav-text">6.4.4 模型搭建和参数优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第7章-迁移学习"><span class="nav-text">第7章 迁移学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-迁移学习入门"><span class="nav-text">7.1 迁移学习入门</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-数据集处理"><span class="nav-text">7.2 数据集处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-1-验证数据集和测试数据集"><span class="nav-text">7.2.1 验证数据集和测试数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-2-数据预览"><span class="nav-text">7.2.2 数据预览</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-模型搭建和参数优化"><span class="nav-text">7.3 模型搭建和参数优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-1-自定义VGGNet"><span class="nav-text">7.3.1 自定义VGGNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-2-迁移VGG16"><span class="nav-text">7.3.2 迁移VGG16</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-3-迁移ResNet50"><span class="nav-text">7.3.3 迁移ResNet50</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-4-小结"><span class="nav-text">7.4 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第8章-图像风格迁移实战"><span class="nav-text">第8章 图像风格迁移实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1-风格迁移入门"><span class="nav-text">8.1 风格迁移入门</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-PyTorch图像风格迁移实战"><span class="nav-text">8.2 PyTorch图像风格迁移实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-1-图像的内容损失"><span class="nav-text">8.2.1 图像的内容损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-2-图像的风格损失"><span class="nav-text">8.2.2 图像的风格损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-3-模型搭建和参数优化"><span class="nav-text">8.2.3 模型搭建和参数优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-4-训练新定义的卷积神经网络"><span class="nav-text">8.2.4 训练新定义的卷积神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-3-小结"><span class="nav-text">8.3 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第9章-多模型融合"><span class="nav-text">第9章 多模型融合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-1-多模型融合入门"><span class="nav-text">9.1 多模型融合入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-1-结果多数表决"><span class="nav-text">9.1.1 结果多数表决</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-2-结果直接平均"><span class="nav-text">9.1.2 结果直接平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-3-结果加权平均"><span class="nav-text">9.1.3 结果加权平均</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2-PyTorch之多模型融合实战"><span class="nav-text">9.2 PyTorch之多模型融合实战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-3-小结"><span class="nav-text">9.3 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第10章-循环神经网络"><span class="nav-text">第10章 循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#10-1-循环神经网络入门"><span class="nav-text">10.1 循环神经网络入门</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-2-PyTorch之循环神经网络实战"><span class="nav-text">10.2 PyTorch之循环神经网络实战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-3-小结"><span class="nav-text">10.3 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第11章-自动编码器"><span class="nav-text">第11章 自动编码器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#11-1-自动编码器入门"><span class="nav-text">11.1 自动编码器入门</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-2-PyTorch之自动编码实战"><span class="nav-text">11.2 PyTorch之自动编码实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-1-通过线性变换实现自动编码器模型"><span class="nav-text">11.2.1 通过线性变换实现自动编码器模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-2-通过卷积变换实现自动编码器模型"><span class="nav-text">11.2.2 通过卷积变换实现自动编码器模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-3-小结"><span class="nav-text">11.3 小结</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hac_lang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  





  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="true"></script>









  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  
  

  
  

  


  

  

  

  

  


  


  




  




  




  



<script>
// GET RESPONSIVE HEIGHT PASSED FROM IFRAME

window.addEventListener("message", function(e) {
  var data = e.data;
  if ((typeof data === 'string') && (data.indexOf('ciu_embed') > -1)) {
    var featureID = data.split(':')[1];
    var height = data.split(':')[2];
    $(`iframe[data-feature=${featureID}]`).height(parseInt(height) + 30);
  }
}, false);
</script>


  

  

  


  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":250,"height":500},"mobile":{"show":false,"scale":0.5},"react":{"opacity":0.7},"log":false,"tagMode":false});</script></body>
</html>
