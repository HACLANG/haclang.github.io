<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">






<link rel="stylesheet" href="/css/main.css?v=7.2.0">






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="出版社: 人民邮电出版社出品方: 异步图书出版年: 2017-8ISBN: 9787115459688 内容提要自然语言处理是计算语言学和人工智能之中与人机交互相关的领域之一。 本书是学习自然语言处理的一本综合学习指南，介绍了如何用Python实现各种NLP任务，以帮助读者创建基于真实生活应用的项目。全书共10章，分别涉及字符串操作、统计语言建模、形态学、词性标注、语法解析、语义分析、情感分析、信">
<meta name="keywords" content="计算机科学,计算机,自评,books,更毕,Python,NLP,码农,AI,nobutdunbuy,豆瓣4">
<meta property="og:type" content="article">
<meta property="og:title" content="book_《精通Python自然语言处理》">
<meta property="og:url" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/index.html">
<meta property="og:site_name" content="Hac_lang">
<meta property="og:description" content="出版社: 人民邮电出版社出品方: 异步图书出版年: 2017-8ISBN: 9787115459688 内容提要自然语言处理是计算语言学和人工智能之中与人机交互相关的领域之一。 本书是学习自然语言处理的一本综合学习指南，介绍了如何用Python实现各种NLP任务，以帮助读者创建基于真实生活应用的项目。全书共10章，分别涉及字符串操作、统计语言建模、形态学、词性标注、语法解析、语义分析、情感分析、信">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00000.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00001.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00002.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00003.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00004.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00005.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00006.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00007.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00008.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00009.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00010.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00011.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/K1.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/K2.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00012.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00013.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00014.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00015.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00016.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00017.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00018.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00019.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00020.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00021.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00022.gif">
<meta property="og:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00023.gif">
<meta property="og:updated_time" content="2020-08-14T04:16:57.161Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="book_《精通Python自然语言处理》">
<meta name="twitter:description" content="出版社: 人民邮电出版社出品方: 异步图书出版年: 2017-8ISBN: 9787115459688 内容提要自然语言处理是计算语言学和人工智能之中与人机交互相关的领域之一。 本书是学习自然语言处理的一本综合学习指南，介绍了如何用Python实现各种NLP任务，以帮助读者创建基于真实生活应用的项目。全书共10章，分别涉及字符串操作、统计语言建模、形态学、词性标注、语法解析、语义分析、情感分析、信">
<meta name="twitter:image" content="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/Image00000.jpg">





  
  
  <link rel="canonical" href="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>book_《精通Python自然语言处理》 | Hac_lang</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hac_lang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">小白hac_lang的笔记，涉及内容包含但不限于：人工智能   基因工程    信息安全   软件工程   嵌入式   天文物理</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-news">

    
    
      
    

    

    <a href="/news/" rel="section"><i class="menu-item-icon fa fa-fw fa-rss"></i> <br>news</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    
  
  

  

  <a href="https://github.com/HACLANG" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://haclang.github.io/2017/11/06/book-《精通Python自然语言处理》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hac_lang">
      <meta itemprop="description" content="小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hac_lang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">book_《精通Python自然语言处理》

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-11-06 21:57:45" itemprop="dateCreated datePublished" datetime="2017-11-06T21:57:45+08:00">2017-11-06</time>
            </span>
          

          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>出版社: 人民邮电出版社<br>出品方: 异步图书<br>出版年: 2017-8<br>ISBN: 9787115459688</p>
<h1 id="内容提要"><a href="#内容提要" class="headerlink" title="内容提要"></a>内容提要</h1><p>自然语言处理是计算语言学和人工智能之中与人机交互相关的领域之一。</p>
<p>本书是学习自然语言处理的一本综合学习指南，介绍了如何用Python实现各种NLP任务，以帮助读者创建基于真实生活应用的项目。全书共10章，分别涉及字符串操作、统计语言建模、形态学、词性标注、语法解析、语义分析、情感分析、信息检索、语篇分析和NLP系统评估等主题。</p>
<p>本书适合熟悉Python语言并对自然语言处理开发有一定了解和兴趣的读者阅读参考。</p>
<h1 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h1><p><strong>Deepti Chopra</strong> 是Banasthali大学的助理教授。她的主要研究领域是计算语言学、自然语言处理以及人工智能，她也参与了将英语转换为印度诸语言的机器翻译引擎的研发。她在各种期刊和会议上发表过一些文章，此外她还担任一些期刊及会议的程序委员会委员。</p>
<p><strong>Nisheeth Joshi</strong> 是Banasthali大学的副教授。他感兴趣的领域包括计算语言学、自然语言处理以及人工智能。除此之外，他也非常积极地参与了将英语转换为印度诸语言的机器翻译引擎的研发。他是印度政府电子和信息技术部TDIL计划选任的专家之一，TDIL是负责印度语言技术资金和研究的主要组织。他在各种期刊和会议上发表过一些文章，并同时担任一些期刊及会议的程序委员会及编审委员会委员。</p>
<p><strong>Iti Mathur</strong> 是Banasthali大学的助理教授。她感兴趣的领域是计算语义和本体工程。除此之外，她也非常积极地参与了将英语转换为印度诸语言的机器翻译引擎的研发。她是印度政府电子和信息技术部TDIL计划选任的专家之一，TDIL是负责印度语言技术资金和研究的主要组织。她在期刊和会议上发表过一些文章，并同时担任一些期刊及会议的程序委员会及编审委员会委员。</p>
<blockquote>
<p>我们要诚挚地感谢所有的亲朋好友，因为你们的祝福促使我们完成了出版这本基于自然语言处理的图书的目标。</p>
</blockquote>
<h1 id="审阅者简介"><a href="#审阅者简介" class="headerlink" title="审阅者简介"></a>审阅者简介</h1><p><strong>Arturo Argueta</strong> 目前是一名在读博士研究生，他专注于高性能计算和自然语言处理领域的研究。他在聚类算法、有关自然语言处理的机器学习算法以及机器翻译等方面有一定的研究。他还精通英语、德语和西班牙语。</p>
<h1 id="译者简介"><a href="#译者简介" class="headerlink" title="译者简介"></a>译者简介</h1><p><strong>王威</strong> 资深研发工程师，曾就职于携程、东方财富等互联网公司。目前专注于互联网分布式架构设计、大数据与机器学习、算法设计等领域的研究，擅长C#、Python、Java、C++等技术。内涵段子手、空想创业家、业余吉他手、重度读书人。</p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在本书中，我们将学习如何使用Python实现各种有关自然语言处理的任务，并了解一些有关自然语言处理的当下和新进的研究主题。本书是一本综合的进阶指南，以期帮助学生和研究人员创建属于他们自己的基于真实生活应用的项目。</p>
<h2 id="本书涵盖内容"><a href="#本书涵盖内容" class="headerlink" title="本书涵盖内容"></a>本书涵盖内容</h2><p>第1章，字符串操作，介绍如何执行文本上的预处理任务，例如切分和标准化，此外还介绍了各种字符串匹配方法。</p>
<p>第2章，统计语言建模，包含如何计算单词的频率以及如何执行各种语言建模的技术。</p>
<p>第3章，形态学：在实践中学习，讨论如何开发词干提取器、形态分析器以及形态生成器。</p>
<p>第4章，词性标注：单词识别，解释词性标注以及有关n-gram方法的统计建模。</p>
<p>第5章，语法解析：分析训练资料，提供关于Tree bank建设、CFG建设、CYK算法、线图分析算法以及音译等概念的相关信息。</p>
<p>第6章，语义分析：意义很重要，介绍浅层语义分析（即NER）的概念和应用以及使用Wordnet执行WSD。</p>
<p>第7章，情感分析：我很快乐，提供可以帮助你理解和应用情感分析相关概念的信息。</p>
<p>第8章，信息检索：访问信息，将帮助你理解和应用信息检索及文本摘要的概念。</p>
<p>第9章，语篇分析：理解才是可信的，探讨语篇分析系统和基于指代消解的系统。</p>
<p>第10章，NLP系统评估：性能分析，谈论NLP系统评估相关概念的理解与应用。</p>
<h2 id="本书的阅读前提"><a href="#本书的阅读前提" class="headerlink" title="本书的阅读前提"></a>本书的阅读前提</h2><p>本书中所有的代码示例均使用Python 2.7或Python 3.2以上的版本编写。不管是32位机还是64位机，都必须安装NLTK（Natural Language Toolkit，NLTK）3.0包。操作系统要求为Windows、Mac或UNIX。</p>
<h2 id="本书的目标读者"><a href="#本书的目标读者" class="headerlink" title="本书的目标读者"></a>本书的目标读者</h2><p>本书主要面向对Python语言有一定认知水平的自然语言处理的中级开发人员。</p>
<h2 id="排版约定"><a href="#排版约定" class="headerlink" title="排版约定"></a>排版约定</h2><p>本书中用不同的文本样式来区分不同种类的信息。下面给出了这些文本样式的示例及其含义。</p>
<p>文本中的代码单词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟URL、用户输入以及推特用户定位表示如下：</p>
<p>“对于法语文本的切分，我们将使用<code>french.pickle</code> 文件。”</p>
<p>代码块的样式如下所示：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; text=&quot; Welcome readers. I hope you find it interesting. Please do</span><br><span class="line">reply.&quot;</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import sent_tokenize</span><br></pre></td></tr></table></figure>

</details>




<blockquote>
<p><img src="Image00000.jpg" alt> &gt;<br>此图标表示警告或需要特别注意的内容。</p>
<p><img src="Image00001.jpg" alt> &gt;<br>此图标表示提示或者技巧。</p>
</blockquote>
<h2 id="读者反馈"><a href="#读者反馈" class="headerlink" title="读者反馈"></a>读者反馈</h2><p>我们始终欢迎来自读者的反馈。请告诉我们你对本书的看法——喜欢或者不喜欢的部分。你的意见对我们来说非常重要，这将有助于我们开发出读者真正感兴趣的东西。</p>
<p>一般的反馈，你只需发送邮件至<code>feedback@packtpub.com</code> ，并在邮件主题中写清楚书名。</p>
<p>如果你擅长某个主题，并有兴趣编写一本书或者想为一本书做贡献，请参考我们的作者指南，网址<code>www.packtpub.com/authors</code> 。</p>
<h2 id="客户支持"><a href="#客户支持" class="headerlink" title="客户支持"></a>客户支持</h2><p>既然你已经是Packt引以为傲的读者了，为了能让你的购买物超所值，我们还为你准备了以下内容。</p>
<h2 id="下载示例代码"><a href="#下载示例代码" class="headerlink" title="下载示例代码"></a>下载示例代码</h2><p>你可以用你的<a href="http://www.packtpub.com" target="_blank" rel="noopener"> <code>http://www.packtpub.com</code> </a> 账户在上面下载本书配套的示例代码。如果你是在别的地方购买的本书，你可以访问<a href="http://www.packtpub.com/support" target="_blank" rel="noopener"> <code>http://www.packtpub.com/support</code> </a> 并注册，我们会用邮件把代码文件直接发给你。</p>
<p>你可以按照以下步骤下载代码文件。</p>
<p>1．使用你的邮箱地址和密码登录或注册我们的网站。</p>
<p>2．将鼠标指针移至顶端的SUPPORT选项卡上。</p>
<p>3．单击Code Downloads &amp; Errata。</p>
<p>4．在搜索框中输入书名。</p>
<p>5．选择你需要下载代码文件的图书。</p>
<p>6．在下拉菜单里选择你从哪里购买的这本书。</p>
<p>7．单击Code Download。</p>
<p>你也可以通过单击Packt出版社官网上关于本书的网页中的“Code Files”按钮来下载代码文件。你可以通过在搜索框中输入书名进入到这个页面。请注意你需要登录你的Packt账户。</p>
<p>一旦下载示例代码文件后，请确保使用以下最新版本的工具解压文件夹：</p>
<ul>
<li>WinRAR / 7-Zip for Windows。</li>
<li>Zipeg / iZip / UnRarX for Mac。</li>
<li>7-Zip / PeaZip for Linux。</li>
</ul>
<p>本书的代码包也托管在Github上，网址是<code>https://github.com/PacktPublishing/</code> <code>Mastering-Natural- Language-Processing-with-Python</code> 。我们也有来自于我们丰富的图书和视频目录的其他代码包，地址是<a href="https://github.com/PacktPublishing/" target="_blank" rel="noopener"> <code>https://github.com/PacktPublishing/</code> </a> 。欢迎访问！</p>
<h2 id="勘误"><a href="#勘误" class="headerlink" title="勘误"></a>勘误</h2><p>虽然我们竭尽全力保证图书内容的准确性，但错误仍在所难免。如果你在我们的任何一本书里发现错误，可能是文字的或者代码中的错误，都烦请报告给我们，我们将不胜感激。这样不仅使其他读者免于困惑，也能帮助我们不断改进后续版本。如果你发现任何错误，请访问<code>http://www.packtpub.com/submit- errata</code> 报告给我们，选择相应图书，单击“Errata Submission Form”链接，并输入勘误详情。一旦你提出的错误被证实，你的勘误将被接收并上传至我们的网站，或加入到已有的勘误列表中。</p>
<p>若要查看之前提交的勘误，请访问<a href="https://www.packtpub.com/books/%20content/support" target="_blank" rel="noopener"> <code>https://www.packtpub.com/books/content/support</code> </a> 并在搜索框中输入书名，所需的信息将会展现在“Errata”部分的下面。</p>
<h2 id="反盗版"><a href="#反盗版" class="headerlink" title="反盗版"></a>反盗版</h2><p>在互联网上，所有媒体都会遭遇盗版问题。对Packt来说，我们严格保护版权和许可证。如果你在互联网上发现我们出版物的任何非法副本，请立即向我们提供侵权网站的地址和名称，以便我们采取补救措施。</p>
<p>请通过<code>copyright@packtpub.com</code> 联系我们，同时请提供涉嫌侵权内容的链接。</p>
<p>非常感激你帮助保护我们的作者，让我们尽力提供更有价值的内容。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>如果你对本书有任何疑问，都可以通过questions@packtpub.com 邮箱联系我们，我们将尽最大努力为你答疑解惑。</p>
<h1 id="第1章-字符串操作"><a href="#第1章-字符串操作" class="headerlink" title="第1章 字符串操作"></a>第1章 字符串操作</h1><p>自然语言处理（Natural Language Processing，NLP）关注的是自然语言与计算机之间的交互。它是人工智能（Artificial Intelligence，AI）和计算语言学的主要分支之一。它提供了计算机和人类之间的无缝交互并使得计算机能够在机器学习的帮助下理解人类语言。在编程语言（例如C、C++、Java、Python等）里用于表示一个文件或文档内容的基础数据类型被称为字符串。在本章中，我们将探索各种可以在字符串上执行的操作，这些操作将有助于完成各种NLP任务。</p>
<p>本章将包含以下主题：</p>
<ul>
<li>文本切分。</li>
<li>文本标准化。</li>
<li>替换和校正标识符。</li>
<li>在文本上应用Zipf定律。</li>
<li>使用编辑距离算法执行相似性度量。</li>
<li>使用Jaccard系数执行相似性度量。</li>
<li>使用Smith Waterman算法执行相似性度量。</li>
</ul>
<h2 id="1-1-切分"><a href="#1-1-切分" class="headerlink" title="1.1 切分"></a>1.1 切分</h2><p>切分可以认为是将文本分割成更小的并被称作标识符的模块的过程，它被认为是NLP的一个重要步骤。</p>
<p>当安装好NLTK包并且Python的交互式开发环境（IDLE）也运行起来时，我们就可以将文本或者段落切分成独立的语句。为了实现切分，我们可以导入语句切分函数，该函数的参数即为需要被切分的文本。<code>sent_tokenize</code> 函数使用了NLTK包的一个叫作<code>PunktSentenceTokenizer</code> 类的实例。基于那些可以标记句子开始和结束的字母和标点符号，NLTK中的这个实例已经被训练用于对不同的欧洲语言执行切分。</p>
<h3 id="1-1-1-将文本切分为语句"><a href="#1-1-1-将文本切分为语句" class="headerlink" title="1.1.1 将文本切分为语句"></a>1.1.1 将文本切分为语句</h3><p>现在，让我们来看看一段给定的文本是如何被切分为独立的句子的：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; text=&quot; Welcome readers. I hope you find it interesting. Please do</span><br><span class="line">reply.&quot;</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import sent_tokenize</span><br><span class="line">&gt;&gt;&gt; sent_tokenize(text)</span><br><span class="line">[&apos; Welcome readers.&apos;, &apos;I hope you find it interesting.&apos;, &apos;Please do</span><br><span class="line">reply.&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>这样，一段给定的文本就被分割成了独立的句子。我们还可以进一步对这些独立的句子进行处理。</p>
<p>要切分大批量的句子，我们可以加载<code>PunktSentenceTokenizer</code> 并使用其<code>tokenize()</code> 函数来进行切分。下面的代码展示了该过程：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; tokenizer=nltk.data.load(&apos;tokenizers/punkt/english.pickle&apos;)</span><br><span class="line">&gt;&gt;&gt; text=&quot; Hello everyone. Hope all are fine and doing well. Hope you</span><br><span class="line">find the book interesting&quot;</span><br><span class="line">&gt;&gt;&gt; tokenizer.tokenize(text)</span><br><span class="line">[&apos; Hello everyone.&apos;, &apos;Hope all are fine and doing well.&apos;, &apos;Hope you</span><br><span class="line">find the book interesting&apos;]</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-1-2-其他语言文本的切分"><a href="#1-1-2-其他语言文本的切分" class="headerlink" title="1.1.2 其他语言文本的切分"></a>1.1.2 其他语言文本的切分</h3><p>为了对除英文之外的其他语言执行切分，我们可以加载它们各自的pickle文件（可以在<code>tokenizers/punkt</code> 里边找到），然后用该语言对文本进行切分，这些文本是<code>tokenize()</code> 函数的参数。对于法语文本的切分，我们将使用如下的<code>french.pickle</code> 文件：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; french_tokenizer=nltk.data.load(&apos;tokenizers/punkt/french.pickle&apos;)</span><br><span class="line">&gt;&gt;&gt; french_tokenizer.tokenize(&apos;Deux agressions en quelques jours,</span><br><span class="line">voilà ce qui a motivé hier matin le débrayage collège franco-</span><br><span class="line">britanniquede Levallois-Perret. Deux agressions en quelques jours,</span><br><span class="line">voilà ce qui a motivé hier matin le débrayage Levallois. L&apos;équipe</span><br><span class="line">pédagogique de ce collège de 750 élèves avait déjà été choquée</span><br><span class="line">par l&apos;agression, janvier , d&apos;un professeur d&apos;histoire. L&apos;équipe</span><br><span class="line">pédagogique de ce collège de 750 élèves avait déjà été choquée par</span><br><span class="line">l&apos;agression, mercredi , d&apos;un professeur d&apos;histoire&apos;)</span><br><span class="line">[&apos;Deux agressions en quelques jours, voilà ce qui a motivé hier</span><br><span class="line">matin le débrayage collège franco-britanniquedeLevallois-Perret.&apos;,</span><br><span class="line">&apos;Deux agressions en quelques jours, voilà ce qui a motivé hier matin</span><br><span class="line">le débrayage Levallois.&apos;, &apos;L&apos;équipe pédagogique de ce collège de</span><br><span class="line">750 élèves avait déjà été choquée par l&apos;agression, janvier , d&apos;un</span><br><span class="line">professeur d&apos;histoire.&apos;, &apos;L&apos;équipe pédagogique de ce collège de</span><br><span class="line">750 élèves avait déjà été choquée par l&apos;agression, mercredi , d&apos;un</span><br><span class="line">professeur d&apos;histoire&apos;]</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-1-3-将句子切分为单词"><a href="#1-1-3-将句子切分为单词" class="headerlink" title="1.1.3 将句子切分为单词"></a>1.1.3 将句子切分为单词</h3><p>现在，我们将对独立的句子执行处理，独立的句子会被切分为单词。通过使用<code>word_tokenize()</code> 函数可以执行单词的切分。<code>word_tokenize</code> 函数使用NLTK包的一个叫作<code>TreebankWordTokenizer</code> 类的实例用于执行单词的切分。</p>
<p>使用<code>word_tokenize</code> 函数切分英文文本的代码如下所示：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; text=nltk.word_tokenize(&quot;PierreVinken , 59 years old , will join</span><br><span class="line">as a nonexecutive director on Nov. 29 .»)</span><br><span class="line">&gt;&gt;&gt; print(text)</span><br><span class="line">[&apos;PierreVinken&apos;, &apos;,&apos;, &apos;59&apos;, &apos; years&apos;, &apos; old&apos;, &apos;,&apos;, &apos;will&apos;, &apos;join&apos;,</span><br><span class="line">&apos;as&apos;, &apos;a&apos;, &apos;nonexecutive&apos;, &apos;director&apos; , &apos;on&apos;, &apos;Nov.&apos;, &apos;29&apos;, &apos;.&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>实现单词的切分还可以通过加载<code>TreebankWordTokenizer</code> ，然后调用<code>tokenize()</code> 函数来完成，其中<code>tokenize()</code> 函数的参数是需要被切分为单词的句子。基于空格和标点符号，NLTK包的这个实例已经被训练用于将句子切分为单词。</p>
<p>如下代码将帮助我们获取用户的输入，然后再将其切分并计算切分后的列表长度：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk import word_tokenize</span><br><span class="line">&gt;&gt;&gt; r=input(&quot;Please write a text&quot;)</span><br><span class="line">Please write a textToday is a pleasant day</span><br><span class="line">&gt;&gt;&gt; print(&quot;The length of text is&quot;,len(word_tokenize(r)),&quot;words&quot;)</span><br><span class="line">The length of text is 5 words</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-1-4-使用TreebankWordTokenizer执行切分"><a href="#1-1-4-使用TreebankWordTokenizer执行切分" class="headerlink" title="1.1.4 使用TreebankWordTokenizer执行切分"></a>1.1.4 使用TreebankWordTokenizer执行切分</h3><p>让我们来看看使用<code>TreebankWordTokenizer</code> 执行切分的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import TreebankWordTokenizer</span><br><span class="line">&gt;&gt;&gt; tokenizer = TreebankWordTokenizer()</span><br><span class="line">&gt;&gt;&gt; tokenizer.tokenize(&quot;Have a nice day. I hope you find the book</span><br><span class="line">interesting&quot;)</span><br><span class="line">[&apos;Have&apos;, &apos;a&apos;, &apos;nice&apos;, &apos;day.&apos;, &apos;I&apos;, &apos;hope&apos;, &apos;you&apos;, &apos;find&apos;, &apos;the&apos;,</span><br><span class="line">&apos;book&apos;, &apos;interesting&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p><code>TreebankWordTokenizer</code> 依据Penn Treebank语料库的约定，通过分离缩略词来实现切分。此过程展示如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; text=nltk.word_tokenize(&quot; Don&apos;t hesitate to ask questions&quot;)</span><br><span class="line">&gt;&gt;&gt; print(text)</span><br><span class="line">[&apos;Do&apos;, &quot;n&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>另一个分词器是<code>PunktWordTokenizer</code> ，它是通过分离标点来实现切分的，每一个单词都会被保留，而不是去创建一个全新的标识符。还有一个分词器是<code>WordPunctTokenizer</code> ，它通过将标点转化为一个全新的标识符来实现切分，我们通常需要这种形式的切分：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.tokenize import WordPunctTokenizer</span><br><span class="line">&gt;&gt;&gt; tokenizer=WordPunctTokenizer()</span><br><span class="line">&gt;&gt;&gt; tokenizer.tokenize(&quot; Don&apos;t hesitate to ask questions&quot;)</span><br><span class="line">[&apos;Don&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>分词器的继承树如图1-1所示。</p>
<p><img src="Image00002.gif" alt></p>
<p>图1-1</p>
<h3 id="1-1-5-使用正则表达式实现切分"><a href="#1-1-5-使用正则表达式实现切分" class="headerlink" title="1.1.5 使用正则表达式实现切分"></a>1.1.5 使用正则表达式实现切分</h3><p>可以通过构建如下两种正则表达式来实现单词的切分：</p>
<ul>
<li>通过匹配单词。</li>
<li>通过匹配空格或间隔。</li>
</ul>
<p>我们可以导入NLTK包的<code>RegexpTokenizer</code> 模块，并构建一个与文本中的标识符相匹配的正则表达式：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import RegexpTokenizer</span><br><span class="line">&gt;&gt;&gt; tokenizer=RegexpTokenizer([\w]+&quot;)</span><br><span class="line">&gt;&gt;&gt; tokenizer.tokenize(&quot;Don&apos;t hesitate to ask questions&quot;)</span><br><span class="line">[&quot;Don&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>另一种不用实例化类的切分方式将使用下面的函数：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import regexp_tokenize</span><br><span class="line">&gt;&gt;&gt; sent=&quot;Don&apos;t hesitate to ask questions&quot;</span><br><span class="line">&gt;&gt;&gt; print(regexp_tokenize(sent, pattern=&apos;\w+|\$[\d\.]+|\S+&apos;))</span><br><span class="line">[&apos;Don&apos;, &quot;&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p><code>RegularexpTokenizer</code> 在使用<code>re.findall()</code> 函数时是通过匹配标识符来执行切分的；在使用<code>re.split()</code> 函数时是通过匹配间隔或者空格来执行切分的。</p>
<p>让我们来看一个如何通过空格来执行切分的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import RegexpTokenizer</span><br><span class="line">&gt;&gt;&gt; tokenizer=RegexpTokenizer(&apos;\s+&apos;,gaps=True)</span><br><span class="line">&gt;&gt;&gt; tokenizer.tokenize(&quot;Don&apos;t hesitate to ask questions&quot;)</span><br><span class="line">[&quot;Don&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>要筛选以大写字母开头的单词，可以使用下面的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import RegexpTokenizer</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X . She is a meritorious</span><br><span class="line">student&quot;</span><br><span class="line">&gt;&gt;&gt; capt = RegexpTokenizer(&apos;[A-Z]\w+&apos;)</span><br><span class="line">&gt;&gt;&gt; capt.tokenize(sent)</span><br><span class="line">[&apos;She&apos;, &apos;She&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>下面的代码展示了<code>RegexpTokenizer</code> 的一个子类是如何使用预定义正则表达式的：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X . She is a meritorious</span><br><span class="line">student&quot;</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import BlanklineTokenizer</span><br><span class="line">&gt;&gt;&gt; BlanklineTokenizer().tokenize(sent)</span><br><span class="line">[&apos; She secured 90.56 % in class X \n. She is a meritorious student\n&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>字符串的切分可以通过空格、间隔、换行等来完成：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X . She is a meritorious</span><br><span class="line">student&quot;</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import WhitespaceTokenizer</span><br><span class="line">&gt;&gt;&gt; WhitespaceTokenizer().tokenize(sent)</span><br><span class="line">[&apos;She&apos;, &apos;secured&apos;, &apos;90.56&apos;, &apos;%&apos;, &apos;in&apos;, &apos;class&apos;, &apos;X&apos;, &apos;.&apos;, &apos;She&apos;, &apos;is&apos;,</span><br><span class="line">&apos;a&apos;, &apos;meritorious&apos;, &apos;student&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p><code>WordPunctTokenizer</code> 使用正则表达式<code>\w+|[^\w\s]+</code> 来执行文本的切分，并将其切分为字母与非字母字符。</p>
<p>使用<code>split()</code> 方法进行切分的代码描述如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sent= She secured 90.56 % in class X. She is a meritorious student&quot;</span><br><span class="line">&gt;&gt;&gt; sent.split()</span><br><span class="line">[&apos;She&apos;, &apos;secured&apos;, &apos;90.56&apos;, &apos;%&apos;, &apos;in&apos;, &apos;class&apos;, &apos;X&apos;, &apos;.&apos;, &apos;She&apos;, &apos;is&apos;,</span><br><span class="line">&apos;a&apos;, &apos;meritorious&apos;, &apos;student&apos;]</span><br><span class="line">&gt;&gt;&gt; sent.split(&apos;&apos;)</span><br><span class="line">[&apos;&apos;, &apos;She&apos;, &apos;secured&apos;, &apos;90.56&apos;, &apos;%&apos;, &apos;in&apos;, &apos;class&apos;, &apos;X&apos;, &apos;.&apos;, &apos;She&apos;,</span><br><span class="line">&apos;is&apos;, &apos;a&apos;, &apos;meritorious&apos;, &apos;student&apos;]</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X \n. She is a meritorious</span><br><span class="line">student\n&quot;</span><br><span class="line">&gt;&gt;&gt; sent.split(&apos;\n&apos;)</span><br><span class="line">[&apos; She secured 90.56 % in class X &apos;, &apos;. She is a meritorious student&apos;,</span><br><span class="line">&apos;&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>类似于<code>sent.split(&#39;\n&#39;)</code> 方法，<code>LineTokenizer</code> 通过将文本切分为行来执行切分：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import BlanklineTokenizer</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X \n. She is a meritorious</span><br><span class="line">student\n&quot;</span><br><span class="line">&gt;&gt;&gt; BlanklineTokenizer().tokenize(sent)</span><br><span class="line">[&apos; She secured 90.56 % in class X \n. She is a meritorious student\n&apos;]</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import LineTokenizer</span><br><span class="line">&gt;&gt;&gt; LineTokenizer(blanklines=&apos;keep&apos;).tokenize(sent)</span><br><span class="line">[&apos; She secured 90.56 % in class X &apos;, &apos;. She is a meritorious student&apos;]</span><br><span class="line">&gt;&gt;&gt; LineTokenizer(blanklines=&apos;discard&apos;).tokenize(sent)</span><br><span class="line">[&apos; She secured 90.56 % in class X &apos;, &apos;. She is a meritorious student&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p><code>SpaceTokenizer</code> 与<code>sent.split(&#39;&#39;)</code> 方法的工作原理类似:</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X \n. She is a meritorious</span><br><span class="line">student\n&quot;</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import SpaceTokenizer</span><br><span class="line">&gt;&gt;&gt; SpaceTokenizer().tokenize(sent)</span><br><span class="line">[&apos;&apos;, &apos;She&apos;, &apos;secured&apos;, &apos;90.56&apos;, &apos;%&apos;, &apos;in&apos;, &apos;class&apos;, &apos;X&apos;, &apos;\n.&apos;, &apos;She&apos;,</span><br><span class="line">&apos;is&apos;, &apos;a&apos;, &apos;meritorious&apos;, &apos;student\n&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p><code>nltk.tokenize.util</code> 模块通过返回元组形式的序列来执行切分，该序列为标识符在语句中的位置和偏移量：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import WhitespaceTokenizer</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X \n. She is a meritorious</span><br><span class="line">student\n&quot;</span><br><span class="line">&gt;&gt;&gt; list(WhitespaceTokenizer().span_tokenize(sent))</span><br><span class="line">[(1, 4), (5, 12), (13, 18), (19, 20), (21, 23), (24, 29), (30, 31),</span><br><span class="line">(33, 34), (35, 38), (39, 41), (42, 43), (44, 55), (56, 63)]</span><br></pre></td></tr></table></figure>

</details>




<p>给定一个标识符的序列，则可以返回其跨度序列：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import WhitespaceTokenizer</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize.util import spans_to_relative</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X \n. She is a meritorious</span><br><span class="line">student\n&quot;</span><br><span class="line">&gt;&gt;&gt;list(spans_to_relative(WhitespaceTokenizer().span_tokenize(sent)))</span><br><span class="line">[(1, 3), (1, 7), (1, 5), (1, 1), (1, 2), (1, 5), (1, 1), (2, 1), (1,</span><br><span class="line">3), (1, 2), (1, 1), (1, 11), (1, 7)]</span><br></pre></td></tr></table></figure>

</details>




<p>通过在每一个分隔符的连接处进行分割，<code>nltk.tokenize.util.string_span_`` ``tokenize(sent,separator)</code> 将返回sent中标识符的偏移量：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize.util import string_span_tokenize</span><br><span class="line">&gt;&gt;&gt; sent=&quot; She secured 90.56 % in class X \n. She is a meritorious</span><br><span class="line">student\n&quot;</span><br><span class="line">&gt;&gt;&gt; list(string_span_tokenize(sent, &quot;&quot;))</span><br><span class="line">[(1, 4), (5, 12), (13, 18), (19, 20), (21, 23), (24, 29), (30, 31),</span><br><span class="line">(32, 34), (35, 38), (39, 41), (42, 43), (44, 55), (56, 64)]</span><br></pre></td></tr></table></figure>

</details>




<h2 id="1-2-标准化"><a href="#1-2-标准化" class="headerlink" title="1.2 标准化"></a>1.2 标准化</h2><p>为了实现对自然语言文本的处理，我们需要对其执行标准化，主要涉及消除标点符号、将整个文本转换为大写或小写、数字转换成单词、扩展缩略词、文本的规范化等操作。</p>
<h3 id="1-2-1-消除标点符号"><a href="#1-2-1-消除标点符号" class="headerlink" title="1.2.1 消除标点符号"></a>1.2.1 消除标点符号</h3><p>有时候，在切分文本的过程中，我们希望删除标点符号。当在NLTK中执行标准化操作时，删除标点符号被认为是主要的任务之一。</p>
<p>考虑下面的代码示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; text=[&quot; It is a pleasant evening.&quot;,&quot;Guests, who came from US</span><br><span class="line">arrived at the venue&quot;,&quot;Food was tasty.&quot;]</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import word_tokenize</span><br><span class="line">&gt;&gt;&gt; tokenized_docs=[word_tokenize(doc) for doc in text]</span><br><span class="line">&gt;&gt;&gt; print(tokenized_docs)</span><br><span class="line">[[&apos;It&apos;, &apos;is&apos;, &apos;a&apos;, &apos;pleasant&apos;, &apos;evening&apos;, &apos;.&apos;], [&apos;Guests&apos;, &apos;,&apos;, &apos;who&apos;,</span><br><span class="line">&apos;came&apos;, &apos;from&apos;, &apos;US&apos;, &apos;arrived&apos;, &apos;at&apos;, &apos;the&apos;, &apos;venue&apos;], [&apos;Food&apos;,</span><br><span class="line">&apos;was&apos;, &apos;tasty&apos;, &apos;.&apos;]]</span><br></pre></td></tr></table></figure>

</details>




<p>以上代码得到了切分后的文本。以下代码将从切分后的文本中删除标点符号：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import re</span><br><span class="line">&gt;&gt;&gt; import string</span><br><span class="line">&gt;&gt;&gt; text=[&quot; It is a pleasant evening.&quot;,&quot;Guests, who came from US</span><br><span class="line">arrived at the venue&quot;,&quot;Food was tasty.&quot;]</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import word_tokenize</span><br><span class="line">&gt;&gt;&gt; tokenized_docs=[word_tokenize(doc) for doc in text]</span><br><span class="line">&gt;&gt;&gt; x=re.compile(&apos;[%s]&apos; % re.escape(string.punctuation))</span><br><span class="line">&gt;&gt;&gt; tokenized_docs_no_punctuation = []</span><br><span class="line">&gt;&gt;&gt; for review in tokenized_docs:</span><br><span class="line">    new_review = []</span><br><span class="line">    for token in review:</span><br><span class="line">    new_token = x.sub(u&apos;&apos;, token)</span><br><span class="line">    if not new_token == u&apos;&apos;:</span><br><span class="line">            new_review.append(new_token)</span><br><span class="line">    tokenized_docs_no_punctuation.append(new_review)</span><br><span class="line">&gt;&gt;&gt; print(tokenized_docs_no_punctuation)</span><br><span class="line">[[&apos;It&apos;, &apos;is&apos;, &apos;a&apos;, &apos;pleasant&apos;, &apos;evening&apos;], [&apos;Guests&apos;, &apos;who&apos;, &apos;came&apos;,</span><br><span class="line">&apos;from&apos;, &apos;US&apos;, &apos;arrived&apos;, &apos;at&apos;, &apos;the&apos;, &apos;venue&apos;], [&apos;Food&apos;, &apos;was&apos;,</span><br><span class="line">&apos;tasty&apos;]]</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-2-2-文本的大小写转换"><a href="#1-2-2-文本的大小写转换" class="headerlink" title="1.2.2 文本的大小写转换"></a>1.2.2 文本的大小写转换</h3><p>通过<code>lower ( )</code> 和<code>upper ( )</code> 函数可以将一段给定的文本彻底转换为小写或大写文本。将文本转换为大小写的任务也属于文本标准化的范畴。</p>
<p>考虑下面的大小写转换例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; text=&apos;HARdWork IS KEy to SUCCESS&apos;</span><br><span class="line">&gt;&gt;&gt; print(text.lower())</span><br><span class="line">hardwork is key to success</span><br><span class="line">&gt;&gt;&gt; print(text.upper())</span><br><span class="line">HARDWORK IS KEY TO SUCCESS</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-2-3-处理停止词"><a href="#1-2-3-处理停止词" class="headerlink" title="1.2.3 处理停止词"></a>1.2.3 处理停止词</h3><p>停止词是指在执行信息检索任务或其他自然语言任务时需要被过滤掉的词，因为这些词对理解句子的整体意思没有多大的意义。许多搜索引擎通过去除停止词来工作，以便缩小搜索范围。消除停止词在NLP中被认为是至关重要的标准化任务之一 。</p>
<p>NLTK库为多种语言提供了一系列的停止词，为了可以从<code>nltk_data/corpora/ stopwords</code> 中访问停止词列表，我们需要解压<code>datafile</code> 文件：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line">&gt;&gt;&gt; stops=set(stopwords.words(&apos;english&apos;))</span><br><span class="line">&gt;&gt;&gt; words=[&quot;Don&apos;t&quot;, &apos;hesitate&apos;,&apos;to&apos;,&apos;ask&apos;,&apos;questions&apos;]</span><br><span class="line">&gt;&gt;&gt; [word for word in words if word not in stops]</span><br><span class="line">[&quot;Don&apos;t&quot;, &apos;hesitate&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p><code>nltk.corpus.reader.WordListCorpusReader</code> 类的实例是一个<code>stopwords</code> 语料库，它拥有一个参数为<code>fileid</code> 的<code>words()</code> 函数。这里参数为English，它指的是在英语文件中存在的所有停止词。如果<code>words()</code> 函数没有参数，那么它指的将是关于所有语言的全部停止词。</p>
<p>可以在其中执行停止词删除的其他语言，或者在NLTK中其文件存在停止词的语言数量都可以通过使用<code>fileids ( )</code> 函数找到：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; stopwords.fileids()</span><br><span class="line">[&apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, &apos;french&apos;, &apos;german&apos;,</span><br><span class="line">&apos;hungarian&apos;, &apos;italian&apos;, &apos;norwegian&apos;, &apos;portuguese&apos;, &apos;russian&apos;,</span><br><span class="line">&apos;spanish&apos;, &apos;swedish&apos;, &apos;turkish&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>上面列出的任何一种语言都可以用作<code>words()</code> 函数的参数，以便获取该语言的停止词。</p>
<h3 id="1-2-4-计算英语中的停止词"><a href="#1-2-4-计算英语中的停止词" class="headerlink" title="1.2.4 计算英语中的停止词"></a>1.2.4 计算英语中的停止词</h3><p>让我们来看一个有关如何计算停止词的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line">&gt;&gt;&gt; stopwords.words(&apos;english&apos;)</span><br><span class="line">[&apos;i&apos;, &apos;me&apos;, &apos;my&apos;, &apos;myself&apos;, &apos;we&apos;, &apos;our&apos;, &apos;ours&apos;, &apos;ourselves&apos;, &apos;you&apos;,</span><br><span class="line">&apos;your&apos;, &apos;yours&apos;, &apos;yourself&apos;, &apos;yourselves&apos;, &apos;he&apos;, &apos;him&apos;, &apos;his&apos;,</span><br><span class="line">&apos;himself&apos;, &apos;she&apos;, &apos;her&apos;, &apos;hers&apos;, &apos;herself&apos;, &apos;it&apos;, &apos;its&apos;, &apos;itself&apos;,</span><br><span class="line">&apos;they&apos;, &apos;them&apos;, &apos;their&apos;, &apos;theirs&apos;, &apos;themselves&apos;, &apos;what&apos;, &apos;which&apos;,</span><br><span class="line">&apos;who&apos;, &apos;whom&apos;, &apos;this&apos;, &apos;that&apos;, &apos;these&apos;, &apos;those&apos;, &apos;am&apos;, &apos;is&apos;, &apos;are&apos;,</span><br><span class="line">&apos;was&apos;, &apos;were&apos;, &apos;be&apos;, &apos;been&apos;, &apos;being&apos;, &apos;have&apos;, &apos;has&apos;, &apos;had&apos;, &apos;having&apos;,</span><br><span class="line">&apos;do&apos;, &apos;does&apos;, &apos;did&apos;, &apos;doing&apos;, &apos;a&apos;, &apos;an&apos;, &apos;the&apos;, &apos;and&apos;, &apos;but&apos;, &apos;if&apos;,</span><br><span class="line">&apos;or&apos;, &apos;because&apos;, &apos;as&apos;, &apos;until&apos;, &apos;while&apos;, &apos;of&apos;, &apos;at&apos;, &apos;by&apos;, &apos;for&apos;,</span><br><span class="line">&apos;with&apos;, &apos;about&apos;, &apos;against&apos;, &apos;between&apos;, &apos;into&apos;, &apos;through&apos;, &apos;during&apos;,</span><br><span class="line">&apos;before&apos;, &apos;after&apos;, &apos;above&apos;, &apos;below&apos;, &apos;to&apos;, &apos;from&apos;, &apos;up&apos;, &apos;down&apos;, &apos;in&apos;,</span><br><span class="line">&apos;out&apos;, &apos;on&apos;, &apos;off&apos;, &apos;over&apos;, &apos;under&apos;, &apos;again&apos;, &apos;further&apos;, &apos;then&apos;,</span><br><span class="line">&apos;once&apos;, &apos;here&apos;, &apos;there&apos;, &apos;when&apos;, &apos;where&apos;, &apos;why&apos;, &apos;how&apos;, &apos;all&apos;, &apos;any&apos;,</span><br><span class="line">&apos;both&apos;, &apos;each&apos;, &apos;few&apos;, &apos;more&apos;, &apos;most&apos;, &apos;other&apos;, &apos;some&apos;, &apos;such&apos;, &apos;no&apos;,</span><br><span class="line">&apos;nor&apos;, &apos;not&apos;, &apos;only&apos;, &apos;own&apos;, &apos;same&apos;, &apos;so&apos;, &apos;than&apos;, &apos;too&apos;, &apos;very&apos;, &apos;s&apos;,</span><br><span class="line">&apos;t&apos;, &apos;can&apos;, &apos;will&apos;, &apos;just&apos;, &apos;don&apos;, &apos;should&apos;, &apos;now&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; def para_fraction(text):</span><br><span class="line">stopwords = nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">para = [w for w in text if w.lower() not in stopwords]</span><br><span class="line">return len(para) / len(text)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; para_fraction(nltk.corpus.reuters.words())</span><br><span class="line">0.7364374824583169</span><br><span class="line">&gt;&gt;&gt; para_fraction(nltk.corpus.inaugural.words())</span><br><span class="line">0.5229560503653893</span><br></pre></td></tr></table></figure>

</details>




<p>标准化操作还涉及将数字转化为单词（例如，1可以替换为<code>one</code> ）和扩展缩略词（例如，<code>can’t</code> 可以替换为<code>cannot</code><br>），这可以通过使用替换模式表示它们来实现。我们将在下一节讨论这些内容。</p>
<h2 id="1-3-替换和校正标识符"><a href="#1-3-替换和校正标识符" class="headerlink" title="1.3 替换和校正标识符"></a>1.3 替换和校正标识符</h2><p>在本节中，我们将讨论用其他类型的标识符来替换标识符。我们还会讨论如何来校正标识符的拼写（通过用正确拼写的标识符替换拼写不正确的标识符）。</p>
<h3 id="1-3-1-使用正则表达式替换单词"><a href="#1-3-1-使用正则表达式替换单词" class="headerlink" title="1.3.1 使用正则表达式替换单词"></a>1.3.1 使用正则表达式替换单词</h3><p>为了消除错误或执行文本的标准化，需要做单词替换。一种可以完成文本替换的方法是使用正则表达式。之前，在执行缩略词切分时我们遇到了问题。通过使用文本替换，我们可以用缩略词的扩展形式来替换缩略词。例如，<code>doesn’t</code> 可以被替换为<code>does not</code> 。</p>
<p>我们将从编写以下代码开始，并命名此程序为<code>replacers.py</code> ，最后将其保存在<code>nltkdata</code> 文件夹中：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">replacement_patterns = [</span><br><span class="line">(r&apos;won\&apos;t&apos;, &apos;will not&apos;),</span><br><span class="line">(r&apos;can\&apos;t&apos;, &apos;cannot&apos;),</span><br><span class="line">(r&apos;i\&apos;m&apos;, &apos;i am&apos;),</span><br><span class="line">(r&apos;ain\&apos;t&apos;, &apos;is not&apos;),</span><br><span class="line">(r&apos;(\w+)\&apos;ll&apos;, &apos;\g&lt;1&gt; will&apos;),</span><br><span class="line">(r&apos;(\w+)n\&apos;t&apos;, &apos;\g&lt;1&gt; not&apos;),</span><br><span class="line">(r&apos;(\w+)\&apos;ve&apos;, &apos;\g&lt;1&gt; have&apos;),</span><br><span class="line">(r&apos;(\w+)\&apos;s&apos;, &apos;\g&lt;1&gt; is&apos;),</span><br><span class="line">(r&apos;(\w+)\&apos;re&apos;, &apos;\g&lt;1&gt; are&apos;),</span><br><span class="line">(r&apos;(\w+)\&apos;d&apos;, &apos;\g&lt;1&gt; would&apos;)</span><br><span class="line">]</span><br><span class="line">class RegexpReplacer(object):</span><br><span class="line">    def __init__(self, patterns=replacement_patterns):</span><br><span class="line">        self.patterns = [(re.compile(regex), repl) for (regex, repl)</span><br><span class="line">in</span><br><span class="line">        patterns]</span><br><span class="line">    def replace(self, text):</span><br><span class="line">        s = text</span><br><span class="line">        for (pattern, repl) in self.patterns:</span><br><span class="line">             (s, count) = re.subn(pattern, repl, s)</span><br><span class="line">        return s</span><br></pre></td></tr></table></figure>

</details>




<p>这里我们定义了替换模式，模式第一项表示需要被匹配的模式，第二项是其对应的替换模式。<code>RegexpReplacer</code> 类被定义用来执行编译模式对的任务，并且它提供了一个叫作<code>replace()</code> 的方法，该方法的功能是用另一种模式来执行模式的替换。</p>
<h3 id="1-3-2-用其他文本替换文本的示例"><a href="#1-3-2-用其他文本替换文本的示例" class="headerlink" title="1.3.2 用其他文本替换文本的示例"></a>1.3.2 用其他文本替换文本的示例</h3><p>让我们来看一个有关如何用其他文本来替换文本的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from replacers import RegexpReplacer</span><br><span class="line">&gt;&gt;&gt; replacer= RegexpReplacer()</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&quot;Don&apos;t hesitate to ask questions&quot;)</span><br><span class="line">&apos;Do not hesitate to ask questions&apos;</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&quot;She must&apos;ve gone to the market but she didn&apos;t</span><br><span class="line">go&quot;)</span><br><span class="line">&apos;She must have gone to the market but she did not go&apos;</span><br></pre></td></tr></table></figure>

</details>




<p><code>RegexpReplacer.replace()</code> 函数用其相应的替换模式来更换被替换模式的每一个实例。在这里，<code>must’ve</code> 被替换为<code>must have</code> , <code>didn’t</code> 被替换为<code>did not</code> ，因为在<code>replacers.py</code> 中已经通过元组对的形式定义了替换模式，也就是<code>（r&#39;（\ w +）\&#39;ve&#39; ，&#39;\ g &lt;1&gt;have&#39;）</code> 和<code>（r&#39;（\ w +）n\&#39;t&#39;，&#39;\ g&lt;1&gt;not&#39;）</code> 。</p>
<p>我们不仅可以执行缩略词的替换，还可以用其他任意标识符来替换一个标识符。</p>
<h3 id="1-3-3-在执行切分前先执行替换操作"><a href="#1-3-3-在执行切分前先执行替换操作" class="headerlink" title="1.3.3 在执行切分前先执行替换操作"></a>1.3.3 在执行切分前先执行替换操作</h3><p>标识符替换操作可以在切分前执行，以避免在切分缩略词的过程中出现问题：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import word_tokenize</span><br><span class="line">&gt;&gt;&gt; from replacers import RegexpReplacer</span><br><span class="line">&gt;&gt;&gt; replacer=RegexpReplacer()</span><br><span class="line">&gt;&gt;&gt; word_tokenize(&quot;Don&apos;t hesitate to ask questions&quot;)</span><br><span class="line">[&apos;Do&apos;, &quot;n&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br><span class="line">&gt;&gt;&gt; word_tokenize(replacer.replace(&quot;Don&apos;t hesitate to ask questions&quot;))</span><br><span class="line">[&apos;Do&apos;, &apos;not&apos;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;questions&apos;]</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-3-4-处理重复字符"><a href="#1-3-4-处理重复字符" class="headerlink" title="1.3.4 处理重复字符"></a>1.3.4 处理重复字符</h3><p>有时候，人们在写作时会涉及一些可以引起语法错误的重复字符。例如考虑这样的一个句子：<code>I like it a lotttttt</code> 。在这里，<code>lotttttt</code> 是指<code>lot</code> 。所以现在我们将使用反向引用方法来去除这些重复的字符，在该方法中，一个字符指的是正则表达式分组中的先前字符。消除重复字符也被认为是标准化任务之一。</p>
<p>首先，将以下代码附加到先前创建的<code>replacers.py</code> 文件中：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class RepeatReplacer(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.repeat_regexp = re.compile(r&apos;(\w*)(\w)\2(\w*)&apos;)</span><br><span class="line">        self.repl = r&apos;\1\2\3&apos;</span><br><span class="line">    def replace(self, word):</span><br><span class="line">        repl_word = self.repeat_regexp.sub(self.repl, word)</span><br><span class="line">        if repl_word != word:</span><br><span class="line">            return self.replace(repl_word)</span><br><span class="line">        else:</span><br><span class="line">            return repl_word</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-3-5-去除重复字符的示例"><a href="#1-3-5-去除重复字符的示例" class="headerlink" title="1.3.5 去除重复字符的示例"></a>1.3.5 去除重复字符的示例</h3><p>让我们来看一个关于如何从一个标识符中去除重复字符的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from replacers import RepeatReplacer</span><br><span class="line">&gt;&gt;&gt; replacer=RepeatReplacer()</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&apos;lotttt&apos;)</span><br><span class="line">&apos;lot&apos;</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&apos;ohhhhh&apos;)</span><br><span class="line">&apos;oh&apos;</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&apos;ooohhhhh&apos;)</span><br><span class="line">&apos;oh&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>在<code>replacers.py</code> 文件中，<code>RepeatReplacer</code> 类通过编译正则表达式和替换的字符串来工作，并使用<code>backreference.Repeat_regexp</code> 来定义。它匹配可能是以零个或多个<code>(\ w
*)</code> 字符开始，以零个或多个<code>(\ w *)</code> ，或者一个<code>(\ w)</code> 其后面带有相同字符的字符而结束的字符。</p>
<p>例如，<code>lotttt</code> 被分拆为<code>(lo)(t)t(tt)</code> 。这里减少了一个<code>t</code> 并且字符串变为<code>lottt</code> 。分拆的过程还将继续，最后得到的结果字符串是lot。</p>
<p>使用<code>RepeatReplacer</code> 的问题是它会将<code>happy</code> 转换为<code>hapy</code> ，这样是不妥的。为了避免这个问题，我们可以嵌入<code>wordnet</code> 与其一起使用。</p>
<p>在先前创建的<code>replacers.py</code> 程序中，添加以下代码行以便包含<code>wordnet</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">from nltk.corpus import wordnet</span><br><span class="line">class RepeatReplacer(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.repeat_regexp = re.compile(r&apos;(\w*)(\w)\2(\w*)&apos;)</span><br><span class="line">        self.repl = r&apos;\1\2\3&apos;</span><br><span class="line">    def replace(self, word):</span><br><span class="line">        if wordnet.synsets(word):</span><br><span class="line">            return word</span><br><span class="line">        repl_word = self.repeat_regexp.sub(self.repl, word)</span><br><span class="line">        if repl_word != word:</span><br><span class="line">            return self.replace(repl_word)</span><br><span class="line">        else:</span><br><span class="line">            return repl_word</span><br></pre></td></tr></table></figure>

</details>




<p>现在，让我们来看看如何解决前面提到的问题：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from replacers import RepeatReplacer</span><br><span class="line">&gt;&gt;&gt; replacer=RepeatReplacer()</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&apos;happy&apos;)</span><br><span class="line">&apos;happy&apos;</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-3-6-用单词的同义词替换"><a href="#1-3-6-用单词的同义词替换" class="headerlink" title="1.3.6 用单词的同义词替换"></a>1.3.6 用单词的同义词替换</h3><p>现在我们将看到如何用其同义词来替代一个给定的单词。对于已经存在的<code>replacers.py</code> 文件，我们可以为其添加一个名为<code>WordReplacer</code> 的类，这个类提供了一个单词与其同义词之间的映射关系：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class WordReplacer(object):</span><br><span class="line">    def __init__(self, word_map):</span><br><span class="line">        self.word_map = word_map</span><br><span class="line">    def replace(self, word):</span><br><span class="line">        return self.word_map.get(word, word)</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-3-7-用单词的同义词替换的示例"><a href="#1-3-7-用单词的同义词替换的示例" class="headerlink" title="1.3.7 用单词的同义词替换的示例"></a>1.3.7 用单词的同义词替换的示例</h3><p>让我们来看一个有关用其同义词来替换单词的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from replacers import WordReplacer</span><br><span class="line">&gt;&gt;&gt; replacer=WordReplacer(&#123;&apos;congrats&apos;:&apos;congratulations&apos;&#125;)</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&apos;congrats&apos;)</span><br><span class="line">&apos;congratulations&apos;</span><br><span class="line">&gt;&gt;&gt; replacer.replace(&apos;maths&apos;)</span><br><span class="line">&apos;maths&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>在这段代码中，<code>replace ( )</code> 函数在<code>word_map</code> 中寻找单词对应的同义词。如果给定的单词存在同义词，则该单词将被其同义词替换；如果给定单词的同义词不存在，则不执行替换，将返回单词本身。</p>
<h2 id="1-4-在文本上应用Zipf定律"><a href="#1-4-在文本上应用Zipf定律" class="headerlink" title="1.4 在文本上应用Zipf定律"></a>1.4 在文本上应用Zipf定律</h2><p>Zipf定律指出，文本中标识符出现的频率与其在排序列表中的排名或位置成反比。该定律描述了标识符在语言中是如何分布的：一些标识符非常频繁地出现，另一些出现频率较低，还有一些基本上不出现。</p>
<p>让我们来看看NLTK中用于获取基于Zipf定律的双对数图（log-log plot）的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import gutenberg</span><br><span class="line">&gt;&gt;&gt; from nltk.probability import FreqDist</span><br><span class="line">&gt;&gt;&gt; import matplotlib</span><br><span class="line">&gt;&gt;&gt; import matplotlib.pyplot as plt</span><br><span class="line">&gt;&gt;&gt; matplotlib.use(&apos;TkAgg&apos;)</span><br><span class="line">&gt;&gt;&gt; fd = FreqDist()</span><br><span class="line">&gt;&gt;&gt; for text in gutenberg.fileids():</span><br><span class="line">. . . for word in gutenberg.words(text):</span><br><span class="line">. . . fd.inc(word)</span><br><span class="line">&gt;&gt;&gt; ranks = []</span><br><span class="line">&gt;&gt;&gt; freqs = []</span><br><span class="line">&gt;&gt;&gt; for rank, word in enumerate(fd):</span><br><span class="line">. . . ranks.append(rank+1)</span><br><span class="line">. . . freqs.append(fd[word])</span><br><span class="line">. . .</span><br><span class="line">&gt;&gt;&gt; plt.loglog(ranks, freqs)</span><br><span class="line">&gt;&gt;&gt; plt.xlabel(&apos;frequency(f)&apos;, fontsize=14, fontweight=&apos;bold&apos;)</span><br><span class="line">&gt;&gt;&gt; plt.ylabel(&apos;rank(r)&apos;, fontsize=14, fontweight=&apos;bold&apos;)</span><br><span class="line">&gt;&gt;&gt; plt.grid(True)</span><br><span class="line">&gt;&gt;&gt; plt.show()</span><br></pre></td></tr></table></figure>

</details>




<p>上述代码将获取一个关于单词在文档中的排名相对其出现的频率的双对数图。因此，我们可以通过查看单词的排名与其频率之间的比例关系来验证Zipf定律是否适用于所有文档。</p>
<h2 id="1-5-相似性度量"><a href="#1-5-相似性度量" class="headerlink" title="1.5 相似性度量"></a>1.5 相似性度量</h2><p>有许多可用于执行NLP任务的相似性度量。NLTK中的<code>nltk.metrics</code> 包用于提供各种评估或相似性度量，这将有利于执行各种各样的NLP任务。</p>
<p>在NLP中，为了测试标注器、分块器等的性能，可以使用从信息检索中检索到的标准分数。</p>
<p>让我们来看看如何使用标准分（从一个训练文件中获取的）来分析命名实体识别器的输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from __future__ import print_function</span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import *</span><br><span class="line">&gt;&gt;&gt; training=&apos;PERSON OTHER PERSON OTHER OTHER ORGANIZATION&apos;.split()</span><br><span class="line">&gt;&gt;&gt; testing=&apos;PERSON OTHER OTHER OTHER OTHER OTHER&apos;.split()</span><br><span class="line">&gt;&gt;&gt; print(accuracy(training,testing))</span><br><span class="line">0.6666666666666666</span><br><span class="line">&gt;&gt;&gt; trainset=set(training)</span><br><span class="line">&gt;&gt;&gt; testset=set(testing)</span><br><span class="line">&gt;&gt;&gt; precision(trainset,testset)</span><br><span class="line">1.0</span><br><span class="line">&gt;&gt;&gt; print(recall(trainset,testset))</span><br><span class="line">0.6666666666666666</span><br><span class="line">&gt;&gt;&gt; print(f_measure(trainset,testset))</span><br><span class="line">0.8</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-5-1-使用编辑距离算法执行相似性度量"><a href="#1-5-1-使用编辑距离算法执行相似性度量" class="headerlink" title="1.5.1 使用编辑距离算法执行相似性度量"></a>1.5.1 使用编辑距离算法执行相似性度量</h3><p>两个字符串之间的编辑距离或Levenshtein编辑距离算法用于计算为了使两个字符串相等所插入、替换或删除的字符数量。</p>
<p>在编辑距离算法中需要执行的操作包含以下内容：</p>
<ul>
<li><p>将字母从第一个字符串复制到第二个字符串（cost为0），并用另一个字母替换字母（cost为1）：</p>
<p>_D_ ( _i−_ 1 _,j−_ 1) _+ d_ ( _si,tj_ )（替换 /复制操作）</p>
</li>
<li><p>删除第一个字符串中的字母（cost为1）：</p>
<p>_D_ ( _i,j−_ 1) _+_ 1（删除操作）</p>
</li>
<li><p>在第二个字符串中插入一个字母（cost为1）：</p>
<p>_D_ ( _i,j_ ) _= min D_ ( _i−_ 1 _,j_ ) _+_ 1 （插入操作）</p>
</li>
</ul>
<p><code>nltk.metrics</code> 包中的Edit Distance算法的Python代码如下所示：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">def _edit_dist_init(len1, len2):</span><br><span class="line">    lev = []</span><br><span class="line">    for i in range(len1):</span><br><span class="line">        lev.append([0] * len2)      # initialize 2D array to zero</span><br><span class="line">    for i in range(len1):</span><br><span class="line">        lev[i][0] = i               # column 0: 0,1,2,3,4,...</span><br><span class="line">    for j in range(len2):</span><br><span class="line">        lev[0][j] = j               # row 0: 0,1,2,3,4,...</span><br><span class="line">    return lev</span><br><span class="line"></span><br><span class="line">def _edit_dist_step(lev,i,j,s1,s2,transpositions=False):</span><br><span class="line">c1 =s1[i-1]</span><br><span class="line">c2 =s2[j-1]</span><br><span class="line"></span><br><span class="line"># skipping a character in s1</span><br><span class="line">a =lev[i-1][j] +1</span><br><span class="line"># skipping a character in s2</span><br><span class="line">b =lev[i][j -1]+1</span><br><span class="line"># substitution</span><br><span class="line">c =lev[i-1][j-1]+(c1!=c2)</span><br><span class="line"># transposition</span><br><span class="line">d =c+1 # never picked by default</span><br><span class="line">if transpositions and i&gt;1 and j&gt;1:</span><br><span class="line">if s1[i -2]==c2 and s2[j -2]==c1:</span><br><span class="line">d =lev[i-2][j-2]+1</span><br><span class="line"># pick the cheapest</span><br><span class="line">lev[i][j] =min(a,b,c,d)</span><br><span class="line"></span><br><span class="line">def edit_distance(s1, s2, transpositions=False):</span><br><span class="line">    # set up a 2-D array</span><br><span class="line">    len1 = len(s1)</span><br><span class="line">    len2 = len(s2)</span><br><span class="line">    lev = _edit_dist_init(len1 + 1, len2 + 1)</span><br><span class="line"></span><br><span class="line">    # iterate over the array</span><br><span class="line">    for i in range(len1):</span><br><span class="line">    for j in range(len2):</span><br><span class="line">        _edit_dist_step(lev, i + 1, j + 1, s1, s2,</span><br><span class="line">transpositions=transpositions)</span><br><span class="line">    return lev[len1][len2]</span><br></pre></td></tr></table></figure>

</details>




<p>让我们看一看使用NLTK中的<code>nltk.metrics</code> 包来计算编辑距离的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import *</span><br><span class="line">&gt;&gt;&gt; edit_distance(&quot;relate&quot;,&quot;relation&quot;)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; edit_distance(&quot;suggestion&quot;,&quot;calculation&quot;)</span><br><span class="line">7</span><br></pre></td></tr></table></figure>

</details>





<p>这里，当我们计算<code>relate</code> 和<code>relation</code> 之间的编辑距离时，需要执行三个操作（一个替换操作和两个插入操作）。当计算<code>suggestion</code> 和<code>calculation</code> 之间的编辑距离时，需要执行七个操作（六个替换操作和一个插入操作）。</p>
<h3 id="1-5-2-使用Jaccard系数执行相似性度量"><a href="#1-5-2-使用Jaccard系数执行相似性度量" class="headerlink" title="1.5.2 使用Jaccard系数执行相似性度量"></a>1.5.2 使用Jaccard系数执行相似性度量</h3><p>Jaccard系数或Tanimoto系数可以认为是两个集合X和Y交集的相似程度。</p>
<p>它可以定义如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* _Jaccard_ ( _X_ , _Y_ )=| _X_ ∩ _Y_ |/| _XUY_ |。</span><br><span class="line">* _Jaccard_ ( _X_ , _X_ )=1。</span><br><span class="line">* _Jaccard_ ( _X_ , _Y_ )=0 _if_ _X_ ∩ _Y_ =0。</span><br></pre></td></tr></table></figure>

</details>

<p>有关Jaccard相似度的代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def jacc_similarity(query, document):</span><br><span class="line">first=set(query).intersection(set(document))</span><br><span class="line">second=set(query).union(set(document))</span><br><span class="line">return len(first)/len(second)</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看NLTK中Jaccard相似性系数的实现：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import *</span><br><span class="line">&gt;&gt;&gt; X=set([10,20,30,40])</span><br><span class="line">&gt;&gt;&gt; Y=set([20,30,60])</span><br><span class="line">&gt;&gt;&gt; print(jaccard_distance(X,Y))</span><br><span class="line">0.6</span><br></pre></td></tr></table></figure>

</details>




<h3 id="1-5-3-使用Smith-Waterman距离算法执行相似性度量"><a href="#1-5-3-使用Smith-Waterman距离算法执行相似性度量" class="headerlink" title="1.5.3 使用Smith Waterman距离算法执行相似性度量"></a>1.5.3 使用Smith Waterman距离算法执行相似性度量</h3><p>Smith Waterman距离算法类似于编辑距离算法。开发这种相似度指标以便检测相关蛋白质序列和DNA之间的光学比对。它包括被分配的成本和将字母表映射到成本值的函数（替换）；成本也分配给gap惩罚（插入或删除）。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1．0 _//_ start over</span><br><span class="line"></span><br><span class="line">2． _D(i−_ 1 _,j−_ 1 _) −d(si,tj)_ //subst/copy</span><br><span class="line"></span><br><span class="line">3． _D(i,j) = max D(i−_ 1 _,j) −G_ //insert</span><br><span class="line"></span><br><span class="line">1． _D(i,j−_ 1 _) −G //_ delete</span><br><span class="line"></span><br><span class="line">&gt; ![](Image00000.jpg) &gt;</span><br><span class="line">&gt; Distance is maximum over all _i_ , _j_ in table of _D(i,j)_ 。</span><br><span class="line"></span><br><span class="line">4． _G =_ 1 _//_ example value for gap</span><br><span class="line"></span><br><span class="line">5． _d(c,c) = −_ 2 _//_ context dependent substitution cost</span><br><span class="line"></span><br><span class="line">6． _d(c,d) = +_ 1 _//_ context dependent substitution cost</span><br></pre></td></tr></table></figure>

</details>


<p>与编辑距离算法类似，Smith Waterman的Python代码可以嵌入到<code>nltk.metrics</code> 包中，以便使用NLTK中的Smith Waterman算法执行字符串相似性度量。</p>
<h3 id="1-5-4-其他字符串相似性度量"><a href="#1-5-4-其他字符串相似性度量" class="headerlink" title="1.5.4 其他字符串相似性度量"></a>1.5.4 其他字符串相似性度量</h3><p>二进制距离是一个字符串相似性指标。如果两个标签相同，它的返回值为<code>0.0</code> ；否则，它的返回值为<code>1.0</code> 。</p>
<p>二进制距离度量的Python代码为：</p>
<pre><code>def binary_distance(label1, label2):
 return 0.0 if label1 == label2 else 1.0
</code></pre><p>让我们来看看在NLTK中如何实现二进制距离算法度量：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import *</span><br><span class="line">&gt;&gt;&gt; X = set([10,20,30,40])</span><br><span class="line">&gt;&gt;&gt; Y= set([30,50,70])</span><br><span class="line">&gt;&gt;&gt; binary_distance(X, Y)</span><br><span class="line">1.0</span><br></pre></td></tr></table></figure>

</details>




<p>当存在多个标签时，Masi距离基于部分协议。</p>
<p>包含在<code>nltk.metrics</code> 包中的<code>masi</code> 距离算法的Python代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def masi_distance(label1, label2):</span><br><span class="line">    len_intersection = len(label1.intersection(label2))</span><br><span class="line">    len_union = len(label1.union(label2))</span><br><span class="line">    len_label1 = len(label1)</span><br><span class="line">    len_label2 = len(label2)</span><br><span class="line">    if len_label1 == len_label2 and len_label1 == len_intersection:</span><br><span class="line">        m = 1</span><br><span class="line">    elif len_intersection == min(len_label1, len_label2):</span><br><span class="line">        m = 0.67</span><br><span class="line">    elif len_intersection &gt; 0:</span><br><span class="line">        m = 0.33</span><br><span class="line">    else:</span><br><span class="line">        m = 0</span><br><span class="line"></span><br><span class="line">return 1 - (len_intersection / float(len_union)) * m</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看NLTK中<code>masi</code> 距离算法的实现：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from __future__ import print_function</span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import *</span><br><span class="line">&gt;&gt;&gt; X = set([10,20,30,40])</span><br><span class="line">&gt;&gt;&gt; Y= set([30,50,70])</span><br><span class="line">&gt;&gt;&gt; print(masi_distance(X,Y))</span><br><span class="line">0.945</span><br></pre></td></tr></table></figure>

</details>




<h2 id="1-6-小结"><a href="#1-6-小结" class="headerlink" title="1.6 小结"></a>1.6 小结</h2><p>在本章中，你已经学会了各种可以在文本（由字符串集合组成）上执行的操作。你已经理解了字符串切分、替换和标准化的概念，以及使用NLTK在字符串上应用各种相似性度量方法。此外我们还讨论了可能适用于一些现存文档的Zipf定律。</p>
<p>在下一章中，我们将讨论各种语言建模技术以及各种不同的NLP任务。</p>
<h1 id="第2章-统计语言建模"><a href="#第2章-统计语言建模" class="headerlink" title="第2章 统计语言建模"></a>第2章 统计语言建模</h1><p>计算语言学是一个广泛应用于分析、软件应用程序和人机交互上下文的新兴领域。我们可以认为其是人工智能的一个子领域。计算语言学的应用范围包括机器翻译、语音识别、智能Web搜索、信息检索和智能拼写检查等。理解各种可以在自然语言文本上执行的预处理任务或者计算是至关重要的。在以下章节中，我们将会讨论一些计算单词频率、最大似然估计（Maximum Likelihood Estimation，MLE）模型、数据插值等的方法。但是首先让我们来看看本章将会涉及的各个主题，具体如下：</p>
<ul>
<li>计算单词频率（1-gram，2-gram，3-gram）。</li>
<li>为给定的文本开发MLE。</li>
<li>在MLE模型上应用平滑。</li>
<li>为MLE开发一个回退机制。</li>
<li>应用数据插值以获得混合搭配。</li>
<li>通过复杂度来评估语言模型。</li>
<li>在语言建模中应用Metropolis-Hastings算法。</li>
<li>在语言处理中应用Gibbs采样法。</li>
</ul>
<h2 id="2-1-理解单词频率"><a href="#2-1-理解单词频率" class="headerlink" title="2.1 理解单词频率"></a>2.1 理解单词频率</h2><p>词的搭配可以被定义为倾向于并存的两个或多个标识符的集合。例如: the United States, the United Kingdom, Union of Soviet Socialist Republics等。</p>
<p>Unigram（一元语法）代表单个标识符。以下代码用于为Alpino语料库生成<code>unigrams</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.util import ngrams</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import alpino</span><br><span class="line">&gt;&gt;&gt; alpino.words()</span><br><span class="line">[&apos;De&apos;, &apos;verzekeringsmaatschappijen&apos;, &apos;verhelen&apos;, ...]&gt;&gt;&gt;</span><br><span class="line">unigrams=ngrams(alpino.words(),1)</span><br><span class="line">&gt;&gt;&gt; for i in unigrams:</span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure>

</details>




<p>考虑另一个有关从alpino语料库生成<code>quadgrams</code> 或<code>fourgrams</code> （四元语法）的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.util import ngrams</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import alpino</span><br><span class="line">&gt;&gt;&gt; alpino.words()</span><br><span class="line">[&apos;De&apos;, &apos;verzekeringsmaatschappijen&apos;, &apos;verhelen&apos;, ...]</span><br><span class="line">&gt;&gt;&gt; quadgrams=ngrams(alpino.words(),4)</span><br><span class="line">&gt;&gt;&gt; for i in quadgrams:</span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure>

</details>




<p><code>bigram</code><br>（二元语法）指的是一对标识符。为了在文本中找到bigrams，首先需要搜索小写单词，把文本创建为小写单词列表后，然后创建<code>BigramCollocationFinder</code> 实例。在<code>nltk.metrics</code> 包中找到的<code>BigramAssocMeasures</code> 可用于在文本中查找bigrams：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.collocations import BigramCollocationFinder</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import webtext</span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import BigramAssocMeasures</span><br><span class="line">&gt;&gt;&gt; tokens=[t.lower() for t in webtext.words(&apos;grail.txt&apos;)]</span><br><span class="line">&gt;&gt;&gt; words=BigramCollocationFinder.from_words(tokens)</span><br><span class="line">&gt;&gt;&gt; words.nbest(BigramAssocMeasures.likelihood_ratio, 10)</span><br><span class="line">[(&quot;&apos;&quot;, &apos;s&apos;), (&apos;arthur&apos;, &apos;:&apos;), (&apos;#&apos;, &apos;1&apos;), (&quot;&apos;&quot;, &apos;t&apos;), (&apos;villager&apos;,</span><br><span class="line">&apos;#&apos;), (&apos;#&apos;, &apos;2&apos;), (&apos;]&apos;, &apos;[&apos;), (&apos;1&apos;, &apos;:&apos;), (&apos;oh&apos;, &apos;,&apos;), (&apos;black&apos;,</span><br><span class="line">&apos;knight&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>在上面的代码中，我们可以添加一个用来消除停止词和标点符号的单词过滤器：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import webtext</span><br><span class="line">&gt;&gt;&gt; from nltk.collocations import BigramCollocationFinder</span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import BigramAssocMeasures</span><br><span class="line">&gt;&gt;&gt; set = set(stopwords.words(&apos;english&apos;))</span><br><span class="line">&gt;&gt;&gt; stops_filter = lambda w: len(w) &lt; 3 or w in set</span><br><span class="line">&gt;&gt;&gt; tokens=[t.lower() for t in webtext.words(&apos;grail.txt&apos;)]</span><br><span class="line">&gt;&gt;&gt; words=BigramCollocationFinder.from_words(tokens)</span><br><span class="line">&gt;&gt;&gt; words.apply_word_filter(stops_filter)</span><br><span class="line">&gt;&gt;&gt; words.nbest(BigramAssocMeasures.likelihood_ratio, 10)</span><br><span class="line">[(&apos;black&apos;, &apos;knight&apos;), (&apos;clop&apos;, &apos;clop&apos;), (&apos;head&apos;, &apos;knight&apos;), (&apos;mumble&apos;,</span><br><span class="line">&apos;mumble&apos;), (&apos;squeak&apos;, &apos;squeak&apos;), (&apos;saw&apos;, &apos;saw&apos;), (&apos;holy&apos;, &apos;grail&apos;),</span><br><span class="line">(&apos;run&apos;, &apos;away&apos;), (&apos;french&apos;, &apos;guard&apos;), (&apos;cartoon&apos;, &apos;character&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>这里，我们可以将bigrams的频率更改为其他数字。</p>
<p>另一种从文本中生成bigrams的方法是使用词汇搭配查找器，如下代码所示：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.collocation import *</span><br><span class="line">&gt;&gt;&gt; text1=&quot;Hardwork is the key to success. Never give up!&quot;</span><br><span class="line">&gt;&gt;&gt; word = nltk.wordpunct_tokenize(text1)</span><br><span class="line">&gt;&gt;&gt; finder = BigramCollocationFinder.from_words(word)</span><br><span class="line">&gt;&gt;&gt; bigram_measures = nltk.collocations.BigramAssocMeasures()</span><br><span class="line">&gt;&gt;&gt; value = finder.score_ngrams(bigram_measures.raw_freq)</span><br><span class="line">&gt;&gt;&gt; sorted(bigram for bigram, score in value)</span><br><span class="line">[(&apos;.&apos;, &apos;Never&apos;), (&apos;Hardwork&apos;, &apos;is&apos;), (&apos;Never&apos;, &apos;give&apos;), (&apos;give&apos;,</span><br><span class="line">&apos;up&apos;), (&apos;is&apos;, &apos;the&apos;), (&apos;key&apos;, &apos;to&apos;), (&apos;success&apos;, &apos;.&apos;), (&apos;the&apos;, &apos;key&apos;),</span><br><span class="line">(&apos;to&apos;, &apos;success&apos;), (&apos;up&apos;, &apos;!&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>现在让我们看看另外一段从<code>alpino</code> 语料库生成bigrams的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.util import ngrams</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import alpino</span><br><span class="line">&gt;&gt;&gt; alpino.words()</span><br><span class="line">[&apos;De&apos;, &apos;verzekeringsmaatschappijen&apos;, &apos;verhelen&apos;, ...]</span><br><span class="line">&gt;&gt;&gt; bigrams_tokens=ngrams(alpino.words(),2)</span><br><span class="line">&gt;&gt;&gt; for i in bigrams_tokens:</span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure>

</details>




<p>此代码将从<code>alpino</code> 语料库生成bigrams。</p>
<p>现在我们来看看用于生成<code>trigrams</code> 的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.util import ngrams</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import alpino</span><br><span class="line">&gt;&gt;&gt; alpino.words()</span><br><span class="line">[&apos;De&apos;, &apos;verzekeringsmaatschappijen&apos;, &apos;verhelen&apos;, ...]&gt;&gt;&gt; trigrams_</span><br><span class="line">tokens=ngrams(alpino.words(),3)</span><br><span class="line">&gt;&gt;&gt; for i in trigrams_tokens:</span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure>

</details>




<p>为了生成<code>fourgrams</code> 并生成<code>fourgrams</code> 的频率，可以使用如下代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.collocations import *</span><br><span class="line">&gt;&gt;&gt; text=&quot;Hello how are you doing ? I hope you find the book</span><br><span class="line">interesting&quot;</span><br><span class="line">&gt;&gt;&gt; tokens=nltk.wordpunct_tokenize(text)</span><br><span class="line">&gt;&gt;&gt; fourgrams=nltk.collocations.QuadgramCollocationFinder.from_</span><br><span class="line">words(tokens)</span><br><span class="line">&gt;&gt;&gt; for fourgram, freq in fourgrams.ngram_fd.items():</span><br><span class="line">print(fourgram,freq)</span><br><span class="line"></span><br><span class="line">(&apos;hope&apos;, &apos;you&apos;, &apos;find&apos;, &apos;the&apos;) 1</span><br><span class="line">(&apos;Hello&apos;, &apos;how&apos;, &apos;are&apos;, &apos;you&apos;) 1</span><br><span class="line">(&apos;you&apos;, &apos;doing&apos;, &apos;?&apos;, &apos;I&apos;) 1</span><br><span class="line">(&apos;are&apos;, &apos;you&apos;, &apos;doing&apos;, &apos;?&apos;) 1</span><br><span class="line">(&apos;how&apos;, &apos;are&apos;, &apos;you&apos;, &apos;doing&apos;) 1</span><br><span class="line">(&apos;?&apos;, &apos;I&apos;, &apos;hope&apos;, &apos;you&apos;) 1</span><br><span class="line">(&apos;doing&apos;, &apos;?&apos;, &apos;I&apos;, &apos;hope&apos;) 1</span><br><span class="line">(&apos;find&apos;, &apos;the&apos;, &apos;book&apos;, &apos;interesting&apos;) 1</span><br><span class="line">(&apos;you&apos;, &apos;find&apos;, &apos;the&apos;, &apos;book&apos;) 1</span><br><span class="line">(&apos;I&apos;, &apos;hope&apos;, &apos;you&apos;, &apos;find&apos;) 1</span><br></pre></td></tr></table></figure>

</details>




<p>现在我们来看看为给定句子生成<code>ngrams</code> （n元语法）的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sent=&quot; Hello , please read the book thoroughly . If you have any</span><br><span class="line">queries , then don&apos;t hesitate to ask . There is no shortcut to success</span><br><span class="line">.&quot;</span><br><span class="line">&gt;&gt;&gt; n=5</span><br><span class="line">&gt;&gt;&gt; fivegrams=ngrams(sent.split(),n)</span><br><span class="line">&gt;&gt;&gt; for grams in fivegrams:</span><br><span class="line">    print(grams)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(&apos;Hello&apos;, &apos;,&apos;, &apos;please&apos;, &apos;read&apos;, &apos;the&apos;)</span><br><span class="line">(&apos;,&apos;, &apos;please&apos;, &apos;read&apos;, &apos;the&apos;, &apos;book&apos;)</span><br><span class="line">(&apos;please&apos;, &apos;read&apos;, &apos;the&apos;, &apos;book&apos;, &apos;thoroughly&apos;)</span><br><span class="line">(&apos;read&apos;, &apos;the&apos;, &apos;book&apos;, &apos;thoroughly&apos;, &apos;.&apos;)</span><br><span class="line">(&apos;the&apos;, &apos;book&apos;, &apos;thoroughly&apos;, &apos;.&apos;, &apos;If&apos;)</span><br><span class="line">(&apos;book&apos;, &apos;thoroughly&apos;, &apos;.&apos;, &apos;If&apos;, &apos;you&apos;)</span><br><span class="line">(&apos;thoroughly&apos;, &apos;.&apos;, &apos;If&apos;, &apos;you&apos;, &apos;have&apos;)</span><br><span class="line">(&apos;.&apos;, &apos;If&apos;, &apos;you&apos;, &apos;have&apos;, &apos;any&apos;)</span><br><span class="line">(&apos;If&apos;, &apos;you&apos;, &apos;have&apos;, &apos;any&apos;, &apos;queries&apos;)</span><br><span class="line">(&apos;you&apos;, &apos;have&apos;, &apos;any&apos;, &apos;queries&apos;, &apos;,&apos;)</span><br><span class="line">(&apos;have&apos;, &apos;any&apos;, &apos;queries&apos;, &apos;,&apos;, &apos;then&apos;)</span><br><span class="line">(&apos;any&apos;, &apos;queries&apos;, &apos;,&apos;, &apos;then&apos;, &quot;don&apos;t&quot;)</span><br><span class="line">(&apos;queries&apos;, &apos;,&apos;, &apos;then&apos;, &quot;don&apos;t&quot;, &apos;hesitate&apos;)</span><br><span class="line">(&apos;,&apos;, &apos;then&apos;, &quot;don&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;)</span><br><span class="line">(&apos;then&apos;, &quot;don&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;)</span><br><span class="line">(&quot;don&apos;t&quot;, &apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;.&apos;)</span><br><span class="line">(&apos;hesitate&apos;, &apos;to&apos;, &apos;ask&apos;, &apos;.&apos;, &apos;There&apos;)</span><br><span class="line">(&apos;to&apos;, &apos;ask&apos;, &apos;.&apos;, &apos;There&apos;, &apos;is&apos;)</span><br><span class="line">(&apos;ask&apos;, &apos;.&apos;, &apos;There&apos;, &apos;is&apos;, &apos;no&apos;)</span><br><span class="line">(&apos;.&apos;, &apos;There&apos;, &apos;is&apos;, &apos;no&apos;, &apos;shortcut&apos;)</span><br><span class="line">(&apos;There&apos;, &apos;is&apos;, &apos;no&apos;, &apos;shortcut&apos;, &apos;to&apos;)</span><br><span class="line">(&apos;is&apos;, &apos;no&apos;, &apos;shortcut&apos;, &apos;to&apos;, &apos;success&apos;)</span><br><span class="line">(&apos;no&apos;, &apos;shortcut&apos;, &apos;to&apos;, &apos;success&apos;, &apos;.&apos;)</span><br></pre></td></tr></table></figure>

</details>




<h3 id="2-1-1-为给定的文本开发MLE"><a href="#2-1-1-为给定的文本开发MLE" class="headerlink" title="2.1.1 为给定的文本开发MLE"></a>2.1.1 为给定的文本开发MLE</h3><p>最大似然估计（Maximum Likelihood Estimate，MLE），是NLP领域中的一项重要任务，其也被称作多元逻辑回归或条件指数分类器。Berger和Della Pietra曾于1996年首次介绍了它。最大熵模型被定义在NLTK中的<code>nltk.classify.maxent</code> 模块里，在该模块中，所有的概率分布被认为是与训练数据保持一致的。该模型用于指代两个特征，即输入特征和联合特征。输入特征可以认为是未加标签单词的特征，而联合特征可以认为是加标签单词的特征。MLE用于生成<code>freqdist</code> ，它包含了文本中给定标识符出现的概率分布。参数<code>freqdist</code> 由作为概率分布基础的频率分布组成。</p>
<p>让我们来看看NLTK中有关最大熵模型的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function,unicode_literals</span><br><span class="line">__docformat__=&apos;epytext en&apos;</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">import numpy</span><br><span class="line">except ImportError:</span><br><span class="line">    pass</span><br><span class="line">import tempfile</span><br><span class="line">import os</span><br><span class="line">from collections import defaultdict</span><br><span class="line">from nltk import compat</span><br><span class="line">from nltk.data import gzip_open_unicode</span><br><span class="line">from nltk.util import OrderedDict</span><br><span class="line">from nltk.probability import DictionaryProbDist</span><br><span class="line">from nltk.classify.api import ClassifierI</span><br><span class="line">from nltk.classify.util import CutoffChecker,accuracy,log_likelihood</span><br><span class="line">from nltk.classify.megam import (call_megam,</span><br><span class="line">write_megam_file,parse_megam_weights)</span><br><span class="line">from nltk.classify.tadm import call_tadm,write_tadm_file,parse_tadm_</span><br><span class="line">weights</span><br></pre></td></tr></table></figure>

</details>




<p>在以上代码中，<code>nltk.probability</code> 包含了<code>FreqDist</code> 类，该类可以用来确定文本中单个标识符出现的频率。</p>
<p><code>ProbDistI</code> 用于确定单个标识符在文本中出现的概率分布。基本上有两种概率分布：派生概率分布和分析概率分布。派生概率分布是从频率分布中获取的，而分析概率分布则是从参数中获取的，例如方差。</p>
<p>为了获取频率分布，可以使用最大似然估计。它基于各个标识符在频率分布中的频率来计算其概率：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   class MLEProbDist(ProbDistI):</span><br><span class="line">   </span><br><span class="line">       def __init__(self, freqdist, bins=None):</span><br><span class="line">           self._freqdist = freqdist</span><br><span class="line">   </span><br><span class="line">       def freqdist(self):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">此函数将在概率分布的基础上找到频率分布：</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       return self._freqdist</span><br><span class="line">   </span><br><span class="line">       def prob(self, sample):</span><br><span class="line">           return self._freqdist.freq(sample)</span><br><span class="line">   </span><br><span class="line">       def max(self):</span><br><span class="line">           return self._freqdist.max()</span><br><span class="line">   </span><br><span class="line">       def samples(self):</span><br><span class="line">           return self._freqdist.keys()</span><br><span class="line">   </span><br><span class="line">       def __repr__(self):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           It will return string representation of ProbDist</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           return &apos;&lt;MLEProbDist based on %d samples&gt;&apos; % self._</span><br><span class="line">   freqdist.N()</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   class LidstoneProbDist(ProbDistI):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">该类用于获取频率分布。该频率分布由实数 _Gamma_ 表示，其取值范围在0到1之间。`LidstoneProbDist` 使用计数 _c_ 、样本结果 _N_ 和能够从概率分布中获取的样本值 _B_ 来计算给定样本概率的公式如下： _(c+Gamma)/(N+B*Gamma)_ 。</span><br><span class="line"></span><br><span class="line">这也意味着将 _Gamma_ 加到了每一个可能的样本结果的计数上，并且从给定的频率分布中计算出了MLE：</span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   SUM_TO_ONE = False</span><br><span class="line">       def __init__(self, freqdist, gamma, bins=None):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">Lidstone用于计算概率分布以便获取`freqdist` 。</span><br><span class="line"></span><br><span class="line">参数`freqdist` 可以定义为概率估计所基于的频率分布。</span><br><span class="line"></span><br><span class="line">参数`bins` 可以被定义为能够从概率分布中获取的样本值，概率的总和等于1：</span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">           if (bins == 0) or (bins is None and freqdist.N() == 0):</span><br><span class="line">               name = self.__class__.__name__[:-8]</span><br><span class="line">               raise ValueError(&apos;A %s probability distribution &apos; % name +</span><br><span class="line">   &apos;must have at least one bin.&apos;)</span><br><span class="line">           if (bins is not None) and (bins &lt; freqdist.B()):</span><br><span class="line">               name = self.__class__.__name__[:-8]</span><br><span class="line">               raise ValueError(&apos;\nThe number of bins in a %s</span><br><span class="line">   distribution &apos; % name +</span><br><span class="line">   &apos;(%d) must be greater than or equal to\n&apos; % bins +</span><br><span class="line">   &apos;the number of bins in the FreqDist used &apos; +</span><br><span class="line">   &apos;to create it (%d).&apos; % freqdist.B())</span><br><span class="line">   </span><br><span class="line">           self._freqdist = freqdist</span><br><span class="line">           self._gamma = float(gamma)</span><br><span class="line">           self._N = self._freqdist.N()</span><br><span class="line">   </span><br><span class="line">           if bins is None:</span><br><span class="line">               bins = freqdist.B()</span><br><span class="line">           self._bins = bins</span><br><span class="line">   </span><br><span class="line">           self._divisor = self._N + bins * gamma</span><br><span class="line">           if self._divisor == 0.0:</span><br><span class="line">               # In extreme cases we force the probability to be 0,</span><br><span class="line">               # which it will be, since the count will be 0:</span><br><span class="line">               self._gamma = 0</span><br><span class="line">               self._divisor = 1</span><br><span class="line">   </span><br><span class="line">   def freqdist(self):</span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">该函数基于概率分布获取了频率分布：</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">           return self._freqdist</span><br><span class="line">   </span><br><span class="line">   def prob(self, sample):</span><br><span class="line">   c = self._freqdist[sample]</span><br><span class="line">           return (c + self._gamma) / self._divisor</span><br><span class="line">   </span><br><span class="line">      def max(self):</span><br><span class="line">    # To obtain most probable sample, choose the one</span><br><span class="line">   # that occurs very frequently.</span><br><span class="line">           return self._freqdist.max()</span><br><span class="line">   </span><br><span class="line">   def samples(self):</span><br><span class="line">           return self._freqdist.keys()</span><br><span class="line">   </span><br><span class="line">   def discount(self):</span><br><span class="line">       gb = self._gamma * self._bins</span><br><span class="line">           return gb / (self._N + gb)</span><br><span class="line">   </span><br><span class="line">       def __repr__(self):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   String representation of ProbDist is obtained.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           return &apos;&lt;LidstoneProbDist based on %d samples&gt;&apos; % self._</span><br><span class="line">   freqdist.N()</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   class LaplaceProbDist(LidstoneProbDist):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">该类用于获取频率分布。它使用计数 _c_ 、样本结果 _N_ 和能够被生成的样本值的频率 _B_ 来计算一个样本的概率，计算公式如下：</span><br><span class="line"></span><br><span class="line">( _c+_ 1) _/_ ( _N+B_ )</span><br><span class="line"></span><br><span class="line">这也意味着将1加到了每一个可能的样本结果的计数上，并且获取了所得频率分布的最大似然估计：</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       def __init__(self, freqdist, bins=None):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">`LaplaceProbDist` 用于获取为生成`freqdist` 的概率分布。</span><br><span class="line"></span><br><span class="line">参数`freqdist` 用于获取基于概率估计的频率分布。</span><br><span class="line"></span><br><span class="line">参数`bins` 可以被认为是能够被生成的样本值的频率。概率的总和必须为1：</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">           LidstoneProbDist.__init__(self, freqdist, 1, bins)</span><br><span class="line">   </span><br><span class="line">       def __repr__(self):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           String representation of ProbDist is obtained.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">           return &apos;&lt;LaplaceProbDist based on %d samples&gt;&apos; % self._</span><br><span class="line">   freqdist.N()</span><br><span class="line">   </span><br><span class="line">   class ELEProbDist(LidstoneProbDist):</span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">该类用于获取频率分布。它使用计数 _c_ ，样本结果 _N_ 和能够被生成的样本值的频率 _B_ 来计算一个样本的概率，计算公式如下：</span><br><span class="line"></span><br><span class="line">( _c+_ 0.5) _/_ ( _N+B/_ 2)</span><br><span class="line"></span><br><span class="line">这也意味着将`0.5` 加到了每一个可能的样本结果的计数上，并且获取了所得频率分布的最大似然估计：</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       def __init__(self, freqdist, bins=None):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">预期似然估计用于获取生成`freqdist` 的概率分布。参数`freqdist` 用于获取基于概率估计的频率分布。</span><br><span class="line"></span><br><span class="line">参数`bins` 可以被认为是能够被生成的样本值的频率。概率的总和必须为1：</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   LidstoneProbDist.__init__(self, freqdist, 0.5, bins)</span><br><span class="line">   </span><br><span class="line">       def __repr__(self):</span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   String representation of ProbDist is obtained.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">           return &apos;&lt;ELEProbDist based on %d samples&gt;&apos; % self._</span><br><span class="line">   freqdist.N()</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   class WittenBellProbDist(ProbDistI):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   `WittenBellProbDist` 类用于获取概率分布。在之前看到的样本频率的基础上，该类用于获取均匀的概率质量。关于样本概率质量的计算公式如下：</span><br><span class="line">_T /_ ( _N + T_ )</span><br><span class="line">这里， _T_ 是观察到的样本数， _N_ 是观察到的事件的总数。样本的概率质量等于即将出现的新样本的最大似然估计。所有概率的总和等于1：</span><br><span class="line"></span><br><span class="line">   Here,</span><br><span class="line">        p = T / Z (N + T), if count = 0</span><br><span class="line">        p = c / (N + T), otherwise</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       def __init__(self, freqdist, bins=None):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">此段代码获取了概率分布。该概率用于向未知的样本提供均匀的概率质量。样本的概率质量计算公式给出如下：</span><br><span class="line"></span><br><span class="line">_T /_ ( _N + T_ )</span><br><span class="line"></span><br><span class="line">这里， _T_ 是观察到的样本数， _N_ 是观察到的事件的总数。样本的概率质量等于即将出现的新样本的最大似然估计。所有概率的总和等于1：</span><br><span class="line"></span><br><span class="line">    Here,</span><br><span class="line">         p = T / Z (N + T), if count = 0</span><br><span class="line">         p = c / (N + T), otherwise</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Z是使用这些值和一个bin值计算出的规范化因子。</span><br><span class="line"></span><br><span class="line">参数`freqdist` 用于估算可以从中获取概率分布的频率计数。</span><br><span class="line"></span><br><span class="line">参数`bins` 可以定义为样本的可能类型的数量：</span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">           assert bins is None or bins &gt;= freqdist.B(),\</span><br><span class="line">   &apos;bins parameter must not be less than %d=freqdist.B()&apos; % freqdist.B()</span><br><span class="line">           if bins is None:</span><br><span class="line">               bins = freqdist.B()</span><br><span class="line">           self._freqdist = freqdist</span><br><span class="line">           self._T = self._freqdist.B()</span><br><span class="line">           self._Z = bins - self._freqdist.B()</span><br><span class="line">           self._N = self._freqdist.N()</span><br><span class="line">           # self._P0 is P(0), precalculated for efficiency:</span><br><span class="line">           if self._N==0:</span><br><span class="line">               # if freqdist is empty, we approximate P(0) by a</span><br><span class="line">   UniformProbDist:</span><br><span class="line">               self._P0 = 1.0 / self._Z</span><br><span class="line">           else:</span><br><span class="line">               self._P0 = self._T / float(self._Z * (self._N + self._T))</span><br><span class="line">   </span><br><span class="line">       def prob(self, sample):</span><br><span class="line">           # inherit docs from ProbDistI</span><br><span class="line">           c = self._freqdist[sample]</span><br><span class="line">           return (c / float(self._N + self._T) if c != 0 else self._P0)</span><br><span class="line">   </span><br><span class="line">       def max(self):</span><br><span class="line">           return self._freqdist.max()</span><br><span class="line">   </span><br><span class="line">       def samples(self):</span><br><span class="line">           return self._freqdist.keys()</span><br><span class="line">   </span><br><span class="line">       def freqdist(self):</span><br><span class="line">           return self._freqdist</span><br><span class="line">   </span><br><span class="line">       def discount(self):</span><br><span class="line">           raise NotImplementedError()</span><br><span class="line">   </span><br><span class="line">       def __repr__(self):</span><br><span class="line"></span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   String representation of ProbDist is obtained.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">           return &apos;&lt;WittenBellProbDist based on %d samples&gt;&apos; % self._</span><br><span class="line">   freqdist.N()</span><br></pre></td></tr></table></figure>

</details>




<p>我们可以使用最大似然估计来执行测试，让我们考虑如下NLTK中有关MLE的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.probability import *</span><br><span class="line">&gt;&gt;&gt; train_and_test(mle)</span><br><span class="line">28.76%</span><br><span class="line">&gt;&gt;&gt; train_and_test(LaplaceProbDist)</span><br><span class="line">69.16%</span><br><span class="line">&gt;&gt;&gt; train_and_test(ELEProbDist)</span><br><span class="line">76.38%</span><br><span class="line">&gt;&gt;&gt; def lidstone(gamma):</span><br><span class="line">    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; train_and_test(lidstone(0.1))</span><br><span class="line">86.17%</span><br><span class="line">&gt;&gt;&gt; train_and_test(lidstone(0.5))</span><br><span class="line">76.38%</span><br><span class="line">&gt;&gt;&gt; train_and_test(lidstone(1.0))</span><br><span class="line">69.16%</span><br></pre></td></tr></table></figure>

</details>





<h3 id="2-1-2-隐马尔科夫模型估计"><a href="#2-1-2-隐马尔科夫模型估计" class="headerlink" title="2.1.2 隐马尔科夫模型估计"></a>2.1.2 隐马尔科夫模型估计</h3><p>隐马尔科夫模型（Hidden Markov Model，HMM）包含观察状态和帮助确定观察状态的隐藏状态。我们来看看关于HMM的图解说明，如图2-1所示，x表示隐藏状态，y表示观察状态。</p>
<p><img src="Image00003.jpg" alt></p>
<p>图2-1</p>
<p>我们可以使用HMM估计执行测试，让我们考虑如下使用<code>Brown</code> 语料库的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; corpus = nltk.corpus.brown.tagged_sents(categories=&apos;adventure&apos;)</span><br><span class="line">[:700]</span><br><span class="line">&gt;&gt;&gt; print(len(corpus))</span><br><span class="line">700</span><br><span class="line">&gt;&gt;&gt; from nltk.util import unique_list</span><br><span class="line">&gt;&gt;&gt; tag_set = unique_list(tag for sent in corpus for (word,tag) in</span><br><span class="line">sent)</span><br><span class="line">&gt;&gt;&gt; print(len(tag_set))</span><br><span class="line">104</span><br><span class="line">&gt;&gt;&gt; symbols = unique_list(word for sent in corpus for (word,tag) in</span><br><span class="line">sent)</span><br><span class="line">&gt;&gt;&gt; print(len(symbols))</span><br><span class="line">1908</span><br><span class="line">&gt;&gt;&gt; print(len(tag_set))</span><br><span class="line">104</span><br><span class="line">&gt;&gt;&gt; symbols = unique_list(word for sent in corpus for (word,tag) in</span><br><span class="line">sent)</span><br><span class="line">&gt;&gt;&gt; print(len(symbols))</span><br><span class="line">1908</span><br><span class="line">&gt;&gt;&gt; trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)</span><br><span class="line">&gt;&gt;&gt; train_corpus = []</span><br><span class="line">&gt;&gt;&gt; test_corpus = []</span><br><span class="line">&gt;&gt;&gt; for i in range(len(corpus)):</span><br><span class="line">if i % 10:</span><br><span class="line">train_corpus += [corpus[i]]</span><br><span class="line">else:</span><br><span class="line">test_corpus += [corpus[i]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(len(train_corpus))</span><br><span class="line">630</span><br><span class="line">&gt;&gt;&gt; print(len(test_corpus))</span><br><span class="line">70</span><br><span class="line">&gt;&gt;&gt; def train_and_test(est):</span><br><span class="line">hmm = trainer.train_supervised(train_corpus, estimator=est)</span><br><span class="line">print(&apos;%.2f%%&apos; % (100 * hmm.evaluate(test_corpus)))</span><br></pre></td></tr></table></figure>

</details>




<p>在上面的代码中，我们创建了一个90％用于训练和10％用于测试的文件，并且我们已经测试了估计量。</p>
<h2 id="2-2-在MLE模型上应用平滑"><a href="#2-2-在MLE模型上应用平滑" class="headerlink" title="2.2 在MLE模型上应用平滑"></a>2.2 在MLE模型上应用平滑</h2><p>平滑（Smoothing）用于处理之前未曾出现过的单词。因此，未知单词的概率为0。为了解决这个问题，我们使用了平滑。</p>
<h3 id="2-2-1-加法平滑"><a href="#2-2-1-加法平滑" class="headerlink" title="2.2.1 加法平滑"></a>2.2.1 加法平滑</h3><p>在18世纪，Laplace发明了加法平滑。在加法平滑中，需要将每个单词的计数加1。除了1之外，任何其他数值均可以被加到未知单词的计数上，以便未知单词可以被处理并且使它们的概率不为0。伪计数是指被加到未知单词计数上以使其概率不为0的值（即1或非0值）。</p>
<p>让我们考虑如下NLTK中有关加法平滑的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; corpus=u&quot;&lt;s&gt; hello how are you doing ? Hope you find the book</span><br><span class="line">interesting. &lt;/s&gt;&quot;.split()</span><br><span class="line">&gt;&gt;&gt; sentence=u&quot;&lt;s&gt;how are you doing&lt;/s&gt;&quot;.split()</span><br><span class="line">&gt;&gt;&gt; vocabulary=set(corpus)</span><br><span class="line">&gt;&gt;&gt; len(vocabulary)</span><br><span class="line">13</span><br><span class="line">&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(nltk.bigrams(corpus))</span><br><span class="line">&gt;&gt;&gt; # The corpus counts of each bigram in the sentence:</span><br><span class="line">&gt;&gt;&gt; [cfd[a][b] for (a,b) in nltk.bigrams(sentence)]</span><br><span class="line">[0, 1, 0]</span><br><span class="line">&gt;&gt;&gt; # The counts for each word in the sentence:</span><br><span class="line">&gt;&gt;&gt; [cfd[a].N() for (a,b) in nltk.bigrams(sentence)]</span><br><span class="line">[0, 1, 2]</span><br><span class="line">&gt;&gt;&gt; # There is already a FreqDist method for MLE probability:</span><br><span class="line">&gt;&gt;&gt; [cfd[a].freq(b) for (a,b) in nltk.bigrams(sentence)]</span><br><span class="line">[0, 1.0, 0.0]</span><br><span class="line">&gt;&gt;&gt; # Laplace smoothing of each bigram count:</span><br><span class="line">&gt;&gt;&gt; [1 + cfd[a][b] for (a,b) in nltk.bigrams(sentence)]</span><br><span class="line">[1, 2, 1]</span><br><span class="line">&gt;&gt;&gt; # We need to normalise the counts for each word:</span><br><span class="line">&gt;&gt;&gt; [len(vocabulary) + cfd[a].N() for (a,b) in nltk.bigrams(sentence)]</span><br><span class="line">[13, 14, 15]</span><br><span class="line">&gt;&gt;&gt; # The smoothed Laplace probability for each bigram:</span><br><span class="line">&gt;&gt;&gt; [1.0 * (1+cfd[a][b]) / (len(vocabulary)+cfd[a].N()) for (a,b) in</span><br><span class="line">nltk.bigrams(sentence)]</span><br><span class="line">[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]</span><br></pre></td></tr></table></figure>

</details>




<p>考虑另一种执行加法平滑或者说生成Laplace概率分布的方法：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; # MLEProbDist is the unsmoothed probability distribution:</span><br><span class="line">&gt;&gt;&gt; cpd_mle = nltk.ConditionalProbDist(cfd, nltk.MLEProbDist,</span><br><span class="line">bins=len(vocabulary))</span><br><span class="line">&gt;&gt;&gt; # Now we can get the MLE probabilities by using the .prob method:</span><br><span class="line">&gt;&gt;&gt; [cpd_mle[a].prob(b) for (a,b) in nltk.bigrams(sentence)]</span><br><span class="line">[0, 1.0, 0.0]</span><br><span class="line">&gt;&gt;&gt; # LaplaceProbDist is the add-one smoothed ProbDist:</span><br><span class="line">&gt;&gt;&gt; cpd_laplace = nltk.ConditionalProbDist(cfd, nltk.LaplaceProbDist,</span><br><span class="line">bins=len(vocabulary))</span><br><span class="line">&gt;&gt;&gt; # Getting the Laplace probabilities is the same as for MLE:</span><br><span class="line">&gt;&gt;&gt; [cpd_laplace[a].prob(b) for (a,b) in nltk.bigrams(sentence)]</span><br><span class="line">[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]</span><br></pre></td></tr></table></figure>

</details>




<h3 id="2-2-2-Good-Turing平滑"><a href="#2-2-2-Good-Turing平滑" class="headerlink" title="2.2.2 Good Turing平滑"></a>2.2.2 Good Turing平滑</h3><p>Good Turing平滑是由Alan Turing和他的统计助理I.J. Good提出的。这是一种有效的平滑方法，这种方法提高了用于执行语言学任务的统计技术的性能，例如词义消歧（WSD）、命名实体识别（NER）、拼写校正、机器翻译等。此方法有助于预测未知对象的概率。在该方法中，我们感兴趣的对象服从二项分布。在大样本量的基础上，该方法可用于计算出现0 次或出现较低次数样本的质量概率。通过对对数空间上的一条线性直线进行线性回归运算，Simple Good Turing可以执行从一个频率到另一个频率的近似估计。如果 _c_ \是调整后的计数，它将计算如下：</p>
<pre><code>_c\ =_ ( _c +_ 1) _N_ ( _c +_ 1) _/ N_ ( _c_ ) _c &gt;=_ 1

_c ==_ 0，训练文件中的零频率的样本 _= N_ (1)。
</code></pre><p>这里， _c_ 是初始计数， _N_ ( _i_ )是用计数 _i_ 观察到的事件类型的数量。</p>
<p>Bill Gale和Geoffrey Sampson已经呈现了Simple Good Turing平滑：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   class SimpleGoodTuringProbDist(ProbDistI):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">       Given a pair (pi, qi), where pi refers to the frequency and</span><br><span class="line">       qi refers to the frequency of frequency, our aim is to minimize</span><br><span class="line">   the</span><br><span class="line">       square variation. E(p) and E(q) is the mean of pi and qi.</span><br><span class="line">   </span><br><span class="line">       - slope, b = sigma ((pi-E(p)(qi-E(q))) / sigma ((pi-E(p))(pi-E(p)))</span><br><span class="line">       - intercept: a = E(q) - b.E(p)</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">       SUM_TO_ONE = False</span><br><span class="line">       def __init__(self, freqdist, bins=None):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           param freqdist refers to the count of frequency from which</span><br><span class="line">   probability</span><br><span class="line">           distribution is estimated.</span><br><span class="line">           Param bins is used to estimate the possible number of samples.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           assert bins is None or bins &gt; freqdist.B(),\</span><br><span class="line">   &apos;bins parameter must not be less than %d=freqdist.B()+1&apos; %</span><br><span class="line">   (freqdist.B()+1)</span><br><span class="line">           if bins is None:</span><br><span class="line">               bins = freqdist.B() + 1</span><br><span class="line">           self._freqdist = freqdist</span><br><span class="line">           self._bins = bins</span><br><span class="line">           r, nr = self._r_Nr()</span><br><span class="line">           self.find_best_fit(r, nr)</span><br><span class="line">           self._switch(r, nr)</span><br><span class="line">           self._renormalize(r, nr)</span><br><span class="line">   </span><br><span class="line">       def _r_Nr_non_zero(self):</span><br><span class="line">           r_Nr = self._freqdist.r_Nr()</span><br><span class="line">           del r_Nr[0]</span><br><span class="line">           return r_Nr</span><br><span class="line">       def _r_Nr(self):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   Split the frequency distribution in two list (r, Nr), where Nr(r) &gt; 0</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           nonzero = self._r_Nr_non_zero()</span><br><span class="line">   </span><br><span class="line">           if not nonzero:</span><br><span class="line">               return [], []</span><br><span class="line">           return zip(*sorted(nonzero.items()))</span><br><span class="line">   </span><br><span class="line">       def find_best_fit(self, r, nr):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           Use simple linear regression to tune parameters self._slope</span><br><span class="line">   and self._intercept in the log-log space based on count and</span><br><span class="line">   Nr(count) (Work in log space to avoid floating point underflow.)</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           # For higher sample frequencies the data points becomes</span><br><span class="line">   horizontal</span><br><span class="line">           # along line Nr=1. To create a more evident linear model in</span><br><span class="line">   log-log</span><br><span class="line">           # space, we average positive Nr values with the surrounding</span><br><span class="line">   zero</span><br><span class="line">           # values. (Church and Gale, 1991)</span><br><span class="line">   </span><br><span class="line">           if not r or not nr:</span><br><span class="line">               # Empty r or nr?</span><br><span class="line">               return</span><br><span class="line">   </span><br><span class="line">           zr = []</span><br><span class="line">           for j in range(len(r)):</span><br><span class="line">               i = (r[j-1] if j &gt; 0 else 0)</span><br><span class="line">               k = (2 * r[j] - i if j == len(r) - 1 else r[j+1])</span><br><span class="line">               zr_ = 2.0 * nr[j] / (k - i)</span><br><span class="line">               zr.append(zr_)</span><br><span class="line">   </span><br><span class="line">           log_r = [math.log(i) for i in r]</span><br><span class="line">           log_zr = [math.log(i) for i in zr]</span><br><span class="line">   </span><br><span class="line">           xy_cov = x_var = 0.0</span><br><span class="line">           x_mean = 1.0 * sum(log_r) / len(log_r)</span><br><span class="line">           y_mean = 1.0 * sum(log_zr) / len(log_zr)</span><br><span class="line">           for (x, y) in zip(log_r, log_zr):</span><br><span class="line">               xy_cov += (x - x_mean) * (y - y_mean)</span><br><span class="line">               x_var += (x - x_mean)**2</span><br><span class="line">           self._slope = (xy_cov / x_var if x_var != 0 else 0.0)</span><br><span class="line">               if self._slope &gt;= -1:</span><br><span class="line">                   warnings.warn(&apos;SimpleGoodTuring did not find a proper best</span><br><span class="line">   fit &apos;</span><br><span class="line">   &apos;line for smoothing probabilities of occurrences. &apos;</span><br><span class="line">   &apos;The probability estimates are likely to be &apos;</span><br><span class="line">   &apos;unreliable.&apos;)</span><br><span class="line">           self._intercept = y_mean - self._slope * x_mean</span><br><span class="line">   </span><br><span class="line">       def _switch(self, r, nr):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           Calculate the r frontier where we must switch from Nr to Sr</span><br><span class="line">           when estimating E[Nr].</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           for i, r_ in enumerate(r):</span><br><span class="line">               if len(r) == i + 1 or r[i+1] != r_ + 1:</span><br><span class="line">                   # We are at the end of r, or there is a gap in r</span><br><span class="line">                   self._switch_at = r_</span><br><span class="line">                   break</span><br><span class="line">   </span><br><span class="line">               Sr = self.smoothedNr</span><br><span class="line">               smooth_r_star = (r_ + 1) * Sr(r_+1) / Sr(r_)</span><br><span class="line">               unsmooth_r_star = 1.0 * (r_ + 1) * nr[i+1] / nr[i]</span><br><span class="line">   </span><br><span class="line">               std = math.sqrt(self._variance(r_, nr[i], nr[i+1]))</span><br><span class="line">               if abs(unsmooth_r_star-smooth_r_star) &lt;= 1.96 * std:</span><br><span class="line">                   self._switch_at = r_</span><br><span class="line">                   break</span><br><span class="line">   </span><br><span class="line">       def _variance(self, r, nr, nr_1):</span><br><span class="line">           r = float(r)</span><br><span class="line">           nr = float(nr)</span><br><span class="line">           nr_1 = float(nr_1)</span><br><span class="line">           return (r + 1.0)**2 * (nr_1 / nr**2) * (1.0 + nr_1 / nr)</span><br><span class="line">   </span><br><span class="line">       def _renormalize(self, r, nr):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">重整化对于确保获取到正确的概率分布是至关重要的。它可以通过公式 _N_ ( _1_ ) _/N_ 对未知的样本进行概率估计，然后对所有之前所见的样本概率进行重整来获取：</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">           prob_cov = 0.0</span><br><span class="line">           for r_, nr_ in zip(r, nr):</span><br><span class="line">               prob_cov += nr_ * self._prob_measure(r_)</span><br><span class="line">           if prob_cov:</span><br><span class="line">               self._renormal = (1 - self._prob_measure(0)) / prob_cov</span><br><span class="line">   </span><br><span class="line">       def smoothedNr(self, r):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           Return the number of samples with count r.</span><br><span class="line">   </span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   </span><br><span class="line">           # Nr = a*r^b (with b &lt; -1 to give the appropriate hyperbolic</span><br><span class="line">           # relationship)</span><br><span class="line">           # Estimate a and b by simple linear regression technique on</span><br><span class="line">           # the logarithmic form of the equation: log Nr = a + b*log(r)</span><br><span class="line">   </span><br><span class="line">           return math.exp(self._intercept + self._slope * math.log(r))</span><br><span class="line">   </span><br><span class="line">       def prob(self, sample):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           Return the sample&apos;s probability.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           count = self._freqdist[sample]</span><br><span class="line">           p = self._prob_measure(count)</span><br><span class="line">           if count == 0:</span><br><span class="line">               if self._bins == self._freqdist.B():</span><br><span class="line">                   p = 0.0</span><br><span class="line">               else:</span><br><span class="line">                   p = p / (1.0 * self._bins - self._freqdist.B())</span><br><span class="line">           else:</span><br><span class="line">               p = p * self._renormal</span><br><span class="line">           return p</span><br><span class="line">   </span><br><span class="line">       def _prob_measure(self, count):</span><br><span class="line">           if count == 0 and self._freqdist.N() == 0 :</span><br><span class="line">               return 1.0</span><br><span class="line">           elif count == 0 and self._freqdist.N() != 0:</span><br><span class="line">               return 1.0 * self._freqdist.Nr(1) / self._freqdist.N()</span><br><span class="line">           if self._switch_at &gt; count:</span><br><span class="line">               Er_1 = 1.0 * self._freqdist.Nr(count+1)</span><br><span class="line">               Er = 1.0 * self._freqdist.Nr(count)</span><br><span class="line">           else:</span><br><span class="line">               Er_1 = self.smoothedNr(count+1)</span><br><span class="line">               Er = self.smoothedNr(count)</span><br><span class="line">   </span><br><span class="line">           r_star = (count + 1) * Er_1 / Er</span><br><span class="line">           return r_star / self._freqdist.N()</span><br><span class="line">   </span><br><span class="line">       def check(self):</span><br><span class="line">           prob_sum = 0.0</span><br><span class="line">           for i in range(0, len(self._Nr)):</span><br><span class="line">               prob_sum += self._Nr[i] * self._prob_measure(i) / self._</span><br><span class="line">   renormal</span><br><span class="line">           print(&quot;Probability Sum:&quot;, prob_sum)</span><br><span class="line">           #assert prob_sum != 1.0, &quot;probability sum should be one!&quot;</span><br><span class="line">   </span><br><span class="line">       def discount(self):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           It is used to provide the total probability transfers from the</span><br><span class="line">           seen events to the unseen events.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           return 1.0 * self.smoothedNr(1) / self._freqdist.N()</span><br><span class="line">   </span><br><span class="line">       def max(self):</span><br><span class="line">           return self._freqdist.max()</span><br><span class="line">   </span><br><span class="line">       def samples(self):</span><br><span class="line">           return self._freqdist.keys()</span><br><span class="line">   </span><br><span class="line">       def freqdist(self):</span><br><span class="line">           return self._freqdist</span><br><span class="line">   </span><br><span class="line">       def __repr__(self):</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           It obtains the string representation of ProbDist.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">           return &apos;&lt;SimpleGoodTuringProbDist based on %d samples&gt;&apos;\</span><br><span class="line">                   % self._freqdist.N()</span><br></pre></td></tr></table></figure>

</details>



<p>让我们来看看NLTK中有关Simple Good Turing的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; gt = lambda fd, bins: SimpleGoodTuringProbDist(fd, bins=1e5)</span><br><span class="line">&gt;&gt;&gt; train_and_test(gt)</span><br><span class="line">5.17%</span><br></pre></td></tr></table></figure>

</details>




<h3 id="2-2-3-Kneser-Ney平滑"><a href="#2-2-3-Kneser-Ney平滑" class="headerlink" title="2.2.3 Kneser Ney平滑"></a>2.2.3 Kneser Ney平滑</h3><p>Kneser Ney平滑是与trigrams一起使用的。让我们来看看下面NLTK中的有关Kneser Ney平滑的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; corpus = [[((x[0],y[0],z[0]),(x[1],y[1],z[1]))</span><br><span class="line">    for x, y, z in nltk.trigrams(sent)]</span><br><span class="line">   for sent in corpus[:100]]</span><br><span class="line">&gt;&gt;&gt; tag_set = unique_list(tag for sent in corpus for (word,tag) in</span><br><span class="line">sent)</span><br><span class="line">&gt;&gt;&gt; len(tag_set)</span><br><span class="line">906</span><br><span class="line">&gt;&gt;&gt; symbols = unique_list(word for sent in corpus for (word,tag) in</span><br><span class="line">sent)</span><br><span class="line">&gt;&gt;&gt; len(symbols)</span><br><span class="line">1341</span><br><span class="line">&gt;&gt;&gt; trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)</span><br><span class="line">&gt;&gt;&gt; train_corpus = []</span><br><span class="line">&gt;&gt;&gt; test_corpus = []</span><br><span class="line">&gt;&gt;&gt; for i in range(len(corpus)):</span><br><span class="line">if i % 10:</span><br><span class="line">train_corpus += [corpus[i]]</span><br><span class="line">else:</span><br><span class="line">test_corpus += [corpus[i]]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; len(train_corpus)</span><br><span class="line">90</span><br><span class="line">&gt;&gt;&gt; len(test_corpus)</span><br><span class="line">10</span><br><span class="line">&gt;&gt;&gt; kn = lambda fd, bins: KneserNeyProbDist(fd)</span><br><span class="line">&gt;&gt;&gt; train_and_test(kn)</span><br><span class="line">0.86%</span><br></pre></td></tr></table></figure>

</details>




<h3 id="2-2-4-Witten-Bell平滑"><a href="#2-2-4-Witten-Bell平滑" class="headerlink" title="2.2.4 Witten Bell平滑"></a>2.2.4 Witten Bell平滑</h3><p>Witten Bell是用于处理具有0概率的未知单词的一种平滑算法。让我们考虑如下NLTK中关于Witten Bell平滑的代码：</p>
<pre><code>&gt;&gt;&gt; train_and_test(WittenBellProbDist)
6.90%
</code></pre><h2 id="2-3-为MLE开发一个回退机制"><a href="#2-3-为MLE开发一个回退机制" class="headerlink" title="2.3 为MLE开发一个回退机制"></a>2.3 为MLE开发一个回退机制</h2><p>Katz回退模型可以认为是一个具备高效生产力的n gram语言模型，如果在n gram中能够给出一个指定标识符的先前信息，那么该模型可以计算出其条件概率。依据这个模型，在训练文件中，如果n gram出现的次数多于n次，在已知其先前信息的条件下，标识符的条件概率与该n gram的MLE成正比。否则，条件概率相当于(n-1) gram的回退条件概率。</p>
<p>以下是NLTK中有关Katz回退模型的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def prob(self, word, context):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Evaluate the probability of this word in this context using Katz</span><br><span class="line">Backoff.</span><br><span class="line">: param word: the word to get the probability of</span><br><span class="line">: type word: str</span><br><span class="line">:param context: the context the word is in</span><br><span class="line">:type context: list(str)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">context = tuple(context)</span><br><span class="line">if(context+(word,) in self._ngrams) or (self._n == 1):</span><br><span class="line">return self[context].prob(word)</span><br><span class="line">else:</span><br><span class="line">return self._alpha(context) * self._backoff.prob(word,context[1:])</span><br></pre></td></tr></table></figure>

</details>




<h2 id="2-4-应用数据的插值以便获取混合搭配"><a href="#2-4-应用数据的插值以便获取混合搭配" class="headerlink" title="2.4 应用数据的插值以便获取混合搭配"></a>2.4 应用数据的插值以便获取混合搭配</h2><p>使用加法平滑模型bigram的局限是当我们处理罕见文本时就会回退到一个不可知的状态。例如，单词captivating在训练数据中出现了五次，其中三次出现在by之前，两次出现在the之前。使用加法平滑模型，在captivating之前，a和new的出现频率是一样的。这两种情况都是合理的，但与后者相比前者出现的可能性更大。这个问题可以通过使用unigram概率模型来修正。我们可以开发一个能够结合unigram和bigram概率模型的插值模型。</p>
<p>在语言模型训练工具SRILM中，我们先通过用<code>-order 1</code> 来训练unigram模型并用<code>-order 2</code> 来训练bigram模型来执行插值模型：</p>
<pre><code>ngram - count - text / home / linux / ieng6 / ln165w / public / data
/ engand hintrain . txt \ - vocab / home / linux / ieng6 / ln165w /
public / data / engandhinlexicon . txt \ - order 1 - addsmooth 0.0001
- lm wsj1 . lm
</code></pre><h2 id="2-5-通过复杂度来评估语言模型"><a href="#2-5-通过复杂度来评估语言模型" class="headerlink" title="2.5 通过复杂度来评估语言模型"></a>2.5 通过复杂度来评估语言模型</h2><p>NLTK中的<code>nltk.model.ngram</code> 模块有一个子模块<code>perplexity(text)</code> 。这个子模块用于评估指定文本的复杂度。复杂度（Perplexity）被定义为文本的2 **交叉熵。复杂度定义了概率模型或概率分布是怎样被用于预测文本的。</p>
<p><code>nltk.model.ngram</code> 模块中所呈现的用于评估文本复杂度的代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def perplexity(self, text):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">        Calculates the perplexity of the given text.</span><br><span class="line">        This is simply 2 ** cross-entropy for the text.</span><br><span class="line"></span><br><span class="line">        :param text: words to calculate perplexity of</span><br><span class="line">        :type text: list(str)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        return pow(2.0, self.entropy(text))</span><br></pre></td></tr></table></figure>

</details>




<h2 id="2-6-在语言建模中应用Metropolis-Hastings算法"><a href="#2-6-在语言建模中应用Metropolis-Hastings算法" class="headerlink" title="2.6 在语言建模中应用Metropolis-Hastings算法"></a>2.6 在语言建模中应用Metropolis-Hastings算法</h2><p>在马尔科夫链蒙特卡罗 (Markov Chain Monte Carlo，MCMC)中有多种关于后验概率的执行处理方法。一种方法是使用Metropolis- Hastings采样器。为了实现Metropolis- Hastings算法，我们需要标准的均匀分布、建议分布和与后验概率成正比的目标分布。下面的话题谈论了一个有关Metropolis- Hastings算法的示例。</p>
<h2 id="2-7-在语言处理中应用Gibbs采样法"><a href="#2-7-在语言处理中应用Gibbs采样法" class="headerlink" title="2.7 在语言处理中应用Gibbs采样法"></a>2.7 在语言处理中应用Gibbs采样法</h2><p>在Gibbs采样法的帮助下，可以通过从条件概率中采样建立马尔科夫链。当完成了对所有参数的迭代时，就完成了一次Gibbs采样周期。当不能从条件分布中采样时，则可以使用Metropolis- Hastings算法，这被称作Metropolis within Gibbs。Gibbs采样法可以认为是具有特殊建议分布的Metropolis- hastings采样法。在每一次迭代中，我们为每一个特定参数的新值抽取一个建议值。</p>
<p>考虑一个关于投掷两枚硬币的例子，它以一枚硬币正面朝上的次数和掷币次数为表征：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def bern(theta,z,N):</span><br><span class="line">&quot;&quot;&quot;Bernoulli likelihood with N trials and z successes.&quot;&quot;&quot;</span><br><span class="line">return np.clip(theta**z*(1-theta)**(N-z),0,1)</span><br><span class="line">def bern2(theta1,theta2,z1,z2,N1,N2):</span><br><span class="line">&quot;&quot;&quot;Bernoulli likelihood with N trials and z successes.&quot;&quot;&quot;</span><br><span class="line">return bern(theta1,z1,N1)*bern(theta2,z2,N2)</span><br><span class="line">def make_thetas(xmin,xmax,n):</span><br><span class="line">xs=np.linspace(xmin,xmax,n)</span><br><span class="line">widths=(xs[1:]-xs[:-1])/2.0</span><br><span class="line">thetas=xs[:-1]+widths</span><br><span class="line">return thetas</span><br><span class="line">def make_plots(X,Y,prior,likelihood,posterior,projection=None):</span><br><span class="line">fig,ax=plt.subplots(1,3,subplot_kw=dict(projection=projection,aspect=&apos;</span><br><span class="line">equal&apos;),figsize=(12,3))</span><br><span class="line">if projection==&apos;3d&apos;:</span><br><span class="line">ax[0].plot_surface(X,Y,prior,alpha=0.3,cmap=plt.cm.jet)</span><br><span class="line">ax[1].plot_surface(X,Y,likelihood,alpha=0.3,cmap=plt.cm.jet)</span><br><span class="line">ax[2].plot_surface(X,Y,posterior,alpha=0.3,cmap=plt.cm.jet)</span><br><span class="line">else:</span><br><span class="line">ax[0].contour(X,Y,prior)</span><br><span class="line">ax[1].contour(X,Y,likelihood)</span><br><span class="line">ax[2].contour(X,Y, posterior)</span><br><span class="line">ax[0].set_title(&apos;Prior&apos;)</span><br><span class="line">ax[1].set_title(&apos;Likelihood&apos;)</span><br><span class="line">ax[2].set_title(&apos;posteior&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">thetas1=make_thetas(0,1,101)</span><br><span class="line">thetas2=make_thetas(0,1,101)</span><br><span class="line">X,Y=np.meshgrid(thetas1,thetas2)</span><br></pre></td></tr></table></figure>

</details>




<p>对于Metropolis算法，可考虑以下值：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">a=2</span><br><span class="line">b=3</span><br><span class="line"></span><br><span class="line">z1=11</span><br><span class="line">N1=14</span><br><span class="line">z2=7</span><br><span class="line">N2=14</span><br><span class="line"></span><br><span class="line">prior=lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.beta(a,b).</span><br><span class="line">pdf(theta2)</span><br><span class="line">lik=partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)</span><br><span class="line">target=lambda theta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)</span><br><span class="line"></span><br><span class="line">theta=np.array([0.5,0.5])</span><br><span class="line">niters=10000</span><br><span class="line">burnin=500</span><br><span class="line">sigma=np.diag([0.2,0.2])</span><br><span class="line"></span><br><span class="line">thetas=np.zeros((niters-burnin,2),np.float)</span><br><span class="line">for i inrange(niters):</span><br><span class="line">new_theta=stats.multivariate_normal(theta,sigma).rvs()</span><br><span class="line">p=min(target(*new_theta)/target(*theta),1)</span><br><span class="line">if np.random.rand()&lt;p:</span><br><span class="line">theta=new_theta</span><br><span class="line">if i&gt;=burnin:</span><br><span class="line">thetas[i-burnin]=theta</span><br><span class="line">kde=stats.gaussian_kde(thetas.T)</span><br><span class="line">XY=np.vstack([X.ravel(),Y.ravel()])</span><br><span class="line">posterior_metroplis=kde(XY).reshape(X.shape)</span><br><span class="line">make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_metroplis)</span><br><span class="line">make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_</span><br><span class="line">metroplis,projection=&apos;3d&apos;)</span><br></pre></td></tr></table></figure>

</details>




<p>对于Gibbs，可考虑以下值：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">a=2</span><br><span class="line">b=3</span><br><span class="line"></span><br><span class="line">z1=11</span><br><span class="line">N1=14</span><br><span class="line">z2=7</span><br><span class="line">N2=14</span><br><span class="line"></span><br><span class="line">prior=lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.</span><br><span class="line">beta(a,b).pdf(theta2)</span><br><span class="line">lik=partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)</span><br><span class="line">target=lambda theta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)</span><br><span class="line"></span><br><span class="line">theta=np.array([0.5,0.5])</span><br><span class="line">niters=10000</span><br><span class="line">burnin=500</span><br><span class="line">sigma=np.diag([0.2,0.2])</span><br><span class="line"></span><br><span class="line">thetas=np.zeros((niters-burnin,2),np.float)</span><br><span class="line">for i inrange(niters):</span><br><span class="line">theta=[stats.beta(a+z1,b+N1-z1).rvs(),theta[1]]</span><br><span class="line">theta=[theta[0],stats.beta(a+z2,b+N2-z2).rvs()]</span><br><span class="line"></span><br><span class="line">if i&gt;=burnin:</span><br><span class="line">thetas[i-burnin]=theta</span><br><span class="line">kde=stats.gaussian_kde(thetas.T)</span><br><span class="line">XY=np.vstack([X.ravel(),Y.ravel()])</span><br><span class="line">posterior_gibbs=kde(XY).reshape(X.shape)</span><br><span class="line">make_plots(X,Y,prior(X,Y),lik(X,Y), posterior_gibbs)</span><br><span class="line">make_plots(X,Y,prior(X,Y),lik(X,Y), posterior_gibbs,projection=&apos;3d&apos;)</span><br></pre></td></tr></table></figure>

</details>




<p>在上面有关Metropolis和Gibbs的代码中，可以获取到先验概率、似然估计和后验概率的2D和3D图。</p>
<h2 id="2-8-小结"><a href="#2-8-小结" class="headerlink" title="2.8 小结"></a>2.8 小结</h2><p>在本章中，我们讨论了单词频率（unigram、bigram和trigram）。你已经学习了最大似然估计以及它在NLTK中的实现。此外我们还讨论了插值法、回退法、Gibbs采样法和Metropolis- hastings算法。同时我们还讨论了如何通过复杂度来进行语言建模。</p>
<p>在下一章中，我们将讨论词干提取器（Stemmer）和词形还原器（Lemmatizer），以及使用机器学习工具创建形态生成器（Morphological generator）。</p>
<h1 id="第3章-形态学：在实践中学习"><a href="#第3章-形态学：在实践中学习" class="headerlink" title="第3章 形态学：在实践中学习"></a>第3章 形态学：在实践中学习</h1><p>形态学可以定义为使用语素对单词的构成进行研究，语素是具有意义的最小语言单位。本章中，我们将会介绍词干提取和词形还原，以及有关非英文语言的词干提取器和词形还原器，使用机器学习工具开发形态分析器和形态生成器，还会介绍搜索引擎及其他诸如此类的概念。</p>
<p>简而言之，本章将包含以下主题：</p>
<ul>
<li>形态学简介。</li>
<li>理解词干提取器。</li>
<li>理解词形还原。</li>
<li>为非英文语言开发词干提取器。</li>
<li>形态分析器。</li>
<li>形态生成器。</li>
<li>搜索引擎。</li>
</ul>
<h2 id="3-1-形态学简介"><a href="#3-1-形态学简介" class="headerlink" title="3.1 形态学简介"></a>3.1 形态学简介</h2><p>形态学可以定义为在语素的帮助下对标识符的构成进行研究。语素是承载意义的基本语言单位。语素有两种类型：词根和词缀（后缀、前缀、中缀和环缀）。</p>
<p>词根也被称作自由语素，因为它们甚至可以在不添加词缀的情况下而存在。词缀被称作粘着语素，因为它们不能以自由的形式而存在，总是与自由语素共存。考虑单词<code>unbelievable</code> ，在这里，<code>believe</code> 是词根或者叫自由语素，它可以独立地存在。语素<code>un</code> 和<code>able</code> 是词缀或者叫粘着语素，它们不能以自由的形式而存在，但是可以与词根共存。语言可分为三类，即孤立语（isolating languages）、粘着语（agglutinative languages）和屈折语（inflecting languages）。形态学在这些语言中有着不同的含义。在孤立语中，单词仅由自由语素构成并且它们不具备任何时态（过去，现在和将来）和数（单数或复数）的信息，其中汉语是孤立语的一个例子。在粘着语中，是将短词结合在一起以传达复合的信息，其中土耳其语是粘着语的一个例子。在屈折语中，单词被分解成更简单的语言单位，但是所有这些语言单位表达了不同的含义，其中拉丁语是屈折语的一个例子。形态学过程包括以下几种类型：屈折、派生、半词缀、组合形式和复缀化。屈折意味着将单词转换为某种形式，以便它可以代表人称、数、时态、性别、所有格、语态和语气，这里，单词的句法类型保持不变。在派生词中，单词的句法类型也被改变了。半词缀是呈现单词的粘着语素，例如quality、noteworthy、antisocial、anticlockwise等词。</p>
<h2 id="3-2-理解词干提取器"><a href="#3-2-理解词干提取器" class="headerlink" title="3.2 理解词干提取器"></a>3.2 理解词干提取器</h2><p>词干提取可以被定义为一个通过去除单词中的词缀以获取词干的过程。以单词raining为例，词干提取器通过从raining中去除词缀来返回其词根或词干rain。为了提高信息检索的准确性，搜索引擎大多会使用词干提取来获取词干并将其存储为索引词。搜索引擎使用具有相同含义的同义词，这可能是一种被称为异文合并的查询扩展。Martin Porter已经设计了一个广为人知的被称作 _Porter_ 的词干提取算法。该算法基本上用于替换和消除英文单词中的一些众所周知的后缀。为了在NLTK中执行词干提取，我们可以简单地对<code>PorterStemmer</code> 类进行实例化，然后通过调用<code>stem</code> 方法来进行词干提取。</p>
<p>让我们来看看有关在NLTK中使用<code>PorterStemmer</code> 类进行词干提取的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import PorterStemmer</span><br><span class="line">&gt;&gt;&gt; stemmerporter = PorterStemmer()</span><br><span class="line">&gt;&gt;&gt; stemmerporter.stem(&apos;working&apos;)</span><br><span class="line">&apos;work&apos;</span><br><span class="line">&gt;&gt;&gt; stemmerporter.stem(&apos;happiness&apos;)</span><br><span class="line">&apos;happi&apos;</span><br></pre></td></tr></table></figure>

</details>




<p><code>PorterStemmer</code> 类被训练并已经掌握了英文的许多词干和单词形式。词干提取的过程需要一系列的步骤，并最终将单词变换成较短的单词或与词根具有相似含义的单词。Stemmer I接口定义了<code>stem()</code> 方法，所有的词干提取器类都继承自Stemmer I接口。继承关系如图3-1所示。</p>
<p><img src="Image00004.jpg" alt></p>
<p>图3-1</p>
<p>另一种被称作Lancaster的词干提取算法是由兰卡斯特大学（Lancaster University）提出的。类似于<code>PorterStemmer</code> 类，<code>LancasterStemmer</code> 类在NLTK中用于实现Lancaster词干提取算法。然而，两种算法之间的主要区别之一是Lancaster词干提取算法比Porter词干提取算法涉及更多不同情感词的使用。</p>
<p>让我们考虑如下用于描述在NLTK中执行Lancaster词干提取的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import LancasterStemmer</span><br><span class="line">&gt;&gt;&gt; stemmerlan=LancasterStemmer()</span><br><span class="line">&gt;&gt;&gt; stemmerlan.stem(&apos;working&apos;)</span><br><span class="line">&apos;work&apos;</span><br><span class="line">&gt;&gt;&gt; stemmerlan.stem(&apos;happiness&apos;)</span><br><span class="line">&apos;happy&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>在NLTK中，我们通过使用<code>RegexpStemmer</code> 类也可以构建属于我们自己的词干提取器。它的工作原理是通过接收一个字符串，并在找到其匹配的单词时删除该单词的前缀或后缀。</p>
<p>让我们考虑一个在NLTK中使用<code>RegexpStemmer</code> 进行词干提取的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import RegexpStemmer</span><br><span class="line">&gt;&gt;&gt; stemmerregexp=RegexpStemmer(&apos;ing&apos;)</span><br><span class="line">&gt;&gt;&gt; stemmerregexp.stem(&apos;working&apos;)</span><br><span class="line">&apos;work&apos;</span><br><span class="line">&gt;&gt;&gt; stemmerregexp.stem(&apos;happiness&apos;)</span><br><span class="line">&apos;happiness&apos;</span><br><span class="line">&gt;&gt;&gt; stemmerregexp.stem(&apos;pairing&apos;)</span><br><span class="line">&apos;pair&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>我们可以在无法使用<code>PorterStemmer</code> 和<code>LancasterStemmer</code> 进行词干提取的情况下使用<code>RegexpStemmer</code> 。</p>
<p><code>SnowballStemmer</code> 用于对除英文之外的其他13种语言进行词干提取。为了使用<code>SnowballStemmer</code> 执行词干提取，首先，为需要执行词干提取的语言创建一个实例，然后调用其<code>stem()</code> 方法，就完成了词干提取。</p>
<p>考虑如下NLTK中的代码示例，它使用<code>SnowballStemmer</code> 对西班牙语和法语执行了词干提取：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import SnowballStemmer</span><br><span class="line">&gt;&gt;&gt; SnowballStemmer.languages</span><br><span class="line">(&apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, &apos;french&apos;, &apos;german&apos;,</span><br><span class="line">&apos;hungarian&apos;, &apos;italian&apos;, &apos;norwegian&apos;, &apos;porter&apos;, &apos;portuguese&apos;,</span><br><span class="line">&apos;romanian&apos;, &apos;russian&apos;, &apos;spanish&apos;, &apos;swedish&apos;)</span><br><span class="line">&gt;&gt;&gt; spanishstemmer=SnowballStemmer(&apos;spanish&apos;)</span><br><span class="line">&gt;&gt;&gt; spanishstemmer.stem(&apos;comiendo&apos;)</span><br><span class="line">&apos;com&apos;</span><br><span class="line">&gt;&gt;&gt; frenchstemmer=SnowballStemmer(&apos;french&apos;)</span><br><span class="line">&gt;&gt;&gt; frenchstemmer.stem(&apos;manger&apos;)</span><br><span class="line">&apos;mang&apos;</span><br></pre></td></tr></table></figure>

</details>




<p><code>nltk.stem.api</code> 由可以在其中执行<code>stem</code> 函数的<code>StemmerI</code> 类组成。</p>
<p>考虑如下NLTK中的代码，它能够让我们执行词干提取：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class StemmerI(object):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">It is an interface that helps to eliminate morphological affixes from</span><br><span class="line">the tokens and the process is known as stemming.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def stem(self, token):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Eliminate affixes from token and stem is returned.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">raise NotImplementedError()</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看使用多个词干提取器进行词干提取的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem.porter import PorterStemmer</span><br><span class="line">&gt;&gt;&gt; from nltk.stem.lancaster import LancasterStemmer</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import SnowballStemmer</span><br><span class="line">&gt;&gt;&gt; def obtain_tokens():</span><br><span class="line">With open(&apos;/home/p/NLTK/sample1.txt&apos;) as stem: tok = nltk.word_</span><br><span class="line">tokenize(stem.read())</span><br><span class="line">return tokens</span><br><span class="line">&gt;&gt;&gt; def stemming(filtered):</span><br><span class="line">stem=[]</span><br><span class="line">for x in filtered:</span><br><span class="line">stem.append(PorterStemmer().stem(x))</span><br><span class="line">return stem</span><br><span class="line">&gt;&gt;&gt; if_name_==&quot;_main_&quot;:</span><br><span class="line">tok= obtain_tokens()</span><br><span class="line">&gt;&gt;&gt; print(&quot;tokens is %s&quot;)%(tok)</span><br><span class="line">&gt;&gt;&gt; stem_tokens= stemming(tok)</span><br><span class="line">&gt;&gt;&gt; print(&quot;After stemming is %s&quot;)%stem_tokens</span><br><span class="line">&gt;&gt;&gt; res=dict(zip(tok,stem_tokens))</span><br><span class="line">&gt;&gt;&gt; print(&quot;&#123;tok:stemmed&#125;=%s&quot;)%(result)</span><br></pre></td></tr></table></figure>

</details>




<h2 id="3-3-理解词形还原"><a href="#3-3-理解词形还原" class="headerlink" title="3.3 理解词形还原"></a>3.3 理解词形还原</h2><p>词形还原是一个用不同的词类将一个单词转换为某种形式的过程。词形还原后的单词形式是完全不同的。<code>WordNetLemmatizer</code> 类中内建的<code>morphy()</code> 函数用于词形还原。如果在WordNet中找不到输入的单词，则其保持不变。参数中，<code>pos</code> 所指的是输入单词的词性类别。</p>
<p>考虑一个在NLTK中执行词形还原的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer</span><br><span class="line">&gt;&gt;&gt; lemmatizer_output=WordNetLemmatizer()</span><br><span class="line">&gt;&gt;&gt; lemmatizer_output.lemmatize(&apos;working&apos;)</span><br><span class="line">&apos;working&apos;</span><br><span class="line">&gt;&gt;&gt; lemmatizer_output.lemmatize(&apos;working&apos;,pos=&apos;v&apos;)</span><br><span class="line">&apos;work&apos;</span><br><span class="line">&gt;&gt;&gt; lemmatizer_output.lemmatize(&apos;works&apos;)</span><br><span class="line">&apos;work&apos;</span><br></pre></td></tr></table></figure>

</details>




<p><code>WordNetLemmatizer</code> 库可以认为是对所谓的WordNet语料库进行的封装，它使用<code>WordNetCorpusReader</code> 中的<code>morphy()</code> 函数来提取词根。如果没有词根可提取，那么单词只返回其原始形式。例如，对于<code>works</code> ，返回的词根是其单数形式<code>work</code> 。</p>
<p>让我们考虑下面的代码，这段代码展示了词干提取和词形还原之间的区别：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import PorterStemmer</span><br><span class="line">&gt;&gt;&gt; stemmer_output=PorterStemmer()</span><br><span class="line">&gt;&gt;&gt; stemmer_output.stem(&apos;happiness&apos;)</span><br><span class="line">&apos;happi&apos;</span><br><span class="line">&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer</span><br><span class="line">&gt;&gt;&gt; lemmatizer_output=WordNetLemmatizer()</span><br><span class="line">&gt;&gt;&gt; lemmatizer_output.lemmatize(&apos;happiness&apos;)</span><br><span class="line">&apos;happiness&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>在上面的代码中，<code>happiness</code> 通过词干提取转化为<code>happi</code> ，词形还原没有找到<code>happiness</code> 的词根，因此它返回了<code>happiness</code> 。</p>
<h2 id="3-4-为非英文语言开发词干提取器"><a href="#3-4-为非英文语言开发词干提取器" class="headerlink" title="3.4 为非英文语言开发词干提取器"></a>3.4 为非英文语言开发词干提取器</h2><p>Polyglot是一个用于提供被称作morfessor模型的软件，该模型用于从标识符中获取语素。Morpho项目的目标是创建无监督的数据驱动流程，其主要目的就是专注于语素（语法的最小单位）的创建。语素在自然语言处理中扮演着重要角色，其在自动识别和语言的生成中是非常有用的。在Polyglot的词汇词典的帮助下，已经使用了涉及不同语言的50000个标识符的morfessor模型。</p>
<p>让我们来看看使用<code>polyglot</code> 获取语言表格的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from polyglot.downloader import downloader</span><br><span class="line">print(downloader.supported_languages_table(&quot;morph2&quot;))</span><br></pre></td></tr></table></figure>

</details>




<p>由以上代码得到的输出就是这里列出的语言：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>1. Piedmontese language</th>
<th></th>
<th></th>
<th>2. Lombard language</th>
<th></th>
<th></th>
<th>3. Gan Chinese  </th>
</tr>
</thead>
<tbody>
<tr>
<td>4. Sicilian</td>
<td></td>
<td></td>
<td>5. Scots</td>
<td></td>
<td></td>
<td>6. Kirghiz, Kyrgyz  </td>
</tr>
<tr>
<td>7. Pashto, Pushto</td>
<td></td>
<td></td>
<td>8. Kurdish</td>
<td></td>
<td></td>
<td>9. Portuguese  </td>
</tr>
<tr>
<td>10. Kannada</td>
<td></td>
<td></td>
<td>11. Korean</td>
<td></td>
<td></td>
<td>12. Khmer  </td>
</tr>
<tr>
<td>13. Kazakh</td>
<td></td>
<td></td>
<td>14. Ilokano</td>
<td></td>
<td></td>
<td>15. Polish  </td>
</tr>
<tr>
<td>16. Panjabi, Punjabi</td>
<td></td>
<td></td>
<td>17. Georgian</td>
<td></td>
<td></td>
<td>18. Chuvash  </td>
</tr>
<tr>
<td>19. Alemannic</td>
<td></td>
<td></td>
<td>20. Czech</td>
<td></td>
<td></td>
<td>21. Welsh  </td>
</tr>
<tr>
<td>22. Chechen</td>
<td></td>
<td></td>
<td>23. Catalan; Valencian</td>
<td></td>
<td></td>
<td>24. Northern Sami  </td>
</tr>
<tr>
<td>25. Sanskrit (Sa?sk?ta)</td>
<td></td>
<td></td>
<td>26. Slovene</td>
<td></td>
<td></td>
<td>27. Javanese  </td>
</tr>
<tr>
<td>28. Slovak</td>
<td></td>
<td></td>
<td>29. Bosnian-Croatian-Serbian</td>
<td></td>
<td></td>
<td>30. Bavarian  </td>
</tr>
<tr>
<td>31. Swedish</td>
<td></td>
<td></td>
<td>32. Swahili</td>
<td></td>
<td></td>
<td>33. Sundanese  </td>
</tr>
<tr>
<td>34. Serbian</td>
<td></td>
<td></td>
<td>35. Albanian</td>
<td></td>
<td></td>
<td>36. Japanese  </td>
</tr>
<tr>
<td>37. Western Frisian</td>
<td></td>
<td></td>
<td>38. French</td>
<td></td>
<td></td>
<td>39. Finnish  </td>
</tr>
<tr>
<td>40. Upper Sorbian</td>
<td></td>
<td></td>
<td>41. Faroese</td>
<td></td>
<td></td>
<td>42. Persian  </td>
</tr>
<tr>
<td>43. Sinhala, Sinhalese</td>
<td></td>
<td></td>
<td>44. Italian</td>
<td></td>
<td></td>
<td>45. Amharic  </td>
</tr>
<tr>
<td>46. Aragonese</td>
<td></td>
<td></td>
<td>47. Volapük</td>
<td></td>
<td></td>
<td>48. Icelandic  </td>
</tr>
<tr>
<td>49. Sakha</td>
<td></td>
<td></td>
<td>50. Afrikaans</td>
<td></td>
<td></td>
<td>51. Indonesian  </td>
</tr>
<tr>
<td>52. Interlingua</td>
<td></td>
<td></td>
<td>53. Azerbaijani</td>
<td></td>
<td></td>
<td>54. Ido  </td>
</tr>
<tr>
<td>55. Arabic</td>
<td></td>
<td></td>
<td>56. Assamese</td>
<td></td>
<td></td>
<td>57. Yoruba  </td>
</tr>
<tr>
<td>58. Yiddish</td>
<td></td>
<td></td>
<td>59. Waray-Waray</td>
<td></td>
<td></td>
<td>60. Croatian  </td>
</tr>
<tr>
<td>61. Hungarian</td>
<td></td>
<td></td>
<td>62. Haitian; Haitian Creole</td>
<td></td>
<td></td>
<td>63. Quechua  </td>
</tr>
<tr>
<td>64. Armenian</td>
<td></td>
<td></td>
<td>65. Hebrew (modern)</td>
<td></td>
<td></td>
<td>66. Silesian  </td>
</tr>
<tr>
<td>67. Hindi</td>
<td></td>
<td></td>
<td>68. Divehi; Dhivehi; Mald…</td>
<td></td>
<td></td>
<td>69. German  </td>
</tr>
<tr>
<td>70. Danish</td>
<td></td>
<td></td>
<td>71. Occitan</td>
<td></td>
<td></td>
<td>72. Tagalog  </td>
</tr>
<tr>
<td>73. Turkmen</td>
<td></td>
<td></td>
<td>74. Thai</td>
<td></td>
<td></td>
<td>75. Tajik  </td>
</tr>
<tr>
<td>76. Greek, Modern</td>
<td></td>
<td></td>
<td>77. Telugu</td>
<td></td>
<td></td>
<td>78. Tamil  </td>
</tr>
<tr>
<td>79. Oriya</td>
<td></td>
<td></td>
<td>80. Ossetian, Ossetic</td>
<td></td>
<td></td>
<td>81. Tatar  </td>
</tr>
<tr>
<td>82. Turkish</td>
<td></td>
<td></td>
<td>83. Kapampangan</td>
<td></td>
<td></td>
<td>84. Venetian  </td>
</tr>
<tr>
<td>85. Manx</td>
<td></td>
<td></td>
<td>86. Gujarati</td>
<td></td>
<td></td>
<td>87. Galician  </td>
</tr>
<tr>
<td>88. Irish</td>
<td></td>
<td></td>
<td>89. Scottish Gaelic; Gaelic</td>
<td></td>
<td></td>
<td>90. Nepali  </td>
</tr>
<tr>
<td>91. Cebuano</td>
<td></td>
<td></td>
<td>92. Zazaki</td>
<td></td>
<td></td>
<td>93. Walloon  </td>
</tr>
<tr>
<td>94. Dutch</td>
<td></td>
<td></td>
<td>95. Norwegian</td>
<td></td>
<td></td>
<td>96. Norwegian Nynorsk  </td>
</tr>
<tr>
<td>97. West Flemish</td>
<td></td>
<td></td>
<td>98. Chinese</td>
<td></td>
<td></td>
<td>99. Bosnian  </td>
</tr>
<tr>
<td>100. Breton</td>
<td></td>
<td></td>
<td>101. Belarusian</td>
<td></td>
<td></td>
<td>102. Bulgarian  </td>
</tr>
<tr>
<td>103. Bashkir</td>
<td></td>
<td></td>
<td>104. Egyptian Arabic</td>
<td></td>
<td></td>
<td>105. Tibetan Standard, Tib…  </td>
</tr>
<tr>
<td>106. Bengali</td>
<td></td>
<td></td>
<td>107. Burmese</td>
<td></td>
<td></td>
<td>108. Romansh  </td>
</tr>
<tr>
<td>109. Marathi (Mara?hi)</td>
<td></td>
<td></td>
<td>110. Malay</td>
<td></td>
<td></td>
<td>111. Maltese  </td>
</tr>
<tr>
<td>112. Russian</td>
<td></td>
<td></td>
<td>113. Macedonian</td>
<td></td>
<td></td>
<td>114. Malayalam  </td>
</tr>
<tr>
<td>115. Mongolian</td>
<td></td>
<td></td>
<td>116. Malagasy</td>
<td></td>
<td></td>
<td>117. Vietnamese  </td>
</tr>
<tr>
<td>118. Spanish; Castilian</td>
<td></td>
<td></td>
<td>119. Estonian</td>
<td></td>
<td></td>
<td>120. Basque  </td>
</tr>
<tr>
<td>121. Bishnupriya Manipuri</td>
<td></td>
<td></td>
<td>122. Asturian</td>
<td></td>
<td></td>
<td>123. English  </td>
</tr>
<tr>
<td>124. Esperanto</td>
<td></td>
<td></td>
<td>125. Luxembourgish, Letzeb…</td>
<td></td>
<td></td>
<td>126. Latin  </td>
</tr>
<tr>
<td>127. Uighur, Uyghur</td>
<td></td>
<td></td>
<td>128. Ukrainian</td>
<td></td>
<td></td>
<td>129. Limburgish, Limburgan…  </td>
</tr>
<tr>
<td>130. Latvian</td>
<td></td>
<td></td>
<td>131. Urdu</td>
<td></td>
<td></td>
<td>132. Lithuanian  </td>
</tr>
<tr>
<td>133. Fiji Hindi</td>
<td></td>
<td></td>
<td>134. Uzbek</td>
<td></td>
<td></td>
<td>135. Romanian, Moldavian, …  </td>
</tr>
</tbody>
</table>
</div>
<p>可使用以下代码下载必要的模型：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line">polyglot download morph2.en morph2.ar</span><br><span class="line"></span><br><span class="line">[polyglot_data] Downloading package morph2.en to</span><br><span class="line">[polyglot_data] /home/rmyeid/polyglot_data...</span><br><span class="line">[polyglot_data] Package morph2.en is already up-to-date!</span><br><span class="line">[polyglot_data] Downloading package morph2.ar to</span><br><span class="line">[polyglot_data] /home/rmyeid/polyglot_data...</span><br><span class="line">[polyglot_data] Package morph2.ar is already up-to-date!</span><br></pre></td></tr></table></figure>

</details>




<p>考虑一个可用于从<code>polyglot</code> 中获取输出的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from polyglot.text import Text, Word</span><br><span class="line">tokens =[&quot;unconditional&quot; ,&quot;precooked&quot;, &quot;impossible&quot;, &quot;painful&quot;,</span><br><span class="line">&quot;entered&quot;]</span><br><span class="line">for s in tokens:</span><br><span class="line">s=Word(s, language=&quot;en&quot;)</span><br><span class="line">print(&quot;&#123;:&lt;20&#125;&#123;&#125;&quot;.format(s,s.morphemes))</span><br><span class="line"></span><br><span class="line">unconditional[&apos;un&apos;,&apos;conditional&apos;]</span><br><span class="line">precooked[&apos;pre&apos;,&apos;cook&apos;,&apos;ed&apos;]</span><br><span class="line">impossible[&apos;im&apos;,&apos;possible&apos;]</span><br><span class="line">painful[&apos;pain&apos;,&apos;ful&apos;]</span><br><span class="line">entered[&apos;enter&apos;,&apos;ed&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>如果没有正确地执行切分，那么我们就可以对将文本分割成原始成分的过程进行形态学分析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sent=&quot;Ihopeyoufindthebookinteresting&quot;</span><br><span class="line">para=Text(sent)</span><br><span class="line">para.language=&quot;en&quot;</span><br><span class="line">para.morphemes</span><br><span class="line">WordList([&apos;I&apos;,&apos;hope&apos;,&apos;you&apos;,&apos;find&apos;,&apos;the&apos;,&apos;book&apos;,&apos;interesting&apos;])</span><br></pre></td></tr></table></figure>

</details>




<h2 id="3-5-形态分析器"><a href="#3-5-形态分析器" class="headerlink" title="3.5 形态分析器"></a>3.5 形态分析器</h2><p>在给定标识符后缀信息的前提下，形态分析可以认为是一个从标识符中获取语法信息的过程。可以通过以下三种方式来执行形态分析：基于语素的形态学（或一个项目和排列方法），基于词位的形态学（或一个项目和过程方法）和基于单词的形态学（或一个单词和范式方法）。形态分析器可以认为是一个程序，该程序负责对给定的输入标识符进行形态学分析。它分析给定的标识符并生成诸如性别、数、词类等形态信息作为输出。</p>
<p>为了对一个给定的没有空格的标识符执行形态学分析，需要使用pyEnchant字典。</p>
<p>让我们考虑下面用于执行形态学分析的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import enchant</span><br><span class="line">&gt;&gt;&gt; s = enchant.Dict(&quot;en_US&quot;)</span><br><span class="line">&gt;&gt;&gt; tok=[]</span><br><span class="line">&gt;&gt;&gt; def tokenize(st1):</span><br><span class="line">if not st1:return</span><br><span class="line">for j in xrange(len(st1),-1,-1):</span><br><span class="line">if s.check(st1[0:j]):</span><br><span class="line">tok.append(st1[0:i])</span><br><span class="line">st1=st[j:]</span><br><span class="line">tokenize(st1)</span><br><span class="line">break</span><br><span class="line">&gt;&gt;&gt; tokenize(&quot;itismyfavouritebook&quot;)</span><br><span class="line">&gt;&gt;&gt; tok</span><br><span class="line">[&apos;it&apos;, &apos;is&apos;, &apos;my&apos;,&apos;favourite&apos;,&apos;book&apos;]</span><br><span class="line">&gt;&gt;&gt; tok=[ ]</span><br><span class="line">&gt;&gt;&gt; tokenize(&quot;ihopeyoufindthebookinteresting&quot;)</span><br><span class="line">&gt;&gt;&gt; tok</span><br><span class="line">[&apos;i&apos;,&apos;hope&apos;,&apos;you&apos;,&apos;find&apos;,&apos;the&apos;,&apos;book&apos;,&apos;interesting&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>我们可以借助以下几点来确定词的类别：</p>
<ul>
<li><strong>形态提示</strong> ：后缀信息有助于我们检测词的类别。例如，-ness和-ment后缀与名词共存。</li>
<li><strong>语法提示</strong> ：上下文信息有利于确定词的类别。例如，如果我们已经找到了具有名词类别的单词，那么语法提示将有助于我们确定是否有形容词会出现在名词之前或者名词之后。</li>
<li><strong>语义提示</strong> ：语义提示对于确定词的类别也是有用的。例如，如果我们已经知道一个单词代表一个地名，那么它将归属在名词类别下。</li>
<li><strong>开放类</strong> ：这是一个单词不固定的类别，无论何时一个新单词被添加到它们的列表中时，该类别单词的数量每天都在保持增长。开放类中的单词通常是名词。介词大多都在一个封闭类别中。例如，在人称（Persons）列表中可以有无限数量的单词，所以它是一个开放类。</li>
<li><strong>由词性标记集获取的形态</strong> ：词性标记集获取了帮助我们执行形态学分析的信息。 例如，单词plays将与第三人称和单数名词一起出现。</li>
<li><strong>Omorfi</strong> ：Omorfi（Open morphology of Finnish）是一个已经被GNU GPL版本3许可的软件包。它可用于执行许多任务，例如语言建模、形态学分析、基于规则的机器翻译、信息检索、统计机器翻译 、形态分割、本体模型以及拼写检查和校正等。</li>
</ul>
<h2 id="3-6-形态生成器"><a href="#3-6-形态生成器" class="headerlink" title="3.6 形态生成器"></a>3.6 形态生成器</h2><p>形态生成器是执行形态生成任务的程序。可以认为形态生成是与形态分析相反的任务。这里，如果给出单词在数、类别、词干等方面的描述，就可以检索出原始的单词。例如，如果词根为go，词性为动词, 时态为现在时，并且如果它与第三人称和单数主语一起出现，则形态生成器将生成其表层形式goes。</p>
<p>有很多基于Python的可用于执行形态学分析和生成的软件，其中一些如下：</p>
<ul>
<li><strong>ParaMorfo</strong> ：用于执行关于西班牙语和瓜拉尼语的名词、形容词和动词的形态学生成和分析。</li>
<li><strong>HornMorpho</strong> ：用于执行关于奥罗莫语和阿姆哈拉语的名词和动词，以及提格里尼亚语的动词的形态学生成和分析。</li>
<li><strong>AntiMorfo</strong> ：用于执行关于盖丘亚语的形容词、动词和名词，以及西班牙语的动词的形态学生成和分析。</li>
<li><strong>MorfoMelayu</strong> ：用于马来语单词的形态学分析。</li>
</ul>
<p>其他用于执行形态学分析和生成的软件示例如下：</p>
<ul>
<li>Morph是用于RASP系统的英语的形态生成器和分析器。</li>
<li>Morphy是用于德语的形态生成器、分析器和词性标注器。</li>
<li>Morphisto是用于德语的形态生成器和分析器。</li>
<li>Morfette用于执行西班牙语和法语的监督学习（屈折形态学）。</li>
</ul>
<h2 id="3-7-搜索引擎"><a href="#3-7-搜索引擎" class="headerlink" title="3.7 搜索引擎"></a>3.7 搜索引擎</h2><p>PyStemmer 1.0.1由可用于执行信息检索任务和构建搜索引擎的Snowball词干提取算法组成。它由Porter词干提取算法和许多其他的词干提取算法组成，这些词干提取算法有助于在多种语言（包括许多欧洲语言）中执行词干提取和信息检索任务。</p>
<p>我们可以通过将文本转换为向量来构建向量空间搜索引擎。</p>
<p>以下是构建一个向量空间搜索引擎所涉及的步骤。</p>
<p>1．考虑以下用于删除停止词和分词的代码：词干提取器是一个用于接收单词并将其转化为词干的程序，拥有相同词干的标识符具有几乎相同的含义，文本中的停止词也被去除了。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def eliminatestopwords(self,list):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Eliminate words which occur often and have not much significance</span><br><span class="line">from context point of view.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">return[ word for word in list if word not in self.stopwords ]</span><br><span class="line"></span><br><span class="line">def tokenize(self,string):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Perform the task of splitting text into stop words and tokens</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Str=self.clean(str)</span><br><span class="line">Words=str.split(&quot;&quot;)</span><br><span class="line">return [self.stemmer.stem(word,0,len(word)-1) for word in words]</span><br></pre></td></tr></table></figure>

</details>




<p>2．考虑如下可用于将关键词映射到向量维度的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def obtainvectorkeywordindex(self, documentList):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">In the document vectors, generate the keyword for the given</span><br><span class="line">position of element</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">#Perform mapping of text into strings</span><br><span class="line">vocabstring = &quot;&quot;.join(documentList)</span><br><span class="line"></span><br><span class="line">vocablist = self.parser.tokenise(vocabstring)</span><br><span class="line">#Eliminate common words that have no search significance</span><br><span class="line">vocablist = self.parser.eliminatestopwords(vocablist)</span><br><span class="line">uniqueVocablist = util.removeDuplicates(vocablist)</span><br><span class="line"></span><br><span class="line">vectorIndex=&#123;&#125;</span><br><span class="line"> offset=0</span><br><span class="line">#Attach a position to keywords that performs mapping with</span><br><span class="line">dimension that is used to depict this token</span><br><span class="line"> for word in uniqueVocablist:</span><br><span class="line">vectorIndex[word]=offset</span><br><span class="line">offset+=1</span><br><span class="line"> return vectorIndex #(keyword:position)</span><br></pre></td></tr></table></figure>

</details>




<p>3．这里使用了一个简单的术语计数模型。考虑下面将文本字符串转换为向量的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def constructVector(self, wordString):</span><br><span class="line"></span><br><span class="line">        # Initialise the vector with 0&apos;s</span><br><span class="line">        Vector_val = [0] * len(self.vectorKeywordIndex)</span><br><span class="line">        tokList = self.parser.tokenize(tokString)</span><br><span class="line">        tokList = self.parser.eliminatestopwords(tokList)</span><br><span class="line">        for word in toklist:</span><br><span class="line">                vector[self.vectorKeywordIndex[word]] += 1;</span><br><span class="line"># simple Term Count Model is used</span><br><span class="line">        return vector</span><br></pre></td></tr></table></figure>

</details>




<p>4．通过找到文档的向量之间的角度的余弦来搜索相似文档，我们可以证明两个给定的文档是否相似。如果余弦值为1，那么角度值为0度，并且向量被认为是平行的（这意味着文档被认为是相关的）。如果余弦值为0并且角度的值为90度，那么向量被认为是垂直的（这意味着文档被认为是不相关的）。让我们看看使用SciPy来计算文本向量之间余弦的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def cosine(vec1, vec2):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">                cosine = ( X * Y ) / ||X|| x ||Y||</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">return float(dot(vec1,vec2) / (norm(vec1) * norm(vec2)))</span><br></pre></td></tr></table></figure>

</details>




<p>5．执行关键词到向量空间的映射。我们首先构建了一个表示搜索项的临时文本，然后在余弦测量的帮助下将其与文档向量进行比较。让我们看看下面用于搜索向量空间的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def searching(self,searchinglist):</span><br><span class="line">&quot;&quot;&quot; search for text that are matched on the basis oflist of</span><br><span class="line">items &quot;&quot;&quot;</span><br><span class="line">        askVector = self.buildQueryVector(searchinglist)</span><br><span class="line"></span><br><span class="line">ratings = [util.cosine(askVector, textVector) for textVector in</span><br><span class="line">self.documentVectors]</span><br><span class="line">        ratings.sort(reverse=True)</span><br><span class="line">        return ratings</span><br></pre></td></tr></table></figure>

</details>




<p>6．现在让我们考虑如下可用于对源文本进行语言检测的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; import sys</span><br><span class="line">&gt;&gt;&gt; try:</span><br><span class="line">from nltk import wordpunct_tokenize</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line">except ImportError:</span><br><span class="line">print( &apos;Error has occured&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-----------------------------------------------------------------</span><br><span class="line">-----</span><br><span class="line">&gt;&gt;&gt; def _calculate_languages_ratios(text):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Compute probability of given document that can be written in</span><br><span class="line">different languages and give a dictionary that appears like</span><br><span class="line">&#123;&apos;german&apos;: 2, &apos;french&apos;: 4, &apos;english&apos;: 1&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">languages_ratios = &#123;&#125;</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">nltk.wordpunct_tokenize() splits all punctuations into separate</span><br><span class="line">tokens</span><br><span class="line">wordpunct_tokenize(&quot;I hope you like the book interesting .&quot;)</span><br><span class="line">[&apos; I&apos;,&apos; hope &apos;,&apos;you &apos;,&apos;like &apos;,&apos;the &apos;,&apos;book&apos; ,&apos;interesting &apos;,&apos;.&apos;]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">tok = wordpunct_tokenize(text)</span><br><span class="line">wor = [word.lower() for word in tok]</span><br><span class="line"></span><br><span class="line">  # Compute occurence of unique stopwords in a text</span><br><span class="line">for language in stopwords.fileids():</span><br><span class="line">stopwords_set = set(stopwords.words(language))</span><br><span class="line">words_set = set(words)</span><br><span class="line">common_elements = words_set.intersection(stopwords_set)</span><br><span class="line">languages_ratios[language] = len(common_elements)</span><br><span class="line"># language &quot;score&quot;</span><br><span class="line">return languages_ratios</span><br><span class="line"></span><br><span class="line">#----------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; def detect_language(text):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Compute the probability of given text that is written in different</span><br><span class="line">languages and obtain the one that is highest scored. It makes</span><br><span class="line">use of stopwords calculation approach, finds out unique stopwords</span><br><span class="line">present in a analyzed text.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">ratios = _calculate_languages_ratios(text)</span><br><span class="line">most_rated_language = max(ratios, key=ratios.get)</span><br><span class="line">return most_rated_language</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__==&apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line"> text = &apos;&apos;&apos;</span><br><span class="line">All over this cosmos, most of the people believe that there is</span><br><span class="line">an invisible supreme power that is the creator and the runner of</span><br><span class="line">this world. Human being is supposed to be the most intelligent and</span><br><span class="line">loved creation by that power and that is being searched by human</span><br><span class="line">beings in different ways into different things. As a result people</span><br><span class="line">reveal His assumed form as per their own perceptions and beliefs.</span><br><span class="line">It has given birth to different religions and people are divided</span><br><span class="line">on the name of religion viz. Hindu, Muslim, Sikhs, Christian etc.</span><br><span class="line">People do not stop at this. They debate the superiority of one</span><br><span class="line">over the other and fight to establish their views. Shrewd people</span><br><span class="line">like politicians oppose and support them at their own convenience</span><br><span class="line">to divide them and control them. It has intensified to the extent</span><br><span class="line">that even parents of a</span><br><span class="line">new born baby teach it about religious differences and recommend</span><br><span class="line">their own religion superior to that of others and let the child</span><br><span class="line">learn to hate other people just because of religion. Jonathan</span><br><span class="line">Swift, an eighteenth century novelist, observes that we have just</span><br><span class="line">enough religion to make us hate, but not enough to make us love</span><br><span class="line">one another.</span><br><span class="line">The word &apos;religion&apos; does not have a derogatory meaning - A literal</span><br><span class="line">meaning of religion is &apos;A</span><br><span class="line">personal or institutionalized system grounded in belief in a God</span><br><span class="line">or Gods and the activities connected</span><br><span class="line">with this&apos;. At its basic level, &apos;religion is just a set of</span><br><span class="line">teachings that tells people how to lead a good</span><br><span class="line">life&apos;. It has never been the purpose of religion to divide people</span><br><span class="line">into groups of isolated followers that</span><br><span class="line">cannot live in harmony together. No religion claims to teach</span><br><span class="line">intolerance or even instructs its believers to segregate a</span><br><span class="line">certain religious group or even take the fundamental rights of</span><br><span class="line">an individual solely based on their religious choices. It is also</span><br><span class="line">said that &apos;Majhab nhi sikhata aaps mai bair krna&apos;.But this very</span><br><span class="line">majhab or religion takes a very heinous form when it is misused</span><br><span class="line">by the shrewd politicians and the fanatics e.g. in Ayodhya on 6th</span><br><span class="line">December, 1992 some right wing political parties</span><br><span class="line">and communal organizations incited the Hindus to demolish the 16th</span><br><span class="line">century Babri Masjid in the</span><br><span class="line">name of religion to polarize Hindus votes. Muslim fanatics in</span><br><span class="line">Bangladesh retaliated and destroyed a</span><br><span class="line">number of temples, assassinated innocent Hindus and raped Hindu</span><br><span class="line">girls who had nothing to do with</span><br><span class="line">the demolition of Babri Masjid. This very inhuman act has been</span><br><span class="line">presented by Taslima Nasrin, a Bangladeshi Doctor-cum-Writer</span><br><span class="line">in her controversial novel &apos;Lajja&apos; (1993) in which, she seems</span><br><span class="line">to utilizes fiction&apos;s mass emotional appeal, rather than its</span><br><span class="line">potential for nuance and universality.</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; language = detect_language(text)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(language)</span><br></pre></td></tr></table></figure>

</details>




<p>以上代码将搜索停止词并检测文本的语言类型，即English。</p>
<h2 id="3-8-小结"><a href="#3-8-小结" class="headerlink" title="3.8 小结"></a>3.8 小结</h2><p>计算语言学领域有许多的应用。为了实现或构建一个应用程序，我们需要对我们的原始文本进行预处理。在本章中，我们已经讨论了词干提取、词形还原、形态分析和生成以及它们在NLTK中的实现。我们还讨论了各种搜索引擎以及它们的实现。</p>
<p>在下一章中，我们将讨论词性、标记和语块。</p>
<h1 id="第4章-词性标注：单词识别"><a href="#第4章-词性标注：单词识别" class="headerlink" title="第4章 词性标注：单词识别"></a>第4章 词性标注：单词识别</h1><p>词性（Parts-of- speech，POS）标注是NLP中的众多任务之一。它被定义为将特定的词性标记分配给句中每个单词的过程。词性标记可以识别一个单词是否为名词、动词还是形容词等等。词性标注有着广泛的应用，例如信息检索、机器翻译、NER、语言分析等。</p>
<p>本章将包含以下主题：</p>
<ul>
<li>创建词性标注语料库。</li>
<li>选择一种机器学习算法。</li>
<li>涉及n-gram的统计建模。</li>
<li>使用词性标注数据开发分块器。</li>
</ul>
<h2 id="4-1-词性标注简介"><a href="#4-1-词性标注简介" class="headerlink" title="4.1 词性标注简介"></a>4.1 词性标注简介</h2><p>词性标注是一个对句中的每个标识符分配词类（例如名词、动词、形容词等）标记的过程。在NLTK中，词性标注器存在于<code>nltk.tag</code> 包中并被<code>TaggerIbase</code> 类所继承。</p>
<p>考虑一个NLTK中的例子，它为指定的句子执行词性标注：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; text1=nltk.word_tokenize(&quot;It is a pleasant day today&quot;)</span><br><span class="line">&gt;&gt;&gt; nltk.pos_tag(text1)</span><br><span class="line">[(&apos;It&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;a&apos;, &apos;DT&apos;), (&apos;pleasant&apos;, &apos;JJ&apos;),</span><br><span class="line">(&apos;day&apos;, &apos;NN&apos;), (&apos;today&apos;, &apos;NN&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>我们可以在<code>TaggerI</code> 的所有子类中实现<code>tag( )</code> 方法。为了评估标注器，<code>TaggerI</code> 提供了<code>evaluate( )</code> 方法。标注器的组合可用于形成回退链，如果其中一个标注器无法完成词性标注时，则可以使用下一个标注器进行词性标注。</p>
<p>让我们看看由Penn Treebank提供的那些可用的标记列表（<code>https://www.ling. upenn.edu/</code> <code>courses/Fall_2003/ling001/penn_treebank_pos.html</code> ）：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">CC - Coordinating conjunction</span><br><span class="line">CD - Cardinal number</span><br><span class="line">DT - Determiner</span><br><span class="line">EX - Existential there</span><br><span class="line">FW - Foreign word</span><br><span class="line">IN - Preposition or subordinating conjunction</span><br><span class="line">JJ - Adjective</span><br><span class="line">JJR - Adjective, comparative</span><br><span class="line">JJS - Adjective, superlative</span><br><span class="line">LS - List item marker</span><br><span class="line">MD - Modal</span><br><span class="line">NN - Noun, singular or mass</span><br><span class="line">NNS - Noun, plural</span><br><span class="line">NNP - Proper noun, singular</span><br><span class="line">NNPS - Proper noun, plural</span><br><span class="line">PDT - Predeterminer</span><br><span class="line">POS - Possessive ending</span><br><span class="line">PRP - Personal pronoun</span><br><span class="line">PRP$ - Possessive pronoun (prolog version PRP-S)</span><br><span class="line">RB - Adverb</span><br><span class="line">RBR - Adverb, comparative</span><br><span class="line">RBS - Adverb, superlative</span><br><span class="line">RP - Particle</span><br><span class="line">SYM - Symbol</span><br><span class="line">TO - to</span><br><span class="line">UH - Interjection</span><br><span class="line">VB - Verb, base form</span><br><span class="line">VBD - Verb, past tense</span><br><span class="line">VBG - Verb, gerund or present participle</span><br><span class="line">VBN - Verb, past participle</span><br><span class="line">VBP - Verb, non-3rd person singular present</span><br><span class="line">VBZ - Verb, 3rd person singular present</span><br><span class="line">WDT - Wh-determiner</span><br><span class="line">WP - Wh-pronoun</span><br><span class="line">WP$ - Possessive wh-pronoun (prolog version WP-S)</span><br><span class="line">WRB - Wh-adverb</span><br></pre></td></tr></table></figure>

</details>




<p>NLTK可以提供以上标记的信息。考虑以下提供了NNS标记信息的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; nltk.help.upenn_tagset(&apos;NNS&apos;)</span><br><span class="line">NNS: noun, common, plural</span><br><span class="line">    undergraduates scotches bric-a-brac products bodyguards facets</span><br><span class="line">coasts</span><br><span class="line">    divestitures storehouses designs clubs fragrances averages</span><br><span class="line">    subjectivists apprehensions muses factory-jobs ...</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看另外一个例子，可以在该例中查询一个正则表达式：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; nltk.help.upenn_tagset(&apos;VB.*&apos;)</span><br><span class="line">VB: verb, base form</span><br><span class="line">    ask assemble assess assign assume atone attention avoid bake</span><br><span class="line">balkanize</span><br><span class="line">    bank begin behold believe bend benefit bevel beware bless boil</span><br><span class="line">bomb</span><br><span class="line">    boost brace break bring broil brush build ...</span><br><span class="line">VBD: verb, past tense</span><br><span class="line">    dipped pleaded swiped regummed soaked tidied convened halted</span><br><span class="line">registered</span><br><span class="line">    cushioned exacted snubbed strode aimed adopted belied figgered</span><br><span class="line">    speculated wore appreciated contemplated ...</span><br><span class="line">VBG: verb, present participle or gerund</span><br><span class="line">    telegraphing stirring focusing angering judging stalling lactating</span><br><span class="line">    hankerin&apos; alleging veering capping approaching traveling besieging</span><br><span class="line">    encrypting interrupting erasing wincing ...</span><br><span class="line">VBN: verb, past participle</span><br><span class="line">    multihulled dilapidated aerosolized chaired languished panelized</span><br><span class="line">used</span><br><span class="line">experimented flourished imitated reunifed factored condensed sheared</span><br><span class="line">    unsettled primed dubbed desired ...</span><br><span class="line">VBP: verb, present tense, not 3rd person singular</span><br><span class="line">    predominate wrap resort sue twist spill cure lengthen brush</span><br><span class="line">terminate</span><br><span class="line">    appear tend stray glisten obtain comprise detest tease attract</span><br><span class="line">    emphasize mold postpone sever return wag ...</span><br><span class="line">VBZ: verb, present tense, 3rd person singular</span><br><span class="line">    bases reconstructs marks mixes displeases seals carps weaves</span><br><span class="line">snatches</span><br><span class="line">    slumps stretches authorizes smolders pictures emerges stockpiles</span><br><span class="line">    seduces fizzes uses bolsters slaps speaks pleads ...R</span><br></pre></td></tr></table></figure>

</details>




<p>以上代码给出了关于动词短语的所有标记信息。</p>
<p>让我们来看一个例子，它描述了通过词性标注来实现词义消歧：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; text=nltk.word_tokenize(&quot;I cannot bear the pain of bear&quot;)</span><br><span class="line">&gt;&gt;&gt; nltk.pos_tag(text)</span><br><span class="line">[(&apos;I&apos;, &apos;PRP&apos;), (&apos;can&apos;, &apos;MD&apos;), (&apos;not&apos;, &apos;RB&apos;), (&apos;bear&apos;, &apos;VB&apos;), (&apos;the&apos;,</span><br><span class="line">&apos;DT&apos;), (&apos;pain&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;bear&apos;, &apos;NN&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>在上面的句子中，这里的<code>bear</code> 是一个动词，意思是容忍，同时bear也是一种动物，这意味着它是一个名词。</p>
<p>在NLTK中，已标注的标识符呈现为一个由标识符及其标记组成的元组。在NLTK中我们可以使用函数<code>str2tuple()</code> 来创建这个元组：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; taggedword=nltk.tag.str2tuple(&apos;bear/NN&apos;) </span><br><span class="line">&gt;&gt;&gt; taggedword</span><br><span class="line">(&apos;bear&apos;, &apos;NN&apos;)</span><br><span class="line">&gt;&gt;&gt; taggedword[0]</span><br><span class="line">&apos;bear&apos;</span><br><span class="line">&gt;&gt;&gt; taggedword[1]</span><br><span class="line">&apos;NN&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>让我们考虑一个能够用给定的文本生成元组序列的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sentence=&apos;&apos;&apos;The/DT sacred/VBN Ganga/NNP flows/VBZ in/IN this/DT</span><br><span class="line">region/NN ./. This/DT is/VBZ a/DT pilgrimage/NN ./. People/NNP from/IN</span><br><span class="line">all/DT over/IN the/DT country/NN visit/NN this/DT place/NN ./. &apos;&apos;&apos;</span><br><span class="line">&gt;&gt;&gt; [nltk.tag.str2tuple(t) for t in sentence.split()]</span><br><span class="line">[(&apos;The&apos;, &apos;DT&apos;), (&apos;sacred&apos;, &apos;VBN&apos;), (&apos;Ganga&apos;, &apos;NNP&apos;), (&apos;flows&apos;, &apos;VBZ&apos;),</span><br><span class="line">(&apos;in&apos;, &apos;IN&apos;), (&apos;this&apos;, &apos;DT&apos;), (&apos;region&apos;, &apos;NN&apos;), (&apos;.&apos;, &apos;.&apos;), (&apos;This&apos;,</span><br><span class="line">&apos;DT&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;a&apos;, &apos;DT&apos;), (&apos;pilgrimage&apos;, &apos;NN&apos;), (&apos;.&apos;, &apos;.&apos;),</span><br><span class="line">(&apos;People&apos;, &apos;NNP&apos;), (&apos;from&apos;, &apos;IN&apos;), (&apos;all&apos;, &apos;DT&apos;), (&apos;over&apos;, &apos;IN&apos;),</span><br><span class="line">(&apos;the&apos;, &apos;DT&apos;), (&apos;country&apos;, &apos;NN&apos;), (&apos;visit&apos;, &apos;NN&apos;), (&apos;this&apos;, &apos;DT&apos;),</span><br><span class="line">(&apos;place&apos;, &apos;NN&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>现在考虑如下将元组（单词及其词性标记）转换为一个单词和一个标记的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; taggedtok = (&apos;bear&apos;, &apos;NN&apos;)</span><br><span class="line">&gt;&gt;&gt; from nltk.tag.util import tuple2str</span><br><span class="line">&gt;&gt;&gt; tuple2str(taggedtok)</span><br><span class="line">&apos;bear/NN&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看Treebank语料库中一些常用标记的出现频率：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; treebank_tagged = treebank.tagged_words(tagset=&apos;universal&apos;)</span><br><span class="line">&gt;&gt;&gt; tag = nltk.FreqDist(tag for (word, tag) in treebank_tagged)</span><br><span class="line">&gt;&gt;&gt; tag.most_common()</span><br><span class="line">[(&apos;NOUN&apos;, 28867), (&apos;VERB&apos;, 13564), (&apos;.&apos;, 11715), (&apos;ADP&apos;, 9857),</span><br><span class="line">(&apos;DET&apos;, 8725), (&apos;X&apos;, 6613), (&apos;ADJ&apos;, 6397), (&apos;NUM&apos;, 3546), (&apos;PRT&apos;,</span><br><span class="line">3219), (&apos;ADV&apos;, 3171), (&apos;PRON&apos;, 2737), (&apos;CONJ&apos;, 2265)]</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下代码，它计算了出现在一个名词标记之前的标记数量：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; treebank_tagged = treebank.tagged_words(tagset=&apos;universal&apos;)</span><br><span class="line">&gt;&gt;&gt; tagpairs = nltk.bigrams(treebank_tagged)</span><br><span class="line">&gt;&gt;&gt; preceders_noun = [x[1] for (x, y) in tagpairs if y[1] == &apos;NOUN&apos;]</span><br><span class="line">&gt;&gt;&gt; freqdist = nltk.FreqDist(preceders_noun)</span><br><span class="line">&gt;&gt;&gt; [tag for (tag, _) in freqdist.most_common()]</span><br><span class="line">[&apos;NOUN&apos;, &apos;DET&apos;, &apos;ADJ&apos;, &apos;ADP&apos;, &apos;.&apos;, &apos;VERB&apos;, &apos;NUM&apos;, &apos;PRT&apos;, &apos;CONJ&apos;,</span><br><span class="line">&apos;PRON&apos;, &apos;X&apos;, &apos;ADV&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>我们也可以通过使用Python中的字典为标识符提供词性标记。让我们看看下面的代码，这段代码展示了使用Python中的字典来创建一个元组（单词：词性标记）：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; tag=&#123;&#125;</span><br><span class="line">&gt;&gt;&gt; tag</span><br><span class="line">&#123;&#125;</span><br><span class="line">&gt;&gt;&gt; tag[&apos;beautiful&apos;]=&apos;ADJ&apos;</span><br><span class="line">&gt;&gt;&gt; tag</span><br><span class="line">&#123;&apos;beautiful&apos;: &apos;ADJ&apos;&#125;</span><br><span class="line">&gt;&gt;&gt; tag[&apos;boy&apos;]=&apos;N&apos;</span><br><span class="line">&gt;&gt;&gt; tag[&apos;read&apos;]=&apos;V&apos;</span><br><span class="line">&gt;&gt;&gt; tag[&apos;generously&apos;]=&apos;ADV&apos;</span><br><span class="line">&gt;&gt;&gt; tag</span><br><span class="line">&#123;&apos;boy&apos;: &apos;N&apos;, &apos;beautiful&apos;: &apos;ADJ&apos;, &apos;generously&apos;: &apos;ADV&apos;, &apos;read&apos;: &apos;V&apos;&#125;</span><br></pre></td></tr></table></figure>

</details>




<h3 id="4-1-1-默认标注"><a href="#4-1-1-默认标注" class="headerlink" title="4.1.1 默认标注"></a>4.1.1 默认标注</h3><p>默认标注是一种为所有标识符分配相同词性标记的标注。<code>SequentialBackoffTagger</code> 类的子类是<code>DefaultTagger</code> 。<code>SequentialBackoffTagger</code> 类必须实现<code>choose_tag()</code> 方法，该方法包含以下参数：</p>
<ul>
<li>标识符集。</li>
<li>需要被标注的标识符索引。</li>
<li>先前的标记列表。</li>
</ul>
<p>标注器的层次结构如图4-1所示：</p>
<p><img src="Image00005.jpg" alt></p>
<p>图4-1</p>
<p>现在让我们来看看下面的代码，它展示了<code>DefaultTagger</code> 类的工作原理：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import DefaultTagger</span><br><span class="line">&gt;&gt;&gt; tag = DefaultTagger(&apos;NN&apos;)</span><br><span class="line">&gt;&gt;&gt; tag.tag([&apos;Beautiful&apos;, &apos;morning&apos;])</span><br><span class="line">[(&apos;Beautiful&apos;, &apos;NN&apos;), (&apos;morning&apos;, &apos;NN&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>我们可以借助<code>nltk.tag.untag()</code> 函数将已标注的句子转换为未标注的句子。调用<code>nltk.tag.untag()</code> 函数后，每个标识符上的标记将会被删除。</p>
<p>让我们来看看用于取消句子标注的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.tag import untag</span><br><span class="line">&gt;&gt;&gt; untag([(&apos;beautiful&apos;, &apos;NN&apos;), (&apos;morning&apos;, &apos;NN&apos;)])</span><br><span class="line">[&apos;beautiful&apos;, &apos;morning&apos;]</span><br></pre></td></tr></table></figure>

</details>




<h2 id="4-2-创建词性标注语料库"><a href="#4-2-创建词性标注语料库" class="headerlink" title="4.2 创建词性标注语料库"></a>4.2 创建词性标注语料库</h2><p>一个语料库可以认为是文档的集合。一个语料库（集）是多个语料库的集合。</p>
<p>让我们来看看下面的代码，它将在主目录里生成一个数据目录：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; import os,os.path</span><br><span class="line">&gt;&gt;&gt; create = os.path.expanduser(&apos;~/nltkdoc&apos;)</span><br><span class="line">&gt;&gt;&gt; if not os.path.exists(create):</span><br><span class="line">   os.mkdir(create)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; os.path.exists(create)</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; import nltk.data</span><br><span class="line">&gt;&gt;&gt; create in nltk.data.path</span><br><span class="line">True</span><br></pre></td></tr></table></figure>

</details>




<p>这段代码将在主目录中创建一个叫作<code>〜/nltkdoc</code> 的数据目录。代码的最后一行将返回<code>True</code> ，这将确保数据目录已创建。如果代码的最后一行返回<code>False</code> ，则意味着数据目录尚未被创建，我们需要手动创建它。手动创建数据目录后，我们可以测试一下最后一行代码，然后它将返回<code>True</code> 。在这个目录中，我们可以创建另一个叫作<code>nltkcorpora</code> 的目录，它将包含全部的语料库。此时路径将是<code>〜/nltkdoc/nltkcorpora</code> 。我们也可以创建一个叫作<code>important</code> 的子目录，它将包含所有必要的文件。</p>
<p>最后路径将是<code>~/nltkdoc/nltkcorpora/important</code> 。</p>
<p>让我们来看看下面用于加载子目录中一个文本文件的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk.data</span><br><span class="line">&gt;&gt;&gt; nltk.data.load(&apos;nltkcorpora/important/firstdoc.txt&apos;,format=&apos;raw&apos;)</span><br><span class="line">&apos;nltk\n&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>以上代码中，我们在这里已经注意到<code>format =&#39;raw&#39;</code> ，因为<code>nltk.data.load()</code> 函数无法解释<code>.txt</code> 文件。</p>
<p>NLTK中有一个单词列表语料库叫作Names语料库。它由两个文件组成，分别称为 <code>male.txt</code> 和<code>female.txt</code> 。</p>
<p>让我们来看看分别用于生成<code>male.txt</code> 和<code>female.txt</code> 文件长度的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import names</span><br><span class="line">&gt;&gt;&gt; names.fileids()</span><br><span class="line">[&apos;female.txt&apos;, &apos;male.txt&apos;]</span><br><span class="line">&gt;&gt;&gt; len(names.words(&apos;male.txt&apos;))</span><br><span class="line">2943</span><br><span class="line">&gt;&gt;&gt; len(names.words(&apos;female.txt&apos;))</span><br><span class="line">5001</span><br></pre></td></tr></table></figure>

</details>




<p>NLTK也囊括了一个大的英文单词集。让我们来看看用于描述英文单词文件所包含的单词数量的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import words</span><br><span class="line">&gt;&gt;&gt; words.fileids()</span><br><span class="line">[&apos;en&apos;, &apos;en-basic&apos;]</span><br><span class="line">&gt;&gt;&gt; len(words.words(&apos;en&apos;))</span><br><span class="line">235886</span><br><span class="line">&gt;&gt;&gt; len(words.words(&apos;en-basic&apos;))</span><br><span class="line">850</span><br></pre></td></tr></table></figure>

</details>




<p>考虑以下NLTK中用于定义Maxent Treebank词性标注器的代码：</p>
<pre><code>def pos_tag(tok):
&quot;&quot;&quot;
</code></pre><p>我们可以使用由NLTK提供的词性标注器来标注一个标识符列表：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.tag import pos_tag</span><br><span class="line">&gt;&gt;&gt; from nltk.tokenize import word_tokenize</span><br><span class="line">&gt;&gt;&gt; pos_tag(word_tokenize(&quot;Papa&apos;s favourite hobby is reading.&quot;))</span><br><span class="line">        [(&apos;Papa&apos;, &apos;NNP&apos;), (&quot;&apos;s&quot;, &apos;POS&apos;), (&apos;favourite&apos;, &apos;JJ&apos;),</span><br><span class="line">(&apos;hobby&apos;, &apos;NN&apos;), (&apos;is&apos;,</span><br><span class="line">        &apos;VBZ&apos;), (&apos;reading&apos;, &apos;VB&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br><span class="line"></span><br><span class="line">    :param tokens: list of tokens that need to be tagged</span><br><span class="line">    :type tok: list(str)</span><br><span class="line">    :return: The tagged tokens</span><br><span class="line">    :rtype: list(tuple(str, str))</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    tagger = load(_POS_TAGGER)</span><br><span class="line">    return tagger.tag(tok)</span><br><span class="line"></span><br><span class="line">def batch_pos_tag(sent):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    We can use part of speech tagger given by NLTK to perform tagging</span><br><span class="line">of list of tokens.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    tagger = load(_POS_TAGGER)</span><br><span class="line">    return tagger.batch_tag(sent)</span><br></pre></td></tr></table></figure>

</details>




<h2 id="4-3-选择一种机器学习算法"><a href="#4-3-选择一种机器学习算法" class="headerlink" title="4.3 选择一种机器学习算法"></a>4.3 选择一种机器学习算法</h2><p>词性标注也被称为词义消歧或语法标注。词性标注算法可分为两种类型：基于规则的（rule- based）或随机（stochastic/probabilistic）的标注算法。E. Brill’s标注器就是在基于规则的标注算法的基础上建立的。</p>
<p>词性分类器将文档作为输入并获取单词的特征。它借助这些与已经可用的训练标签相结合的单词特征来训练它自己。这种类型的分类器被称为二阶分类器，并且它使用引导分类器以便为单词生成标记。</p>
<p>一个<code>backoff</code> 分类器是可以执行回退过程的分类器。可以通过这样的方式获取输出：三元词性标注器依赖于二元词性标注器，二元词性标注器依赖于一元词性标注器。</p>
<p>在训练词性分类器时，会生成一个特征集。该特征集大体组成如下：</p>
<ul>
<li>当前单词的信息。</li>
<li>上一个单词或前缀的信息。</li>
<li>下一个单词或后缀的信息。</li>
</ul>
<p>在NLTK中，<code>FastBrillTagger</code> 类是基于一元语法的。它使用一个包含已知单词及其词性标记信息的字典。</p>
<p>让我们来看看NLTK中使用<code>FastBrillTagger</code> 的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from nltk.tag import UnigramTagger</span><br><span class="line">from nltk.tag import FastBrillTaggerTrainer</span><br><span class="line"></span><br><span class="line">from nltk.tag.brill import SymmetricProximateTokensTemplate</span><br><span class="line">from nltk.tag.brill import ProximateTokensTemplate</span><br><span class="line">from nltk.tag.brill import ProximateTagsRule</span><br><span class="line">from nltk.tag.brill import ProximateWordsRule</span><br><span class="line"></span><br><span class="line">ctx = [ # Context = surrounding words and tags.</span><br><span class="line">    SymmetricProximateTokensTemplate(ProximateTagsRule, (1, 1)),</span><br><span class="line">    SymmetricProximateTokensTemplate(ProximateTagsRule, (1, 2)),</span><br><span class="line">    SymmetricProximateTokensTemplate(ProximateTagsRule, (1, 3)),</span><br><span class="line">    SymmetricProximateTokensTemplate(ProximateTagsRule, (2, 2)),</span><br><span class="line">    SymmetricProximateTokensTemplate(ProximateWordsRule, (0, 0)),</span><br><span class="line">    SymmetricProximateTokensTemplate(ProximateWordsRule, (1, 1)),</span><br><span class="line">    SymmetricProximateTokensTemplate(ProximateWordsRule, (1, 2)),</span><br><span class="line">    ProximateTokensTemplate(ProximateTagsRule, (-1, -1), (1, 1)),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">tagger = UnigramTagger(sentences)</span><br><span class="line">tagger = FastBrillTaggerTrainer(tagger, ctx, trace=0)</span><br><span class="line">tagger = tagger.train(sentences, max_rules=100)</span><br></pre></td></tr></table></figure>

</details>




<p>文本分类可以认为是一个为给定的输入确定词性标记的过程。</p>
<p>在监督式分类中，使用了一个包含单词及其正确标记的训练语料库。在非监督式分类中，不存在任何单词对和一个正确的标记列表。</p>
<p>在监督式分类中，在训练期间，特征提取器接受输入和标签并生成特征集，如图4-2所示。这些特征集与标签一起作为机器学习算法的输入。在测试或预测阶段，使用特征提取器从未知的输入中生成特征集，并且将输出的特征集发送到分类模型中，该模型在机器学习算法的帮助下生成了以标签或词性标记信息形式呈现的输出。</p>
<p><img src="Image00006.jpg" alt></p>
<p>图4-2</p>
<p>最大熵分类器通过搜索参数集以便最大化用于训练的语料库的整体似然性。</p>
<p>它可以定义如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">P(features_word)=Σx|in|corpus </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">P(label_word(x)|features_word(x))</span><br><span class="line">P(label_word|features_word)=P(label_word, features_word)</span><br><span class="line">/Σlabel_word </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">P(label_word, features_word)</span><br></pre></td></tr></table></figure>

</details>




<h2 id="4-4-涉及n-gram的统计建模"><a href="#4-4-涉及n-gram的统计建模" class="headerlink" title="4.4 涉及n-gram的统计建模"></a>4.4 涉及n-gram的统计建模</h2><p>一元语法意味着一个独立的单词，在一元语法标注器中，单个的标识符用于查找特定的词性标记。</p>
<p>可以通过在初始化标注器时提供一个句子的列表来执行<code>UnigramTagger</code> 的训练。</p>
<p>让我们来看看下面在NLTK中用于执行<code>UnigramTagger</code> 训练的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import UnigramTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; unitagger=UnigramTagger(training)</span><br><span class="line">&gt;&gt;&gt; treebank.sents()[0]</span><br><span class="line">[&apos;Pierre&apos;, &apos;Vinken&apos;, &apos;,&apos;, &apos;61&apos;, &apos;years&apos;, &apos;old&apos;, &apos;,&apos;, &apos;will&apos;, &apos;join&apos;,</span><br><span class="line">&apos;the&apos;, &apos;board&apos;, &apos;as&apos;, &apos;a&apos;, &apos;nonexecutive&apos;, &apos;director&apos;, &apos;Nov.&apos;, &apos;29&apos;,</span><br><span class="line">&apos;.&apos;]</span><br><span class="line">&gt;&gt;&gt; unitagger.tag(treebank.sents()[0])</span><br><span class="line">[(&apos;Pierre&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;61&apos;, &apos;CD&apos;),</span><br><span class="line">(&apos;years&apos;, &apos;NNS&apos;), (&apos;old&apos;, &apos;JJ&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;will&apos;, &apos;MD&apos;), (&apos;join&apos;,</span><br><span class="line">&apos;VB&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;board&apos;, &apos;NN&apos;), (&apos;as&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;DT&apos;),</span><br><span class="line">(&apos;nonexecutive&apos;, &apos;JJ&apos;), (&apos;director&apos;, &apos;NN&apos;), (&apos;Nov.&apos;, &apos;NNP&apos;), (&apos;29&apos;,</span><br><span class="line">&apos;CD&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>以上代码中，我们使用Treebank语料库的前7000个句子进行了训练。</p>
<p><code>UnigramTagger</code> 的层次结构继承图如图4-3所示。</p>
<p><img src="Image00007.jpg" alt></p>
<p>图4-3</p>
<p>要评估<code>UnigramTagger</code> ，让我们来看看下面用于计算其准确性的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import UnigramTagger</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; unitagger=UnigramTagger(training)</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; unitagger.evaluate(testing)</span><br><span class="line">0.963400866227395</span><br></pre></td></tr></table></figure>

</details>




<p>因此，在正确地执行词性标注时，它的准确率为96%。</p>
<p>既然<code>UnigramTagger</code> 继承于<code>ContextTagger</code> ，那么我们可以用一个特定的标记映射上下文键。</p>
<p>考虑以下有关使用<code>UnigramTagger</code> 进行词性标注的代码示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import UnigramTagger</span><br><span class="line">&gt;&gt;&gt; unitag = UnigramTagger(model=&#123;&apos;Vinken&apos;: &apos;NN&apos;&#125;)</span><br><span class="line">&gt;&gt;&gt; unitag.tag(treebank.sents()[0])</span><br><span class="line">[(&apos;Pierre&apos;, None), (&apos;Vinken&apos;, &apos;NN&apos;), (&apos;,&apos;, None), (&apos;61&apos;, None),</span><br><span class="line">(&apos;years&apos;, None), (&apos;old&apos;, None), (&apos;,&apos;, None), (&apos;will&apos;, None), (&apos;join&apos;,</span><br><span class="line">None), (&apos;the&apos;, None), (&apos;board&apos;, None), (&apos;as&apos;, None), (&apos;a&apos;, None),</span><br><span class="line">(&apos;nonexecutive&apos;, None), (&apos;director&apos;, None), (&apos;Nov.&apos;, None), (&apos;29&apos;,</span><br><span class="line">None), (&apos;.&apos;, None)]</span><br></pre></td></tr></table></figure>

</details>




<p>在以上代码中，这里的<code>UnigramTagger</code> 仅用<code>&#39;NN&#39;</code> 标记标注了<code>&#39;Vinken&#39;</code> ，其余的用<code>&#39;None&#39;</code> 标记进行了标注，这是因为我们已经在上下文模型中提供了单词<code>&#39;Vinken&#39;</code> 的标记，并且该模型不包括其他单词。</p>
<p>在一个给定的上下文中，<code>ContextTagger</code> 使用给定标记的频率来决定最有可能出现的标记。为了使用最小阈值频率，我们可以将特定值传递到截止值。让我们来看看评估<code>UnigramTagger</code> 的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; unitagger = UnigramTagger(training, cutoff=5)</span><br><span class="line">&gt;&gt;&gt; unitagger.evaluate(testing)</span><br><span class="line">0.7974218445306567</span><br></pre></td></tr></table></figure>

</details>




<p>回退标注可以认为是<code>SequentialBackoffTagger</code> 的一个特征。所有标注器被链接在一起，以便如果其中一个标注器不能标注标识符时，那么将使用下一个标注器来标注它。</p>
<p>让我们来看看下面使用了回退标注的代码。这里，<code>DefaultTagger</code> 和<code>UnigramTagger</code> 用于标注一个标识符。如果它们中的任何一个都无法标注一个单词，那么可使用下一个标注器来标注这个单词：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import UnigramTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import DefaultTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; tag1=DefaultTagger(&apos;NN&apos;)</span><br><span class="line">&gt;&gt;&gt; tag2=UnigramTagger(training,backoff=tag1)</span><br><span class="line">&gt;&gt;&gt; tag2.evaluate(testing)</span><br><span class="line">0.963400866227395</span><br></pre></td></tr></table></figure>

</details>




<p><code>NgramTagger</code> 的子类是<code>UnigramTagger</code> 、<code>BigramTagger</code> 和<code>TrigramTagger</code> 。<code>BigramTagger</code> 使用前一个标记作为上下文信息，<code>TrigramTagger</code> 使用前两个标记作为上下文信息。</p>
<p>考虑下面的代码，这段代码展示了<code>BigramTagger</code> 的实现：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import BigramTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; training_1= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; bigramtagger=BigramTagger(training_1)</span><br><span class="line">&gt;&gt;&gt; treebank.sents()[0]</span><br><span class="line">[&apos;Pierre&apos;, &apos;Vinken&apos;, &apos;,&apos;, &apos;61&apos;, &apos;years&apos;, &apos;old&apos;, &apos;,&apos;, &apos;will&apos;, &apos;join&apos;,</span><br><span class="line">&apos;the&apos;, &apos;board&apos;, &apos;as&apos;, &apos;a&apos;, &apos;nonexecutive&apos;, &apos;director&apos;, &apos;Nov.&apos;, &apos;29&apos;,</span><br><span class="line">&apos;.&apos;]</span><br><span class="line">&gt;&gt;&gt; bigramtagger.tag(treebank.sents()[0])</span><br><span class="line">[(&apos;Pierre&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;61&apos;, &apos;CD&apos;),</span><br><span class="line">(&apos;years&apos;, &apos;NNS&apos;), (&apos;old&apos;, &apos;JJ&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;will&apos;, &apos;MD&apos;), (&apos;join&apos;,</span><br><span class="line">&apos;VB&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;board&apos;, &apos;NN&apos;), (&apos;as&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;DT&apos;),</span><br><span class="line">(&apos;nonexecutive&apos;, &apos;JJ&apos;), (&apos;director&apos;, &apos;NN&apos;), (&apos;Nov.&apos;, &apos;NNP&apos;), (&apos;29&apos;,</span><br><span class="line">&apos;CD&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br><span class="line">&gt;&gt;&gt; testing_1 = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; bigramtagger.evaluate(testing_1)</span><br><span class="line">0.922942709936983</span><br></pre></td></tr></table></figure>

</details>




<p>让我们看看另一段有关<code>BigramTagger</code> 和<code>TrigramTagger</code> 的代码：</p>
 <details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line"> &gt;&gt;&gt; from nltk.tag import BigramTagger, TrigramTagger</span><br><span class="line"> &gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line"> &gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line"> &gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line"> &gt;&gt;&gt; bigramtag = BigramTagger(training)</span><br><span class="line"> &gt;&gt;&gt; bigramtag.evaluate(testing)</span><br><span class="line"> 0.9190426339881356</span><br><span class="line"> &gt;&gt;&gt; trigramtag = TrigramTagger(training)</span><br><span class="line"> &gt;&gt;&gt; trigramtag.evaluate(testing)</span><br><span class="line"> 0.9101956195989079</span><br></pre></td></tr></table></figure>

</details>




<p><code>NgramTagger</code> 也可以用于生成n大于3的标注器。让我们看看如下在NLTK中开发一个<code>QuadgramTagger</code> 标注器的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; from nltk import NgramTagger</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; quadgramtag = NgramTagger(4, training)</span><br><span class="line">&gt;&gt;&gt; quadgramtag.evaluate(testing)</span><br><span class="line">0.9429767842847466</span><br></pre></td></tr></table></figure>

</details>




<p><code>AffixTagger</code> 也是一个<code>ContextTagger</code> ，它使用前缀或后缀作为上下文信息。</p>
<p>让我们来看看如下使用了<code>AffixTagger</code> 的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import AffixTagger</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; affixtag = AffixTagger(training)</span><br><span class="line">&gt;&gt;&gt; affixtag.evaluate(testing)</span><br><span class="line">0.29043249789601167</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看下面有关学习和使用4个字符前缀的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import AffixTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; prefixtag = AffixTagger(training, affix_length=4)</span><br><span class="line">&gt;&gt;&gt; prefixtag.evaluate(testing)</span><br><span class="line">0.21103516226368618</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下有关学习和使用3个字符后缀的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import AffixTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; suffixtag = AffixTagger(training, affix_length=-3)</span><br><span class="line">&gt;&gt;&gt; suffixtag.evaluate(testing)</span><br><span class="line">0.29043249789601167</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中的代码，它组合了许多回退链中的词缀标注器：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import AffixTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; prefixtagger=AffixTagger(training,affix_length=4)</span><br><span class="line">&gt;&gt;&gt; prefixtagger.evaluate(testing)</span><br><span class="line">0.21103516226368618</span><br><span class="line">&gt;&gt;&gt; prefixtagger3=AffixTagger(training,affix_</span><br><span class="line">length=3,backoff=prefixtagger)</span><br><span class="line">&gt;&gt;&gt; prefixtagger3.evaluate(testing)</span><br><span class="line">0.25906767658107027</span><br><span class="line">&gt;&gt;&gt; suffixtagger3=AffixTagger(training,affix_length=-</span><br><span class="line">3,backoff=prefixtagger3)</span><br><span class="line">&gt;&gt;&gt; suffixtagger3.evaluate(testing)</span><br><span class="line">0.2939630929654946</span><br><span class="line">&gt;&gt;&gt; suffixtagger4=AffixTagger(training,affix_length=-</span><br><span class="line">4,backoff=suffixtagger3)</span><br><span class="line">&gt;&gt;&gt; suffixtagger4.evaluate(testing)</span><br><span class="line">0.3316090892296324</span><br></pre></td></tr></table></figure>

</details>




<p>TnT 代表的是Trigrams n Tags。<code>TnT</code> 建立在二阶马尔科夫模型的基础之上，是一个基于统计的标注器。</p>
<p>让我们来看看NLTK中有关<code>TnT</code> 的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import tnt</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; tnt_tagger=tnt.TnT()</span><br><span class="line">&gt;&gt;&gt; tnt_tagger.train(training)</span><br><span class="line">&gt;&gt;&gt; tnt_tagger.evaluate(testing)</span><br><span class="line">0.9882176652913768</span><br></pre></td></tr></table></figure>

</details>




<p><code>TnT</code> 从训练文本中计算了<code>ConditionalFreqDist</code> 和<code>internalFreqDist</code> 。这些实例用于计算一元语法模型、二元语法模型和三元语法模型。为了选择最佳的标记，TnT使用了n元语法模型。</p>
<p>考虑如下有关<code>DefaultTagger</code> 的代码。在这段代码中，如果明确给出了未知标注器的值，那么<code>TRAINED</code> 将被设置为<code>TRUE</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import DefaultTagger</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import tnt</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; testing = treebank.tagged_sents()[2000:]</span><br><span class="line">&gt;&gt;&gt; training= treebank.tagged_sents()[:7000]</span><br><span class="line">&gt;&gt;&gt; tnt_tagger=tnt.TnT()</span><br><span class="line">&gt;&gt;&gt; unknown=DefaultTagger(&apos;NN&apos;)</span><br><span class="line">&gt;&gt;&gt; tagger_tnt=tnt.TnT(unk=unknown,Trained=True)</span><br><span class="line">&gt;&gt;&gt; tnt_tagger.train(training)</span><br><span class="line">&gt;&gt;&gt; tnt_tagger.evaluate(testing)</span><br><span class="line">0.988238192006897</span><br></pre></td></tr></table></figure>

</details>




<h2 id="4-5-使用词性标注语料库开发分块器"><a href="#4-5-使用词性标注语料库开发分块器" class="headerlink" title="4.5 使用词性标注语料库开发分块器"></a>4.5 使用词性标注语料库开发分块器</h2><p>分块是一个可用于执行实体识别的过程。它用于分割和标记句中的多个标识符序列。</p>
<p>为了设计一个分块器，应该定义分块语法。分块语法包含了有关如何进行分块的规则。</p>
<p>让我们考虑如下通过构建分块规则来执行名词短语分块（ _Noun Phrase Chunking_ ）的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sent=[(&quot;A&quot;,&quot;DT&quot;),(&quot;wise&quot;, &quot;JJ&quot;), (&quot;small&quot;, &quot;JJ&quot;),(&quot;girl&quot;, &quot;NN&quot;),</span><br><span class="line">(&quot;of&quot;, &quot;IN&quot;), (&quot;village&quot;, &quot;N&quot;), (&quot;became&quot;, &quot;VBD&quot;), (&quot;leader&quot;, &quot;NN&quot;)]</span><br><span class="line">&gt;&gt;&gt; sent=[(&quot;A&quot;,&quot;DT&quot;),(&quot;wise&quot;, &quot;JJ&quot;), (&quot;small&quot;, &quot;JJ&quot;),(&quot;girl&quot;, &quot;NN&quot;),</span><br><span class="line">(&quot;of&quot;, &quot;IN&quot;), (&quot;village&quot;, &quot;NN&quot;), (&quot;became&quot;, &quot;VBD&quot;), (&quot;leader&quot;, &quot;NN&quot;)]</span><br><span class="line">&gt;&gt;&gt; grammar = &quot;NP: &#123;&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;&lt;IN&gt;?&lt;NN&gt;*&#125;&quot;</span><br><span class="line">&gt;&gt;&gt; find = nltk.RegexpParser(grammar)</span><br><span class="line">&gt;&gt;&gt; res = find.parse(sent)</span><br><span class="line">&gt;&gt;&gt; print(res)</span><br><span class="line">(S</span><br><span class="line">  (NP A/DT wise/JJ small/JJ girl/NN of/IN village/NN)</span><br><span class="line"> became/VBD</span><br><span class="line">  (NP leader/NN))</span><br><span class="line">&gt;&gt;&gt; res.draw()</span><br></pre></td></tr></table></figure>

</details>




<p>生成了如图4-4所示的解析树。</p>
<p><img src="Image00008.jpg" alt></p>
<p>图4-4</p>
<p>在这里，有关名词短语的语块规则被定义为由可选的DT（限定词）、后跟任意数量的JJ（形容词）、再跟一个NN（名词）、可选的IN（介词）以及任意数量的NN所组成。</p>
<p>考虑另一个用任意数量的名词构建的名词短语分块规则的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; noun1=[(&quot;financial&quot;,&quot;NN&quot;),(&quot;year&quot;,&quot;NN&quot;),(&quot;account&quot;,&quot;NN&quot;),(&quot;summar</span><br><span class="line">y&quot;,&quot;NN&quot;)]</span><br><span class="line">&gt;&gt;&gt; gram=&quot;NP:&#123;&lt;NN&gt;+&#125;&quot;</span><br><span class="line">&gt;&gt;&gt; find = nltk.RegexpParser(gram)</span><br><span class="line">&gt;&gt;&gt; print(find.parse(noun1))</span><br><span class="line">(S (NP financial/NN year/NN account/NN summary/NN))</span><br><span class="line">&gt;&gt;&gt; x=find.parse(noun1)</span><br><span class="line">&gt;&gt;&gt; x.draw()</span><br></pre></td></tr></table></figure>

</details>




<p>输出结果以解析树的形式如图4-5所示。</p>
<p><img src="Image00009.jpg" alt></p>
<p>图4-5</p>
<p>分块是语块的一部分被消除的过程。既可以使用整个语块，也可以使用语块中间的一部分并删除剩余的部分，或者也可以使用语块从开始或结尾截取的一部分并删除剩余的部分。</p>
<p>考虑在NLTK中有关<code>UnigramChunker</code> 的代码，这段代码被开发用来执行分块和解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class UnigramChunker(nltk.ChunkParserI):</span><br><span class="line">  def _init_(self,training):</span><br><span class="line">    training_data=[[(x,y) for p,x,y in nltk.chunk.treeconlltags(sent)]</span><br><span class="line">        for sent in training]</span><br><span class="line">    self.tagger=nltk.UnigramTagger(training_data)</span><br><span class="line">  def parsing(self,sent):</span><br><span class="line">    postags=[pos1 for (word1,pos1) in sent]</span><br><span class="line">    tagged_postags=self.tagger.tag(postags)</span><br><span class="line">    chunk_tags=[chunking for (pos1,chunktag) in tagged_postags]</span><br><span class="line">    conll_tags=[(word,pos1,chunktag) for ((word,pos1),chunktag)</span><br><span class="line">        in zip(sent, chunk_tags)]</span><br><span class="line">    return nltk.chunk.conlltaags2tree(conlltags)</span><br></pre></td></tr></table></figure>

</details>




<p>考虑下面的代码，它可以用来评估分块器在训练之后的准确度：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import nltk.corpus, nltk.tag</span><br><span class="line"></span><br><span class="line">def ubt_conll_chunk_accuracy(train_sents, test_sents):</span><br><span class="line">    chunks_train =conll_tag_chunks(training)</span><br><span class="line">    chunks_test =conll_tag_chunks(testing)</span><br><span class="line"></span><br><span class="line">    chunker1 =nltk.tag.UnigramTagger(chunks_train)</span><br><span class="line">    print &apos;u:&apos;, nltk.tag.accuracy(chunker1, chunks_test)</span><br><span class="line"></span><br><span class="line">    chunker2 =nltk.tag.BigramTagger(chunks_train, backoff=chunker1)</span><br><span class="line">    print &apos;ub:&apos;, nltk.tag.accuracy(chunker2, chunks_test)</span><br><span class="line"></span><br><span class="line">    chunker3 =nltk.tag.TrigramTagger(chunks_train, backoff=chunker2)</span><br><span class="line">    print &apos;ubt:&apos;, nltk.tag.accuracy(chunker3, chunks_test)</span><br><span class="line"></span><br><span class="line">    chunker4 =nltk.tag.TrigramTagger(chunks_train, backoff=chunker1)</span><br><span class="line">    print &apos;ut:&apos;, nltk.tag.accuracy(chunker4, chunks_test)</span><br><span class="line"></span><br><span class="line">    chunker5 =nltk.tag.BigramTagger(chunks_train, backoff=chunker4)</span><br><span class="line">    print &apos;utb:&apos;, nltk.tag.accuracy(chunker5, chunks_test)</span><br><span class="line"></span><br><span class="line"># accuracy test for conll chunking</span><br><span class="line">conll_train =nltk.corpus.conll2000.chunked_sents(&apos;train.txt&apos;)</span><br><span class="line">conll_test =nltk.corpus.conll2000.chunked_sents(&apos;test.txt&apos;)</span><br><span class="line">ubt_conll_chunk_accuracy(conll_train, conll_test)</span><br><span class="line"></span><br><span class="line"># accuracy test for treebank chunking</span><br><span class="line">treebank_sents =nltk.corpus.treebank_chunk.chunked_sents()</span><br><span class="line">ubt_conll_chunk_accuracy(treebank_sents[:2000], treebank_sents[2000:])</span><br></pre></td></tr></table></figure>

</details>




<h2 id="4-6-小结"><a href="#4-6-小结" class="headerlink" title="4.6 小结"></a>4.6 小结</h2><p>在本章中，我们讨论了词性标注、各种不同的词性标注器以及用于词性标注的各种方法。此外你还学习了涉及n-gram的统计建模，并使用词性标记信息开发了一个分块器。</p>
<p>在下一章中，我们将讨论Treebank建设、CFG建设以及各种不同的解析算法等。</p>
<h1 id="第5章-语法解析：分析训练资料"><a href="#第5章-语法解析：分析训练资料" class="headerlink" title="第5章 语法解析：分析训练资料"></a>第5章 语法解析：分析训练资料</h1><p>语法解析（也被称作句法分析）是NLP中的任务之一。其被定义为一个检查用自然语言书写的字符序列是否合乎正式语法中所定义的规则的过程。它是一个将句子分解为单词或短语序列并为它们提供特定的成分类别（名词、动词、介词等）的过程。</p>
<p>本章将包含以下主题：</p>
<ul>
<li>Treebank建设。</li>
<li>从Treebank提取上下文无关文法规则。</li>
<li>从CFG创建概率上下文无关文法。</li>
<li>CYK线图解析算法。</li>
<li>Earley线图解析算法。</li>
</ul>
<h2 id="5-1-语法解析简介"><a href="#5-1-语法解析简介" class="headerlink" title="5.1 语法解析简介"></a>5.1 语法解析简介</h2><p>语法解析是NLP中涉及的步骤之一。它被定义为一个确定句中每个句子成分的词性类别并分析给定的句子是否合乎语法规则的过程。术语 _parsing_ 是从拉丁语 _pars_ （ _oration is_ ）派生的，意为词性。考虑一个例子： _Ram bought a book_ 。这个句子在语法上是正确的。但是，如果我们换掉这个句子，用这样一个句子： _Book bought a Ram_ ，然后通过将语义信息添加到如此构建的解析树上，我们可以得出结论：尽管句子是语法正确的，但却是语义错误的。因此，生成解析树后还要对其添加含义。解析器是一个可以接受输入文本并构造解析树或句法树的软件。语法解析可分为两类：自顶向下的语法解析和自底向上的语法解析。在自顶向下的语法解析中，我们从起始符开始一直持续到单个的句子成分。一些自顶向下的解析器包括递归下降解析器（Recursive Descent Parser）、LL解析器和Earley解析器。在自底向上的语法解析中，我们从单个的句子成分开始一直持续到起始符。一些自底向上的解析器包括运算符优先解析器（Operator- precedence parser）、简单优先解析器（Simple precedence parser）、简单LR解析器（Simple LR Parser）、LALR解析器（LALR Parser）、规范LR（LR(1)）解析器（Canonical LR (LR(1)) Parser）、GLR解析器（GLR Parser）、CYK或（CKY）解析器（CYK or(alternatively CKY) Parser）、递归提升解析器（Recursive ascent parser）和移位归约解析器（Shift-reduce parser）。</p>
<p>NLTK中定义了<code>nltk.parse.api.ParserI</code> 类。此类用于获取一个给定句子的解析或句法结构。解析器可用于获取句法结构、语篇结构和形态树。</p>
<p>线图分析法遵循动态规划方法。在此过程中，一旦获得了一些结果，这些结果就可以被视为中间结果，并且可以被重新使用以获得未来的结果。不像在自顶向下的解析法中，这里相同的任务不会一次又一次地执行。</p>
<h2 id="5-2-Treebank建设"><a href="#5-2-Treebank建设" class="headerlink" title="5.2 Treebank建设"></a>5.2 Treebank建设</h2><p><code>nltk.corpus.package</code> 包含许多<code>corpus reader</code> 类，这些类可用于获取各种语料库的内容。</p>
<p>Treebank语料库也可以通过<code>nltk.corpus</code> 访问到。文件的标识符亦可以通过使用<code>fileids()</code> 函数获取到：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; import nltk.corpus</span><br><span class="line">&gt;&gt;&gt; print(str(nltk.corpus.treebank).replace(&apos;\\\\&apos;,&apos;/&apos;))</span><br><span class="line">&lt;BracketParseCorpusReader in &apos;C:/nltk_data/corpora/treebank/combined&apos;&gt;</span><br><span class="line">&gt;&gt;&gt; nltk.corpus.treebank.fileids()</span><br><span class="line">[&apos;wsj_0001.mrg&apos;, &apos;wsj_0002.mrg&apos;, &apos;wsj_0003.mrg&apos;, &apos;wsj_0004.</span><br><span class="line">mrg&apos;, &apos;wsj_0005.mrg&apos;, &apos;wsj_0006.mrg&apos;, &apos;wsj_0007.mrg&apos;, &apos;wsj_0008.</span><br><span class="line">mrg&apos;, &apos;wsj_0009.mrg&apos;, &apos;wsj_0010.mrg&apos;, &apos;wsj_0011.mrg&apos;, &apos;wsj_0012.</span><br><span class="line">mrg&apos;, &apos;wsj_0013.mrg&apos;, &apos;wsj_0014.mrg&apos;, &apos;wsj_0015.mrg&apos;, &apos;wsj_0016.</span><br><span class="line">mrg&apos;, &apos;wsj_0017.mrg&apos;, &apos;wsj_0018.mrg&apos;, &apos;wsj_0019.mrg&apos;, &apos;wsj_0020.</span><br><span class="line">mrg&apos;, &apos;wsj_0021.mrg&apos;, &apos;wsj_0022.mrg&apos;, &apos;wsj_0023.mrg&apos;, &apos;wsj_0024.</span><br><span class="line">mrg&apos;, &apos;wsj_0025.mrg&apos;, &apos;wsj_0026.mrg&apos;, &apos;wsj_0027.mrg&apos;, &apos;wsj_0028.mrg&apos;,</span><br><span class="line">&apos;wsj_0029.mrg&apos;, &apos;wsj_0030.mrg&apos;, &apos;wsj_0031.mrg&apos;, &apos;wsj_0032.</span><br><span class="line">mrg&apos;, &apos;wsj_0033.mrg&apos;, &apos;wsj_0034.mrg&apos;, &apos;wsj_0035.mrg&apos;, &apos;wsj_0036.</span><br><span class="line">mrg&apos;, &apos;wsj_0037.mrg&apos;, &apos;wsj_0038.mrg&apos;, &apos;wsj_0039.mrg&apos;, &apos;wsj_0040.</span><br><span class="line">mrg&apos;, &apos;wsj_0041.mrg&apos;, &apos;wsj_0042.mrg&apos;, &apos;wsj_0043.mrg&apos;, &apos;wsj_0044.</span><br><span class="line">mrg&apos;, &apos;wsj_0045.mrg&apos;, &apos;wsj_0046.mrg&apos;, &apos;wsj_0047.mrg&apos;, &apos;wsj_0048.</span><br><span class="line">mrg&apos;, &apos;wsj_0049.mrg&apos;, &apos;wsj_0050.mrg&apos;, &apos;wsj_0051.mrg&apos;, &apos;wsj_0052.</span><br><span class="line">mrg&apos;, &apos;wsj_0053.mrg&apos;, &apos;wsj_0054.mrg&apos;, &apos;wsj_0055.mrg&apos;, &apos;wsj_0056.</span><br><span class="line">mrg&apos;, &apos;wsj_0057.mrg&apos;, &apos;wsj_0058.mrg&apos;, &apos;wsj_0059.mrg&apos;, &apos;wsj_0060.</span><br><span class="line">mrg&apos;, &apos;wsj_0061.mrg&apos;, &apos;wsj_0062.mrg&apos;, &apos;wsj_0063.mrg&apos;, &apos;wsj_0064.</span><br><span class="line">mrg&apos;, &apos;wsj_0065.mrg&apos;, &apos;wsj_0066.mrg&apos;, &apos;wsj_0067.mrg&apos;, &apos;wsj_0068.</span><br><span class="line">mrg&apos;, &apos;wsj_0069.mrg&apos;, &apos;wsj_0070.mrg&apos;, &apos;wsj_0071.mrg&apos;, &apos;wsj_0072.</span><br><span class="line">mrg&apos;, &apos;wsj_0073.mrg&apos;, &apos;wsj_0074.mrg&apos;, &apos;wsj_0075.mrg&apos;, &apos;wsj_0076.</span><br><span class="line">mrg&apos;, &apos;wsj_0077.mrg&apos;, &apos;wsj_0078.mrg&apos;, &apos;wsj_0079.mrg&apos;, &apos;wsj_0080.</span><br><span class="line">mrg&apos;, &apos;wsj_0081.mrg&apos;, &apos;wsj_0082.mrg&apos;, &apos;wsj_0083.mrg&apos;, &apos;wsj_0084.</span><br><span class="line">mrg&apos;, &apos;wsj_0085.mrg&apos;, &apos;wsj_0086.mrg&apos;, &apos;wsj_0087.mrg&apos;, &apos;wsj_0088.</span><br><span class="line">mrg&apos;, &apos;wsj_0089.mrg&apos;, &apos;wsj_0090.mrg&apos;, &apos;wsj_0091.mrg&apos;, &apos;wsj_0092.</span><br><span class="line">mrg&apos;, &apos;wsj_0093.mrg&apos;, &apos;wsj_0094.mrg&apos;, &apos;wsj_0095.mrg&apos;, &apos;wsj_0096.</span><br><span class="line">mrg&apos;, &apos;wsj_0097.mrg&apos;, &apos;wsj_0098.mrg&apos;, &apos;wsj_0099.mrg&apos;, &apos;wsj_0100.</span><br><span class="line">mrg&apos;, &apos;wsj_0101.mrg&apos;, &apos;wsj_0102.mrg&apos;, &apos;wsj_0103.mrg&apos;, &apos;wsj_0104.</span><br><span class="line">mrg&apos;, &apos;wsj_0105.mrg&apos;, &apos;wsj_0106.mrg&apos;, &apos;wsj_0107.mrg&apos;, &apos;wsj_0108.</span><br><span class="line">mrg&apos;, &apos;wsj_0109.mrg&apos;, &apos;wsj_0110.mrg&apos;, &apos;wsj_0111.mrg&apos;, &apos;wsj_0112.</span><br><span class="line">mrg&apos;, &apos;wsj_0113.mrg&apos;, &apos;wsj_0114.mrg&apos;, &apos;wsj_0115.mrg&apos;, &apos;wsj_0116.</span><br><span class="line">mrg&apos;, &apos;wsj_0117.mrg&apos;, &apos;wsj_0118.mrg&apos;, &apos;wsj_0119.mrg&apos;, &apos;wsj_0120.</span><br><span class="line">mrg&apos;, &apos;wsj_0121.mrg&apos;, &apos;wsj_0122.mrg&apos;, &apos;wsj_0123.mrg&apos;, &apos;wsj_0124.</span><br><span class="line">mrg&apos;, &apos;wsj_0125.mrg&apos;, &apos;wsj_0126.mrg&apos;, &apos;wsj_0127.mrg&apos;, &apos;wsj_0128.</span><br><span class="line">mrg&apos;, &apos;wsj_0129.mrg&apos;, &apos;wsj_0130.mrg&apos;, &apos;wsj_0131.mrg&apos;, &apos;wsj_0132.</span><br><span class="line">mrg&apos;, &apos;wsj_0133.mrg&apos;, &apos;wsj_0134.mrg&apos;, &apos;wsj_0135.mrg&apos;, &apos;wsj_0136.</span><br><span class="line">mrg&apos;, &apos;wsj_0137.mrg&apos;, &apos;wsj_0138.mrg&apos;, &apos;wsj_0139.mrg&apos;, &apos;wsj_0140.</span><br><span class="line">mrg&apos;, &apos;wsj_0141.mrg&apos;, &apos;wsj_0142.mrg&apos;, &apos;wsj_0143.mrg&apos;, &apos;wsj_0144.</span><br><span class="line">mrg&apos;, &apos;wsj_0145.mrg&apos;, &apos;wsj_0146.mrg&apos;, &apos;wsj_0147.mrg&apos;, &apos;wsj_0148.</span><br><span class="line">mrg&apos;, &apos;wsj_0149.mrg&apos;, &apos;wsj_0150.mrg&apos;, &apos;wsj_0151.mrg&apos;, &apos;wsj_0152.</span><br><span class="line">mrg&apos;, &apos;wsj_0153.mrg&apos;, &apos;wsj_0154.mrg&apos;, &apos;wsj_0155.mrg&apos;, &apos;wsj_0156.</span><br><span class="line">mrg&apos;, &apos;wsj_0157.mrg&apos;, &apos;wsj_0158.mrg&apos;, &apos;wsj_0159.mrg&apos;, &apos;wsj_0160.</span><br><span class="line">mrg&apos;, &apos;wsj_0161.mrg&apos;, &apos;wsj_0162.mrg&apos;, &apos;wsj_0163.mrg&apos;, &apos;wsj_0164.</span><br><span class="line">mrg&apos;, &apos;wsj_0165.mrg&apos;, &apos;wsj_0166.mrg&apos;, &apos;wsj_0167.mrg&apos;, &apos;wsj_0168.</span><br><span class="line">mrg&apos;, &apos;wsj_0169.mrg&apos;, &apos;wsj_0170.mrg&apos;, &apos;wsj_0171.mrg&apos;, &apos;wsj_0172.</span><br><span class="line">mrg&apos;, &apos;wsj_0173.mrg&apos;, &apos;wsj_0174.mrg&apos;, &apos;wsj_0175.mrg&apos;, &apos;wsj_0176.</span><br><span class="line">mrg&apos;, &apos;wsj_0177.mrg&apos;, &apos;wsj_0178.mrg&apos;, &apos;wsj_0179.mrg&apos;, &apos;wsj_0180.</span><br><span class="line">mrg&apos;, &apos;wsj_0181.mrg&apos;, &apos;wsj_0182.mrg&apos;, &apos;wsj_0183.mrg&apos;, &apos;wsj_0184.</span><br><span class="line">mrg&apos;, &apos;wsj_0185.mrg&apos;, &apos;wsj_0186.mrg&apos;, &apos;wsj_0187.mrg&apos;, &apos;wsj_0188.</span><br><span class="line">mrg&apos;, &apos;wsj_0189.mrg&apos;, &apos;wsj_0190.mrg&apos;, &apos;wsj_0191.mrg&apos;, &apos;wsj_0192.</span><br><span class="line">mrg&apos;, &apos;wsj_0193.mrg&apos;, &apos;wsj_0194.mrg&apos;, &apos;wsj_0195.mrg&apos;, &apos;wsj_0196.mrg&apos;,</span><br><span class="line">&apos;wsj_0197.mrg&apos;, &apos;wsj_0198.mrg&apos;, &apos;wsj_0199.mrg&apos;]</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; print(treebank.words(&apos;wsj_0007.mrg&apos;))</span><br><span class="line">[&apos;McDermott&apos;, &apos;International&apos;, &apos;Inc.&apos;, &apos;said&apos;, &apos;0&apos;, ...]</span><br><span class="line">&gt;&gt;&gt; print(treebank.tagged_words(&apos;wsj_0007.mrg&apos;))</span><br><span class="line">[(&apos;McDermott&apos;, &apos;NNP&apos;), (&apos;International&apos;, &apos;NNP&apos;), ...]</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看NLTK中有关访问Penn Treebank语料库的代码，它使用了语料库模块中的Treebank语料库阅读器（Treebank Corpus Reader）：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; print(treebank.parsed_sents(&apos;wsj_0007.mrg&apos;)[2])</span><br><span class="line">(S</span><br><span class="line">  (NP-SBJ</span><br><span class="line">    (NP (NNP Bailey) (NNP Controls))</span><br><span class="line">    (, ,)</span><br><span class="line">    (VP</span><br><span class="line">      (VBN based)</span><br><span class="line">      (NP (-NONE- *))</span><br><span class="line">      (PP-LOC-CLR</span><br><span class="line">        (IN in)</span><br><span class="line">        (NP (NP (NNP Wickliffe)) (, ,) (NP (NNP Ohio)))))</span><br><span class="line">    (, ,))</span><br><span class="line">  (VP</span><br><span class="line">    (VBZ makes)</span><br><span class="line">    (NP</span><br><span class="line">      (JJ computerized)</span><br><span class="line">      (JJ industrial)</span><br><span class="line">      (NNS controls)</span><br><span class="line">      (NNS systems)))</span><br><span class="line">  (. .))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank_chunk</span><br><span class="line">&gt;&gt;&gt; treebank_chunk.chunked_sents()[1]</span><br><span class="line">Tree(&apos;S&apos;, [Tree(&apos;NP&apos;, [(&apos;Mr.&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;)]), (&apos;is&apos;,</span><br><span class="line">&apos;VBZ&apos;), Tree(&apos;NP&apos;, [(&apos;chairman&apos;, &apos;NN&apos;)]), (&apos;of&apos;, &apos;IN&apos;), Tree(&apos;NP&apos;,</span><br><span class="line">[(&apos;Elsevier&apos;, &apos;NNP&apos;), (&apos;N.V.&apos;, &apos;NNP&apos;)]), (&apos;,&apos;, &apos;,&apos;), Tree(&apos;NP&apos;,</span><br><span class="line">[(&apos;the&apos;, &apos;DT&apos;), (&apos;Dutch&apos;, &apos;NNP&apos;), (&apos;publishing&apos;, &apos;VBG&apos;), (&apos;group&apos;,</span><br><span class="line">&apos;NN&apos;)]), (&apos;.&apos;, &apos;.&apos;)])</span><br><span class="line">&gt;&gt;&gt; treebank_chunk.chunked_sents()[1].draw()</span><br></pre></td></tr></table></figure>

</details>




<p>以上代码获取了如图5-1所示的解析树。</p>
<p><img src="Image00010.gif" alt></p>
<p>图5-1</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank_chunk</span><br><span class="line">&gt;&gt;&gt; treebank_chunk.chunked_sents()[1].leaves()</span><br><span class="line">[(&apos;Mr.&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;chairman&apos;,</span><br><span class="line">&apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;Elsevier&apos;, &apos;NNP&apos;), (&apos;N.V.&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;),</span><br><span class="line">(&apos;the&apos;, &apos;DT&apos;), (&apos;Dutch&apos;, &apos;NNP&apos;), (&apos;publishing&apos;, &apos;VBG&apos;), (&apos;group&apos;,</span><br><span class="line">&apos;NN&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br><span class="line">&gt;&gt;&gt; treebank_chunk.chunked_sents()[1].pos()</span><br><span class="line">[((&apos;Mr.&apos;, &apos;NNP&apos;), &apos;NP&apos;), ((&apos;Vinken&apos;, &apos;NNP&apos;), &apos;NP&apos;), ((&apos;is&apos;, &apos;VBZ&apos;),</span><br><span class="line">&apos;S&apos;), ((&apos;chairman&apos;, &apos;NN&apos;), &apos;NP&apos;), ((&apos;of&apos;, &apos;IN&apos;), &apos;S&apos;), ((&apos;Elsevier&apos;,</span><br><span class="line">&apos;NNP&apos;), &apos;NP&apos;), ((&apos;N.V.&apos;, &apos;NNP&apos;), &apos;NP&apos;), ((&apos;,&apos;, &apos;,&apos;), &apos;S&apos;), ((&apos;the&apos;,</span><br><span class="line">&apos;DT&apos;), &apos;NP&apos;), ((&apos;Dutch&apos;, &apos;NNP&apos;), &apos;NP&apos;), ((&apos;publishing&apos;, &apos;VBG&apos;), &apos;NP&apos;),</span><br><span class="line">((&apos;group&apos;, &apos;NN&apos;), &apos;NP&apos;), ((&apos;.&apos;, &apos;.&apos;), &apos;S&apos;)]</span><br><span class="line">&gt;&gt;&gt; treebank_chunk.chunked_sents()[1].productions()</span><br><span class="line">[S -&gt; NP (&apos;is&apos;, &apos;VBZ&apos;) NP (&apos;of&apos;, &apos;IN&apos;) NP (&apos;,&apos;, &apos;,&apos;) NP (&apos;.&apos;, &apos;.&apos;),</span><br><span class="line">NP -&gt; (&apos;Mr.&apos;, &apos;NNP&apos;) (&apos;Vinken&apos;, &apos;NNP&apos;), NP -&gt; (&apos;chairman&apos;, &apos;NN&apos;), NP</span><br><span class="line">-&gt; (&apos;Elsevier&apos;, &apos;NNP&apos;) (&apos;N.V.&apos;, &apos;NNP&apos;), NP -&gt; (&apos;the&apos;, &apos;DT&apos;) (&apos;Dutch&apos;,</span><br><span class="line">&apos;NNP&apos;) (&apos;publishing&apos;, &apos;VBG&apos;) (&apos;group&apos;, &apos;NN&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p><code>tagged_words()</code> 方法包含了词性注释：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; nltk.corpus.treebank.tagged_words()</span><br><span class="line">[(&apos;Pierre&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;), ...]</span><br></pre></td></tr></table></figure>

</details>





<p>Penn Treebank语料库中所用的标记类型及这些标记的数量展示如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>#</th>
<th></th>
<th></th>
<th>16  </th>
</tr>
</thead>
<tbody>
<tr>
<td>$</td>
<td></td>
<td></td>
<td>724  </td>
</tr>
<tr>
<td>‘’</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>,</td>
<td></td>
<td></td>
<td>4886  </td>
</tr>
<tr>
<td>-LRB-</td>
<td></td>
<td></td>
<td>120  </td>
</tr>
<tr>
<td>-NONE-</td>
<td></td>
<td></td>
<td>6592  </td>
</tr>
<tr>
<td>-RRB-</td>
<td></td>
<td></td>
<td>126  </td>
</tr>
<tr>
<td>.</td>
<td></td>
<td></td>
<td>384  </td>
</tr>
<tr>
<td>:</td>
<td></td>
<td></td>
<td>563  </td>
</tr>
<tr>
<td>``</td>
<td></td>
<td></td>
<td>712  </td>
</tr>
<tr>
<td>CC</td>
<td></td>
<td></td>
<td>2265  </td>
</tr>
<tr>
<td>CD</td>
<td></td>
<td></td>
<td>3546  </td>
</tr>
<tr>
<td>DT</td>
<td></td>
<td></td>
<td>8165  </td>
</tr>
<tr>
<td>EX</td>
<td></td>
<td></td>
<td>88  </td>
</tr>
<tr>
<td>FW</td>
<td></td>
<td></td>
<td>4  </td>
</tr>
<tr>
<td>IN</td>
<td></td>
<td></td>
<td>9857  </td>
</tr>
<tr>
<td>JJ</td>
<td></td>
<td></td>
<td>5834  </td>
</tr>
<tr>
<td>JJR</td>
<td></td>
<td></td>
<td>381  </td>
</tr>
<tr>
<td>JJS</td>
<td></td>
<td></td>
<td>182  </td>
</tr>
<tr>
<td>LS</td>
<td></td>
<td></td>
<td>13  </td>
</tr>
<tr>
<td>MD</td>
<td></td>
<td></td>
<td>927  </td>
</tr>
<tr>
<td>NN</td>
<td></td>
<td></td>
<td>13166  </td>
</tr>
<tr>
<td>NNP</td>
<td></td>
<td></td>
<td>9410  </td>
</tr>
<tr>
<td>NNPS</td>
<td></td>
<td></td>
<td>244  </td>
</tr>
<tr>
<td>NNS</td>
<td></td>
<td></td>
<td>6047  </td>
</tr>
<tr>
<td>PDT</td>
<td></td>
<td></td>
<td>27  </td>
</tr>
<tr>
<td>POS</td>
<td></td>
<td></td>
<td>824  </td>
</tr>
<tr>
<td>PRP</td>
<td></td>
<td></td>
<td>1716  </td>
</tr>
<tr>
<td>PRP$</td>
<td></td>
<td></td>
<td>766  </td>
</tr>
<tr>
<td>RB</td>
<td></td>
<td></td>
<td>2822  </td>
</tr>
<tr>
<td>RBR</td>
<td></td>
<td></td>
<td>136  </td>
</tr>
<tr>
<td>RBS</td>
<td></td>
<td></td>
<td>35  </td>
</tr>
<tr>
<td>RP</td>
<td></td>
<td></td>
<td>216  </td>
</tr>
<tr>
<td>SYM</td>
<td></td>
<td></td>
<td>1  </td>
</tr>
<tr>
<td>TO</td>
<td></td>
<td></td>
<td>2179  </td>
</tr>
<tr>
<td>UH</td>
<td></td>
<td></td>
<td>3  </td>
</tr>
<tr>
<td>VB</td>
<td></td>
<td></td>
<td>2554  </td>
</tr>
<tr>
<td>VBD</td>
<td></td>
<td></td>
<td>3043  </td>
</tr>
<tr>
<td>VBG</td>
<td></td>
<td></td>
<td>1460  </td>
</tr>
<tr>
<td>VBN</td>
<td></td>
<td></td>
<td>2134  </td>
</tr>
<tr>
<td>VBP</td>
<td></td>
<td></td>
<td>1321  </td>
</tr>
<tr>
<td>VBZ</td>
<td></td>
<td></td>
<td>2125  </td>
</tr>
<tr>
<td>WDT</td>
<td></td>
<td></td>
<td>445  </td>
</tr>
<tr>
<td>WP</td>
<td></td>
<td></td>
<td>241  </td>
</tr>
<tr>
<td>WP$</td>
<td></td>
<td></td>
<td>14  </td>
</tr>
</tbody>
</table>
</div>
<p>从以下代码可以获取到标签及其频率：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.probability import FreqDist</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; fd = FreqDist()</span><br><span class="line">&gt;&gt;&gt; fd.items()</span><br><span class="line">dict_items([])</span><br></pre></td></tr></table></figure>

</details>




<p>上面的代码获取了Treebank语料库中的标记列表以及每个标记的频率。</p>
<p>让我们来看看NLTK中有关访问Sinica Treebank语料库的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import sinica_treebank</span><br><span class="line">&gt;&gt;&gt; print(sinica_treebank.sents())</span><br><span class="line">[[&apos;一&apos;], [&apos;友情&apos;], [&apos;嘉珍&apos;, &apos;和&apos;, &apos;我&apos;, &apos;住在&apos;, &apos;同一条&apos;, &apos;巷子&apos;], ...]</span><br><span class="line">&gt;&gt;&gt; sinica_treebank.parsed_sents()[27]</span><br><span class="line">Tree(&apos;S&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;N·的&apos;, [Tree(&apos;Nhaa&apos;, [&apos;我&apos;]),</span><br><span class="line">Tree(&apos;DE&apos;, [&apos;的&apos;])]), Tree(&apos;Ncb&apos;, [&apos;脑海&apos;])]), Tree(&apos;Ncda&apos;, [&apos;中&apos;])]),</span><br><span class="line">Tree(&apos;Dd&apos;, [&apos;顿时&apos;]), Tree(&apos;DM&apos;, [&apos;一片&apos;]), Tree(&apos;VH11&apos;, [&apos;空白&apos;])])</span><br></pre></td></tr></table></figure>

</details>




<h2 id="5-3-从Treebank提取上下文无关文法规则"><a href="#5-3-从Treebank提取上下文无关文法规则" class="headerlink" title="5.3 从Treebank提取上下文无关文法规则"></a>5.3 从Treebank提取上下文无关文法规则</h2><p>上下文无关文法（Context-free Grammar，CFG）是在1957年由Noam Chomsky为自然语言定义的。一个CFG由以下部分组成：</p>
<ul>
<li>非终结符的有限集合（N）。</li>
<li>终结符的有限集合（T）。</li>
<li>开始符号（S）。</li>
<li>产生式的有限集合（P），形如：A→ <strong>a</strong> 。</li>
</ul>
<p>CFG规则有两种类型：短语结构规则和句子结构规则。</p>
<p>短语结构规则可以定义如下：A→a，其中A Î N和a由终结符和非终结符组成。</p>
<p>在句子级别的CFG构建中，有如下四种结构：</p>
<ul>
<li>陈述结构：处理陈述句（主语后面跟着谓语）。</li>
<li>祈使结构：处理祈使句，命令或建议（句子以动词短语开头，没有主语）。</li>
<li>一般疑问结构：处理疑问句。这些问句的答案是 _yes_ 或 _no_ 。</li>
<li>特殊疑问结构：处理疑问句。以特殊疑问词（Who, What, How, When, Where, Why, Which）开头的问句。</li>
</ul>
<p>常用的CFG规则总结如下：</p>
<ul>
<li>S→NP VP</li>
<li>S→VP</li>
<li>S→Aux NP VP</li>
<li>S→Wh-NP VP</li>
<li>S→Wh-NP Aux NP VP</li>
<li>NP→(Det) (AP) Nom (PP)</li>
<li>VP→Verb (NP) (NP) (PP)*</li>
<li>VP→Verb S</li>
<li>PP→Prep (NP)</li>
<li>AP→(Adv) Adj (PP)</li>
</ul>
<p>考虑一个描述了在NLTK中使用上下文无关文法规则的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk import Nonterminal, nonterminals, Production, CFG</span><br><span class="line">&gt;&gt;&gt; nonterminal1 = Nonterminal(&apos;NP&apos;)</span><br><span class="line">&gt;&gt;&gt; nonterminal2 = Nonterminal(&apos;VP&apos;)</span><br><span class="line">&gt;&gt;&gt; nonterminal3 = Nonterminal(&apos;PP&apos;)</span><br><span class="line">&gt;&gt;&gt; nonterminal1.symbol()</span><br><span class="line">&apos;NP&apos;</span><br><span class="line">&gt;&gt;&gt; nonterminal2.symbol()</span><br><span class="line">&apos;VP&apos;</span><br><span class="line">&gt;&gt;&gt; nonterminal3.symbol()</span><br><span class="line">&apos;PP&apos;</span><br><span class="line">&gt;&gt;&gt; nonterminal1==nonterminal2</span><br><span class="line">False</span><br><span class="line">&gt;&gt;&gt; nonterminal2==nonterminal3</span><br><span class="line">False</span><br><span class="line">&gt;&gt;&gt; nonterminal1==nonterminal3</span><br><span class="line">False</span><br><span class="line">&gt;&gt;&gt; S, NP, VP, PP = nonterminals(&apos;S, NP, VP, PP&apos;)</span><br><span class="line">&gt;&gt;&gt; N, V, P, DT = nonterminals(&apos;N, V, P, DT&apos;)</span><br><span class="line">&gt;&gt;&gt; production1 = Production(S, [NP, VP])</span><br><span class="line">&gt;&gt;&gt; production2 = Production(NP, [DT, NP])</span><br><span class="line">&gt;&gt;&gt; production3 = Production(VP, [V, NP,NP,PP])</span><br><span class="line">&gt;&gt;&gt; production1.lhs()</span><br><span class="line">S</span><br><span class="line">&gt;&gt;&gt; production1.rhs()</span><br><span class="line">(NP, VP)</span><br><span class="line">&gt;&gt;&gt; production3.lhs()</span><br><span class="line">VP</span><br><span class="line">&gt;&gt;&gt; production3.rhs()</span><br><span class="line">(V, NP, NP, PP)</span><br><span class="line">&gt;&gt;&gt; production3 == Production(VP, [V,NP,NP,PP])</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; production2 == production3</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

</details>




<p>在NLTK中用于访问ATIS语法的示例如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; gram1</span><br><span class="line">&lt;Grammar with 5517 productions&gt;</span><br></pre></td></tr></table></figure>

</details>




<p>从ATIS提取测试句子的示例如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; len(sent)</span><br><span class="line">98</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; testingsent[1]</span><br><span class="line">11</span><br><span class="line">&gt;&gt;&gt; testingsent[0]</span><br><span class="line">[&apos;list&apos;, &apos;those&apos;, &apos;flights&apos;, &apos;that&apos;, &apos;stop&apos;, &apos;over&apos;, &apos;in&apos;, &apos;salt&apos;,</span><br><span class="line">&apos;lake&apos;, &apos;city&apos;, &apos;.&apos;]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br></pre></td></tr></table></figure>

</details>




<p>自底向上的语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser1 = nltk.parse.BottomUpChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart1 = parser1.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart1.num_edges()))</span><br><span class="line">13454</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart1.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>自底向上，左角（Left Corner）语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser2 = nltk.parse.BottomUpLeftCornerChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart2 = parser2.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart2.num_edges()))</span><br><span class="line">8781</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart2.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>使用了自底向上过滤器的左角语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser3 = nltk.parse.LeftCornerChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart3 = parser3.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart3.num_edges()))</span><br><span class="line">1280</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart3.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>自顶向下的语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser4 = nltk.parse.TopDownChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart4 = parser4.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart4.num_edges()))</span><br><span class="line">37763</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart4.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>增量式自底向上语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser5 = nltk.parse.IncrementalBottomUpChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart5 = parser5.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart5.num_edges()))</span><br><span class="line">13454</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart5.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>增量式自底向上、左角语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser6 = nltk.parse.IncrementalBottomUpLeftCornerChartParser(gr</span><br><span class="line">am1)</span><br><span class="line">&gt;&gt;&gt; chart6 = parser6.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart6.num_edges()))</span><br><span class="line">8781</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart6.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>使用了自底向上过滤器的增量式左角语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser7 = nltk.parse.IncrementalLeftCornerChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart7 = parser7.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart7.num_edges()))</span><br><span class="line">1280</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart7.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>增量式自顶向下语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser8 = nltk.parse.IncrementalTopDownChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart8 = parser8.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart8.num_edges()))</span><br><span class="line">37763</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart8.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<p>Earley语法解析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; gram1 = nltk.data.load(&apos;grammars/large_grammars/atis.cfg&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.data.load(&apos;grammars/large_grammars/atis_sentences.</span><br><span class="line">txt&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = nltk.parse.util.extract_test_sentences(sent)</span><br><span class="line">&gt;&gt;&gt; testingsent=sent[25]</span><br><span class="line">&gt;&gt;&gt; sent=testingsent[0]</span><br><span class="line">&gt;&gt;&gt; parser9 = nltk.parse.EarleyChartParser(gram1)</span><br><span class="line">&gt;&gt;&gt; chart9 = parser9.chart_parse(sent)</span><br><span class="line">&gt;&gt;&gt; print((chart9.num_edges()))</span><br><span class="line">37763</span><br><span class="line">&gt;&gt;&gt; print((len(list(chart9.parses(gram1.start())))))</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

</details>




<h2 id="5-4-从CFG创建概率上下文无关文法"><a href="#5-4-从CFG创建概率上下文无关文法" class="headerlink" title="5.4 从CFG创建概率上下文无关文法"></a>5.4 从CFG创建概率上下文无关文法</h2><p>在概率上下文无关文法（Probabilistic Context-free Grammar，PCFG）中，概率被附加到CFG中呈现的所有产生式中，这些概率之和为1。它生成与CFG相同的解析结构，但是也为每个解析树分配了一个概率。解析树的概率是在构建树的过程中用到的所有产生式概率的乘积。</p>
<p>让我们来看看以下有关NLTK的代码，这段代码说明了PCFG中的规则信息：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; from itertools import islice</span><br><span class="line">&gt;&gt;&gt; from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2</span><br><span class="line">&gt;&gt;&gt; gram2 = PCFG.from string(&quot;&quot;&quot;</span><br><span class="line">A -&gt; B B [.3] | C B C [.7]</span><br><span class="line">B -&gt; B D [.5] | C [.5]</span><br><span class="line">C -&gt; &apos;a&apos; [.1] | &apos;b&apos; [0.9]</span><br><span class="line">D -&gt; &apos;b&apos; [1.0]</span><br><span class="line">&quot;&quot;&quot;)</span><br><span class="line">&gt;&gt;&gt; prod1 = gram2.productions()[0]</span><br><span class="line">&gt;&gt;&gt; prod1</span><br><span class="line">A -&gt; B B [0.3]</span><br><span class="line">&gt;&gt;&gt; prod2 = gram2.productions()[1]</span><br><span class="line">&gt;&gt;&gt; prod2</span><br><span class="line">A -&gt; C B C [0.7]</span><br><span class="line">&gt;&gt;&gt; prod2.lhs()</span><br><span class="line">A</span><br><span class="line">&gt;&gt;&gt; prod2.rhs()</span><br><span class="line">(C, B, C)</span><br><span class="line">&gt;&gt;&gt; print((prod2.prob()))</span><br><span class="line">0.7</span><br><span class="line">&gt;&gt;&gt; gram2.start()</span><br><span class="line">A</span><br><span class="line">&gt;&gt;&gt; gram2.productions()</span><br><span class="line">[A -&gt; B B [0.3], A -&gt; C B C [0.7], B -&gt; B D [0.5], B -&gt; C [0.5], C -&gt;</span><br><span class="line">&apos;a&apos; [0.1], C -&gt; &apos;b&apos; [0.9], D -&gt; &apos;b&apos; [1.0]]</span><br></pre></td></tr></table></figure>

</details>




<p>我们来看看NLTK中用于说明概率分布图解析的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import treebank</span><br><span class="line">&gt;&gt;&gt; from itertools import islice</span><br><span class="line">&gt;&gt;&gt; from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2</span><br><span class="line">&gt;&gt;&gt; tokens = &quot;Jack told Bob to bring my cookie&quot;.split()</span><br><span class="line">&gt;&gt;&gt; grammar = toy_pcfg2</span><br><span class="line">&gt;&gt;&gt; print(grammar)</span><br><span class="line">Grammar with 23 productions (start state = S)</span><br><span class="line">    S -&gt; NP VP [1.0]</span><br><span class="line">    VP -&gt; V NP [0.59]</span><br><span class="line">    VP -&gt; V [0.4]</span><br><span class="line">    VP -&gt; VP PP [0.01]</span><br><span class="line">    NP -&gt; Det N [0.41]</span><br><span class="line">    NP -&gt; Name [0.28]</span><br><span class="line">    NP -&gt; NP PP [0.31]</span><br><span class="line">    PP -&gt; P NP [1.0]</span><br><span class="line">    V -&gt; &apos;saw&apos; [0.21]</span><br><span class="line">    V -&gt; &apos;ate&apos; [0.51]</span><br><span class="line">    V -&gt; &apos;ran&apos; [0.28]</span><br><span class="line">    N -&gt; &apos;boy&apos; [0.11]</span><br><span class="line">    N -&gt; &apos;cookie&apos; [0.12]</span><br><span class="line">    N -&gt; &apos;table&apos; [0.13]</span><br><span class="line">    N -&gt; &apos;telescope&apos; [0.14]</span><br><span class="line">    N -&gt; &apos;hill&apos; [0.5]</span><br><span class="line">    Name -&gt; &apos;Jack&apos; [0.52]</span><br><span class="line">    Name -&gt; &apos;Bob&apos; [0.48]</span><br><span class="line">    P -&gt; &apos;with&apos; [0.61]</span><br><span class="line">    P -&gt; &apos;under&apos; [0.39]</span><br><span class="line">    Det -&gt; &apos;the&apos; [0.41]</span><br><span class="line">    Det -&gt; &apos;a&apos; [0.31]</span><br><span class="line">    Det -&gt; &apos;my&apos; [0.28]</span><br></pre></td></tr></table></figure>

</details>




<h2 id="5-5-CYK线图解析算法"><a href="#5-5-CYK线图解析算法" class="headerlink" title="5.5 CYK线图解析算法"></a>5.5 CYK线图解析算法</h2><p>递归下降解析的缺点是它会导致左递归问题并且非常复杂，所以引入了CYK线图解析。CYK线图解析使用动态规划方法，是最简单的线图解析算法之一。CYK算法构建线图的时间复杂度为O（n3）。CYK和Earley都是自底向上的线图解析算法。但是，当构建了无效的解析时，Earley算法也使用自顶向下的预测。</p>
<p>考虑如下有关CYK线图解析的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tok = [&quot;the&quot;, &quot;kids&quot;, &quot;opened&quot;, &quot;the&quot;, &quot;box&quot;, &quot;on&quot;, &quot;the&quot;, &quot;floor&quot;]</span><br><span class="line">gram = nltk.parse_cfg(&quot;&quot;&quot;</span><br><span class="line">S -&gt; NP VP</span><br><span class="line">NP -&gt; Det N | NP PP</span><br><span class="line">VP -&gt; V NP | VP PP</span><br><span class="line">PP -&gt; P NP</span><br><span class="line">Det -&gt; &apos;the&apos;</span><br><span class="line">N -&gt; &apos;kids&apos; | &apos;box&apos; | &apos;floor&apos;</span><br><span class="line">V -&gt; &apos;opened&apos; P -&gt; &apos;on&apos;</span><br><span class="line">&quot;&quot;&quot;)</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下用于构建初始化线图的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def init_nfst(tok, gram):</span><br><span class="line">numtokens1 = len(tok)</span><br><span class="line"> # fill w/ dots</span><br><span class="line">nfst = [[&quot;.&quot; for i in range(numtokens1+1)] !!!!!!! for j in</span><br><span class="line">range(numtokens1+1)]</span><br><span class="line"># fill in diagonal</span><br><span class="line">for i in range(numtokens1):</span><br><span class="line">prod= gram.productions(rhs=tok[i])</span><br><span class="line">nfst[i][i+1] = prod[0].lhs()</span><br><span class="line">return nfst</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下用于填充线图的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def complete_nfst(nfst, tok, trace=False):</span><br><span class="line">index1 = &#123;&#125; for prod in gram.productions():</span><br><span class="line">#make lookup reverse</span><br><span class="line">index1[prod.rhs()] = prod.lhs()</span><br><span class="line">numtokens1 = len(tok) for span in range(2, numtokens1+1):</span><br><span class="line">for start in range(numtokens1+1-span):</span><br><span class="line">#go down towards diagonal</span><br><span class="line">end1 = start1 + span for mid in range(start1+1, end1):</span><br><span class="line">nt1, nt2 = nfst[start1][mid1], nfst[mid1][end1]</span><br><span class="line">if (nt1,nt2) in index1:</span><br><span class="line">if trace:</span><br><span class="line">print &quot;[%s] %3s [%s] %3s [%s] ==&gt; [%s] %3s [%s]&quot; % \ (start, nt1,mid1, nt2, end1, start1, index[(nt1,nt2)], end) </span><br><span class="line">nfst[start1][end1] =</span><br><span class="line">index[(nt1,nt2)]</span><br><span class="line">return nfst</span><br></pre></td></tr></table></figure>

</details>




<p>下面是在Python中用于构建显示线图的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def display(wfst, tok):</span><br><span class="line">print &apos;\nWFST &apos; + &apos; &apos;.join([(&quot;%-4d&quot; % i) for i in range(1,</span><br><span class="line">len(wfst))])</span><br><span class="line">for i in range(len(wfst)-1):</span><br><span class="line">print &quot;%d &quot; % i,</span><br><span class="line">for j in range(1, len(wfst)):</span><br><span class="line">print &quot;%-4s&quot; % wfst[i][j],</span><br><span class="line">print</span><br></pre></td></tr></table></figure>

</details>




<p>以下代码用于获取输出结果：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tok = [&quot;the&quot;, &quot;kids&quot;, &quot;opened&quot;, &quot;the&quot;, &quot;box&quot;, &quot;on&quot;, &quot;the&quot;, &quot;floor&quot;]</span><br><span class="line">res1 = init_wfst(tok, gram)</span><br><span class="line">display(res1, tok)</span><br><span class="line">res2 = complete_wfst(res1,tok)</span><br><span class="line">display(res2, tok)</span><br></pre></td></tr></table></figure>

</details>




<h2 id="5-6-Earley线图解析算法"><a href="#5-6-Earley线图解析算法" class="headerlink" title="5.6 Earley线图解析算法"></a>5.6 Earley线图解析算法</h2><p>Earley算法由Earley于1970年提出。该算法类似于自顶向下的语句解析。它可以处理左递归问题，并且不需要 CNF（乔姆斯基范式）转化。Earley算法以从左到右的方式填充线图。</p>
<p>考虑一个展示了用Earley线图解析器来进行语法解析的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; nltk.parse.earleychart.demo(print_times=False, trace=1,sent=&apos;I saw</span><br><span class="line">a dog&apos;, numparses=2)</span><br><span class="line">* Sentence:</span><br><span class="line">I saw a dog</span><br><span class="line">[&apos;I&apos;, &apos;saw&apos;, &apos;a&apos;, &apos;dog&apos;]</span><br><span class="line"></span><br><span class="line">|.   I     .   saw   .   a     .   dog   .|</span><br><span class="line">|[---------]         .         .         .| [0:1] &apos;I&apos;</span><br><span class="line">|.         [---------]         .         .| [1:2] &apos;saw&apos;</span><br><span class="line">|.         .         [---------]         .| [2:3] &apos;a&apos;</span><br><span class="line">|.         .         .         [---------]| [3:4] &apos;dog&apos;</span><br><span class="line">|&gt;         .         .         .         .| [0:0] S -&gt; * NP VP</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * NP PP</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * Det Noun</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * &apos;I&apos;</span><br><span class="line">|[---------]         .         .         .| [0:1] NP -&gt; &apos;I&apos; *</span><br><span class="line">|[---------&gt;         .         .         .| [0:1] S -&gt; NP * VP</span><br><span class="line">|[---------&gt;         .         .         .| [0:1] NP -&gt; NP * PP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * VP PP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * Verb NP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * Verb</span><br><span class="line">|.         &gt;         .         .         .| [1:1] Verb -&gt; * &apos;saw&apos;</span><br><span class="line">|.         [---------]         .         .| [1:2] Verb -&gt; &apos;saw&apos; *</span><br><span class="line">|.         [---------&gt;         .         .| [1:2] VP -&gt; Verb * NP</span><br><span class="line">|.         [---------]         .         .| [1:2] VP -&gt; Verb *</span><br><span class="line">|[-------------------]         .         .| [0:2] S -&gt; NP VP *</span><br><span class="line">|.         [---------&gt;         .         .| [1:2] VP -&gt; VP * PP</span><br><span class="line">|.         .         &gt;         .         .| [2:2] NP -&gt; * NP PP</span><br><span class="line">|.         .         &gt;         .         .| [2:2] NP -&gt; * Det Noun</span><br><span class="line">|.         .         &gt;         .         .| [2:2] Det -&gt; * &apos;a&apos;</span><br><span class="line">|.         .         [---------]         .| [2:3] Det -&gt; &apos;a&apos; *</span><br><span class="line">|.         .         [---------&gt;         .| [2:3] NP -&gt; Det * Noun</span><br><span class="line">|.         .         .         &gt;         .| [3:3] Noun -&gt; * &apos;dog&apos;</span><br><span class="line">|.         .         .         [---------]| [3:4] Noun -&gt; &apos;dog&apos; *</span><br><span class="line">|.         .         [-------------------]| [2:4] NP -&gt; Det Noun *</span><br><span class="line">|.         [-----------------------------]| [1:4] VP -&gt; Verb NP *</span><br><span class="line">|.         .         [-------------------&gt;| [2:4] NP -&gt; NP * PP</span><br><span class="line">|[=======================================]| [0:4] S -&gt; NP VP *</span><br><span class="line">|.         [-----------------------------&gt;| [1:4] VP -&gt; VP * PP</span><br></pre></td></tr></table></figure>

</details>




<p>考虑一个使用NLTK中的线图解析器来进行语法解析的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; nltk.parse.chart.demo(2, print_times=False, trace=1,sent=&apos;John saw</span><br><span class="line">a dog&apos;, numparses=1)</span><br><span class="line">* Sentence:</span><br><span class="line">John saw a dog</span><br><span class="line">[&apos;John&apos;, &apos;saw&apos;, &apos;a&apos;, &apos;dog&apos;]</span><br><span class="line"></span><br><span class="line">* Strategy: Bottom-up</span><br><span class="line"></span><br><span class="line">|.   John  .   saw   .   a     .    dog  .|</span><br><span class="line">|[---------]         .         .         .| [0:1] &apos;John&apos;</span><br><span class="line">|.         [---------]         .         .| [1:2] &apos;saw&apos;</span><br><span class="line">|.         .         [---------]         .| [2:3] &apos;a&apos;</span><br><span class="line">|.         .         .         [---------]| [3:4] &apos;dog&apos;</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * &apos;John&apos;</span><br><span class="line">|[---------]         .         .         .| [0:1] NP -&gt; &apos;John&apos; *</span><br><span class="line">|&gt;         .         .         .         .| [0:0] S -&gt; * NP VP</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * NP PP</span><br><span class="line">|[---------&gt;         .         .         .| [0:1] S -&gt; NP * VP</span><br><span class="line">|[---------&gt;         .         .         .| [0:1] NP -&gt; NP * PP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] Verb -&gt; * &apos;saw&apos;</span><br><span class="line">|.         [---------]         .         .| [1:2] Verb -&gt; &apos;saw&apos; *</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * Verb NP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * Verb</span><br><span class="line">|.         [---------&gt;         .         .| [1:2] VP -&gt; Verb * NP</span><br><span class="line">|.         [---------]         .         .| [1:2] VP -&gt; Verb *</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * VP PP</span><br><span class="line">|[-------------------]         .         .| [0:2] S -&gt; NP VP *</span><br><span class="line">|.         [---------&gt;         .         .| [1:2] VP -&gt; VP * PP</span><br><span class="line">|.        .          &gt;         .         .| [2:2] Det -&gt; * &apos;a&apos;</span><br><span class="line">|.        .          [---------]         .| [2:3] Det -&gt; &apos;a&apos; *</span><br><span class="line">|.        .          &gt;         .         .| [2:2] NP -&gt; * Det Noun</span><br><span class="line">|.        .          [---------&gt;         .| [2:3] NP -&gt; Det * Noun</span><br><span class="line">|.        .          .         &gt;         .| [3:3] Noun -&gt; * &apos;dog&apos;</span><br><span class="line">|.        .          .         [---------]| [3:4] Noun -&gt; &apos;dog&apos; *</span><br><span class="line">|.        .          [-------------------]| [2:4] NP -&gt; Det Noun *</span><br><span class="line">|.        .          &gt;         .         .| [2:2] S -&gt; * NP VP</span><br><span class="line">|.        .          &gt;         .         .| [2:2] NP -&gt; * NP PP</span><br><span class="line">|.        [------------------------------]| [1:4] VP -&gt; Verb NP *</span><br><span class="line">|.        .          [-------------------&gt;| [2:4] S -&gt; NP * VP</span><br><span class="line">|.        .          [-------------------&gt;| [2:4] NP -&gt; NP * PP</span><br><span class="line">|[=======================================]| [0:4] S -&gt; NP VP *</span><br><span class="line">|.        [------------------------------&gt;| [1:4] VP -&gt; VP * PP</span><br><span class="line">Nr edges in chart: 33</span><br><span class="line">(S (NP John) (VP (Verb saw) (NP (Det a) (Noun dog))))</span><br></pre></td></tr></table></figure>

</details>




<p>考虑一个使用了NLTK中的Stepping线图解析器来进行语法解析的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; nltk.parse.chart.demo(5, print_times=False, trace=1,sent=&apos;John saw</span><br><span class="line">a dog&apos;, numparses=2)</span><br><span class="line">* Sentence:</span><br><span class="line">John saw a dog</span><br><span class="line">[&apos;John&apos;, &apos;saw&apos;, &apos;a&apos;, &apos;dog&apos;]</span><br><span class="line"></span><br><span class="line">* Strategy: Stepping (top-down vs bottom-up)</span><br><span class="line"></span><br><span class="line">*** SWITCH TO TOP DOWN</span><br><span class="line">|[---------]         .      .            .| [0:1] &apos;John&apos;</span><br><span class="line">|.         [---------]      .            .| [1:2] &apos;saw&apos;</span><br><span class="line">|.         .         [---------]         .| [2:3] &apos;a&apos;</span><br><span class="line">|.         .         .         [---------]| [3:4] &apos;dog&apos;</span><br><span class="line">|&gt;         .         .         .         .| [0:0] S -&gt; * NP VP</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * NP PP</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * Det Noun</span><br><span class="line">|&gt;         .         .         .         .| [0:0] NP -&gt; * &apos;John&apos;</span><br><span class="line">|[---------]         .         .         .| [0:1] NP -&gt; &apos;John&apos; *</span><br><span class="line">|[---------&gt;         .         .         .| [0:1] S -&gt; NP * VP</span><br><span class="line">|[---------&gt;         .         .         .| [0:1] NP -&gt; NP * PP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * VP PP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * Verb NP</span><br><span class="line">|.         &gt;         .         .         .| [1:1] VP -&gt; * Verb</span><br><span class="line">|.         &gt;         .         .         .| [1:1] Verb -&gt; * &apos;saw&apos;</span><br><span class="line">|.         [---------]         .         .| [1:2] Verb -&gt; &apos;saw&apos; *</span><br><span class="line">|.         [---------&gt;         .         .| [1:2] VP -&gt; Verb * NP</span><br><span class="line">|.         [---------]         .         .| [1:2] VP -&gt; Verb *</span><br><span class="line">|[-------------------]         .         .| [0:2] S -&gt; NP VP *</span><br><span class="line">|.         [---------&gt;         .         .| [1:2] VP -&gt; VP * PP</span><br><span class="line">|.         .         &gt;         .         .| [2:2] NP -&gt; * NP PP</span><br><span class="line">|.         .         &gt;         .         .| [2:2] NP -&gt; * Det Noun</span><br><span class="line">*** SWITCH TO BOTTOM UP</span><br><span class="line">|.         .         &gt;         .         .| [2:2] Det -&gt; * &apos;a&apos;</span><br><span class="line">|.         .         .         &gt;         .| [3:3] Noun -&gt; * &apos;dog&apos;</span><br><span class="line">|.         .         [---------]         .| [2:3] Det -&gt; &apos;a&apos; *</span><br><span class="line">|.         .         .         [---------]| [3:4] Noun -&gt; &apos;dog&apos; *</span><br><span class="line">|.         .         [---------&gt;         .| [2:3] NP -&gt; Det * Noun</span><br><span class="line">|.         .         [-------------------]| [2:4] NP -&gt; Det Noun *</span><br><span class="line">|.         [-----------------------------]| [1:4] VP -&gt; Verb NP *</span><br><span class="line">|.         .         [-------------------&gt;| [2:4] NP -&gt; NP * PP</span><br><span class="line">|[=======================================]| [0:4] S -&gt; NP VP *</span><br><span class="line">|.         [-----------------------------&gt;| [1:4] VP -&gt; VP * PP</span><br><span class="line">|.         .         &gt;         .         .| [2:2] S -&gt; * NP VP</span><br><span class="line">|.         .         [-------------------&gt;| [2:4] S -&gt; NP * VP</span><br><span class="line">*** SWITCH TO TOP DOWN</span><br><span class="line">|.         .         .         .         &gt;| [4:4] VP -&gt; * VP PP</span><br><span class="line">|.         .         .         .         &gt;| [4:4] VP -&gt; * Verb NP</span><br><span class="line">|.         .         .         .         &gt;| [4:4] VP -&gt; * Verb</span><br><span class="line">*** SWITCH TO BOTTOM UP</span><br><span class="line">*** SWITCH TO TOP DOWN</span><br><span class="line">*** SWITCH TO BOTTOM UP</span><br><span class="line">*** SWITCH TO TOP DOWN</span><br><span class="line">*** SWITCH TO BOTTOM UP</span><br><span class="line">*** SWITCH TO TOP DOWN</span><br><span class="line">*** SWITCH TO BOTTOM UP</span><br><span class="line">Nr edges in chart: 37</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看NLTK中有关Feature线图解析的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; nltk.parse.featurechart.demo(print_times=False,print_</span><br><span class="line">grammar=True,parser=nltk.parse.featurechart.FeatureChartParser,sent=&apos;I</span><br><span class="line">saw a dog&apos;)</span><br><span class="line"></span><br><span class="line">Grammar with 18 productions (start state = S[])</span><br><span class="line">    S[] -&gt; NP[] VP[]</span><br><span class="line">    PP[] -&gt; Prep[] NP[]</span><br><span class="line">    NP[] -&gt; NP[] PP[]</span><br><span class="line">    VP[] -&gt; VP[] PP[]</span><br><span class="line">    VP[] -&gt; Verb[] NP[]</span><br><span class="line">    VP[] -&gt; Verb[]</span><br><span class="line">    NP[] -&gt; Det[pl=?x] Noun[pl=?x]</span><br><span class="line">    NP[] -&gt; &apos;John&apos;</span><br><span class="line">    NP[] -&gt; &apos;I&apos;</span><br><span class="line">    Det[] -&gt; &apos;the&apos;</span><br><span class="line">    Det[] -&gt; &apos;my&apos;</span><br><span class="line">    Det[-pl] -&gt; &apos;a&apos;</span><br><span class="line">    Noun[-pl] -&gt; &apos;dog&apos;</span><br><span class="line">    Noun[-pl] -&gt; &apos;cookie&apos;</span><br><span class="line">    Verb[] -&gt; &apos;ate&apos;</span><br><span class="line">    Verb[] -&gt; &apos;saw&apos;</span><br><span class="line">    Prep[] -&gt; &apos;with&apos;</span><br><span class="line">    Prep[] -&gt; &apos;under&apos;</span><br><span class="line"></span><br><span class="line">* FeatureChartParser</span><br><span class="line">Sentence: I saw a dog</span><br><span class="line">|. I .saw. a .dog.|</span><br><span class="line">|[---]   .   .   .| [0:1] &apos;I&apos;</span><br><span class="line">|.   [---]   .   .| [1:2] &apos;saw&apos;</span><br><span class="line">|.   .   [---]   .| [2:3] &apos;a&apos;</span><br><span class="line">|.   .   .   [---]| [3:4] &apos;dog&apos;</span><br><span class="line">|[---]   .   .   .| [0:1] NP[] -&gt; &apos;I&apos; *</span><br><span class="line">|[---&gt;   .   .   .| [0:1] S[] -&gt; NP[] * VP[] &#123;&#125;</span><br><span class="line">|[---&gt;   .   .   .| [0:1] NP[] -&gt; NP[] * PP[] &#123;&#125;</span><br><span class="line">|.   [---]   .   .| [1:2] Verb[] -&gt; &apos;saw&apos; *</span><br><span class="line">|.   [---&gt;   .   .| [1:2] VP[] -&gt; Verb[] * NP[] &#123;&#125;</span><br><span class="line">|.   [---]   .   .| [1:2] VP[] -&gt; Verb[] *</span><br><span class="line">|.   [---&gt;   .   .| [1:2] VP[] -&gt; VP[] * PP[] &#123;&#125;</span><br><span class="line">|[-------]   .   .| [0:2] S[] -&gt; NP[] VP[] *</span><br><span class="line">|.   .   [---]   .| [2:3] Det[-pl] -&gt; &apos;a&apos; *</span><br><span class="line">|.   .   [---&gt;   .| [2:3] NP[] -&gt; Det[pl=?x] * Noun[pl=?x] &#123;?x: False&#125;</span><br><span class="line">|.   .   .   [---]| [3:4] Noun[-pl] -&gt; &apos;dog&apos; *</span><br><span class="line">|.   .   [-------]| [2:4] NP[] -&gt; Det[-pl] Noun[-pl] *</span><br><span class="line">|.   .   [-------&gt;| [2:4] S[] -&gt; NP[] * VP[] &#123;&#125;</span><br><span class="line">|.   .   [-------&gt;| [2:4] NP[] -&gt; NP[] * PP[] &#123;&#125;</span><br><span class="line">|.   [-----------]| [1:4] VP[] -&gt; Verb[] NP[] *</span><br><span class="line">|.   [-----------&gt;| [1:4] VP[] -&gt; VP[] * PP[] &#123;&#125;</span><br><span class="line">|[===============]| [0:4] S[] -&gt; NP[] VP[] *</span><br><span class="line">(S[]</span><br><span class="line">  (NP[] I)</span><br><span class="line">  (VP[] (Verb[] saw) (NP[] (Det[-pl] a) (Noun[-pl] dog))))</span><br></pre></td></tr></table></figure>

</details>




<p>如下是NLTK中用于实现Earley算法的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">def demo(print_times=True, print_grammar=False,</span><br><span class="line">        print_trees=True, trace=2,</span><br><span class="line">        sent=&apos;I saw John with a dog with my cookie&apos;, numparses=5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A demonstration of the Earley parsers.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    import sys, time</span><br><span class="line">    from nltk.parse.chart import demo_grammar</span><br><span class="line"></span><br><span class="line">    # The grammar for ChartParser and SteppingChartParser:</span><br><span class="line">    grammar = demo_grammar()</span><br><span class="line">    if print_grammar:</span><br><span class="line">        print(&quot;* Grammar&quot;)</span><br><span class="line">        print(grammar)</span><br><span class="line"></span><br><span class="line">    # Tokenize the sample sentence.</span><br><span class="line">    print(&quot;* Sentence:&quot;)</span><br><span class="line">    print(sent)</span><br><span class="line">    tokens = sent.split()</span><br><span class="line">    print(tokens)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line">    # Do the parsing.</span><br><span class="line">    earley = EarleyChartParser(grammar, trace=trace)</span><br><span class="line">    t = time.clock()</span><br><span class="line">    chart = earley.chart_parse(tokens)</span><br><span class="line">    parses = list(chart.parses(grammar.start()))</span><br><span class="line">    t = time.clock()-t</span><br><span class="line"></span><br><span class="line">    # Print results.</span><br><span class="line">    if numparses:</span><br><span class="line">        assert len(parses)==numparses, &apos;Not all parses found&apos;</span><br><span class="line">    if print_trees:</span><br><span class="line">        for tree in parses: print(tree)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;Nr trees:&quot;, len(parses))</span><br><span class="line">    if print_times:</span><br><span class="line">        print(&quot;Time:&quot;, t)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;: demo()</span><br></pre></td></tr></table></figure>

</details>




<h2 id="5-7-小结"><a href="#5-7-小结" class="headerlink" title="5.7 小结"></a>5.7 小结</h2><p>在本章中，我们讨论了语法解析，Treebank语料库的访问，以及上下文无关文法、概率上下文无关文法、CYK算法和Earley算法等的实现。因此在本章中，我们讨论的是NLP的句法分析阶段。</p>
<p>在下一章中，我们将讨论语义分析，这是NLP的另一个阶段。我们将讨论使用各种不同方法的NER，并获取用于执行消歧任务的各种方法。</p>
<h1 id="第6章-语义分析：意义很重要"><a href="#第6章-语义分析：意义很重要" class="headerlink" title="第6章 语义分析：意义很重要"></a>第6章 语义分析：意义很重要</h1><p>语义分析（或者叫意义生成）是NLP中的任务之一。它被定义为确定字符或单词序列意义的过程，其可用于执行语义消歧任务。</p>
<p>本章将包含以下主题：</p>
<ul>
<li>NER。</li>
<li>使用HMM的NER系统。</li>
<li>使用机器学习工具包训练NER。</li>
<li>使用词性标注执行NER。</li>
<li>使用Wordnet生成同义词集id。</li>
<li>使用Wordnet进行词义消歧。</li>
</ul>
<h2 id="6-1-语义分析简介"><a href="#6-1-语义分析简介" class="headerlink" title="6.1 语义分析简介"></a>6.1 语义分析简介</h2><p>NLP指的是在自然语言上执行计算。语义分析是处理自然语言时需要执行的步骤之一。在分析一个给定的句子时，如果已经构建了句子的句法结构，那么这个句子的语义分析就算完成了。语义解释指的是将意义分配给句子，上下文解释指的是将逻辑形式分配给知识表示。语义分析的原语或基本单位被称为意义或语义（meaning或sense）。ELIZA是处理语义的工具之一，是由Joseph Weizenbaum在六十年代开发出来的，它使用替换和模式匹配技术来分析句子并且为给定的输入提供输出。MARGIE是由Robert Schank在七十年代开发出来的，它可以使用11种原语来表示所有的英语动词。MARGIE可以解释一个句子的语义并借助原语来表示其语义。MARGIE之后进一步让位于脚本的概念，脚本应用机制（Script Applier Mechanism，SAM）就是基于MARGIE开发出来的，它可以翻译来自不同语言的句子，例如英语、汉语、俄语、荷兰语和西班牙语等。为了处理文本数据，使用了一个Python库也就是TextBlob库。TextBlob提供了用于执行NLP任务的API，例如词性标注、名词短语提取、文本分类、机器翻译、情感分析等。</p>
<p>语义分析可用于查询数据库和检索信息。另一个Python库Gensim可用于执行文档索引、主题建模和相似性检索。Polyglot是一个支持多语言应用的NLP工具，它提供了40种语言的命名实体识别、165种语言的分词、196种语言的语言检测、136种语言的情感分析、16种语言的词性标注、135种语言的形态分析、137种语言的嵌入以及69种语言的音译。MontyLingua是一个用于执行有关英语文本语义解释的NLP工具，它可以从英文句子中提取诸如动词、名词、形容词、日期、短语等语义信息。</p>
<p>可以使用逻辑学来正式地表示句子。命题逻辑中的基本表达式或句子可以用诸如P、Q、R等命题符号来表示。命题逻辑中的复杂表达式可以用布尔运算符来表示。例如，为了表示句子If it is raining, I’ll wear a raincoat，可以使用命题逻辑：</p>
<ul>
<li>P: It is raining.</li>
<li>Q: I’ll wear raincoat.</li>
<li>P→Q: If it is raining, I’ll wear a raincoat.</li>
</ul>
<p>考虑下面NLTK中用于展示所使用的运算符的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; nltk.boolean_ops()</span><br><span class="line">negation -</span><br><span class="line">conjunction   &amp;</span><br><span class="line">disjunction   |</span><br><span class="line">implication   -&gt;</span><br><span class="line">equivalence   &lt;-&gt;</span><br></pre></td></tr></table></figure>

</details>




<p>合式公式（Well-formed Formulas，WFF）是使用命题符号或命题符号与布尔运算符的组合构成的。</p>
<p>让我们来看看如下NLTK中的代码，它将逻辑表达式分解为不同的子类：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; input_expr = nltk.sem.Expression.from string</span><br><span class="line">&gt;&gt;&gt; input_expr(&apos;X | (Y -&gt; Z)&apos;)</span><br><span class="line">&lt;OrExpression (X | (Y -&gt; Z))&gt;</span><br><span class="line">&gt;&gt;&gt; input_expr(&apos;-(X &amp; Y)&apos;)</span><br><span class="line">&lt;NegatedExpression -(X &amp; Y)&gt;</span><br><span class="line">&gt;&gt;&gt; input_expr(&apos;X &amp; Y&apos;)</span><br><span class="line">&lt;AndExpression (X &amp; Y)&gt;</span><br><span class="line">&gt;&gt;&gt; input_expr(&apos;X &lt;-&gt; -- X&apos;)</span><br><span class="line">&lt;IffExpression (X &lt;-&gt; --X)&gt;</span><br></pre></td></tr></table></figure>

</details>




<p>为了将<code>True</code> 或<code>False</code> 值赋值给逻辑表达式，使用了NLTK中的<code>Valuation</code> 函数：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; value = nltk.Valuation([(&apos;X&apos;, True), (&apos;Y&apos;, False), (&apos;Z&apos;, True)])</span><br><span class="line">&gt;&gt;&gt; value[&apos;Z&apos;]</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; domain = set()</span><br><span class="line">&gt;&gt;&gt; v = nltk.Assignment(domain)</span><br><span class="line">&gt;&gt;&gt; u = nltk.Model(domain, value)</span><br><span class="line">&gt;&gt;&gt; print(u.evaluate(&apos;(X &amp; Y)&apos;, v))</span><br><span class="line">False</span><br><span class="line">&gt;&gt;&gt; print(u.evaluate(&apos;-(X &amp; Y)&apos;, v))</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; print(u.evaluate(&apos;(X &amp; Z)&apos;, v))</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; print(u.evaluate(&apos;(X | Y)&apos;, v))</span><br><span class="line">True</span><br></pre></td></tr></table></figure>

</details>




<p>下面的代码描述了NLTK中包含常量和谓词的一阶谓词逻辑：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; input_expr = nltk.sem.Expression.fromstring</span><br><span class="line">&gt;&gt;&gt; expression = input_expr(&apos;run(marcus)&apos;, type_check=True)</span><br><span class="line">&gt;&gt;&gt; expression.argument</span><br><span class="line">&lt;ConstantExpression marcus&gt;</span><br><span class="line">&gt;&gt;&gt; expression.argument.type</span><br><span class="line">e</span><br><span class="line">&gt;&gt;&gt; expression.function</span><br><span class="line">&lt;ConstantExpression run&gt;</span><br><span class="line">&gt;&gt;&gt; expression.function.type</span><br><span class="line">&lt;e,?&gt;</span><br><span class="line">&gt;&gt;&gt; sign = &#123;&apos;run&apos;: &apos;&lt;e, t&gt;&apos;&#125;</span><br><span class="line">&gt;&gt;&gt; expression = input_expr(&apos;run(marcus)&apos;, signature=sign)</span><br><span class="line">&gt;&gt;&gt; expression.function.type</span><br><span class="line">e</span><br></pre></td></tr></table></figure>

</details>




<p>在NLTK中使用<code>signature</code> 是为了映射关联类型和非逻辑常量。</p>
<p>考虑如下NLTK中的代码，它有助于生成查询指令，并可以从数据库中检索数据：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; nltk.data.show_cfg(&apos;grammars/book_grammars/sql1.fcfg&apos;)</span><br><span class="line">% start S</span><br><span class="line">S[SEM=(?np + WHERE + ?vp)] -&gt; NP[SEM=?np] VP[SEM=?vp]</span><br><span class="line">VP[SEM=(?v + ?pp)] -&gt; IV[SEM=?v] PP[SEM=?pp]</span><br><span class="line">VP[SEM=(?v + ?ap)] -&gt; IV[SEM=?v] AP[SEM=?ap]</span><br><span class="line">VP[SEM=(?v + ?np)] -&gt; TV[SEM=?v] NP[SEM=?np]</span><br><span class="line">VP[SEM=(?vp1 + ?c + ?vp2)] -&gt; VP[SEM=?vp1] Conj[SEM=?c] VP[SEM=?vp2]</span><br><span class="line">NP[SEM=(?det + ?n)] -&gt;Det[SEM=?det] N[SEM=?n]</span><br><span class="line">NP[SEM=(?n + ?pp)] -&gt; N[SEM=?n] PP[SEM=?pp]</span><br><span class="line">NP[SEM=?n] -&gt; N[SEM=?n] | CardN[SEM=?n]</span><br><span class="line">CardN[SEM=&apos;1000&apos;] -&gt; &apos;1,000,000&apos;</span><br><span class="line">PP[SEM=(?p + ?np)] -&gt; P[SEM=?p] NP[SEM=?np]</span><br><span class="line">AP[SEM=?pp] -&gt; A[SEM=?a] PP[SEM=?pp]</span><br><span class="line">NP[SEM=&apos;Country=&quot;greece&quot;&apos;] -&gt; &apos;Greece&apos;</span><br><span class="line">NP[SEM=&apos;Country=&quot;china&quot;&apos;] -&gt; &apos;China&apos;</span><br><span class="line">Det[SEM=&apos;SELECT&apos;] -&gt; &apos;Which&apos; | &apos;What&apos;</span><br><span class="line">Conj[SEM=&apos;AND&apos;] -&gt; &apos;and&apos;</span><br><span class="line">N[SEM=&apos;City FROM city_table&apos;] -&gt; &apos;cities&apos;</span><br><span class="line">N[SEM=&apos;Population&apos;] -&gt; &apos;populations&apos;</span><br><span class="line">IV[SEM=&apos;&apos;] -&gt; &apos;are&apos;</span><br><span class="line">TV[SEM=&apos;&apos;] -&gt; &apos;have&apos;</span><br><span class="line">A -&gt; &apos;located&apos;</span><br><span class="line">P[SEM=&apos;&apos;] -&gt; &apos;in&apos;</span><br><span class="line">P[SEM=&apos;&gt;&apos;] -&gt; &apos;above&apos;</span><br><span class="line">&gt;&gt;&gt; from nltk import load_parser</span><br><span class="line">&gt;&gt;&gt; test = load_parser(&apos;grammars/book_grammars/sql1.fcfg&apos;)</span><br><span class="line">&gt;&gt;&gt; q=&quot; What cities are in Greece&quot;</span><br><span class="line">&gt;&gt;&gt; t = list(test.parse(q.split()))</span><br><span class="line">&gt;&gt;&gt; ans = t[0].label()[&apos;SEM&apos;]</span><br><span class="line">&gt;&gt;&gt; ans = [s for s in ans if s]</span><br><span class="line">&gt;&gt;&gt; q = &apos; &apos;.join(ans)</span><br><span class="line">&gt;&gt;&gt; print(q)</span><br><span class="line">SELECT City FROM city_table WHERE Country=&quot;greece&quot;</span><br><span class="line">&gt;&gt;&gt; from nltk.sem import chat80</span><br><span class="line">&gt;&gt;&gt; r = chat80.sql_query(&apos;corpora/city_database/city.db&apos;, q)</span><br><span class="line">&gt;&gt;&gt; for p in r:</span><br><span class="line">print(p[0], end=&quot; &quot;)</span><br><span class="line"></span><br><span class="line">athens</span><br></pre></td></tr></table></figure>

</details>




<h3 id="6-1-1-NER简介"><a href="#6-1-1-NER简介" class="headerlink" title="6.1.1 NER简介"></a>6.1.1 NER简介</h3><p>命名实体识别（Named entity recognition，NER）是定位文档中的专有名词或命名实体的过程。而且，这些命名实体被分成了不同的类别，例如人名、地名、机构名等。</p>
<p>由IIIT-Hyderabad IJCNLP 2008所定义的NER标签集有12个，描述如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>SNO.</th>
<th></th>
<th></th>
<th>Named entity tag</th>
<th></th>
<th></th>
<th>Meaning  </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td></td>
<td></td>
<td>NEP</td>
<td></td>
<td></td>
<td>Name of Person  </td>
</tr>
<tr>
<td>2</td>
<td></td>
<td></td>
<td>NED</td>
<td></td>
<td></td>
<td>Name of Designation  </td>
</tr>
<tr>
<td>3</td>
<td></td>
<td></td>
<td>NEO</td>
<td></td>
<td></td>
<td>Name of Organization  </td>
</tr>
<tr>
<td>4</td>
<td></td>
<td></td>
<td>NEA</td>
<td></td>
<td></td>
<td>Name of Abbreviation  </td>
</tr>
<tr>
<td>5</td>
<td></td>
<td></td>
<td>NEB</td>
<td></td>
<td></td>
<td>Name of Brand  </td>
</tr>
<tr>
<td>6</td>
<td></td>
<td></td>
<td>NETP</td>
<td></td>
<td></td>
<td>Title of Person  </td>
</tr>
<tr>
<td>7</td>
<td></td>
<td></td>
<td>NETO</td>
<td></td>
<td></td>
<td>Title of Object  </td>
</tr>
<tr>
<td>8</td>
<td></td>
<td></td>
<td>NEL</td>
<td></td>
<td></td>
<td>Name of Location  </td>
</tr>
<tr>
<td>9</td>
<td></td>
<td></td>
<td>NETI</td>
<td></td>
<td></td>
<td>Time  </td>
</tr>
<tr>
<td>10</td>
<td></td>
<td></td>
<td>NEN</td>
<td></td>
<td></td>
<td>Number  </td>
</tr>
<tr>
<td>11</td>
<td></td>
<td></td>
<td>NEM</td>
<td></td>
<td></td>
<td>Measure  </td>
</tr>
<tr>
<td>12</td>
<td></td>
<td></td>
<td>NETE</td>
<td></td>
<td></td>
<td>Terms</td>
</tr>
</tbody>
</table>
</div>
<p>NER的应用之一是信息提取。在NLTK中，我们可以通过存储元组（实体，关系，实体）来执行信息提取任务，之后就可以提取到实体值。</p>
<p>考虑一个NLTK中的示例，它展示了如何进行信息提取：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; locations=[(&apos;Jaipur&apos;, &apos;IN&apos;, &apos;Rajasthan&apos;),(&apos;Ajmer&apos;, &apos;IN&apos;,</span><br><span class="line">&apos;Rajasthan&apos;),(&apos;Udaipur&apos;, &apos;IN&apos;, &apos;Rajasthan&apos;),(&apos;Mumbai&apos;, &apos;IN&apos;,</span><br><span class="line">&apos;Maharashtra&apos;),(&apos;Ahmedabad&apos;, &apos;IN&apos;, &apos;Gujrat&apos;)]</span><br><span class="line">&gt;&gt;&gt; q = [x1 for (x1, relation, x2) in locations if x2==&apos;Rajasthan&apos;]</span><br><span class="line">&gt;&gt;&gt; print(q)</span><br><span class="line">[&apos;Jaipur&apos;, &apos;Ajmer&apos;, &apos;Udaipur&apos;]</span><br></pre></td></tr></table></figure>

</details>





<p>使用<code>nltk.tag.stanford</code> 模块以便可以使用斯坦福标注器来执行NER。我们可以通过网址<a href="http://nlp.stanford.edu/software" target="_blank" rel="noopener">http://nlp.stanford.edu/software</a> 来下载该标注器模型。</p>
<p>让我们来看看如下NLTK中的示例，它使用了斯坦福标注器用于执行NER：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.tag import StanfordNERTagger</span><br><span class="line">&gt;&gt;&gt; sentence = StanfordNERTagger(&apos;english.all.3class.distsim.crf.ser.</span><br><span class="line">gz&apos;)</span><br><span class="line">&gt;&gt;&gt; sentence.tag(&apos;John goes to NY&apos;.split())</span><br><span class="line">[(&apos;John&apos;, &apos;PERSON&apos;), (&apos;goes&apos;, &apos;O&apos;), (&apos;to&apos;, &apos;O&apos;),(&apos;NY&apos;, &apos;LOCATION&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>NLTK提供了一个已经训练好的可用于识别命名实体的分类器。通过使用函数<code>nltk.ne.chunk()</code> ，可以识别一个文本中的命名实体。如果参数binary被置为true，则可以识别出命名实体并使用NE标记来标注它们；否则，就使用诸如PERSON、GPE和ORGANIZATION等标记来标注命名实体。</p>
<p>让我们来看看如下用于识别命名实体的代码，如果命名实体存在，就用NE标记来标注它们：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sentences1 = nltk.corpus.treebank.tagged_sents()[17]</span><br><span class="line">&gt;&gt;&gt; print(nltk.ne_chunk(sentences1, binary=True))</span><br><span class="line">(S</span><br><span class="line">  The/DT</span><br><span class="line">total/NN</span><br><span class="line">of/IN</span><br><span class="line">  18/CD</span><br><span class="line">deaths/NNS</span><br><span class="line">from/IN</span><br><span class="line">malignant/JJ</span><br><span class="line">mesothelioma/NN</span><br><span class="line">  ,/,</span><br><span class="line">lung/NN</span><br><span class="line">cancer/NN</span><br><span class="line">and/CC</span><br><span class="line">asbestosis/NN</span><br><span class="line">was/VBD</span><br><span class="line">far/RB</span><br><span class="line">higher/JJR</span><br><span class="line">than/IN</span><br><span class="line">  */-NONE-</span><br><span class="line">expected/VBN</span><br><span class="line">  *?*/-NONE-</span><br><span class="line">  ,/,</span><br><span class="line">the/DT</span><br><span class="line">researchers/NNS</span><br><span class="line">said/VBD</span><br><span class="line">  0/-NONE-</span><br><span class="line">  *T*-1/-NONE-</span><br><span class="line">  ./.)</span><br><span class="line">&gt;&gt;&gt; sentences2 = nltk.corpus.treebank.tagged_sents()[7]</span><br><span class="line">&gt;&gt;&gt; print(nltk.ne_chunk(sentences2, binary=True))</span><br><span class="line">(S</span><br><span class="line">  A/DT</span><br><span class="line">   (NE Lorillard/NNP)</span><br><span class="line">spokewoman/NN</span><br><span class="line">said/VBD</span><br><span class="line">  ,/,</span><br><span class="line">  ``/``</span><br><span class="line">  This/DT</span><br><span class="line">is/VBZ</span><br><span class="line">an/DT</span><br><span class="line">old/JJ</span><br><span class="line">story/NN</span><br><span class="line">  ./.)</span><br><span class="line">&gt;&gt;&gt; print(nltk.ne_chunk(sentences2))</span><br><span class="line">(S</span><br><span class="line">  A/DT</span><br><span class="line">  (ORGANIZATION Lorillard/NNP)</span><br><span class="line">spokewoman/NN</span><br><span class="line">said/VBD</span><br><span class="line">  ,/,</span><br><span class="line">  ``/``</span><br><span class="line">  This/DT</span><br><span class="line">is/VBZ</span><br><span class="line">an/DT</span><br><span class="line">old/JJ</span><br><span class="line">story/NN</span><br><span class="line">  ./.)</span><br></pre></td></tr></table></figure>

</details>




<p>考虑NLTK中另一个可用于识别命名实体的示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import conll2002</span><br><span class="line">&gt;&gt;&gt; for documents in conll2002.chunked_sents(&apos;ned.train&apos;)[25]:</span><br><span class="line">print(documents)</span><br><span class="line"></span><br><span class="line">(PER Vandenbussche/Adj)</span><br><span class="line">(&apos;zelf&apos;, &apos;Pron&apos;)</span><br><span class="line">(&apos;besloot&apos;, &apos;V&apos;)</span><br><span class="line">(&apos;dat&apos;, &apos;Conj&apos;)</span><br><span class="line">(&apos;het&apos;, &apos;Art&apos;)</span><br><span class="line">(&apos;hof&apos;, &apos;N&apos;)</span><br><span class="line">(&apos;&quot;&apos;, &apos;Punc&apos;)</span><br><span class="line">(&apos;de&apos;, &apos;Art&apos;)</span><br><span class="line">(&apos;politieke&apos;, &apos;Adj&apos;)</span><br><span class="line">(&apos;zeden&apos;, &apos;N&apos;)</span><br><span class="line">(&apos;uit&apos;, &apos;Prep&apos;)</span><br><span class="line">(&apos;het&apos;, &apos;Art&apos;)</span><br><span class="line">(&apos;verleden&apos;, &apos;N&apos;)</span><br><span class="line">(&apos;&quot;&apos;, &apos;Punc&apos;)</span><br><span class="line">(&apos;heeft&apos;, &apos;V&apos;)</span><br><span class="line">(&apos;willen&apos;, &apos;V&apos;)</span><br><span class="line">(&apos;veroordelen&apos;, &apos;V&apos;)</span><br><span class="line">(&apos;.&apos;, &apos;Punc&apos;)</span><br></pre></td></tr></table></figure>

</details>




<p>分块器是一个用于将纯文本分割为语义相关的单词序列的程序。为了在NLTK中执行NER，我们需要使用默认的分块器。默认分块器是指基于在ACE语料库上训练过的分类器的分块器。其他分块器已经在解析过的或已分块的NLTK语料库上被训练过了。这些NLTK分块器涉及的语言如下：</p>
<ul>
<li>Dutch（荷兰语）。</li>
<li>Spanish（西班牙语）。</li>
<li>Portuguese（葡萄牙语）。</li>
<li>English（英语）。</li>
</ul>
<p>考虑NLTK中的另一个示例，它用于识别命名实体并将其划分为不同的命名实体类别：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; sentence = &quot;I went to Greece to meet John&quot;;</span><br><span class="line">&gt;&gt;&gt; tok=nltk.word_tokenize(sentence)</span><br><span class="line">&gt;&gt;&gt; pos_tag=nltk.pos_tag(tok)</span><br><span class="line">&gt;&gt;&gt; print(nltk.ne_chunk(pos_tag))</span><br><span class="line">(S</span><br><span class="line">  I/PRP</span><br><span class="line">went/VBD</span><br><span class="line">to/TO</span><br><span class="line">  (GPE Greece/NNP)</span><br><span class="line">to/TO</span><br><span class="line">meet/VB</span><br><span class="line">  (PERSON John/NNP))</span><br></pre></td></tr></table></figure>

</details>




<h3 id="6-1-2-使用隐马尔科夫模型的NER系统"><a href="#6-1-2-使用隐马尔科夫模型的NER系统" class="headerlink" title="6.1.2 使用隐马尔科夫模型的NER系统"></a>6.1.2 使用隐马尔科夫模型的NER系统</h3><p>HMM是关于NER的流行统计学方法之一。HMM被定义为一个随机有限状态自动机（Stochastic Finite State Automaton，SFSA），它由与确定的概率分布相关联的有限状态集组成，状态是不可观察或是隐蔽的。HMM生成最优的状态序列作为输出。HMM基于马尔科夫链属性。依据马尔科夫链属性，下一个状态发生的概率取决于上一个状态，这是最简单的实现方法。HMM的缺点是它需要进行大量的训练并且不能用于大的依赖。HMM包括以下内容：</p>
<p>状态集 _S_ , 其中| _S_ |= _N_ 。这里， _N_ 指的是状态的总数。</p>
<ul>
<li>初始状态 _S0_ 。</li>
<li>输出字符表 _O_ ，其中| _O_ |= _k_ 。这里， _k_ 指的是输出字母的总数。</li>
<li>转移概率 _A_ 。</li>
<li>发射概率 _B_ 。</li>
<li>初始状态概率 _π_ 。</li>
</ul>
<p>HMM可以由如下的元组呈现—— _λ=_ ( _A, B, π_ )。</p>
<p>启动概率或初始状态概率可以认为是一个特定的标记首次在句中出现的概率。</p>
<p>转移概率（ _A=aij_ ）指的是在给定当前特定标记 _i_ 出现的情况下下一个标记 _j_ 出现的概率。</p>
<p>_A=aij=_ 从状态 _si_ 到 _sj_ 的转换的数量/从状态 _si_ 开始转换的数量。</p>
<p>发射概率（ _B=bj(O)_ ）指的是在给定一个状态 _j_ 的情况下输出序列出现的概率。</p>
<p>_B=bj(k)=_ 在状态 _j_ 时输出观察符号k 的概率/在状态 _j_ 时预期观察值出现的次数。</p>
<p>Baum Welch算法用于找到HMM参数的最大似然值和后验模型估计。前向-后向算法用于在给定一个输出或观察序列的情况下找到所有隐藏状态变量的后验边缘概率。</p>
<p>使用HMM来执行NER有三个步骤：注释、HMM训练和HMM测试。注释模块将原始文本转换为注释或可训练的数据。在HMM训练环节，我们要计算HMM参数，包括启动概率、转移概率和发射概率。在HMM测试环节，使用了Viterbi算法以便找出最佳标记序列。</p>
<p>考虑一个NLTK中有关使用HMM进行分块的示例。通过分块，我们可以获得NP和VP组块。NP组块可以进一步被处理以便获取专有名词或命名实体：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; nltk.tag.hmm.demo_pos()</span><br><span class="line"></span><br><span class="line">HMM POS tagging demo</span><br><span class="line"></span><br><span class="line">Training HMM...</span><br><span class="line">Testing...</span><br><span class="line"></span><br><span class="line">Test: the/AT fulton/NP county/NN grand/JJ jury/NN said/VBD friday/</span><br><span class="line">NR an/AT investigation/NN of/IN atlanta&apos;s/NP$ recent/JJ primary/NN</span><br><span class="line">election/NN produced/VBD ``/`` no/AT evidence/NN &apos;&apos;/&apos;&apos; that/CS any/DTI</span><br><span class="line">irregularities/NNS took/VBD place/NN ./.</span><br><span class="line"></span><br><span class="line">Untagged: the fulton county grand jury said friday an investigation of</span><br><span class="line">atlanta&apos;s recent primary election produced `` no evidence &apos;&apos; that any</span><br><span class="line">irregularities took place .</span><br><span class="line"></span><br><span class="line">HMM-tagged: the/AT fulton/NP county/NN grand/JJ jury/NN said/</span><br><span class="line">VBD friday/NR an/AT investigation/NN of/IN atlanta&apos;s/NP$ recent/</span><br><span class="line">JJ primary/NN election/NN produced/VBD ``/`` no/AT evidence/NN &apos;&apos;/&apos;&apos;</span><br><span class="line">that/CS any/DTI irregularities/NNS took/VBD place/NN ./.</span><br><span class="line"></span><br><span class="line">Entropy: 18.7331739705</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: the/AT jury/NN further/RBR said/VBD in/IN term-end/NN</span><br><span class="line">presentments/NNS that/CS the/AT city/NN executive/JJ committee/NN ,/,</span><br><span class="line">which/WDT had/HVD over-all/JJ charge/NN of/IN the/AT election/NN ,/,</span><br><span class="line">``/`` deserves/VBZ the/AT praise/NN and/CC thanks/NNS of/IN the/AT</span><br><span class="line">city/NN of/IN atlanta/NP &apos;&apos;/&apos;&apos; for/IN the/AT manner/NN in/IN which/WDT</span><br><span class="line">the/AT election/NN was/BEDZ conducted/VBN ./.</span><br><span class="line"></span><br><span class="line">Untagged: the jury further said in term-end presentments that the</span><br><span class="line">city executive committee , which had over-all charge of the election</span><br><span class="line">, `` deserves the praise and thanks of the city of atlanta &apos;&apos; for the</span><br><span class="line">manner in which the election was conducted .</span><br><span class="line"></span><br><span class="line">HMM-tagged: the/AT jury/NN further/RBR said/VBD in/IN term-end/AT</span><br><span class="line">presentments/NN that/CS the/AT city/NN executive/NN committee/NN ,/,</span><br><span class="line">which/WDT had/HVD over-all/VBN charge/NN of/IN the/AT election/NN ,/,</span><br><span class="line">``/`` deserves/VBZ the/AT praise/NN and/CC thanks/NNS of/IN the/AT</span><br><span class="line">city/NN of/IN atlanta/NP &apos;&apos;/&apos;&apos; for/IN the/AT manner/NN in/IN which/WDT</span><br><span class="line">the/AT election/NN was/BEDZ conducted/VBN ./.</span><br><span class="line"></span><br><span class="line">Entropy: 27.0708725519</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: the/AT september-october/NP term/NN jury/NN had/HVD been/BEN</span><br><span class="line">charged/VBN by/IN fulton/NP superior/JJ court/NN judge/NN durwood/</span><br><span class="line">NP pye/NP to/TO investigate/VB reports/NNS of/IN possible/JJ ``/``</span><br><span class="line">irregularities/NNS &apos;&apos;/&apos;&apos; in/IN the/AT hard-fought/JJ primary/NN which/</span><br><span class="line">WDT was/BEDZ won/VBN by/IN mayor-nominate/NN ivan/NP allen/NP jr./NP</span><br><span class="line">./.</span><br><span class="line"></span><br><span class="line">Untagged: the september-october term jury had been charged by fulton</span><br><span class="line">superior court judge durwoodpye to investigate reports of possible ``</span><br><span class="line">irregularities &apos;&apos; in the hard-fought primary which was won by mayor-</span><br><span class="line">nominate ivanallenjr. .</span><br><span class="line"></span><br><span class="line">HMM-tagged: the/AT september-october/JJ term/NN jury/NN had/HVD been/</span><br><span class="line">BEN charged/VBN by/IN fulton/NP superior/JJ court/NN judge/NN durwood/</span><br><span class="line">TO pye/VB to/TO investigate/VB reports/NNS of/IN possible/JJ ``/``</span><br><span class="line">irregularities/NNS &apos;&apos;/&apos;&apos; in/IN the/AT hard-fought/JJ primary/NN which/</span><br><span class="line">WDT was/BEDZ won/VBN by/IN mayor-nominate/NP ivan/NP allen/NP jr./NP</span><br><span class="line">./.</span><br><span class="line"></span><br><span class="line">Entropy: 33.8281874237</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: ``/`` only/RB a/AT relative/JJ handful/NN of/IN such/JJ reports/</span><br><span class="line">NNS was/BEDZ received/VBN &apos;&apos;/&apos;&apos; ,/, the/AT jury/NN said/VBD ,/, ``/``</span><br><span class="line">considering/IN the/AT widespread/JJ interest/NN in/IN the/AT election/</span><br><span class="line">NN ,/, the/AT number/NN of/IN voters/NNS and/CC the/AT size/NN of/IN</span><br><span class="line">this/DT city/NN &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Untagged: `` only a relative handful of such reports was received &apos;&apos; ,</span><br><span class="line">the jury said , `` considering the widespread interest in the election</span><br><span class="line">, the number of voters and the size of this city &apos;&apos; .</span><br><span class="line"></span><br><span class="line">HMM-tagged: ``/`` only/RB a/AT relative/JJ handful/NN of/IN such/JJ</span><br><span class="line">reports/NNS was/BEDZ received/VBN &apos;&apos;/&apos;&apos; ,/, the/AT jury/NN said/VBD</span><br><span class="line">,/, ``/`` considering/IN the/AT widespread/JJ interest/NN in/IN the/AT</span><br><span class="line">election/NN ,/, the/AT number/NN of/IN voters/NNS and/CC the/AT size/</span><br><span class="line">NN of/IN this/DT city/NN &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Entropy: 11.4378198596</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: the/AT jury/NN said/VBD it/PPS did/DOD find/VB that/CS many/AP</span><br><span class="line">of/IN georgia&apos;s/NP$ registration/NN and/CC election/NN laws/NNS ``/``</span><br><span class="line">are/BER outmoded/JJ or/CC inadequate/JJ and/CC often/RB ambiguous/JJ</span><br><span class="line">&apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Untagged: the jury said it did find that many of georgia&apos;s</span><br><span class="line">registration and election laws `` are outmoded or inadequate and often</span><br><span class="line">ambiguous &apos;&apos; .</span><br><span class="line"></span><br><span class="line">HMM-tagged: the/AT jury/NN said/VBD it/PPS did/DOD find/VB that/CS</span><br><span class="line">many/AP of/IN georgia&apos;s/NP$ registration/NN and/CC election/NN laws/</span><br><span class="line">NNS ``/`` are/BER outmoded/VBG or/CC inadequate/JJ and/CC often/RB</span><br><span class="line">ambiguous/VB &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Entropy: 20.8163623192</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: it/PPS recommended/VBD that/CS fulton/NP legislators/NNS act/VB</span><br><span class="line">``/`` to/TO have/HV these/DTS laws/NNS studied/VBN and/CC revised/VBN</span><br><span class="line">to/IN the/AT end/NN of/IN modernizing/VBG and/CC improving/VBG them/</span><br><span class="line">PPO &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Untagged: it recommended that fulton legislators act `` to have these</span><br><span class="line">laws studied and revised to the end of modernizing and improving them</span><br><span class="line">&apos;&apos; .</span><br><span class="line"></span><br><span class="line">HMM-tagged: it/PPS recommended/VBD that/CS fulton/NP legislators/</span><br><span class="line">NNS act/VB ``/`` to/TO have/HV these/DTS laws/NNS studied/VBD and/CC</span><br><span class="line">revised/VBD to/IN the/AT end/NN of/IN modernizing/NP and/CC improving/</span><br><span class="line">VBG them/PPO &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Entropy: 20.3244921203</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: the/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/</span><br><span class="line">IN other/AP topics/NNS ,/, among/IN them/PPO the/AT atlanta/NP and/</span><br><span class="line">CC fulton/NP county/NN purchasing/VBG departments/NNS which/WDT it/</span><br><span class="line">PPS said/VBD ``/`` are/BER well/QL operated/VBN and/CC follow/VB</span><br><span class="line">generally/RB accepted/VBN practices/NNS which/WDT inure/VB to/IN the/</span><br><span class="line">AT best/JJT interest/NN of/IN both/ABX governments/NNS &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Untagged: the grand jury commented on a number of other topics ,</span><br><span class="line">among them the atlanta and fulton county purchasing departments which</span><br><span class="line">it said `` are well operated and follow generally accepted practices</span><br><span class="line">which inure to the best interest of both governments &apos;&apos; .</span><br><span class="line"></span><br><span class="line">HMM-tagged: the/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/</span><br><span class="line">NN of/IN other/AP topics/NNS ,/, among/IN them/PPO the/AT atlanta/</span><br><span class="line">NP and/CC fulton/NP county/NN purchasing/NN departments/NNS which/WDT</span><br><span class="line">it/PPS said/VBD ``/`` are/BER well/RB operated/VBN and/CC follow/VB</span><br><span class="line">generally/RB accepted/VBN practices/NNS which/WDT inure/VBZ to/IN the/</span><br><span class="line">AT best/JJT interest/NN of/IN both/ABX governments/NNS &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Entropy: 31.3834231469</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: merger/NN proposed/VBN</span><br><span class="line"></span><br><span class="line">Untagged: merger proposed</span><br><span class="line"></span><br><span class="line">HMM-tagged: merger/PPS proposed/VBD</span><br><span class="line"></span><br><span class="line">Entropy: 5.6718203946</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: however/WRB ,/, the/AT jury/NN said/VBD it/PPS believes/VBZ</span><br><span class="line">``/`` these/DTS two/CD offices/NNS should/MD be/BE combined/VBN to/TO</span><br><span class="line">achieve/VB greater/JJR efficiency/NN and/CC reduce/VB the/AT cost/NN</span><br><span class="line">of/IN administration/NN &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Untagged: however , the jury said it believes `` these two offices</span><br><span class="line">should be combined to achieve greater efficiency and reduce the cost</span><br><span class="line">of administration &apos;&apos; .</span><br><span class="line"></span><br><span class="line">HMM-tagged: however/WRB ,/, the/AT jury/NN said/VBD it/PPS believes/</span><br><span class="line">VBZ ``/`` these/DTS two/CD offices/NNS should/MD be/BE combined/VBN</span><br><span class="line">to/TO achieve/VB greater/JJR efficiency/NN and/CC reduce/VB the/AT</span><br><span class="line">cost/NN of/IN administration/NN &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Entropy: 8.27545943909</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Test: the/AT city/NN purchasing/VBG department/NN ,/, the/AT jury/NN</span><br><span class="line">said/VBD ,/, ``/`` is/BEZ lacking/VBG in/IN experienced/VBN clerical/</span><br><span class="line">JJ personnel/NNS as/CS a/AT result/NN of/IN city/NN personnel/NNS</span><br><span class="line">policies/NNS &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Untagged: the city purchasing department , the jury said , `` is</span><br><span class="line">lacking in experienced clerical personnel as a result of city</span><br><span class="line">personnel policies &apos;&apos; .</span><br><span class="line"></span><br><span class="line">HMM-tagged: the/AT city/NN purchasing/NN department/NN ,/, the/</span><br><span class="line">AT jury/NN said/VBD ,/, ``/`` is/BEZ lacking/VBG in/IN experienced/</span><br><span class="line">AT clerical/JJ personnel/NNS as/CS a/AT result/NN of/IN city/NN</span><br><span class="line">personnel/NNS policies/NNS &apos;&apos;/&apos;&apos; ./.</span><br><span class="line"></span><br><span class="line">Entropy: 16.7622537278</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">accuracy over 284 tokens: 92.96</span><br></pre></td></tr></table></figure>

</details>






<p>可以认为NER标注器的结果是一个回答，人们的解释称作答案要点。因此，我们提供了如下定义：</p>
<ul>
<li><strong>Correct</strong> ：如果回答与答案要点完全相同。</li>
<li><strong>Incorrect</strong> ：如果回答与答案要点不同。</li>
<li><strong>Missing</strong> ：如果答案要点被标注，但回答未被标注。</li>
<li><strong>Spurious</strong> ：如果回答被标注，但答案要点未被标注。</li>
</ul>
<p>通过使用以下参数可以评价一个基于NER的系统的性能：</p>
<ul>
<li><strong>Precision (P)</strong> ：定义如下：</li>
</ul>
<p>P=Correct/ (Correct+Incorrect+Missing)</p>
<ul>
<li><strong>Recall (R)</strong> ：定义如下：</li>
</ul>
<p>R=Correct/ (Correct+Incorrect+Spurious)</p>
<ul>
<li><strong>F-Measure</strong> ：定义如下：</li>
</ul>
<p>F-Measure = (2 _PREC_ REC)/(PRE+REC)</p>
<h3 id="6-1-3-使用机器学习工具包训练NER"><a href="#6-1-3-使用机器学习工具包训练NER" class="headerlink" title="6.1.3 使用机器学习工具包训练NER"></a>6.1.3 使用机器学习工具包训练NER</h3><p>可以使用以下方法执行NER：</p>
<ul>
<li>基于规则的或手工的方法：<ul>
<li>列表查找方法。</li>
<li>语言学方法。</li>
</ul>
</li>
<li>基于机器学习的方法或自动化方法：<ul>
<li>隐马尔科夫模型。</li>
<li>最大熵马尔科夫模型。</li>
<li>条件随机场。</li>
<li>支持向量机。</li>
<li>决策树。</li>
</ul>
</li>
</ul>
<p>实践表明，基于机器学习的方法优于基于规则的方法。此外，如果使用了基于规则的和基于机器学习的方法的组合，那么NER的性能也将提升。</p>
<h3 id="6-1-4-使用词性标注执行NER"><a href="#6-1-4-使用词性标注执行NER" class="headerlink" title="6.1.4 使用词性标注执行NER"></a>6.1.4 使用词性标注执行NER</h3><p>通过使用词性标注可以执行NER。可使用的词性标记如下所示（可访问<code>https://www.ling.upenn.edu/</code> <code>courses/Fall_2003/ling001/penn_treebank_pos.html</code> ）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Tag</th>
<th></th>
<th></th>
<th>Description  </th>
</tr>
</thead>
<tbody>
<tr>
<td>CC</td>
<td></td>
<td></td>
<td>Coordinating conjunction  </td>
</tr>
<tr>
<td>CD</td>
<td></td>
<td></td>
<td>Cardinal number  </td>
</tr>
<tr>
<td>DT</td>
<td></td>
<td></td>
<td>Determiner  </td>
</tr>
<tr>
<td>EX</td>
<td></td>
<td></td>
<td>Existential there  </td>
</tr>
<tr>
<td>FW</td>
<td></td>
<td></td>
<td>Foreign word  </td>
</tr>
<tr>
<td>IN</td>
<td></td>
<td></td>
<td>Preposition or subordinating conjunction  </td>
</tr>
<tr>
<td>JJ</td>
<td></td>
<td></td>
<td>Adjective  </td>
</tr>
<tr>
<td>JJR</td>
<td></td>
<td></td>
<td>Adjective, comparative  </td>
</tr>
<tr>
<td>JJS</td>
<td></td>
<td></td>
<td>Adjective, superlative  </td>
</tr>
<tr>
<td>LS</td>
<td></td>
<td></td>
<td>List item marker  </td>
</tr>
<tr>
<td>MD</td>
<td></td>
<td></td>
<td>Modal  </td>
</tr>
<tr>
<td>NN</td>
<td></td>
<td></td>
<td>Noun, singular or mass  </td>
</tr>
<tr>
<td>NNS</td>
<td></td>
<td></td>
<td>Noun, plural  </td>
</tr>
<tr>
<td>NNP</td>
<td></td>
<td></td>
<td>Proper noun, singular  </td>
</tr>
<tr>
<td>NNPS</td>
<td></td>
<td></td>
<td>Proper noun, plural  </td>
</tr>
<tr>
<td>PDT</td>
<td></td>
<td></td>
<td>Predeterminer  </td>
</tr>
<tr>
<td>POS</td>
<td></td>
<td></td>
<td>Possessive ending  </td>
</tr>
<tr>
<td>PRP</td>
<td></td>
<td></td>
<td>Personal pronoun  </td>
</tr>
<tr>
<td>PRP$</td>
<td></td>
<td></td>
<td>Possessive pronoun  </td>
</tr>
<tr>
<td>RB</td>
<td></td>
<td></td>
<td>Adverb  </td>
</tr>
<tr>
<td>RBR</td>
<td></td>
<td></td>
<td>Adverb, comparative  </td>
</tr>
<tr>
<td>RBS</td>
<td></td>
<td></td>
<td>Adverb, superlative  </td>
</tr>
<tr>
<td>RP</td>
<td></td>
<td></td>
<td>Particle  </td>
</tr>
<tr>
<td>SYM</td>
<td></td>
<td></td>
<td>Symbol  </td>
</tr>
<tr>
<td>TO</td>
<td></td>
<td></td>
<td>To  </td>
</tr>
<tr>
<td>UH</td>
<td></td>
<td></td>
<td>Interjection  </td>
</tr>
<tr>
<td>VB</td>
<td></td>
<td></td>
<td>Verb, base form  </td>
</tr>
<tr>
<td>VBD</td>
<td></td>
<td></td>
<td>Verb, past tense  </td>
</tr>
<tr>
<td>VBG</td>
<td></td>
<td></td>
<td>Verb, gerund or present participle  </td>
</tr>
<tr>
<td>VBN</td>
<td></td>
<td></td>
<td>Verb, past participle  </td>
</tr>
<tr>
<td>VBP</td>
<td></td>
<td></td>
<td>Verb, non-3rd person singular present  </td>
</tr>
<tr>
<td>VBZ</td>
<td></td>
<td></td>
<td>Verb, 3rd person singular present  </td>
</tr>
<tr>
<td>WDT</td>
<td></td>
<td></td>
<td>Wh-determiner  </td>
</tr>
<tr>
<td>WP</td>
<td></td>
<td></td>
<td>Wh-pronoun  </td>
</tr>
<tr>
<td>WP$</td>
<td></td>
<td></td>
<td>Possessive wh-pronoun  </td>
</tr>
<tr>
<td>WRB</td>
<td></td>
<td></td>
<td>Wh-adverb  </td>
</tr>
</tbody>
</table>
</div>
<p>   如果执行了词性标注，那么使用词性信息就可以识别出命名实体。用NNP标记标注的标识符就是命名实体。</p>
<p>考虑如下NLTK中的示例，它使用词性标注来执行NER：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk import pos_tag, word_tokenize</span><br><span class="line">&gt;&gt;&gt; pos_tag(word_tokenize(&quot;John and Smith are going to NY and</span><br><span class="line">Germany&quot;))</span><br><span class="line">[(&apos;John&apos;, &apos;NNP&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;Smith&apos;, &apos;NNP&apos;), (&apos;are&apos;, &apos;VBP&apos;),</span><br><span class="line">(&apos;going&apos;, &apos;VBG&apos;), (&apos;to&apos;, &apos;TO&apos;), (&apos;NY&apos;, &apos;NNP&apos;), (&apos;and&apos;, &apos;CC&apos;),</span><br><span class="line">(&apos;Germany&apos;, &apos;NNP&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<p>在这里，命名实体是<code>John</code> 、<code>Smith</code> 、<code>NY</code> 以及<code>Germany</code> ，因为它们被标注了NNP标记。</p>
<p>让我们来看看另一个NLTK中的示例，其中执行了词性标注并且词性标记信息被用于识别命名实体：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; from nltk.tag import UnigramTagger</span><br><span class="line">&gt;&gt;&gt; tagger = UnigramTagger(brown.tagged_sents(categories=&apos;news&apos;)</span><br><span class="line">[:700])</span><br><span class="line">&gt;&gt;&gt; sentence = [&apos;John&apos;,&apos;and&apos;,&apos;Smith&apos;,&apos;went&apos;,&apos;to&apos;,&apos;NY&apos;,&apos;and&apos;,&apos;Germany&apos;]</span><br><span class="line">&gt;&gt;&gt; for word, tag in tagger.tag(sentence):</span><br><span class="line">print(word,&apos;-&gt;&apos;,tag)</span><br><span class="line"></span><br><span class="line">John -&gt; NP</span><br><span class="line">and -&gt; CC</span><br><span class="line">Smith -&gt; None</span><br><span class="line">went -&gt; VBD</span><br><span class="line">to -&gt; TO</span><br><span class="line">NY -&gt; None</span><br><span class="line">and -&gt; CC</span><br><span class="line">Germany -&gt; None</span><br></pre></td></tr></table></figure>

</details>




<p>在这里，单词John已经被标注了NP标记，因此它被识别为命名实体。这里的一些标识符用None标记标注是因为这些标识符还没有经过训练。</p>
<h2 id="6-2-使用Wordnet生成同义词集id"><a href="#6-2-使用Wordnet生成同义词集id" class="headerlink" title="6.2 使用Wordnet生成同义词集id"></a>6.2 使用Wordnet生成同义词集id</h2><p>Wordnet可以定义为一个英语词汇数据库。通过使用同义词集，可以找到单词之间的概念依存，例如上位词、同义词、反义词和下位词。</p>
<p>考虑如下NLTK中用于生成同义词集的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">def all_synsets(self, pos=None):</span><br><span class="line">        &quot;&quot;&quot;Iterate over all synsets with a given part of speech tag.</span><br><span class="line">        If no pos is specified, all synsets for all parts of speech</span><br><span class="line">        will be loaded.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if pos is None:</span><br><span class="line">            pos_tags = self._FILEMAP.keys()</span><br><span class="line">        else:</span><br><span class="line">            pos_tags = [pos]</span><br><span class="line"></span><br><span class="line">        cache = self._synset_offset_cache</span><br><span class="line">        from_pos_and_line = self._synset_from_pos_and_line</span><br><span class="line"></span><br><span class="line">        # generate all synsets for each part of speech</span><br><span class="line">        for pos_tag in pos_tags:</span><br><span class="line">            # Open the file for reading. Note that we can not re-use</span><br><span class="line">            # the file pointers from self._data_file_map here, because</span><br><span class="line">            # we&apos;re defining an iterator, and those file pointers</span><br><span class="line">might</span><br><span class="line">            # be moved while we&apos;re not looking.</span><br><span class="line">            if pos_tag == ADJ_SAT:</span><br><span class="line">                pos_tag = ADJ</span><br><span class="line">            fileid = &apos;data.%s&apos; % self._FILEMAP[pos_tag]</span><br><span class="line">            data_file = self.open(fileid)</span><br><span class="line"></span><br><span class="line">            try:</span><br><span class="line">                # generate synsets for each line in the POS file</span><br><span class="line">                offset = data_file.tell()</span><br><span class="line">                line = data_file.readline()</span><br><span class="line">                while line:</span><br><span class="line">                    if not line[0].isspace():</span><br><span class="line">                        if offset in cache[pos_tag]:</span><br><span class="line">                            # See if the synset is cached</span><br><span class="line">                            synset = cache[pos_tag][offset]</span><br><span class="line">                        else:</span><br><span class="line">                            # Otherwise, parse the line</span><br><span class="line">                            synset = from_pos_and_line(pos_tag, line)</span><br><span class="line">                            cache[pos_tag][offset] = synset</span><br><span class="line"></span><br><span class="line">                        # adjective satellites are in the same file as</span><br><span class="line">                        # adjectives so only yield the synset if it&apos;s</span><br><span class="line">actually</span><br><span class="line">                        # a satellite</span><br><span class="line">                        if synset._pos == ADJ_SAT:</span><br><span class="line">                            yield synset</span><br><span class="line"></span><br><span class="line">                        # for all other POS tags, yield all synsets</span><br><span class="line">(this means</span><br><span class="line">                        # that adjectives also include adjective</span><br><span class="line">satellites)</span><br><span class="line">                        else:</span><br><span class="line">                            yield synset</span><br><span class="line">                    offset = data_file.tell()</span><br><span class="line">                    line = data_file.readline()</span><br><span class="line"></span><br><span class="line">            # close the extra file handle we opened</span><br><span class="line">            except:</span><br><span class="line">                data_file.close()</span><br><span class="line">                raise</span><br><span class="line">            else:</span><br><span class="line">                data_file.close()</span><br></pre></td></tr></table></figure>

</details>



<p>让我们看看如下NLTK中的代码，它通过使用同义词集来查找单词：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet as wn</span><br><span class="line">&gt;&gt;&gt; wn.synsets(&apos;cat&apos;)</span><br><span class="line">[Synset(&apos;cat.n.01&apos;), Synset(&apos;guy.n.01&apos;), Synset(&apos;cat.n.03&apos;),</span><br><span class="line">Synset(&apos;kat.n.01&apos;), Synset(&apos;cat-o&apos;-nine-tails.n.01&apos;),</span><br><span class="line">Synset(&apos;caterpillar.n.02&apos;), Synset(&apos;big_cat.n.01&apos;),</span><br><span class="line">Synset(&apos;computerized_tomography.n.01&apos;), Synset(&apos;cat.v.01&apos;),</span><br><span class="line">Synset(&apos;vomit.v.01&apos;)]</span><br><span class="line">&gt;&gt;&gt; wn.synsets(&apos;cat&apos;, pos=wn.VERB)</span><br><span class="line">[Synset(&apos;cat.v.01&apos;), Synset(&apos;vomit.v.01&apos;)]</span><br><span class="line">&gt;&gt;&gt; wn.synset(&apos;cat.n.01&apos;)</span><br><span class="line">Synset(&apos;cat.n.01&apos;)</span><br></pre></td></tr></table></figure>

</details>




<p>这里，<code>cat.n.01</code> 表示单词<code>cat</code> 属于名词类别并且只有一种含义的<code>cat</code> 存在：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print(wn.synset(&apos;cat.n.01&apos;).definition())</span><br><span class="line">feline mammal usually having thick soft fur and no ability to roar:</span><br><span class="line">domestic cats; wildcats</span><br><span class="line">&gt;&gt;&gt; len(wn.synset(&apos;cat.n.01&apos;).examples())</span><br><span class="line">0</span><br><span class="line">&gt;&gt;&gt; wn.synset(&apos;cat.n.01&apos;).lemmas()</span><br><span class="line">[Lemma(&apos;cat.n.01.cat&apos;), Lemma(&apos;cat.n.01.true_cat&apos;)]</span><br><span class="line">&gt;&gt;&gt; [str(lemma.name()) for lemma in wn.synset(&apos;cat.n.01&apos;).lemmas()]</span><br><span class="line">[&apos;cat&apos;, &apos;true_cat&apos;]</span><br><span class="line">&gt;&gt;&gt; wn.lemma(&apos;cat.n.01.cat&apos;).synset()</span><br><span class="line">Synset(&apos;cat.n.01&apos;)</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下NLTK中的示例，它描述了同义词集以及使用了ISO 639语种代码的开放多语言Wordnet的用法：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet as wn</span><br><span class="line">&gt;&gt;&gt; sorted(wn.langs())</span><br><span class="line">[&apos;als&apos;, &apos;arb&apos;, &apos;cat&apos;, &apos;cmn&apos;, &apos;dan&apos;, &apos;eng&apos;, &apos;eus&apos;, &apos;fas&apos;, &apos;fin&apos;, &apos;fra&apos;,</span><br><span class="line">&apos;fre&apos;, &apos;glg&apos;, &apos;heb&apos;, &apos;ind&apos;, &apos;ita&apos;, &apos;jpn&apos;, &apos;nno&apos;, &apos;nob&apos;, &apos;pol&apos;, &apos;por&apos;,</span><br><span class="line">&apos;spa&apos;, &apos;tha&apos;, &apos;zsm&apos;]</span><br><span class="line">&gt;&gt;&gt; wn.synset(&apos;cat.n.01&apos;).lemma_names(&apos;ita&apos;)</span><br><span class="line">[&apos;gatto&apos;]</span><br><span class="line">&gt;&gt;&gt; sorted(wn.synset(&apos;cat.n.01&apos;).lemmas(&apos;dan&apos;))</span><br><span class="line">[Lemma(&apos;cat.n.01.kat&apos;), Lemma(&apos;cat.n.01.mis&apos;), Lemma(&apos;cat.n.01.</span><br><span class="line">missekat&apos;)]</span><br><span class="line">&gt;&gt;&gt; sorted(wn.synset(&apos;cat.n.01&apos;).lemmas(&apos;por&apos;))</span><br><span class="line">[Lemma(&apos;cat.n.01.Gato-doméstico&apos;), Lemma(&apos;cat.n.01.Gato_doméstico&apos;),</span><br><span class="line">Lemma(&apos;cat.n.01.gato&apos;), Lemma(&apos;cat.n.01.gato&apos;)]</span><br><span class="line">&gt;&gt;&gt; len(wordnet.all_lemma_names(pos=&apos;n&apos;, lang=&apos;jpn&apos;))</span><br><span class="line">66027</span><br><span class="line">&gt;&gt;&gt; cat = wn.synset(&apos;cat.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; cat.hypernyms()</span><br><span class="line">[Synset(&apos;feline.n.01&apos;)]</span><br><span class="line">&gt;&gt;&gt; cat.hyponyms()</span><br><span class="line">[Synset(&apos;domestic_cat.n.01&apos;), Synset(&apos;wildcat.n.03&apos;)]</span><br><span class="line">&gt;&gt;&gt; cat.member_holonyms()</span><br><span class="line">[]</span><br><span class="line">&gt;&gt;&gt; cat.root_hypernyms()</span><br><span class="line">[Synset(&apos;entity.n.01&apos;)]</span><br><span class="line">&gt;&gt;&gt; wn.synset(&apos;cat.n.01&apos;).lowest_common_hypernyms(wn.</span><br><span class="line">synset(&apos;dog.n.01&apos;))</span><br><span class="line">[Synset(&apos;carnivore.n.01&apos;)]</span><br></pre></td></tr></table></figure>

</details>




<h2 id="6-3-使用Wordnet进行词义消歧"><a href="#6-3-使用Wordnet进行词义消歧" class="headerlink" title="6.3 使用Wordnet进行词义消歧"></a>6.3 使用Wordnet进行词义消歧</h2><p>词义消歧是基于单词的含义或意义来区分两个或更多拼写相同或发音相同的单词的任务。</p>
<p>以下是使用Python技术实现的词义消歧或WSD任务：</p>
<ul>
<li>Lesk算法：<ul>
<li>原始的Lesk算法。</li>
<li>余弦 Lesk算法（使用余弦定理而不是原始计数来计算重叠）。</li>
<li>简单的 Lesk算法（例如定义上位词 + 下位词）。</li>
<li>自适应的/可扩展的Lesk算法。</li>
<li>增强的Lesk算法。</li>
</ul>
</li>
<li>最大相似性：<ul>
<li>信息内容。</li>
<li>路径相似性。</li>
</ul>
</li>
<li>有指导的WSD：<ul>
<li>It Makes Sense（IMS）。</li>
<li>支持向量机WSD。</li>
</ul>
</li>
<li>向量空间模型：<ul>
<li>主题模型，LDA。</li>
<li>LSI / LSA。</li>
<li>NMF。</li>
</ul>
</li>
<li>基于图表的模型：<ul>
<li>Babelfly。</li>
<li>UKB。</li>
</ul>
</li>
<li>基准：<ul>
<li>随机含义。</li>
<li>最高引理计数。</li>
<li>第一NLTK含义。</li>
</ul>
</li>
</ul>
<p>NLTK中的Wordnet语义相似度涉及以下算法：</p>
<ul>
<li><p><strong>Resnik Score相似度算法</strong> ：在比较两个标识符时，返回一个决定两个标识符相似度的得分（最小公共包含，Least Common Subsumer）。</p>
</li>
<li><p><strong>Wu-Palmer相似度算法</strong> ：基于两个概念的深度和最小公共包含来定义两个标识符之间的相似度。</p>
</li>
<li><p><strong>Path Distance相似度算法</strong> ：基于在is-a分类结构中计算的最短距离来决定两个标识符的相似度。</p>
</li>
<li><p><strong>Leacock Chodorow相似度算法</strong> ：基于最短路径和语义在分类结构中的最大深度返回一个相似度得分。</p>
</li>
<li><p><strong>Lin相似度算法</strong> ：基于最小公共包含的信息内容和两个输入的同义词集返回一个相似度得分。</p>
</li>
<li><p><strong>Jiang-Conrath相似度算法</strong> ：基于最小公共包含的内容信息和两个输入的同义词集返回一个相似度得分。</p>
</li>
</ul>
<p>考虑如下NLTK中用于描述路径相似性的代码示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet as wn</span><br><span class="line">&gt;&gt;&gt; lion = wn.synset(&apos;lion.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; cat = wn.synset(&apos;cat.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; lion.path_similarity(cat)</span><br><span class="line">0.25</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中用于描述Leacock Chodorow相似性的代码示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet as wn</span><br><span class="line">&gt;&gt;&gt; lion = wn.synset(&apos;lion.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; cat = wn.synset(&apos;cat.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; lion.lch_similarity(cat)</span><br><span class="line">2.2512917986064953</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中用于描述Wu-Palmer相似性的代码示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet as wn</span><br><span class="line">&gt;&gt;&gt; lion = wn.synset(&apos;lion.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; cat = wn.synset(&apos;cat.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; lion.wup_similarity(cat)</span><br><span class="line">0.896551724137931</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中用于描述Resnik相似性、Lin相似性和Jiang-Conrath相似性的代码示例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet as wn</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet_ic</span><br><span class="line">&gt;&gt;&gt; brown_ic = wordnet_ic.ic(&apos;ic-brown.dat&apos;)</span><br><span class="line">&gt;&gt;&gt; semcor_ic = wordnet_ic.ic(&apos;ic-semcor.dat&apos;)</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import genesis</span><br><span class="line">&gt;&gt;&gt; genesis_ic = wn.ic(genesis, False, 0.0)</span><br><span class="line">&gt;&gt;&gt; lion = wn.synset(&apos;lion.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; cat = wn.synset(&apos;cat.n.01&apos;)</span><br><span class="line">&gt;&gt;&gt; lion.res_similarity(cat, brown_ic)</span><br><span class="line">8.663481537685325</span><br><span class="line">&gt;&gt;&gt; lion.res_similarity(cat, genesis_ic)</span><br><span class="line">7.339696591781995</span><br><span class="line">&gt;&gt;&gt; lion.jcn_similarity(cat, brown_ic)</span><br><span class="line">0.36425897775957294</span><br><span class="line">&gt;&gt;&gt; lion.jcn_similarity(cat, genesis_ic)</span><br><span class="line">0.3057800856788946</span><br><span class="line">&gt;&gt;&gt; lion.lin_similarity(cat, semcor_ic)</span><br><span class="line">0.8560734335071154</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下NLTK中基于Wu-Palmer相似性和路径距离相似性的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">from nltk.corpus import wordnet as wn</span><br><span class="line">def getSenseSimilarity(worda,wordb):</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">find similarity between word senses of two words</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">wordasynsets = wn.synsets(worda)</span><br><span class="line"></span><br><span class="line">wordbsynsets = wn.synsets(wordb)</span><br><span class="line"></span><br><span class="line">synsetnamea = [wn.synset(str(syns.name)) for syns in wordasynsets]</span><br><span class="line"></span><br><span class="line">    synsetnameb = [wn.synset(str(syns.name)) for syns in wordbsynsets]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for sseta, ssetb in [(sseta,ssetb) for sseta in synsetnamea\</span><br><span class="line"></span><br><span class="line">for ssetb in synsetnameb]:</span><br><span class="line"></span><br><span class="line">pathsim = sseta.path_similarity(ssetb)</span><br><span class="line"></span><br><span class="line">wupsim = sseta.wup_similarity(ssetb)</span><br><span class="line"></span><br><span class="line">if pathsim != None:</span><br><span class="line"></span><br><span class="line">print &quot;Path Sim Score: &quot;,pathsim,&quot; WUP Sim Score: &quot;,wupsim,\</span><br><span class="line"></span><br><span class="line">&quot;\t&quot;,sseta.definition, &quot;\t&quot;, ssetb.definition</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">#getSenseSimilarity(&apos;walk&apos;,&apos;dog&apos;)</span><br><span class="line"></span><br><span class="line">getSenseSimilarity(&apos;cricket&apos;,&apos;ball&apos;)</span><br></pre></td></tr></table></figure>

</details>




<p>让我们考虑如下NLTK中有关Lesk算法的代码，它用于执行词义消歧任务：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">from nltk.corpus import wordnet</span><br><span class="line"></span><br><span class="line">def lesk(context_sentence, ambiguous_word, pos=None, synsets=None):</span><br><span class="line">    &quot;&quot;&quot;Return a synset for an ambiguous word in a context.</span><br><span class="line"></span><br><span class="line">    :param iter context_sentence: The context sentence where the</span><br><span class="line">ambiguous word</span><br><span class="line">    occurs, passed as an iterable of words.</span><br><span class="line">    :param str ambiguous_word: The ambiguous word that requires WSD.</span><br><span class="line">    :param str pos: A specified Part-of-Speech (POS).</span><br><span class="line">    :param iter synsets: Possible synsets of the ambiguous word.</span><br><span class="line">    :return: ``lesk_sense`` The Synset() object with the highest</span><br><span class="line">signature overlaps.</span><br><span class="line"></span><br><span class="line">// This function is an implementation of the original Lesk</span><br><span class="line">algorithm (1986) [1].</span><br><span class="line"></span><br><span class="line">    Usage example::</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; lesk([&apos;I&apos;, &apos;went&apos;, &apos;to&apos;, &apos;the&apos;, &apos;bank&apos;, &apos;to&apos;, &apos;deposit&apos;, &apos;money&apos;,</span><br><span class="line">&apos;.&apos;], &apos;bank&apos;, &apos;n&apos;)</span><br><span class="line">    Synset(&apos;savings_bank.n.02&apos;)</span><br><span class="line"></span><br><span class="line">    context = set(context_sentence)</span><br><span class="line">    if synsets is None:</span><br><span class="line">        synsets = wordnet.synsets(ambiguous_word)</span><br><span class="line"></span><br><span class="line">    if pos:</span><br><span class="line">        synsets = [ss for ss in synsets if str(ss.pos()) == pos]</span><br><span class="line"></span><br><span class="line">    if not synsets:</span><br><span class="line">        return None</span><br><span class="line"></span><br><span class="line">    _, sense = max(</span><br><span class="line">         (len(context.intersection(ss.definition().split())), ss) for</span><br><span class="line">ss in synsets</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">return sense</span><br></pre></td></tr></table></figure>

</details>




<h2 id="6-4-小结"><a href="#6-4-小结" class="headerlink" title="6.4 小结"></a>6.4 小结</h2><p>在本章中，我们讨论了语义分析，它也是自然语言处理的阶段之一。我们还讨论了NER、使用HMM执行NER、使用机器学习工具包执行NER、NER的性能指标、使用词性标注执行NER、使用Wordnet的WSD和同义词集生成。</p>
<p>在下一章中，我们将使用NER和机器学习的方法来讨论情感分析，还将讨论NER系统的评估。</p>
<h1 id="第7章-情感分析：我很快乐"><a href="#第7章-情感分析：我很快乐" class="headerlink" title="第7章 情感分析：我很快乐"></a>第7章 情感分析：我很快乐</h1><p>情感分析（或者叫情感生成）是NLP中的众多任务之一，其被定义为确定一个字符序列背后所隐含的情感信息的过程。情感分析可用于确定表达文本思想的演讲者或人们的心情是愉快的还是悲伤的，或者仅代表一次中性的表达。</p>
<p>本章将包含以下主题：</p>
<ul>
<li>情感分析简介。</li>
<li>使用NER执行情感分析。</li>
<li>使用机器学习执行情感分析。</li>
<li>NER系统的评估。</li>
</ul>
<h2 id="7-1-情感分析简介"><a href="#7-1-情感分析简介" class="headerlink" title="7.1 情感分析简介"></a>7.1 情感分析简介</h2><p>情感分析可以认为是一个在自然语言上执行的任务。这里，对用自然语言表达的句子或单词执行了计算，以便确定它们是在表达积极的、消极的还是中性的情感。情感分析是一个主观的任务，因为它提供了所表达的文本的有关信息。情感分析可以认为是一个分类问题，有两种分类类型，即二元分类（积极的或消极的）和多元分类（积极的、消极的或中性的）。情感分析也被称作文本情感分析，这是一种文本挖掘的方法，通过该方法我们可以知晓文本隐含的情感或情绪。当我们将情感分析与主题挖掘相结合时，就可以称之为主题情感分析。通过使用词典可以执行情感分析。词典可以是特定领域的抑或是通用类型的，词典可以包含一个由积极的表达、消极的表达、中性的表达和停止词组成的列表。当出现一个测试的句子时，可以通过该词典来执行简单的查找操作。</p>
<p>单词列表的一个例子是标准英语情感词汇库（Affective Norms for English Words，ANEW）。这个库是一个英语单词列表，是由Bradley和Lang在佛罗里达大学创建的，它包含了涉及情绪的三个维度（优势度、愉悦度、激活度）的1034个单词。当初构建这个单词列表是为了学术目的并不是为了研究的目的。其他变体有DANEW<br>(Dutch ANEW)和SPANEW (Spanish ANEW)。</p>
<p>AFINN由2477个单词组成（更早为1468个单词）。这个单词列表是由Finn Arup Nielson创建的。创建这个单词列表的主要目的是对Twitter上的文本执行情感分析，并将评价值（范围从-5到+5）分配给每一个单词。</p>
<p>Balance Affective单词列表包括277个英语单词。评价编码范围从1到4。1表示积极的，2表示消极的，3表示焦虑的，4表示中立的。</p>
<p>Berlin Affective Word List (BAWL)，包含2200个德语单词。BAWL的另一个版本是Berlin Affective Word List Reloaded (BAWL-R)，其由单词的额外激活度组成。</p>
<p>Bilingual Finnish Affective Norms，包括210个英式英语和芬兰语名词，此外还包括禁忌词。</p>
<p>Compass DeRose Guide to Emotion Words，由英文中的情绪词组成，它是由Steve J. DeRose创立的。虽然单词被分了类，但是并不存在评价值和激活度。</p>
<p>Dictionary of Affect in Language (DAL)，包括可用于情感分析的情绪词，它是由Cynthia M. Whissell创立的。因此，它也被称为Whissell’s Dictionary of Affect in Language (WDAL)。</p>
<p>General Inquirer，由许多字典组成。其中，积极情感列表包含1915个单词，消极情感列表包含2291个单词。</p>
<p>Hu-Liu opinion Lexicon (HL)，由一个包含了6800个单词（积极的和消极的）的列表组成。</p>
<p>Leipzig Affective Norms for German (LANG)，是一个由1000个德语名词组成的列表，其评级基于评价值、性别和激活度。</p>
<p>Loughran and McDonald Financial Sentiment Dictionaries，它是由Tim Loughran和Bill McDonald创建的。这些词典由财务文档词汇组成，其包含积极的、消极的或者有关语气的单词。</p>
<p>Moors，由一个与优势度、激活度和评价值相关的荷兰语单词列表组成。</p>
<p>NRC Emotion Lexicon，包含由Saif M. Mohammad通过亚马逊土耳其机器人（Amazon Mechanical Turk）开发的一个词汇列表。</p>
<p>OpinionFinder的Subjectivity Lexicon由一个包含了8221个单词（积极的或消极的）的列表组成。</p>
<p>SentiSense，由2190个同义词集和5496个单词组成，这些单词涉及14种情感分类。</p>
<p>Warringer，包含13915个英文单词，这些单词由亚马逊土耳其机器人（Amazon Mechanical Turk）收集且与优势度、激活度和评价值相关。</p>
<p>labMT，是一个由10000个单词组成的单词列表。</p>
<p>让我们考虑如下NLTK中的代码示例，其可用于对电影评论进行情感分析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line">import random</span><br><span class="line">from nltk.corpus import movie_reviews</span><br><span class="line">docs = [(list(movie_reviews.words(fid)), cat)</span><br><span class="line">         for cat in movie_reviews.categories()</span><br><span class="line">         for fid in movie_reviews.fileids(cat)]</span><br><span class="line">random.shuffle(docs)</span><br><span class="line"></span><br><span class="line">all_tokens = nltk.FreqDist(x.lower() for x in movie_reviews.words())</span><br><span class="line">token_features = all_tokens.keys()[:2000]</span><br><span class="line">print token_features[:100]</span><br><span class="line"></span><br><span class="line">    [&apos;,&apos;, &apos;the&apos;, &apos;.&apos;, &apos;a&apos;, &apos;and&apos;, &apos;of&apos;, &apos;to&apos;, &quot;&apos;&quot;, &apos;is&apos;, &apos;in&apos;, &apos;s&apos;,</span><br><span class="line">&apos;&quot;&apos;, &apos;it&apos;, &apos;that&apos;, &apos;-&apos;, &apos;)&apos;, &apos;(&apos;, &apos;as&apos;, &apos;with&apos;, &apos;for&apos;, &apos;his&apos;, &apos;this&apos;,</span><br><span class="line">&apos;film&apos;, &apos;i&apos;, &apos;he&apos;, &apos;but&apos;, &apos;on&apos;, &apos;are&apos;, &apos;t&apos;, &apos;by&apos;, &apos;be&apos;, &apos;one&apos;,</span><br><span class="line">&apos;movie&apos;, &apos;an&apos;, &apos;who&apos;, &apos;not&apos;, &apos;you&apos;, &apos;from&apos;, &apos;at&apos;, &apos;was&apos;, &apos;have&apos;,</span><br><span class="line">&apos;they&apos;, &apos;has&apos;, &apos;her&apos;, &apos;all&apos;, &apos;?&apos;, &apos;there&apos;, &apos;like&apos;, &apos;so&apos;, &apos;out&apos;,</span><br><span class="line">&apos;about&apos;, &apos;up&apos;, &apos;more&apos;, &apos;what&apos;, &apos;when&apos;, &apos;which&apos;, &apos;or&apos;, &apos;she&apos;, &apos;their&apos;,</span><br><span class="line">&apos;:&apos;, &apos;some&apos;, &apos;just&apos;, &apos;can&apos;, &apos;if&apos;, &apos;we&apos;, &apos;him&apos;, &apos;into&apos;, &apos;even&apos;, &apos;only&apos;,</span><br><span class="line">&apos;than&apos;, &apos;no&apos;, &apos;good&apos;, &apos;time&apos;, &apos;most&apos;, &apos;its&apos;, &apos;will&apos;, &apos;story&apos;, &apos;would&apos;,</span><br><span class="line">&apos;been&apos;, &apos;much&apos;, &apos;character&apos;, &apos;also&apos;, &apos;get&apos;, &apos;other&apos;, &apos;do&apos;, &apos;two&apos;,</span><br><span class="line">&apos;well&apos;, &apos;them&apos;, &apos;very&apos;, &apos;characters&apos;, &apos;;&apos;, &apos;first&apos;, &apos;--&apos;, &apos;after&apos;,</span><br><span class="line">&apos;see&apos;, &apos;!&apos;, &apos;way&apos;, &apos;because&apos;, &apos;make&apos;, &apos;life&apos;]</span><br><span class="line"></span><br><span class="line">def doc_features(doc):</span><br><span class="line">    doc_words = set(doc)</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    for word in token_features:</span><br><span class="line">        features[&apos;contains(%s)&apos; % word] = (word in doc_words)</span><br><span class="line">    return features</span><br><span class="line"></span><br><span class="line">print doc_features(movie_reviews.words(&apos;pos/cv957_8737.txt</span><br><span class="line">feature_sets = [(doc_features(d), c) for (d,c) in doc]</span><br><span class="line">train_sets, test_sets = feature_sets[100:], feature_sets[:100]</span><br><span class="line">classifiers = nltk.NaiveBayesClassifier.train(train_sets)</span><br><span class="line">print nltk.classify.accuracy(classifiers, test_sets)</span><br><span class="line"></span><br><span class="line">0.86</span><br><span class="line"></span><br><span class="line">classifier.show_most_informative_features(5)</span><br><span class="line"></span><br><span class="line">    Most Informative Features</span><br><span class="line">contains(damon) = True         pos : neg  =  11.2 : 1.0</span><br><span class="line">contains(outstanding) = True   pos : neg  =  10.6 : 1.0</span><br><span class="line">contains(mulan) = True         pos : neg  =   8.8 : 1.0</span><br><span class="line">contains(seagal) = True        neg : pos  =   8.4 : 1.0</span><br><span class="line">contains(wonderfully) = True   pos : neg  =   7.4 : 1.0</span><br></pre></td></tr></table></figure>

</details>




<p>这里，我们检测了文档中是否存在有益的特征信息。</p>
<p>考虑另一个语义分析的例子。首先需要进行文本的预处理。在此过程中，标识出了给定文本中的句子，然后再标识出句子中的标识符。每个标识符还包括三个实体，即：单词、词条和标记。</p>
<p>让我们来看看如下NLTK中用于执行文本预处理的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line"></span><br><span class="line">class Splitter(object):</span><br><span class="line">def __init__(self):</span><br><span class="line">self.nltk_splitter = nltk.data.load(&apos;tokenizers/punkt/english.pickle&apos;)</span><br><span class="line">self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()</span><br><span class="line"></span><br><span class="line">def split(self, text):</span><br><span class="line">sentences = self.nltk_splitter.tokenize(text)</span><br><span class="line">tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in</span><br><span class="line">sentences]</span><br><span class="line">return tokenized_sentences</span><br><span class="line">class POSTagger(object):</span><br><span class="line">def __init__(self):</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">def pos_tag(self, sentences):</span><br><span class="line"></span><br><span class="line">pos = [nltk.pos_tag(sentence) for sentence in sentences]</span><br><span class="line">pos = [[(word, word, [postag]) for (word, postag) in sentence] for</span><br><span class="line">sentence in pos]</span><br><span class="line">return pos</span><br></pre></td></tr></table></figure>

</details>




<p>生成的词条将与单词的形式相同，标记指的是词性标记。考虑如下代码，它为每个标识符生成了包含三个元素的元组（即单词、词条和词性标记）。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">text = &quot;&quot;&quot;Why are you looking disappointed. We will go to restaurant</span><br><span class="line">for dinner.&quot;&quot;&quot;</span><br><span class="line">splitter = Splitter()</span><br><span class="line">postagger = POSTagger()</span><br><span class="line">splitted_sentences = splitter.split(text)</span><br><span class="line">print splitted_sentences</span><br><span class="line">[[&apos;Why&apos;,&apos;are&apos;,&apos;you&apos;,&apos;looking&apos;,&apos;disappointed&apos;,&apos;.&apos;], [&apos;We&apos;,&apos;will&apos;,&apos;go&apos;,&apos;</span><br><span class="line">to&apos;,&apos;restaurant&apos;,&apos;for&apos;,&apos;dinner&apos;,&apos;.&apos;]]</span><br><span class="line"></span><br><span class="line">pos_tagged_sentences = postagger.pos_tag(splitted_sentences)</span><br><span class="line"></span><br><span class="line">print pos_tagged_sentences</span><br><span class="line">[[(&apos;Why&apos;,&apos;Why&apos;,[&apos;WP&apos;]),(&apos;are&apos;,&apos;are&apos;,[&apos;VBZ&apos;]),(&apos;you&apos;,&apos;you&apos;,[&apos;PRP&apos;]</span><br><span class="line">),(&apos;looking&apos;,&apos;looking&apos;,[&apos;VB&apos;]),(&apos;disappointed&apos;,&apos;disappointed&apos;,[&apos;</span><br><span class="line">VB&apos;]),(&apos;.&apos;,&apos;.&apos;,[&apos;.&apos;])],[(&apos;We&apos;,&apos;We&apos;,[&apos;PRP&apos;]),(&apos;will&apos;,&apos;will&apos;,[&apos;VBZ&apos;]),(&apos;</span><br><span class="line">go&apos;,&apos;go&apos;,[&apos;VB&apos;]),(&apos;to&apos;,&apos;to&apos;,[&apos;TO&apos;]),(&apos;restaurant&apos;,&apos;restaurant&apos;,[&apos;NN&apos;])</span><br><span class="line">,(&apos;for&apos;,&apos;for&apos;,[&apos;IN&apos;]),(&apos;dinner&apos;,&apos;dinner&apos;,[&apos;NN&apos;]),(&apos;.&apos;,&apos;.&apos;,[&apos;.&apos;])]]</span><br></pre></td></tr></table></figure>

</details>




<p>我们可以构建两种类型的字典，其包含了积极的表达和消极的表达，然后我们就可以使用字典对我们处理过的文本执行词性标注。</p>
<p>让我们考虑如下NLTK中有关使用字典来执行词性标注的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">class DictionaryTagger(object):</span><br><span class="line">def __init__(self, dictionary_paths):</span><br><span class="line">files = [open(path, &apos;r&apos;) for path in dictionary_paths]</span><br><span class="line">dictionaries = [yaml.load(dict_file) for dict_file in files]</span><br><span class="line">map(lambda x: x.close(), files)</span><br><span class="line">self.dictionary = &#123;&#125;</span><br><span class="line">self.max_key_size = 0</span><br><span class="line">for curr_dict in dictionaries:</span><br><span class="line">for key in curr_dict:</span><br><span class="line">if key in self.dictionary:</span><br><span class="line">self.dictionary[key].extend(curr_dict[key])</span><br><span class="line">else:</span><br><span class="line">self.dictionary[key] = curr_dict[key]</span><br><span class="line">self.max_key_size = max(self.max_key_size, len(key))</span><br><span class="line"></span><br><span class="line">def tag(self, postagged_sentences):</span><br><span class="line">return [self.tag_sentence(sentence) for sentence in postagged_</span><br><span class="line">sentences]</span><br><span class="line"></span><br><span class="line">def tag_sentence(self, sentence, tag_with_lemmas=False):</span><br><span class="line">tag_sentence = []</span><br><span class="line">        N = len(sentence)</span><br><span class="line">if self.max_key_size == 0:</span><br><span class="line">self.max_key_size = N</span><br><span class="line">i = 0</span><br><span class="line">while (i&lt; N):</span><br><span class="line">j = min(i + self.max_key_size, N) #avoid overflow</span><br><span class="line">tagged = False</span><br><span class="line">while (j &gt;i):</span><br><span class="line">expression_form = &apos; &apos;.join([word[0] for word in sentence[i:j]]).</span><br><span class="line">lower()</span><br><span class="line">expression_lemma = &apos; &apos;.join([word[1] for word in sentence[i:j]]).</span><br><span class="line">lower()</span><br><span class="line">if tag_with_lemmas:</span><br><span class="line">literal = expression_lemma</span><br><span class="line">else:</span><br><span class="line">literal = expression_form</span><br><span class="line">if literal in self.dictionary:</span><br><span class="line">    is_single_token = j - i == 1</span><br><span class="line">original_position = i</span><br><span class="line">i = j</span><br><span class="line">taggings = [tag for tag in self.dictionary[literal]]</span><br><span class="line">tagged_expression = (expression_form, expression_lemma, taggings)</span><br><span class="line">if is_single_token: #if the tagged literal is a single token, conserve</span><br><span class="line">its previous taggings:</span><br><span class="line">original_token_tagging = sentence[original_position][2]</span><br><span class="line">tagged_expression[2].extend(original_token_tagging)</span><br><span class="line">tag_sentence.append(tagged_expression)</span><br><span class="line">tagged = True</span><br><span class="line">else:</span><br><span class="line">                    j = j - 1</span><br><span class="line">if not tagged:</span><br><span class="line">tag_sentence.append(sentence[i])</span><br><span class="line">i += 1</span><br><span class="line">return tag_sentence</span><br></pre></td></tr></table></figure>

</details>




<p>这里，在字典的帮助下，文本中预处理过的单词被标注为积极的或者消极的。</p>
<p>让我们来看看如下NLTK中的代码，其可用于计算积极表达和消极表达的数量：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def value_of(sentiment):</span><br><span class="line">if sentiment == &apos;positive&apos;: return 1</span><br><span class="line">if sentiment == &apos;negative&apos;: return -1</span><br><span class="line">return 0</span><br><span class="line">def sentiment_score(review):</span><br><span class="line">return sum ([value_of(tag) for sentence in dict_tagged_sentences for</span><br><span class="line">token in sentence for tag in token[2]])</span><br></pre></td></tr></table></figure>

</details>




<p>在NLTK中，<code>nltk.sentiment.util</code> 模块通过使用Hu- Liu字典来进行情感分析。在字典的帮助下，该模块对积极表达、消极表达以及中立表达的数量进行了统计，然后基于多数原则来确定该文本是由积极、消极还是中立的情感所组成的。字典不支持的单词被认为是中立的。</p>
<h3 id="7-1-1-使用NER执行情感分析"><a href="#7-1-1-使用NER执行情感分析" class="headerlink" title="7.1.1 使用NER执行情感分析"></a>7.1.1 使用NER执行情感分析</h3><p>NER是一个找出命名实体并将其分类为不同的命名实体类的过程。我们可以使用不同的技术来执行NER，例如基于规则的方法、列表查找方法和统计学方法（隐马尔科夫模型、最大熵马尔科夫模型、支持向量机、条件随机场和决策树）。</p>
<p>如果识别出了一个列表中的命名实体，那么就可以将它们从句子中移除或过滤掉。类似地，停止词也可以被删除。现在我们就可以对剩余的单词进行情感分析了，因为命名实体是与情感分析无关的单词。</p>
<h3 id="7-1-2-使用机器学习执行情感分析"><a href="#7-1-2-使用机器学习执行情感分析" class="headerlink" title="7.1.2 使用机器学习执行情感分析"></a>7.1.2 使用机器学习执行情感分析</h3><p>NLTK中的<code>nltk.sentiment.sentiment_analyzer</code> 模块可用于执行情感分析，它是基于机器学习技术的。</p>
<p>让我们来看看如下NLTK中有关<code>nltk.sentiment.sentiment_analyzer</code> 模块的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">from nltk.classify.util import apply_features, accuracy as eval_</span><br><span class="line">accuracy</span><br><span class="line">from nltk.collocations import BigramCollocationFinder</span><br><span class="line">from nltk.metrics import (BigramAssocMeasures, precision as eval_</span><br><span class="line">precision,</span><br><span class="line">    recall as eval_recall, f_measure as eval_f_measure)</span><br><span class="line"></span><br><span class="line">from nltk.probability import FreqDist</span><br><span class="line"></span><br><span class="line">from nltk.sentiment.util import save_file, timer</span><br><span class="line">class SentimentAnalyzer(object):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A tool for Sentiment Analysis which is based on machine learning</span><br><span class="line">techniques.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, classifier=None):</span><br><span class="line">        self.feat_extractors = defaultdict(list)</span><br><span class="line">        self.classifier = classifier</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下代码，它将返回文本中所有（重复的）单词：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def all_words(self, documents, labeled=None):</span><br><span class="line">  all_words = []</span><br><span class="line">  if labeled is None:</span><br><span class="line">      labeled = documents and isinstance(documents[0], tuple)</span><br><span class="line">  if labeled == True:</span><br><span class="line">      for words, sentiment in documents:</span><br><span class="line">          all_words.extend(words)</span><br><span class="line">  elif labeled == False:</span><br><span class="line">      for words in documents:</span><br><span class="line">        all_words.extend(words)</span><br><span class="line">  return all_words</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下代码，它将在文本上应用特征提取函数：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def apply_features(self, documents, labeled=None):</span><br><span class="line"></span><br><span class="line">        return apply_features(self.extract_features, documents,</span><br><span class="line">labeled)</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下代码，它将返回单词的特征：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def unigram_word_feats(self, words, top_n=None, min_freq=0):</span><br><span class="line">        unigram_feats_freqs = FreqDist(word for word in words)</span><br><span class="line">        return [w for w, f in unigram_feats_freqs.most_common(top_n)</span><br><span class="line">                if unigram_feats_freqs[w] &gt; min_freq]</span><br></pre></td></tr></table></figure>

</details>




<p>以下代码返回的是二元语法特征：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def bigram_collocation_feats(self, documents, top_n=None, min_freq=3,</span><br><span class="line">                                 assoc_measure=BigramAssocMeasures.</span><br><span class="line">pmi):</span><br><span class="line">       finder = BigramCollocationFinder.from_documents(documents)</span><br><span class="line">       finder.apply_freq_filter(min_freq)</span><br><span class="line">       return finder.nbest(assoc_measure, top_n)</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下代码，通过使用特征集其可用于分类一个给定的实例：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def classify(self, instance):</span><br><span class="line">        instance_feats = self.apply_features([instance],</span><br><span class="line">labeled=False)</span><br><span class="line">        return self.classifier.classify(instance_feats[0])</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下代码，其可用于抽取文本的特征：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def add_feat_extractor(self, function, **kwargs):</span><br><span class="line">        self.feat_extractors[function].append(kwargs)</span><br><span class="line"></span><br><span class="line">def extract_features(self, document):</span><br><span class="line">        all_features = &#123;&#125;</span><br><span class="line">        for extractor in self.feat_extractors:</span><br><span class="line">            for param_set in self.feat_extractors[extractor]:</span><br><span class="line">                feats = extractor(document, **param_set)</span><br><span class="line">            all_features.update(feats)</span><br><span class="line">        return all_features</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下可在训练文件上执行训练的代码，其中<code>save_classifier</code> 用于将输出结果保存到一个文件中：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def train(self, trainer, training_set, save_classifier=None,</span><br><span class="line">**kwargs):</span><br><span class="line">        print(&quot;Training classifier&quot;)</span><br><span class="line">        self.classifier = trainer(training_set, **kwargs)</span><br><span class="line">        if save_classifier:</span><br><span class="line">            save_file(self.classifier, save_classifier)</span><br><span class="line"></span><br><span class="line">        return self.classifier</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下代码，其可用于执行测试，而且其通过使用测试数据能够对我们的分类器执行性能评估：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">def evaluate(self, test_set, classifier=None, accuracy=True, f_</span><br><span class="line">measure=True,</span><br><span class="line">                precision=True, recall=True, verbose=False):</span><br><span class="line">        if classifier is None:</span><br><span class="line">            classifier = self.classifier</span><br><span class="line">        print(&quot;Evaluating &#123;0&#125; results...&quot;.format(type(classifier).__</span><br><span class="line">name__))</span><br><span class="line">        metrics_results = &#123;&#125;</span><br><span class="line">        if accuracy == True:</span><br><span class="line">            accuracy_score = eval_accuracy(classifier, test_set)</span><br><span class="line">            metrics_results[&apos;Accuracy&apos;] = accuracy_score</span><br><span class="line"></span><br><span class="line">        gold_results = defaultdict(set)</span><br><span class="line">        test_results = defaultdict(set)</span><br><span class="line">        labels = set()</span><br><span class="line">        for i, (feats, label) in enumerate(test_set):</span><br><span class="line">            labels.add(label)</span><br><span class="line">            gold_results[label].add(i)</span><br><span class="line">            observed = classifier.classify(feats)</span><br><span class="line">            test_results[observed].add(i)</span><br><span class="line"></span><br><span class="line">        for label in labels:</span><br><span class="line">            if precision == True:</span><br><span class="line">                precision_score = eval_precision(gold_results[label],</span><br><span class="line">                    test_results[label])</span><br><span class="line">                metrics_results[&apos;Precision [&#123;0&#125;]&apos;.format(label)] =</span><br><span class="line">precision_score</span><br><span class="line">            if recall == True:</span><br><span class="line">                recall_score = eval_recall(gold_results[label],</span><br><span class="line">                    test_results[label])</span><br><span class="line">                metrics_results[&apos;Recall [&#123;0&#125;]&apos;.format(label)] =</span><br><span class="line">recall_score</span><br><span class="line">            if f_measure == True:</span><br><span class="line">                f_measure_score = eval_f_measure(gold_results[label],</span><br><span class="line">                    test_results[label])</span><br><span class="line">                metrics_results[&apos;F-measure [&#123;0&#125;]&apos;.format(label)] = f_</span><br><span class="line">measure_score</span><br><span class="line"></span><br><span class="line">        if verbose == True:</span><br><span class="line">            for result in sorted(metrics_results):</span><br><span class="line">                print(&apos;&#123;0&#125;: &#123;1&#125;&apos;.format(result, metrics_</span><br><span class="line">results[result]))</span><br><span class="line"></span><br><span class="line">        return metrics_results</span><br></pre></td></tr></table></figure>

</details>




<p>Twitter被认为是最流行的博客服务之一，它可用于创建那些被称作推文的消息，这些推文由相关积极、消极或中立情感的单词所组成。</p>
<p>为了执行情感分析，我们可以使用机器学习分类器、统计学分类器或自动分类器，例如朴素贝叶斯分类器（Naive Bayes Classifier）、最大熵分类器（Maximum Entropy Classifier）以及支持向量机分类器（Support Vector Machine Classifier）等。</p>
<p>这些机器学习分类器或自动分类器用于执行有监督的分类，因为它们需要训练数据才能执行分类。</p>
<p>让我们来看看如下NLTK中用于执行特征提取的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">stopWords = []</span><br><span class="line"></span><br><span class="line">#If there is occurrence of two or more same character, then replace it</span><br><span class="line">with the character itself.</span><br><span class="line">def replaceTwoOrMore(s):</span><br><span class="line">    pattern = re.compile(r&quot;(.)\1&#123;1,&#125;&quot;, re.DOTALL)</span><br><span class="line">    return pattern.sub(r&quot;\1\1&quot;, s)</span><br><span class="line">def getStopWordList(stopWordListFileName):</span><br><span class="line">    # This function will read the stopwords from a file and builds a</span><br><span class="line">list.</span><br><span class="line">    stopWords = []</span><br><span class="line">    stopWords.append(&apos;AT_USER&apos;)</span><br><span class="line">    stopWords.append(&apos;URL&apos;)</span><br><span class="line"></span><br><span class="line">    fp = open(stopWordListFileName, &apos;r&apos;)</span><br><span class="line">    line = fp.readline()</span><br><span class="line">    while line:</span><br><span class="line">        word = line.strip()</span><br><span class="line">        stopWords.append(word)</span><br><span class="line">        line = fp.readline()</span><br><span class="line">    fp.close()</span><br><span class="line">    return stopWords</span><br><span class="line"></span><br><span class="line">def getFeatureVector(tweet):</span><br><span class="line">    featureVector = []</span><br><span class="line">    #Tweets are firstly split into words</span><br><span class="line">    words = tweet.split()</span><br><span class="line">    for w in words:</span><br><span class="line">        #replace two or more with two occurrences</span><br><span class="line">        w = replaceTwoOrMore(w)</span><br><span class="line">        #strip punctuation</span><br><span class="line">        w = w.strip(&apos;\&apos;&quot;?,.&apos;)</span><br><span class="line">        #Words begin with alphabet is checked.</span><br><span class="line">        val = re.search(r&quot;^[a-zA-Z][a-zA-Z0-9]*$&quot;, w)</span><br><span class="line">        #If there is a stop word, then it is ignored.</span><br><span class="line">        if(w in stopWords or val is None):</span><br><span class="line">            continue</span><br><span class="line">        else:</span><br><span class="line">            featureVector.append(w.lower())</span><br><span class="line">    return featureVector</span><br><span class="line">#end</span><br><span class="line"></span><br><span class="line">#Tweets are read one by one and then processed.</span><br><span class="line">fp = open(&apos;data/sampleTweets.txt&apos;, &apos;r&apos;)</span><br><span class="line">line = fp.readline()</span><br><span class="line"></span><br><span class="line">st = open(&apos;data/feature_list/stopwords.txt&apos;, &apos;r&apos;)</span><br><span class="line">stopWords = getStopWordList(&apos;data/feature_list/stopwords.txt&apos;)</span><br><span class="line"></span><br><span class="line">while line:</span><br><span class="line">    processedTweet = processTweet(line)</span><br><span class="line">    featureVector = getFeatureVector(processedTweet)</span><br><span class="line">    print featureVector</span><br><span class="line">    line = fp.readline()</span><br><span class="line">#end loop</span><br><span class="line">fp.close()</span><br><span class="line"></span><br><span class="line">#Tweets are read one by one and then processed.</span><br><span class="line">inpTweets = csv.reader(open(&apos;data/sampleTweets.csv&apos;, &apos;rb&apos;),</span><br><span class="line">delimiter=&apos;,&apos;, quotechar=&apos;|&apos;)</span><br><span class="line">tweets = []</span><br><span class="line">for row in inpTweets:</span><br><span class="line">    sentiment = row[0]</span><br><span class="line">    tweet = row[1]</span><br><span class="line">    processedTweet = processTweet(tweet)</span><br><span class="line">    featureVector = getFeatureVector(processedTweet, stopWords)</span><br><span class="line">    tweets.append((featureVector, sentiment));</span><br><span class="line"></span><br><span class="line">#Features Extraction takes place using following method</span><br><span class="line">def extract_features(tweet):</span><br><span class="line">    tweet_words = set(tweet)</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    for word in featureList:</span><br><span class="line">        features[&apos;contains(%s)&apos; % word] = (word in tweet_words)</span><br><span class="line">    return features</span><br></pre></td></tr></table></figure>

</details>




<p>在训练分类器期间，机器学习算法的输入是标签和特征。当输入被给到特征提取器时，就可以从特征提取器获取特征。在预测期间，分类器模型的输出是一个标签，并且其输入是那些使用特征提取器获取的特征。让我们来看看用于阐述这一相同过程的流程图，如图7-1所示。</p>
<p><img src="Image00011.gif" alt></p>
<p>图7-1</p>
<p>现在，让我们来看看如下代码，通过使用朴素贝叶斯分类器，其可用于执行情感分析：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NaiveBClassifier = nltk.NaiveBayesClassifier.train(training_set)</span><br><span class="line"># Testing the classifier testTweet = &apos;I liked this book on Sentiment</span><br><span class="line">Analysis a lot.&apos;</span><br><span class="line">processedTestTweet = processTweet(testTweet)</span><br><span class="line">print NaiveBClassifier.classify(extract_features(getFeatureVector(proc</span><br><span class="line">essedTestTweet)))</span><br><span class="line">testTweet = &apos;I am so badly hurt&apos;</span><br><span class="line">processedTestTweet = processTweet(testTweet)</span><br><span class="line">print NBClassifier.classify(extract_features(getFeatureVector(process</span><br><span class="line">edTestTweet)))</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下使用最大熵执行情感分析的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MaxEntClassifier = nltk.classify.maxent.MaxentClassifier.</span><br><span class="line">train(training_set, &apos;GIS&apos;, trace=3, \</span><br><span class="line">                    encoding=None, labels=None, sparse=True, gaussian_</span><br><span class="line">prior_sigma=0, max_iter = 10)</span><br><span class="line">testTweet = &apos;I liked the book on sentiment analysis a lot&apos;</span><br><span class="line">processedTestTweet = processTweet(testTweet)</span><br><span class="line">print MaxEntClassifier.classify(extract_features(getFeatureVector(proc</span><br><span class="line">essedTestTweet)))</span><br><span class="line">print MaxEntClassifier.show_most_informative_features(10)</span><br></pre></td></tr></table></figure>

</details>




<h3 id="7-1-3-NER系统的评估"><a href="#7-1-3-NER系统的评估" class="headerlink" title="7.1.3 NER系统的评估"></a>7.1.3 NER系统的评估</h3><p>性能指标或评估有助于展示一个NER系统的性能。NER标注器的结果可以认为是一个回答，人们的一种解释称作答案要点。因此，我们提供了如下定义：</p>
<ul>
<li><strong>Correct</strong> ：如果回答与答案要点完全相同。</li>
<li><strong>Incorrect</strong> ：如果回答与答案要点不同。</li>
<li><strong>Missing</strong> ：如果答案要点被标注，但回答未被标注。</li>
<li><strong>Spurious</strong> ：如果回答被标注，但答案要点未被标注。</li>
</ul>
<p>通过使用以下参数可以评价一个基于NER的系统的性能：</p>
<ul>
<li><strong>精确率(P)</strong> : P=Correct/(Correct+Incorrect+Missing)。</li>
<li><strong>召回率(R)</strong> : R=Correct/(Correct+Incorrect+Spurious)。</li>
<li><strong>F值</strong> ：F-Measure = (2<em>P</em>R)/(P+R)。</li>
</ul>
<p>让我们来看看使用HMM执行NER的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br></pre></td><td class="code"><pre><span class="line">#******* Function to find all tags in corpus **********</span><br><span class="line"></span><br><span class="line">def find_tag_set(tra_lines):</span><br><span class="line">global tag_set</span><br><span class="line"></span><br><span class="line">tag_set = [ ]</span><br><span class="line"></span><br><span class="line">for line in tra_lines:</span><br><span class="line">tok = line.split()</span><br><span class="line">for t in tok:</span><br><span class="line">wd = t.split(&quot;/&quot;)</span><br><span class="line">if not wd[1] in tag_set:</span><br><span class="line">tag_set.append(wd[1])</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">#******* Function to find frequency of each tag in tagged corpus</span><br><span class="line">**********</span><br><span class="line"></span><br><span class="line">def cnt_tag(tr_ln):</span><br><span class="line">global start_li</span><br><span class="line">global li</span><br><span class="line">global tag_set</span><br><span class="line">global c</span><br><span class="line">global line_cnt</span><br><span class="line">global lines</span><br><span class="line"></span><br><span class="line">lines = tr_ln</span><br><span class="line"></span><br><span class="line">start_li = [ ] # list of starting tags</span><br><span class="line"></span><br><span class="line">find_tag_set(tr_ln)</span><br><span class="line"></span><br><span class="line">line_cnt = 0</span><br><span class="line">for line in lines:</span><br><span class="line">tok = line.split()</span><br><span class="line">x = tok[0].split(&quot;/&quot;)</span><br><span class="line">if not x[1] in start_li:</span><br><span class="line">start_li.append(x[1])</span><br><span class="line">line_cnt = line_cnt + 1</span><br><span class="line"></span><br><span class="line">find_freq_tag()</span><br><span class="line"></span><br><span class="line">find_freq_srttag()</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">def find_freq_tag():</span><br><span class="line">global tag_cnt</span><br><span class="line">global tag_set</span><br><span class="line">tag_cnt=&#123;&#125;</span><br><span class="line">i = 0</span><br><span class="line">for w in tag_set:</span><br><span class="line">cal_freq_tag(tag_set[i])</span><br><span class="line">i = i + 1</span><br><span class="line">tag_cnt.update(&#123;w:freq_tg&#125;)</span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">def cal_freq_tag(tg):</span><br><span class="line">global freq_tg</span><br><span class="line">global lines</span><br><span class="line">freq_tg = 0</span><br><span class="line"></span><br><span class="line">for line in lines:</span><br><span class="line">freq_tg = freq_tg + line.count(tg)</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">#******* Function to find frequency of each starting tag in tagged</span><br><span class="line">corpus **********</span><br><span class="line"></span><br><span class="line">def find_freq_srttag():</span><br><span class="line">global lst</span><br><span class="line">lst = &#123;&#125; # start probability</span><br><span class="line"></span><br><span class="line">i = 0</span><br><span class="line">for w in start_li:</span><br><span class="line">cc = freq_srt_tag(start_li[i])</span><br><span class="line">prob = cc / line_cnt</span><br><span class="line"></span><br><span class="line">lst.update(&#123;start_li[i]:prob&#125;)</span><br><span class="line">i = i + 1</span><br><span class="line">return</span><br><span class="line">def freq_srt_tag(stg):</span><br><span class="line">global lines</span><br><span class="line">freq_srt_tg = 0</span><br><span class="line"></span><br><span class="line">for line in lines:</span><br><span class="line"></span><br><span class="line">tok = line.split()</span><br><span class="line">if stg in tok[0]:</span><br><span class="line">freq_srt_tg = freq_srt_tg + 1</span><br><span class="line">return(freq_srt_tg)</span><br><span class="line"></span><br><span class="line">import tkinter as tk</span><br><span class="line">import vit</span><br><span class="line">import random</span><br><span class="line">import cal_start_p</span><br><span class="line">import calle_prob</span><br><span class="line">import trans_mat</span><br><span class="line">import time</span><br><span class="line">import trans</span><br><span class="line">import dict5</span><br><span class="line">from tkinter import *</span><br><span class="line">from tkinter import ttk</span><br><span class="line">from tkinter.filedialog import askopenfilename</span><br><span class="line">from tkinter.messagebox import showerror</span><br><span class="line">import languagedetect1</span><br><span class="line">import languagedetect3</span><br><span class="line">e_dict = dict()</span><br><span class="line">t_dict = dict()</span><br><span class="line"></span><br><span class="line">def calculate1(*args):</span><br><span class="line">import listbox1</span><br><span class="line">def calculate2(*args):</span><br><span class="line">import listbox2</span><br><span class="line">def calculate3(*args):</span><br><span class="line">import listbox3</span><br><span class="line"></span><br><span class="line">def dispdlg():</span><br><span class="line">global file_name</span><br><span class="line">root = tk.Tk()</span><br><span class="line">root.withdraw()</span><br><span class="line">file_name = askopenfilename()</span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">def tranhmm():</span><br><span class="line">ttk.Style().configure(&quot;TButton&quot;, padding=6, relief=&quot;flat&quot;,background=&quot;</span><br><span class="line">Pink&quot;,foreground=&quot;Red&quot;)</span><br><span class="line">ttk.Button(mainframe, text=&quot;BROWSE&quot;, command=find_train_corpus).</span><br><span class="line">grid(column=7, row=5, sticky=W)</span><br><span class="line"></span><br><span class="line"># The following code will be used to display or accept the testing</span><br><span class="line">corpus from the user.</span><br><span class="line">def testhmm():</span><br><span class="line">ttk.Button(mainframe, text=&quot;Develop a new testing Corpus&quot;,</span><br><span class="line">command=calculate3).grid(column=9, row=5, sticky=E)</span><br><span class="line"></span><br><span class="line">ttk.Button(mainframe, text=&quot;BROWSE&quot;, command=find_obs).grid(column=9,</span><br><span class="line">row=7, sticky=E)</span><br><span class="line"></span><br><span class="line">#In HMM, We require parameters such as Start Probability, Transition</span><br><span class="line">Probability and Emission Probability. The following code is used to</span><br><span class="line">calculate emission probability matrix</span><br><span class="line"></span><br><span class="line">def cal_emit_mat():</span><br><span class="line">global emission_probability</span><br><span class="line">global corpus</span><br><span class="line">global tlines</span><br><span class="line"></span><br><span class="line">calle_prob.m_prg(e_dict,corpus,tlines)</span><br><span class="line"></span><br><span class="line">emission_probability = e_dict</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line"># to calculate states</span><br><span class="line"></span><br><span class="line">def cal_states():</span><br><span class="line">global states</span><br><span class="line">global tlines</span><br><span class="line"></span><br><span class="line">cal_start_p.cnt_tag(tlines)</span><br><span class="line"></span><br><span class="line">states = cal_start_p.tag_set</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line"># to take observations</span><br><span class="line"></span><br><span class="line">def find_obs():</span><br><span class="line">global observations</span><br><span class="line">global test_lines</span><br><span class="line">global tra</span><br><span class="line">global w4</span><br><span class="line">global co</span><br><span class="line">global tra</span><br><span class="line">global wo1</span><br><span class="line">global wo2</span><br><span class="line">global testl</span><br><span class="line">global wo3</span><br><span class="line">global te</span><br><span class="line">global definitionText</span><br><span class="line">global definitionScroll</span><br><span class="line">global dt2</span><br><span class="line">global ds2</span><br><span class="line">global dt11</span><br><span class="line">global ds11</span><br><span class="line"></span><br><span class="line">wo3=[ ]</span><br><span class="line">woo=[ ]</span><br><span class="line">wo1=[ ]</span><br><span class="line">wo2=[ ]</span><br><span class="line">co=0</span><br><span class="line">w4=[ ]</span><br><span class="line">if(flag2!=0):</span><br><span class="line">definitionText11.pack_forget()</span><br><span class="line">definitionScroll11.pack_forget()</span><br><span class="line">dt1.pack_forget()</span><br><span class="line">ds1.pack_forget()</span><br><span class="line">dispdlg()</span><br><span class="line">f = open(file_name,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">test_lines = f.readlines()</span><br><span class="line">f.close()</span><br><span class="line">fname=&quot;C:/Python32/file_name1&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for x in states:</span><br><span class="line">if not x in start_probability:</span><br><span class="line">start_probability.update(&#123;x:0.0&#125;)</span><br><span class="line">for line in test_lines:</span><br><span class="line">ob = line.split()</span><br><span class="line">observations = ( ob )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fe=open(&quot;C:\Python32\output3_file&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">fe.write(&quot;&quot;)</span><br><span class="line">fe.close()</span><br><span class="line">ff=open(&quot;C:\Python32\output4_file&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">ff.write(&quot;&quot;)</span><br><span class="line">ff.close()</span><br><span class="line">ff7=open(&quot;C:\Python32\output5_file&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">ff7.write(&quot;&quot;)</span><br><span class="line">ff7.close()</span><br><span class="line">ff8=open(&quot;C:\Python32\output6_file&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">ff8.write(&quot;&quot;)</span><br><span class="line">ff8.close()</span><br><span class="line">ff81=open(&quot;C:\Python32\output7_file&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">ff81.write(&quot;&quot;)</span><br><span class="line">ff81.close()</span><br><span class="line">dict5.search_obs_train_corpus(file1,fname,tlines,test_</span><br><span class="line">lines,observations, states, start_probability, transition_probability,</span><br><span class="line">emission_probability)</span><br><span class="line"></span><br><span class="line">f20 = open(&quot;C:\Python32\output5_file&quot;,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">te = f20.readlines()</span><br><span class="line">tee=f20.read()</span><br><span class="line">f = open(fname,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">train_llines = f.readlines()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds11 = Scrollbar(root)</span><br><span class="line">dt11 = Text(root, width=10, height=20,fg=&apos;black&apos;,bg=&apos;pink&apos;,yscrollcom</span><br><span class="line">mand=ds11.set)</span><br><span class="line">ds11.config(command=dt11.yview)</span><br><span class="line">dt11.insert(&quot;1.0&quot;,train_llines)</span><br><span class="line">dt11.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt11.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">dt11.insert(&quot;1.0&quot;,&quot;******TRAINING SENTENCES******&quot;)</span><br><span class="line"></span><br><span class="line">    # an example of how to add new text to the text area</span><br><span class="line">dt11.pack(padx=10,pady=150)</span><br><span class="line">ds11.pack(padx=10,pady=150)</span><br><span class="line"></span><br><span class="line">ds11.pack(side=LEFT, fill=BOTH)</span><br><span class="line">dt11.pack(side=LEFT, fill=BOTH, expand=True)</span><br><span class="line"></span><br><span class="line">ds2 = Scrollbar(root)</span><br><span class="line">dt2 = Text(root, width=10, height=10,fg=&apos;black&apos;,bg=&apos;pink&apos;,yscrollcomm</span><br><span class="line">and=ds2.set)</span><br><span class="line">ds2.config(command=dt2.yview)</span><br><span class="line">dt2.insert(&quot;1.0&quot;,test_lines)</span><br><span class="line">dt2.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt2.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt2.insert(&quot;1.0&quot;,&quot;*********TESTING SENTENCES*********&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # an example of how to add new text to the text area</span><br><span class="line">dt2.pack(padx=10,pady=150)</span><br><span class="line">ds2.pack(padx=10,pady=150)</span><br><span class="line"></span><br><span class="line">ds2.pack(side=LEFT, fill=BOTH)</span><br><span class="line">dt2.pack(side=LEFT, fill=BOTH, expand=True)</span><br><span class="line"></span><br><span class="line">definitionScroll = Scrollbar(root)</span><br><span class="line">definitionText = Text(root, width=10, height=10,fg=&apos;black&apos;,bg=&apos;pink&apos;,y</span><br><span class="line">scrollcommand=definitionScroll.set)</span><br><span class="line">definitionScroll.config(command=definitionText.yview)</span><br><span class="line">definitionText.insert(&quot;1.0&quot;,te)</span><br><span class="line">definitionText.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">definitionText.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">definitionText.insert(&quot;1.0&quot;,&quot;*********OUTPUT*********&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # an example of how to add new text to the text area</span><br><span class="line">definitionText.pack(padx=10,pady=150)</span><br><span class="line">definitionScroll.pack(padx=10,pady=150)</span><br><span class="line"></span><br><span class="line">definitionScroll.pack(side=LEFT, fill=BOTH)</span><br><span class="line">definitionText.pack(side=LEFT, fill=BOTH, expand=True)</span><br><span class="line"></span><br><span class="line">l = tk.Label(root, text=&quot;NOTE:*****The Entities which are not tagged</span><br><span class="line">in Output are not Named Entities*****&quot; , fg=&apos;black&apos;, bg=&apos;pink&apos;)</span><br><span class="line">l.place(x = 500, y = 650, width=500, height=25)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #ttk.Button(mainframe, text=&quot;View Parameters&quot;, command=parame).</span><br><span class="line">grid(column=11, row=10, sticky=E)</span><br><span class="line">    #definitionText.place(x= 19, y = 200,height=25)</span><br><span class="line"></span><br><span class="line">f20.close()</span><br><span class="line"></span><br><span class="line">f14 = open(&quot;C:\Python32\output2_file&quot;,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">testl = f14.readlines()</span><br><span class="line">for lines in testl:</span><br><span class="line">toke = lines.split()</span><br><span class="line">for t in toke:</span><br><span class="line">w4.append(t)</span><br><span class="line">f14.close()</span><br><span class="line">f12 = open(&quot;C:\Python32\output_file&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">f12.write(&quot;&quot;)</span><br><span class="line">f12.close()</span><br><span class="line"></span><br><span class="line">ttk.Button(mainframe, text=&quot;SAVE OUTPUT&quot;, command=save_output).</span><br><span class="line">grid(column=11, row=7, sticky=E)</span><br><span class="line">ttk.Button(mainframe, text=&quot;NER EVALUATION&quot;, command=evaluate).</span><br><span class="line">grid(column=13, row=7, sticky=E)</span><br><span class="line">ttk.Button(mainframe, text=&quot;REFRESH&quot;, command=ref).grid(column=15,</span><br><span class="line">row=7, sticky=E)</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line">def ref():</span><br><span class="line">root.destroy()</span><br><span class="line">import new1</span><br><span class="line">return</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下Python中的代码，它将用于评估通过HMM来执行NER后所生成的输出结果：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br></pre></td><td class="code"><pre><span class="line">def evaluate():</span><br><span class="line">global wDict</span><br><span class="line">global woe</span><br><span class="line">global woe1</span><br><span class="line">global woe2</span><br><span class="line">woe1=[ ]</span><br><span class="line">woe=[ ]</span><br><span class="line">woe2=[ ]</span><br><span class="line">ws=[ ]</span><br><span class="line">wDict = &#123;&#125;</span><br><span class="line">i=0</span><br><span class="line">    j=0</span><br><span class="line">    k=0</span><br><span class="line">sp=0</span><br><span class="line">f141 = open(&quot;C:\Python32\output1_file&quot;,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">tesl = f141.readlines()</span><br><span class="line">for lines in tesl:</span><br><span class="line">toke = lines.split()</span><br><span class="line">for t in toke:</span><br><span class="line">ws.append(t)</span><br><span class="line">if t in wDict: wDict[t] += 1</span><br><span class="line">else: wDict[t] = 1</span><br><span class="line">for line in tlines:</span><br><span class="line">tok = line.split()</span><br><span class="line"></span><br><span class="line">for t in tok:</span><br><span class="line">wd = t.split(&quot;/&quot;)</span><br><span class="line">if(wd[1]!=&apos;OTHER&apos;):</span><br><span class="line">if t in wDict: wDict[t] += 1</span><br><span class="line">else: wDict[t] = 1</span><br><span class="line">print (&quot;words in train corpus &quot;,wDict)</span><br><span class="line">for key in wDict:</span><br><span class="line">i=i+1</span><br><span class="line">print(&quot;total words in Dictionary are:&quot;,i)</span><br><span class="line">for line in train_lines:</span><br><span class="line">toe=line.split()</span><br><span class="line">for t1 in toe:</span><br><span class="line">if &apos;/&apos; not in t1:</span><br><span class="line">sp=sp+1</span><br><span class="line">woe2.append(t1)</span><br><span class="line">print(&quot;Spurious words are&quot;)</span><br><span class="line">for w in woe2:</span><br><span class="line">print(w)</span><br><span class="line">print(&quot;Total spurious words are:&quot;,sp)</span><br><span class="line">for l in te:</span><br><span class="line">to=l.split()</span><br><span class="line">for t1 in to:</span><br><span class="line">if &apos;/&apos; in t1:</span><br><span class="line">                #print(t1)</span><br><span class="line">if t1 in ws or t1 in wDict:</span><br><span class="line">woe.append(t1)</span><br><span class="line">                    j=j+1</span><br><span class="line">if t1 not in wDict:</span><br><span class="line">wdd=t1.split(&quot;/&quot;)</span><br><span class="line">if wdd[0] not in woe2:</span><br><span class="line">woe1.append(t1)</span><br><span class="line">                         k=k+1</span><br><span class="line">print(&quot;Word found in Dict are:&quot;)</span><br><span class="line">for w in woe:</span><br><span class="line">print(w)</span><br><span class="line">print(&quot;Word not found in Dict are:&quot;)</span><br><span class="line">for w in woe1:</span><br><span class="line">print(w)</span><br><span class="line">print(&quot;Total correctly tagged words are:&quot;,j)</span><br><span class="line">print(&quot;Total incorrectly tagged words are:&quot;,k)</span><br><span class="line">pr=(j)/(j+k)</span><br><span class="line">re=(j)/(j+k+sp)</span><br><span class="line">f141.close()</span><br><span class="line">root=Tk()</span><br><span class="line">root.title(&quot;NER EVALUATION&quot;)</span><br><span class="line">root.geometry(&quot;1000x1000&quot;)</span><br><span class="line"></span><br><span class="line">ds21 = Scrollbar(root)</span><br><span class="line">dt21 = Text(root, width=10, height=10,fg=&apos;black&apos;,bg=&apos;pink&apos;,yscrollcom</span><br><span class="line">mand=ds21.set)</span><br><span class="line">ds21.config(command=dt21.yview)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,(2*pr*re)/(pr+re))</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;F-MEASURE=&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;F-MEASURE=(2*PRECISION*RECALL)/(PRECISION+RECALL)&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,re)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;RECALL=&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;RECALL= CORRECT/(CORRECT +INCORRECT +SPURIOUS)&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,pr)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;PRECISION=&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;PRECISION= CORRECT/(CORRECT +INCORRECT +MISSING)&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;Total No. of Missing words are: 0&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,sp)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;Total No. of Spurious Words are:&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">for w in woe2:</span><br><span class="line">dt21.insert(&quot;1.0&quot;,w)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot; &quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;Total Spurious Words are:&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,k)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;Total No. of Incorrectly tagged words are:&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">for w in woe1:</span><br><span class="line">dt21.insert(&quot;1.0&quot;,w)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot; &quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;Total Incorrectly tagged words are:&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,j)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;Total No. of Correctly tagged words are:&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">for w in woe:</span><br><span class="line">dt21.insert(&quot;1.0&quot;,w)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot; &quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;Total Correctly tagged words are:&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt21.insert(&quot;1.0&quot;,&quot;***************PERFORMANCE EVALUATION OF</span><br><span class="line">NERHMM***************&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # an example of how to add new text to the text area</span><br><span class="line">dt21.pack(padx=5,pady=5)</span><br><span class="line">ds21.pack(padx=5,pady=5)</span><br><span class="line">ds21.pack(side=LEFT, fill=BOTH)</span><br><span class="line">dt21.pack(side=LEFT, fill=BOTH, expand=True)</span><br><span class="line">root.mainloop()</span><br><span class="line">return</span><br><span class="line">def save_output():</span><br><span class="line">    #dispdlg()</span><br><span class="line">f = open(&quot;C:\Python32\save&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">f20 = open(&quot;C:\Python32\output5_file&quot;,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">te = f20.readlines()</span><br><span class="line">for t in te:</span><br><span class="line">f.write(t)</span><br><span class="line">f.close()</span><br><span class="line">f20.close()</span><br><span class="line"></span><br><span class="line"># to calculate start probability matrix</span><br><span class="line"></span><br><span class="line">def cal_srt_prob():</span><br><span class="line">global start_probability</span><br><span class="line"></span><br><span class="line">start_probability = cal_start_p.lst</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line"># to print vitarbi parameter if required</span><br><span class="line"></span><br><span class="line">def pr_param():</span><br><span class="line">l1 = tk.Label(root, text=&quot;HMM Training is going on.....Don&apos;t Click any</span><br><span class="line">Button!!&quot;,fg=&apos;black&apos;,bg=&apos;pink&apos;)</span><br><span class="line">l1.place(x = 300, y = 150,height=25)</span><br><span class="line"></span><br><span class="line">print(&quot;states&quot;)</span><br><span class="line">print(states)</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(&quot;start probability&quot;)</span><br><span class="line">print(start_probability)</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(&quot;transition probability&quot;)</span><br><span class="line">print(transition_probability)</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(&quot;emission probability&quot;)</span><br><span class="line">print(emission_probability)</span><br><span class="line">l1 = tk.Label(root, text=&quot;</span><br><span class="line">&quot;)</span><br><span class="line">l1.place(x = 300, y = 150,height=25)</span><br><span class="line">global flag1</span><br><span class="line">    flag1=0</span><br><span class="line">global flag2</span><br><span class="line">    flag2=0</span><br><span class="line">ttk.Button(mainframe, text=&quot;View Parameters&quot;, command=parame).</span><br><span class="line">grid(column=7, row=5, sticky=W)</span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">def parame():</span><br><span class="line">global flag2</span><br><span class="line">    flag2=flag1+1</span><br><span class="line">global definitionText11</span><br><span class="line">global definitionScroll11</span><br><span class="line">definitionScroll11 = Scrollbar(root)</span><br><span class="line">definitionText11 = Text(root, width=10, height=10,fg=&apos;black&apos;,bg=&apos;pink&apos;</span><br><span class="line">,yscrollcommand=definitionScroll11.set)</span><br><span class="line"></span><br><span class="line">    #definitionText.place(x= 19, y = 200,height=25)</span><br><span class="line">definitionScroll11.config(command=definitionText11.yview)</span><br><span class="line"></span><br><span class="line">definitionText11.delete(&quot;1.0&quot;, END) # an example of how to delete</span><br><span class="line">all current text</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,emission_probability )</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,&quot;Emission Probability&quot;)</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,transition_probability)</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,&quot;Transition Probability&quot;)</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,start_probability)</span><br><span class="line">definitionText11.insert(&quot;1.0&quot;,&quot;Start Probability&quot;)</span><br><span class="line"></span><br><span class="line">    # an example of how to add new text to the text area</span><br><span class="line">definitionText11.pack(padx=10,pady=175)</span><br><span class="line">definitionScroll11.pack(padx=10,pady=175)</span><br><span class="line"></span><br><span class="line">definitionScroll11.pack(side=LEFT, fill=BOTH)</span><br><span class="line">definitionText11.pack(side=LEFT, fill=BOTH, expand=True)</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line"># to calculate transition probability matrix</span><br><span class="line"></span><br><span class="line">def cat_trans_prob():</span><br><span class="line">global transition_probability</span><br><span class="line">global corpus</span><br><span class="line">global tlines</span><br><span class="line"></span><br><span class="line">trans_mat.main_prg(t_dict,corpus,tlines)</span><br><span class="line">transition_probability = t_dict</span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">def find_train_corpus():</span><br><span class="line">global train_lines</span><br><span class="line">global tlines</span><br><span class="line">global c</span><br><span class="line">global corpus</span><br><span class="line">global words1</span><br><span class="line">global w1</span><br><span class="line">global train1</span><br><span class="line">global fname</span><br><span class="line">global file1</span><br><span class="line">global ds1</span><br><span class="line">global dt1</span><br><span class="line">global w21</span><br><span class="line">words1=[ ]</span><br><span class="line">    c=0</span><br><span class="line">w1=[ ]</span><br><span class="line">w21=[ ]</span><br><span class="line">f11 = open(&quot;C:\Python32\output1_file&quot;,&quot;w+&quot;,encoding=&apos;utf-8&apos;)</span><br><span class="line">f11.write(&quot;&quot;)</span><br><span class="line">f11.close()</span><br><span class="line">fr = open(&quot;C:\Python32\output_file&quot;,&quot;w+&quot;,encoding=&apos;utf-8&apos;)</span><br><span class="line">fr.write(&quot;&quot;)</span><br><span class="line">fr.close()</span><br><span class="line">fgl=open(&quot;C:\Python32\ladetect1&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">fgl.write(&quot;&quot;)</span><br><span class="line">fgl.close()</span><br><span class="line"></span><br><span class="line">fgl=open(&quot;C:\Python32\ladetect&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">fgl.write(&quot;&quot;)</span><br><span class="line">fgl.close()</span><br><span class="line">dispdlg()</span><br><span class="line">f = open(file_name,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">train_lines = f.readlines()</span><br><span class="line"></span><br><span class="line">ds1 = Scrollbar(root)</span><br><span class="line">dt1 = Text(root, width=10, height=10,fg=&apos;black&apos;,bg=&apos;pink&apos;,yscrollcomm</span><br><span class="line">and=ds1.set)</span><br><span class="line">ds1.config(command=dt1.yview)</span><br><span class="line">dt1.insert(&quot;1.0&quot;,train_lines)</span><br><span class="line">dt1.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt1.insert(&quot;1.0&quot;,&quot;\n&quot;)</span><br><span class="line">dt1.insert(&quot;1.0&quot;,&quot;*********TRAINING SENTENCES*********&quot;)</span><br><span class="line"></span><br><span class="line">    # an example of how to add new text to the text area</span><br><span class="line">dt1.pack(padx=10,pady=175)</span><br><span class="line">ds1.pack(padx=10,pady=175)</span><br><span class="line"></span><br><span class="line">ds1.pack(side=LEFT, fill=BOTH)</span><br><span class="line">dt1.pack(side=LEFT, fill=BOTH, expand=True)</span><br><span class="line">fname=&quot;C:/Python32/file_name1&quot;</span><br><span class="line">f = open(file_name,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">    file1=file_name</span><br><span class="line">p = open(fname,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">corpus = f.read()</span><br><span class="line">for line in train_lines:</span><br><span class="line">tok = line.split()</span><br><span class="line">for t in tok:</span><br><span class="line">n=t.split()</span><br><span class="line"></span><br><span class="line">le=len(t)</span><br><span class="line">i=0</span><br><span class="line">             j=0</span><br><span class="line">for n1 in n:</span><br><span class="line">while(j&lt;le):</span><br><span class="line"></span><br><span class="line">if(n1[j]!=&apos;/&apos;):</span><br><span class="line">i=i+1</span><br><span class="line">               j=j+1</span><br><span class="line">else:</span><br><span class="line">               j=j+1</span><br><span class="line">if(i==le):</span><br><span class="line">p.write(t)</span><br><span class="line">p.write(&quot;/OTHER &quot;) #Handling Spurious words</span><br><span class="line">else:</span><br><span class="line">p.write(t)</span><br><span class="line">p.write(&quot; &quot;)</span><br><span class="line"></span><br><span class="line">p.write(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p.close()</span><br><span class="line">fname=&quot;C:/Python32/file_name1&quot;</span><br><span class="line">f00 = open(fname,&quot;r+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">tlines = f00.readlines()</span><br><span class="line">for line in tlines:</span><br><span class="line">tok = line.split()</span><br><span class="line">for t in tok:</span><br><span class="line">wd = t.split(&quot;/&quot;)</span><br><span class="line">if(wd[1]!=&apos;OTHER&apos;):</span><br><span class="line">if not wd[0] in words1:</span><br><span class="line">words1.append(wd[0])</span><br><span class="line">w1.append(wd[1])</span><br><span class="line">f00.close()</span><br><span class="line"></span><br><span class="line">f157 = open(&quot;C:\Python32\input_file&quot;,&quot;w+&quot;,encoding=&apos;utf-8&apos;)</span><br><span class="line">f157.write(&quot;&quot;)</span><br><span class="line">f157.close()</span><br><span class="line">f1 = open(&quot;C:\Python32\input_file&quot;,&quot;w+&quot;,encoding=&apos;utf-8&apos;) #input_</span><br><span class="line">file has list of Named Entities of training file</span><br><span class="line">for w in words1:</span><br><span class="line">f1.write(w)</span><br><span class="line">f1.write(&quot;\n&quot;)</span><br><span class="line">f1.close()</span><br><span class="line">fr=open(&quot;C:\Python32\detect&quot;,&quot;w+&quot;,encoding = &apos;utf-8&apos;)</span><br><span class="line">fr.write(&quot;&quot;)</span><br><span class="line">fr.close()</span><br><span class="line"></span><br><span class="line">f.close()</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">cal_states()</span><br><span class="line">cal_emit_mat()</span><br><span class="line">cal_srt_prob()</span><br><span class="line">cat_trans_prob()</span><br><span class="line">pr_param()</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">root=Tk()</span><br><span class="line">root.title(&quot;NAMED ENTITY RECOGNITION IN NATURAL LANGUAGES USING HIDDEN</span><br><span class="line">MARKOV MODEL&quot;)</span><br><span class="line">root.geometry(&quot;1000x1000&quot;)</span><br><span class="line"></span><br><span class="line">mainframe = ttk.Frame(root, padding=&quot;20 20 12 12&quot;)</span><br><span class="line">mainframe.grid(column=0, row=0, sticky=(N, W, E, S))</span><br><span class="line"></span><br><span class="line">b=StringVar()</span><br><span class="line">a=StringVar()</span><br><span class="line"></span><br><span class="line">ttk.Style().configure(&quot;TButton&quot;, padding=6, relief=&quot;flat&quot;,background=&quot;</span><br><span class="line">Pink&quot;, foreground=&quot;Red&quot;)</span><br><span class="line">ttk.Button(mainframe, text=&quot;ANNOTATION&quot;, command=calculate1).</span><br><span class="line">grid(column=5, row=3, sticky=W)</span><br><span class="line"></span><br><span class="line">ttk.Button(mainframe, text=&quot;TRAIN HMM&quot;, command=tranhmm).</span><br><span class="line">grid(column=7, row=3, sticky=E)</span><br><span class="line"></span><br><span class="line">ttk.Button(mainframe, text=&quot;TEST HMM&quot;, command=testhmm).grid(column=9,</span><br><span class="line">row=3, sticky=E)</span><br><span class="line"></span><br><span class="line">ttk.Button(mainframe, text=&quot;HELP&quot;, command=hmmhelp).grid(column=11,</span><br><span class="line">row=3, sticky=E)</span><br><span class="line"></span><br><span class="line"># To call viterbi for particular observations find in find_obs</span><br><span class="line"></span><br><span class="line">def call_vitar():</span><br><span class="line">global test_lines</span><br><span class="line">global train_lines</span><br><span class="line">global corpus</span><br><span class="line">global observations</span><br><span class="line">global states</span><br><span class="line">global start_probability</span><br><span class="line">global transition_probability</span><br><span class="line">global emission_probability</span><br><span class="line"></span><br><span class="line">find_train_corpus()</span><br><span class="line">cal_states()</span><br><span class="line">find_obs()</span><br><span class="line">cal_emit_mat()</span><br><span class="line">cal_srt_prob()</span><br><span class="line">cat_trans_prob()</span><br><span class="line"></span><br><span class="line">    # print(&quot;Vitarbi Parameters are for selected corpus&quot;)</span><br><span class="line">    # pr_param()</span><br><span class="line">     # -----------------To add all states not in start probability ---</span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line">for x in states:</span><br><span class="line">if not x in start_probability:</span><br><span class="line">start_probability.update(&#123;x:0.0&#125;)</span><br><span class="line"></span><br><span class="line">for line in test_lines:</span><br><span class="line"></span><br><span class="line">ob = line.split()</span><br><span class="line">observations = ( ob )</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(&quot; &quot;)</span><br><span class="line">print(line)</span><br><span class="line">print(&quot;**************************&quot;)</span><br><span class="line">print(vit.viterbi(observations, states, start_probability, transition_</span><br><span class="line">probability, emission_probability),bg=&apos;Pink&apos;,fg=&apos;Red&apos;)</span><br><span class="line">return</span><br><span class="line"></span><br><span class="line">root.mainloop()</span><br></pre></td></tr></table></figure>

</details>







<p>以上Python代码展示了如何通过HMM来执行NER，以及如何使用性能指标（精确率、召回率和F值）来评估一个NER系统的性能。</p>
<h2 id="7-2-小结"><a href="#7-2-小结" class="headerlink" title="7.2 小结"></a>7.2 小结</h2><p>在本章中，我们讨论了使用NER和机器学习技术进行的情感分析。此外我们还讨论了基于NER的系统的评估。</p>
<p>在下一章中，我们将会讨论信息检索、文本摘要、停止词删除以及问答系统等。</p>
<h1 id="第8章-信息检索：访问信息"><a href="#第8章-信息检索：访问信息" class="headerlink" title="第8章 信息检索：访问信息"></a>第8章 信息检索：访问信息</h1><p>信息检索是自然语言处理的众多应用之一。信息检索可以定义为检索用户一次查询所对应的相关信息（例如，单词Ganga 在文档中所出现的次数）的过程。</p>
<p>本章将涵盖以下主题：</p>
<ul>
<li>信息检索简介。</li>
<li>停止词删除。</li>
<li>使用向量空间模型进行信息检索。</li>
<li>向量空间评分及查询操作符关联。</li>
<li>使用隐性语义索引开发一个IR系统。</li>
<li>文本摘要。</li>
<li>问答系统。</li>
</ul>
<h2 id="8-1-信息检索简介"><a href="#8-1-信息检索简介" class="headerlink" title="8.1 信息检索简介"></a>8.1 信息检索简介</h2><p>信息检索可以定义为检索最合适的信息作为用户查询响应的过程。在信息检索中，搜索是基于元数据或基于上下文的索引进行的。Google搜索就是信息检索的一个例子，其中对于用户的每一次查询，Google搜索都会基于所使用的信息检索算法为其提供一个响应。信息检索算法使用了索引机制，其所使用的索引机制被称为倒排索引。为了执行信息检索任务，信息检索（IR）系统会建立一个索引标记列表。</p>
<p>布尔检索是一种信息检索任务，在该任务中，布尔操作符被应用在标记列表上以便检索相关的信息。</p>
<p>信息检索任务的准确度是依据精确率和召回率来度量的。</p>
<p>假定一个给定的信息检索系统执行一次查询时返回X个文档。但是需要返回的实际或黄金文档集个数是Y。</p>
<p>召回率可以定义为信息检索系统所查找到的部分黄金文档数。它也可以定义为真正类与真正类和假负类的并集之比。</p>
<pre><code>Recall (R) = ( X ∩ Y ) / Y
</code></pre><p>精确率可以定义为信息检索系统检测到并且正确的部分文档数。</p>
<pre><code>Precision (P) = ( X ∩ Y ) / X
</code></pre><p>F值可以定义为精确率和召回率的调合平均值。</p>
<pre><code>F-Measure = 2 * ( X ∩ Y ) / ( X + Y )
</code></pre><h3 id="8-1-1-停止词删除"><a href="#8-1-1-停止词删除" class="headerlink" title="8.1.1 停止词删除"></a>8.1.1 停止词删除</h3><p>在执行信息检索任务时，检测文档中的停止词并删除它们是至关重要的。</p>
<p>让我们来看看如下NLTK中的代码，其用于获取英文中可以被检测到的停止词集合。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line">&gt;&gt;&gt; stopwords.words(&apos;english&apos;)</span><br><span class="line">[&apos;i&apos;, &apos;me&apos;, &apos;my&apos;, &apos;myself&apos;, &apos;we&apos;, &apos;our&apos;, &apos;ours&apos;, &apos;ourselves&apos;, &apos;you&apos;,</span><br><span class="line">&apos;your&apos;, &apos;yours&apos;, &apos;yourself&apos;, &apos;yourselves&apos;, &apos;he&apos;, &apos;him&apos;, &apos;his&apos;,</span><br><span class="line">&apos;himself&apos;, &apos;she&apos;, &apos;her&apos;, &apos;hers&apos;, &apos;herself&apos;, &apos;it&apos;, &apos;its&apos;, &apos;itself&apos;,</span><br><span class="line">&apos;they&apos;, &apos;them&apos;, &apos;their&apos;, &apos;theirs&apos;, &apos;themselves&apos;, &apos;what&apos;, &apos;which&apos;,</span><br><span class="line">&apos;who&apos;, &apos;whom&apos;, &apos;this&apos;, &apos;that&apos;, &apos;these&apos;, &apos;those&apos;, &apos;am&apos;, &apos;is&apos;, &apos;are&apos;,</span><br><span class="line">&apos;was&apos;, &apos;were&apos;, &apos;be&apos;, &apos;been&apos;, &apos;being&apos;, &apos;have&apos;, &apos;has&apos;, &apos;had&apos;, &apos;having&apos;,</span><br><span class="line">&apos;do&apos;, &apos;does&apos;, &apos;did&apos;, &apos;doing&apos;, &apos;a&apos;, &apos;an&apos;, &apos;the&apos;, &apos;and&apos;, &apos;but&apos;, &apos;if&apos;,</span><br><span class="line">&apos;or&apos;, &apos;because&apos;, &apos;as&apos;, &apos;until&apos;, &apos;while&apos;, &apos;of&apos;, &apos;at&apos;, &apos;by&apos;, &apos;for&apos;,</span><br><span class="line">&apos;with&apos;, &apos;about&apos;, &apos;against&apos;, &apos;between&apos;, &apos;into&apos;, &apos;through&apos;, &apos;during&apos;,</span><br><span class="line">&apos;before&apos;, &apos;after&apos;, &apos;above&apos;, &apos;below&apos;, &apos;to&apos;, &apos;from&apos;, &apos;up&apos;, &apos;down&apos;, &apos;in&apos;,</span><br><span class="line">&apos;out&apos;, &apos;on&apos;, &apos;off&apos;, &apos;over&apos;, &apos;under&apos;, &apos;again&apos;, &apos;further&apos;, &apos;then&apos;,</span><br><span class="line">&apos;once&apos;, &apos;here&apos;, &apos;there&apos;, &apos;when&apos;, &apos;where&apos;, &apos;why&apos;, &apos;how&apos;, &apos;all&apos;, &apos;any&apos;,</span><br><span class="line">&apos;both&apos;, &apos;each&apos;, &apos;few&apos;, &apos;more&apos;, &apos;most&apos;, &apos;other&apos;, &apos;some&apos;, &apos;such&apos;, &apos;no&apos;,</span><br><span class="line">&apos;nor&apos;, &apos;not&apos;, &apos;only&apos;, &apos;own&apos;, &apos;same&apos;, &apos;so&apos;, &apos;than&apos;, &apos;too&apos;, &apos;very&apos;, &apos;s&apos;,</span><br><span class="line">&apos;t&apos;, &apos;can&apos;, &apos;will&apos;, &apos;just&apos;, &apos;don&apos;, &apos;should&apos;, &apos;now&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>NLTK包含一个由2400个停止词（涉及11种不同语言）所组成的停止词语料库。</p>
<p>让我们来看看下面NLTK中的代码，其可用于找出一篇文章中那些不是停止词的单词个数。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def not_stopwords(text):</span><br><span class="line">    stopwords = nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">    content = [w for w in text if w.lower() not in stopwords]</span><br><span class="line">    return len(content) / len(text)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; not_stopwords(nltk.corpus.reuters.words())</span><br><span class="line">0.7364374824583169</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看下面NLTK中的代码，其可用于从给定的文本中删除停止词。这里，在删除停止词之前调用了<code>lower()</code> 函数，以便使形如A这样的大写字母的停止词首先转换为小写字母，然后再去除停止词。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line">from collections import Counter</span><br><span class="line">import string</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line"></span><br><span class="line">def get_tokens():</span><br><span class="line">    with open(&apos;/home/d/TRY/NLTK/STOP.txt&apos;) as stopl:</span><br><span class="line">        tokens = nltk.word_tokenize(stopl.read().lower().</span><br><span class="line">translate(None, string.punctuation))</span><br><span class="line">    return tokens</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    tokens = get_tokens()</span><br><span class="line">    print(&quot;tokens[:20]=%s&quot;) %(tokens[:20])</span><br><span class="line"></span><br><span class="line">    count1 = Counter(tokens)</span><br><span class="line">    print(&quot;before: len(count1) = %s&quot;) %(len(count1))</span><br><span class="line"></span><br><span class="line">    filtered1 = [w for w in tokens if not w in stopwords.</span><br><span class="line">words(&apos;english&apos;)]</span><br><span class="line"></span><br><span class="line">    print(&quot;filtered1 tokens[:20]=%s&quot;) %(filtered1[:20])</span><br><span class="line"></span><br><span class="line">    count1 = Counter(filtered1)</span><br><span class="line">    print(&quot;after: len(count1) = %s&quot;) %(len(count1))</span><br><span class="line"></span><br><span class="line">    print(&quot;most_common = %s&quot;) %(count.most_common(10))</span><br><span class="line"></span><br><span class="line">    tagged1 = nltk.pos_tag(filtered1)</span><br><span class="line">    print(&quot;tagged1[:20]=%s&quot;) %(tagged1[:20])</span><br></pre></td></tr></table></figure>

</details>




<h3 id="8-1-2-使用向量空间模型进行信息检索"><a href="#8-1-2-使用向量空间模型进行信息检索" class="headerlink" title="8.1.2 使用向量空间模型进行信息检索"></a>8.1.2 使用向量空间模型进行信息检索</h3><p>在向量空间模型中，所有的文档都使用向量来表示。将文档表示为向量的方法之一是使用TF-IDF（词频–反文档频率，Term Frequency-Inverse Document Frequency）。</p>
<p>词频可以被定义为一个给定的标识符在文档中出现的总数除以该文档中标识符的总数。它也可以被定义为给定文档中某些特征项出现的频率。</p>
<p>词频（TF）的公式如下：</p>
<pre><code>_TF(t,d) =_ 0.5 _\+ (_ 0.5 _* f(t,d)) / max {f(w,d) : w_ ϵ _d}_
</code></pre><p>IDF可以认为是反文档频率，也可以认为其是语料库中包含给定特征项的文档数目。</p>
<p>通过将给定的语料库中存在的文档总数除以包含某特定标识符的文档数，再取商的对数就可以计算IDF。</p>
<pre><code>_IDF(t,D)_ 的公式可以表示如下：

_IDF(t,D)= log(N/{d_ ϵ _D :t_ ϵ _d})_
</code></pre><p>通过将以上两个评分相乘可以获取TF-IDF评分，表示如下：</p>
<pre><code>_TF-IDF(t, d, D) = TF(t,d) * IDF(t,D)_
</code></pre><p>TF-IDF提供了一个特征项在给定的文档中出现频率的估计以及该特征项在语料库中出现的总次数。</p>
<p>为了计算一篇给定文档的TF-IDF，需要执行如下步骤：</p>
<ul>
<li>文档切分。</li>
<li>计算向量空间模型。</li>
<li>计算每个文档的TF-IDF。</li>
</ul>
<p>文档切分是一个首先将文本切分为句子，然后再将独立的句子切分为单词的过程。之后我们可以删除在信息检索的过程中没有意义的单词（也叫停止词）。</p>
<p>让我们来看看下面的代码，其可用于对语料库中的每个文档执行切分：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">authen = OAuthHandler(CLIENT_ID, CLIENT_SECRET, CALLBACK)</span><br><span class="line">authen.set_access_token(ACCESS_TOKEN)</span><br><span class="line">ap = API(authen)</span><br><span class="line"></span><br><span class="line">venue = ap.venues(id=&apos;4bd47eeb5631c9b69672a230&apos;)</span><br><span class="line">stopwords = nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">tokenizer = RegexpTokenizer(&quot;[\w&apos;]+&quot;, flags=re.UNICODE)</span><br><span class="line"></span><br><span class="line">def freq(word, tokens):</span><br><span class="line">return tokens.count(word)</span><br><span class="line"></span><br><span class="line">#Compute the frequency for each term.</span><br><span class="line">vocabulary = []</span><br><span class="line">docs = &#123;&#125;</span><br><span class="line">all_tips = []</span><br><span class="line">for tip in (venue.tips()):</span><br><span class="line">tokens = tokenizer.tokenize(tip.text)</span><br><span class="line"></span><br><span class="line">bitokens = bigrams(tokens)</span><br><span class="line">tritokens = trigrams(tokens)</span><br><span class="line">tokens = [token.lower() for token in tokens if len(token) &gt; 2]</span><br><span class="line">tokens = [token for token in tokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">bitokens = [&apos; &apos;.join(token).lower() for token in bitokens]</span><br><span class="line">bitokens = [token for token in bitokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">tritokens = [&apos; &apos;.join(token).lower() for token in tritokens]</span><br><span class="line">tritokens = [token for token in tritokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">ftokens = []</span><br><span class="line">ftokens.extend(tokens)</span><br><span class="line">ftokens.extend(bitokens)</span><br><span class="line">ftokens.extend(tritokens)</span><br><span class="line">docs[tip.text] = &#123;&apos;freq&apos;: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">for token in ftokens:</span><br><span class="line">docs[tip.text][&apos;freq&apos;][token] = freq(token, ftokens)</span><br><span class="line"></span><br><span class="line">print docs</span><br></pre></td></tr></table></figure>

</details>




<p>当文档被切分之后，下一个需要执行的步骤是<code>tf</code> 向量的标准化。让我们来看看下面用于执行<code>tf</code> 向量标准化的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">authen = OAuthHandler(CLIENT_ID, CLIENT_SECRET, CALLBACK)</span><br><span class="line">authen.set_access_token(ACCESS_TOKEN)</span><br><span class="line">ap = API(auth)</span><br><span class="line"></span><br><span class="line">venue = ap.venues(id=&apos;4bd47eeb5631c9b69672a230&apos;)</span><br><span class="line">stopwords = nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">tokenizer = RegexpTokenizer(&quot;[\w&apos;]+&quot;, flags=re.UNICODE)</span><br><span class="line"></span><br><span class="line">def freq(word, tokens):</span><br><span class="line">return tokens.count(word)</span><br><span class="line"></span><br><span class="line">def word_count(tokens):</span><br><span class="line">return len(tokens)</span><br><span class="line"></span><br><span class="line">def tf(word, tokens):</span><br><span class="line">return (freq(word, tokens) / float(word_count(tokens)))</span><br><span class="line"></span><br><span class="line">#Compute the frequency for each term.</span><br><span class="line">vocabulary = []</span><br><span class="line">docs = &#123;&#125;</span><br><span class="line">all_tips = []</span><br><span class="line">for tip in (venue.tips()):</span><br><span class="line">tokens = tokenizer.tokenize(tip.text)</span><br><span class="line"></span><br><span class="line">bitokens = bigrams(tokens)</span><br><span class="line">tritokens = trigrams(tokens)</span><br><span class="line">tokens = [token.lower() for token in tokens if len(token) &gt; 2]</span><br><span class="line">tokens = [token for token in tokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">bitokens = [&apos; &apos;.join(token).lower() for token in bitokens]</span><br><span class="line">bitokens = [token for token in bitokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">tritokens = [&apos; &apos;.join(token).lower() for token in tritokens]</span><br><span class="line">tritokens = [token for token in tritokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">ftokens = []</span><br><span class="line">ftokens.extend(tokens)</span><br><span class="line">ftokens.extend(bitokens)</span><br><span class="line">ftokens.extend(tritokens)</span><br><span class="line">docs[tip.text] = &#123;&apos;freq&apos;: &#123;&#125;, &apos;tf&apos;: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">for token in ftokens:</span><br><span class="line">         #The Computed Frequency</span><br><span class="line">docs[tip.text][&apos;freq&apos;][token] = freq(token, ftokens)</span><br><span class="line">         # Normalized Frequency</span><br><span class="line">docs[tip.text][&apos;tf&apos;][token] = tf(token, ftokens)</span><br><span class="line"></span><br><span class="line">print docs</span><br></pre></td></tr></table></figure>

</details>




<p>我们来看看以下用于计算TF-IDF值的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">authen = OAuthHandler(CLIENT_ID, CLIENT_SECRET, CALLBACK)</span><br><span class="line">authen.set_access_token(ACCESS_TOKEN)</span><br><span class="line">ap = API(authen)</span><br><span class="line"></span><br><span class="line">venue = ap.venues(id=&apos;4bd47eeb5631c9b69672a230&apos;)</span><br><span class="line">stopwords = nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">tokenizer = RegexpTokenizer(&quot;[\w&apos;]+&quot;, flags=re.UNICODE)</span><br><span class="line"></span><br><span class="line">def freq(word, doc):</span><br><span class="line">return doc.count(word)</span><br><span class="line"></span><br><span class="line">def word_count(doc):</span><br><span class="line">return len(doc)</span><br><span class="line"></span><br><span class="line">def tf(word, doc):</span><br><span class="line">return (freq(word, doc) / float(word_count(doc)))</span><br><span class="line"></span><br><span class="line">def num_docs_containing(word, list_of_docs):</span><br><span class="line">count = 0</span><br><span class="line">for document in list_of_docs:</span><br><span class="line">if freq(word, document) &gt; 0:</span><br><span class="line">count += 1</span><br><span class="line">return 1 + count</span><br><span class="line"></span><br><span class="line">def idf(word, list_of_docs):</span><br><span class="line">return math.log(len(list_of_docs) /</span><br><span class="line">float(num_docs_containing(word, list_of_docs)))</span><br><span class="line"></span><br><span class="line">#Compute the frequency for each term.</span><br><span class="line">vocabulary = []</span><br><span class="line">docs = &#123;&#125;</span><br><span class="line">all_tips = []</span><br><span class="line">for tip in (venue.tips()):</span><br><span class="line">tokens = tokenizer.tokenize(tip.text)</span><br><span class="line"></span><br><span class="line">bitokens = bigrams(tokens)</span><br><span class="line">tritokens = trigrams(tokens)</span><br><span class="line">tokens = [token.lower() for token in tokens if len(token) &gt; 2]</span><br><span class="line">tokens = [token for token in tokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">bitokens = [&apos; &apos;.join(token).lower() for token in bitokens]</span><br><span class="line">bitokens = [token for token in bitokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">tritokens = [&apos; &apos;.join(token).lower() for token in tritokens]</span><br><span class="line">tritokens = [token for token in tritokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">ftokens = []</span><br><span class="line">ftokens.extend(tokens)</span><br><span class="line">ftokens.extend(bitokens)</span><br><span class="line">ftokens.extend(tritokens)</span><br><span class="line">docs[tip.text] = &#123;&apos;freq&apos;: &#123;&#125;, &apos;tf&apos;: &#123;&#125;, &apos;idf&apos;: &#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line">for token in ftokens:</span><br><span class="line">        #The frequency computed for each tip</span><br><span class="line">docs[tip.text][&apos;freq&apos;][token] = freq(token, ftokens)</span><br><span class="line">        #The term-frequency (Normalized Frequency)</span><br><span class="line">docs[tip.text][&apos;tf&apos;][token] = tf(token, ftokens)</span><br><span class="line"></span><br><span class="line">vocabulary.append(ftokens)</span><br><span class="line"></span><br><span class="line">for doc in docs:</span><br><span class="line">for token in docs[doc][&apos;tf&apos;]:</span><br><span class="line">        #The Inverse-Document-Frequency</span><br><span class="line">docs[doc][&apos;idf&apos;][token] = idf(token, vocabulary)</span><br><span class="line"></span><br><span class="line">print docs</span><br></pre></td></tr></table></figure>

</details>




<p>可以通过找出TF和IDF的乘积来计算TF-IDF值。当出现高特征项频率和低文档频率时，计算所得到的TF-IDF值就比较大。</p>
<p>让我们来看看如下的代码，其用于计算文档中每个特征项的TF-IDF值：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">authen = OAuthHandler(CLIENT_ID, CLIENT_SECRET, CALLBACK)</span><br><span class="line">authen.set_access_token(ACCESS_TOKEN)</span><br><span class="line">ap = API(authen)</span><br><span class="line"></span><br><span class="line">venue = ap.venues(id=&apos;4bd47eeb5631c9b69672a230&apos;)</span><br><span class="line">stopwords = nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">tokenizer = RegexpTokenizer(&quot;[\w&apos;]+&quot;, flags=re.UNICODE)</span><br><span class="line"></span><br><span class="line">def freq(word, doc):</span><br><span class="line">return doc.count(word)</span><br><span class="line"></span><br><span class="line">def word_count(doc):</span><br><span class="line">return len(doc)</span><br><span class="line"></span><br><span class="line">def tf(word, doc):</span><br><span class="line">return (freq(word, doc) / float(word_count(doc)))</span><br><span class="line"></span><br><span class="line">def num_docs_containing(word, list_of_docs):</span><br><span class="line">count = 0</span><br><span class="line">for document in list_of_docs:</span><br><span class="line">if freq(word, document) &gt; 0:</span><br><span class="line">count += 1</span><br><span class="line">return 1 + count</span><br><span class="line"></span><br><span class="line">def idf(word, list_of_docs):</span><br><span class="line">return math.log(len(list_of_docs) /</span><br><span class="line">float(num_docs_containing(word, list_of_docs)))</span><br><span class="line"></span><br><span class="line">def tf_idf(word, doc, list_of_docs):</span><br><span class="line">return (tf(word, doc) * idf(word, list_of_docs))</span><br><span class="line"></span><br><span class="line">#Compute the frequency for each term.</span><br><span class="line">vocabulary = []</span><br><span class="line">docs = &#123;&#125;</span><br><span class="line">all_tips = []</span><br><span class="line">for tip in (venue.tips()):</span><br><span class="line">tokens = tokenizer.tokenize(tip.text)</span><br><span class="line"></span><br><span class="line">bitokens = bigrams(tokens)</span><br><span class="line">tritokens = trigrams(tokens)</span><br><span class="line">tokens = [token.lower() for token in tokens if len(token) &gt; 2]</span><br><span class="line">tokens = [token for token in tokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">bitokens = [&apos; &apos;.join(token).lower() for token in bitokens]</span><br><span class="line">bitokens = [token for token in bitokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">tritokens = [&apos; &apos;.join(token).lower() for token in tritokens]</span><br><span class="line">tritokens = [token for token in tritokens if token not in stopwords]</span><br><span class="line"></span><br><span class="line">ftokens = []</span><br><span class="line">ftokens.extend(tokens)</span><br><span class="line">ftokens.extend(bitokens)</span><br><span class="line">ftokens.extend(tritokens)</span><br><span class="line">docs[tip.text] = &#123;&apos;freq&apos;: &#123;&#125;, &apos;tf&apos;: &#123;&#125;, &apos;idf&apos;: &#123;&#125;,</span><br><span class="line">                        &apos;tf-idf&apos;: &#123;&#125;, &apos;tokens&apos;: []&#125;</span><br><span class="line"></span><br><span class="line">for token in ftokens:</span><br><span class="line">        #The frequency computed for each tip</span><br><span class="line">docs[tip.text][&apos;freq&apos;][token] = freq(token, ftokens)</span><br><span class="line">        #The term-frequency (Normalized Frequency)</span><br><span class="line">docs[tip.text][&apos;tf&apos;][token] = tf(token, ftokens)</span><br><span class="line">docs[tip.text][&apos;tokens&apos;] = ftokens</span><br><span class="line">vocabulary.append(ftokens)</span><br><span class="line"></span><br><span class="line">for doc in docs:</span><br><span class="line">for token in docs[doc][&apos;tf&apos;]:</span><br><span class="line">        #The Inverse-Document-Frequency</span><br><span class="line">docs[doc][&apos;idf&apos;][token] = idf(token, vocabulary)</span><br><span class="line">        #The tf-idf</span><br><span class="line">docs[doc][&apos;tf-idf&apos;][token] = tf_idf(token, docs[doc][&apos;tokens&apos;],</span><br><span class="line">vocabulary)</span><br><span class="line"></span><br><span class="line">#Now let&apos;s find out the most relevant words by tf-idf.</span><br><span class="line">words = &#123;&#125;</span><br><span class="line">for doc in docs:</span><br><span class="line">for token in docs[doc][&apos;tf-idf&apos;]:</span><br><span class="line">if token not in words:</span><br><span class="line">words[token] = docs[doc][&apos;tf-idf&apos;][token]</span><br><span class="line">else:</span><br><span class="line">if docs[doc][&apos;tf-idf&apos;][token] &gt; words[token]:</span><br><span class="line">words[token] = docs[doc][&apos;tf-idf&apos;][token]</span><br><span class="line"></span><br><span class="line">for item in sorted(words.items(), key=lambda x: x[1], reverse=True):</span><br><span class="line">print &quot;%f &lt;= %s&quot; % (item[1], item[0])</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下可用于映射关键词到向量维数的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def getVectkeyIndex(self,documentList):</span><br><span class="line">    vocabString=&quot; &quot;.join(documentList)</span><br><span class="line">    vocabList=self.parser.tokenise(vocabString)</span><br><span class="line">    vocabList=self.parser.removeStopWords(vocabList)</span><br><span class="line">    uniquevocabList=util.removeDuplicates(vocabList)</span><br><span class="line">    vectorIndex=&#123;&#125;</span><br><span class="line">    offset=0</span><br><span class="line"></span><br><span class="line">for word in uniquevocabList:</span><br><span class="line">        vectorIndex[word]=offset</span><br><span class="line">        offset+=1</span><br><span class="line">return vectorIndex</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下可用于映射文档字符串到向量的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def makeVect(self,wordString):</span><br><span class="line">    vector=[0]*len(self.vectorkeywordIndex)</span><br><span class="line">    wordList=self.parser.tokenise(wordString)</span><br><span class="line">    wordList=self.parser.removeStopWords(wordList)</span><br><span class="line">    for word in wordList:</span><br><span class="line">        vector[self.vectorkeywordIndex[word]]+=1;</span><br><span class="line">return vector</span><br></pre></td></tr></table></figure>

</details>




<h2 id="8-2-向量空间评分及查询操作符关联"><a href="#8-2-向量空间评分及查询操作符关联" class="headerlink" title="8.2 向量空间评分及查询操作符关联"></a>8.2 向量空间评分及查询操作符关联</h2><p>向量空间模型以词组向量的形式来表示意义。使用线性代数可以很容易地构建一个向量空间模型。因此，可以很容易地计算出向量间的相似度。</p>
<p>向量大小用于表示我们所使用的代表了特定上下文的向量的大小。对于上下文建模，可以使用基于窗口的方法和基于依赖的方法。在基于窗口的方法中，根据特定大小的窗口内出现的单词来确定上下文；在基于依赖的方法中，当存在一个单词与其相应的目标词具有特定的句法关系时，就可以确定上下文。可以对特征或上下文单词执行词干提取和词形还原。相似性度量可用于计算两个向量之间的相似性。</p>
<p>Let’s see the following list of similarity metrics：</p>
<p><img src="K1.jpg" alt></p>
<p>加权方案是另一个非常重要的术语，因为它提供了给定上下文中与目标词更相关的信息。</p>
<p>让我们来看看可以想到的加权方案列表：</p>
<p><img src="K2.jpg" alt></p>
<h2 id="8-3-使用隐性语义索引开发IR系统"><a href="#8-3-使用隐性语义索引开发IR系统" class="headerlink" title="8.3 使用隐性语义索引开发IR系统"></a>8.3 使用隐性语义索引开发IR系统</h2><p>在最小训练集的帮助下，隐性语义索引可用于执行文本分类。</p>
<p>隐性语义索引是一种可用于处理文本的技术，它可以执行以下任务：</p>
<ul>
<li>文本自动分类。</li>
<li>概念信息检索。</li>
<li>跨语言信息检索。</li>
</ul>
<p>隐性语义方法可以认为是一种信息检索和索引的方法，它使用了一种被称为奇异值矩阵分解（Singular Value Decomposition，SVD）的数学方法。SVD用于模式（与给定的非结构化文本中的概念具有特定关系）的识别。</p>
<p>隐性语义索引的一些应用如下：</p>
<ul>
<li>信息探索。</li>
<li>文档自动分类与文本摘要（电子探索、出版）。</li>
<li>关系探索。</li>
<li>自动生成个人和组织的链接图表。</li>
<li>将技术论文和资助与审阅者相匹配。</li>
<li>在线客服。</li>
<li>确定文档作者身份。</li>
<li>自动标注图像关键词。</li>
<li>理解软件源代码。</li>
<li>过滤垃圾邮件。</li>
<li>信息可视化。</li>
<li>论文评分。</li>
<li>基于文献的知识探索。</li>
</ul>
<h2 id="8-4-文本摘要"><a href="#8-4-文本摘要" class="headerlink" title="8.4 文本摘要"></a>8.4 文本摘要</h2><p>文本摘要是为一个给定的长文本生成摘要的过程。基于Luhn出版的著作 _The Automatic Creation of Literature Abstracts_<br>（1958），人们开发了一种被称作NaiveSumm的朴素归纳方法。它使用单词的频率来完成句子的计算和提取，这些句子由出现频率最高的单词组成。使用该方法，就可以通过提取少量特定的句子来完成文本摘要。</p>
<p>让我们来看看如下NLTK中可用于执行文本摘要的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">from nltk.tokenize import sent_tokenize,word_tokenize</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line">from collections import defaultdict</span><br><span class="line">from string import punctuation</span><br><span class="line">from heapq import nlargest</span><br><span class="line"></span><br><span class="line">class Summarize_Frequency:</span><br><span class="line">  def __init__(self, cut_min=0.2, cut_max=0.8):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">     Initilize the text summarizer.</span><br><span class="line">     Words that have a frequency term lower than cut_min</span><br><span class="line">     or higer than cut_max will be ignored.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self._cut_min = cut_min</span><br><span class="line">    self._cut_max = cut_max</span><br><span class="line">    self._stopwords = set(stopwords.words(&apos;english&apos;) +</span><br><span class="line">list(punctuation))</span><br><span class="line"></span><br><span class="line">  def _compute_frequencies(self, word_sent):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">     Compute the frequency of each of word.</span><br><span class="line">     Input:</span><br><span class="line">      word_sent, a list of sentences already tokenized.</span><br><span class="line">     Output:</span><br><span class="line">      freq, a dictionary where freq[w] is the frequency of w.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    freq = defaultdict(int)</span><br><span class="line">    for s in word_sent:</span><br><span class="line">      for word in s:</span><br><span class="line">        if word not in self._stopwords:</span><br><span class="line">          freq[word] += 1</span><br><span class="line">    # frequencies normalization and filtering</span><br><span class="line">    m = float(max(freq.values()))</span><br><span class="line">    for w in freq.keys():</span><br><span class="line">      freq[w] = freq[w]/m</span><br><span class="line">      if freq[w] &gt;= self._cut_max or freq[w] &lt;= self._cut_min:</span><br><span class="line">        del freq[w]</span><br><span class="line">    return freq</span><br><span class="line"></span><br><span class="line">  def summarize(self, text, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">list of (n) sentences are returned.</span><br><span class="line">summary of text is returned.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sents = sent_tokenize(text)</span><br><span class="line">    assert n &lt;= len(sents)</span><br><span class="line">    word_sent = [word_tokenize(s.lower()) for s in sents]</span><br><span class="line">    self._freq = self._compute_frequencies(word_sent)</span><br><span class="line">    ranking = defaultdict(int)</span><br><span class="line">    for i,sent in enumerate(word_sent):</span><br><span class="line">      for w in sent:</span><br><span class="line">        if w in self._freq:</span><br><span class="line">          ranking[i] += self._freq[w]</span><br><span class="line">    sents_idx = self._rank(ranking, n)</span><br><span class="line">    return [sents[j] for j in sents_idx]</span><br><span class="line"></span><br><span class="line">  def _rank(self, ranking, n):</span><br><span class="line">    &quot;&quot;&quot; return the first n sentences with highest ranking &quot;&quot;&quot;</span><br><span class="line">    return nlargest(n, ranking, key=ranking.get)</span><br></pre></td></tr></table></figure>

</details>




<p>在执行信息检索任务的过程中，以上代码计算了每个单词的词频，出现频率最高的单词如限定词等没有多大用处，因此可以删除。</p>
<h2 id="8-5-问答系统"><a href="#8-5-问答系统" class="headerlink" title="8.5 问答系统"></a>8.5 问答系统</h2><p>问答系统指的是智能系统，基于存储在知识库中的某些事实或规则，其可用于为用户的提问提供答案。因此一个问答系统提供正确答案的准确率取决于存储在知识库中的规则或事实。</p>
<p>问答系统中涉及的众多难题之一便是如何在系统中表示问题和答案。可以先检索出答案，然后使用文本摘要或文本解析来表示它。问答系统中涉及的另一个难题是如何在知识库中表示问题及其相应的答案。</p>
<p>为了构建一个问答系统，我们可以应用各种各样的方法，例如命名实体识别、信息检索、信息提取等。</p>
<p>问答系统包含三个阶段：</p>
<ul>
<li>提取事实。</li>
<li>理解问题。</li>
<li>生成答案。</li>
</ul>
<p>为了理解特定领域的数据并生成给定查询的响应，需要进行事实提取。</p>
<p>可以使用如下两种方式来执行事实的提取：提取实体和提取关系。实体或专有名词的提取过程被称作NER，关系的提取建立在从文本中提取的语义信息的基础之上。</p>
<p>理解问题涉及基于给定的文本来生成一个解析树。</p>
<p>生成答案涉及为给定的查询获取最有可能且能够被用户理解的答案。</p>
<p>让我们来看看如下NLTK中的代码，其可用于接受用户的查询。可以通过删除其停止词来处理该查询，以便可以在后续的处理步骤中执行信息检索。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line">from nltk import *</span><br><span class="line">import string</span><br><span class="line">print &quot;Enter your question&quot;</span><br><span class="line">ques=raw input()</span><br><span class="line">ques=ques.lower()</span><br><span class="line">stopwords=nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">cont=nltk.word_tokenize(question)</span><br><span class="line">analysis_keywords=list( set(cont) -set(stopwords) )</span><br></pre></td></tr></table></figure>

</details>




<h2 id="8-6-小结"><a href="#8-6-小结" class="headerlink" title="8.6 小结"></a>8.6 小结</h2><p>在本章中，我们讨论了信息检索，主要学习了停止词删除。为了可以更快地执行信息检索和文本摘要任务，我们删除了停止词。此外我们还讨论了文本摘要、问答系统和向量空间模型等的实现。</p>
<p>在下一章中，我们将学习语篇分析和指代消解的概念。</p>
<h1 id="第9章-语篇分析：理解才是可信的"><a href="#第9章-语篇分析：理解才是可信的" class="headerlink" title="第9章 语篇分析：理解才是可信的"></a>第9章 语篇分析：理解才是可信的</h1><p>语篇分析是自然语言处理的另一种应用。语篇分析可以认为是确定上下文信息（语境）的过程，这些信息有助于执行其他类型的任务，例如：指代消解（anaphora resolution，AR）（随后我们将在本章讨论这一部分内容）以及NER等。</p>
<p>本章将涵盖以下主题：</p>
<ul>
<li>语篇分析简介。</li>
<li>使用中心理论执行语篇分析。</li>
<li>指代消解。</li>
</ul>
<h2 id="9-1-语篇分析简介"><a href="#9-1-语篇分析简介" class="headerlink" title="9.1 语篇分析简介"></a>9.1 语篇分析简介</h2><p>语言学术语单词discourse是指使用中的语言。语篇分析可以认为是执行文本或语言分析的过程，其包含了文本解释以及对社交互动的理解。语篇分析可能涉及对语素、n元语法模型、时态、口语范畴以及页面布局等的处理。语篇可以认为是一个有序的句子集。</p>
<p>在大多数情况下，基于其前面的句子，我们可以解释一个句子的含义。</p>
<p>考虑如下话语” _John went to the club on Saturday. He met Sam_ .”，这里 _He_ 指的是John。</p>
<p>人们开发语篇表述理论（Discourse Representation Theory，DRT）用于提供执行AR的方法，开发语篇表述结构（Discourse Representation Structure，DRS）用于提供语篇的含义（在语篇指称对象和条件的帮助下）。语篇指称对象是指在一阶逻辑中使用的变量以及在语篇中正在考虑的事物。语篇表述结构的条件是指在一阶谓词逻辑中使用的原子公式。</p>
<p>人们开发一阶谓词逻辑（First Order Predicate Logic，FOPL）用于扩展命题逻辑的概念。FOPL涉及函数、参数和量词的使用。两种类型的量词（也就是通用量词和存在量词）用于表示常规的句子。在FOPL中，也使用了连接词、常量和变量，例如：<code>Robin is a bird</code> ，在FOPL中可以表示为<code>bird</code> （<code>robin</code> ）。</p>
<p>让我们来看一个有关语篇表述结构的例子，如图9-1所示。</p>
<p><img src="Image00012.gif" alt></p>
<p>图9-1</p>
<p>上图是下面句子的一种表述：</p>
<p>1．John went to a club.</p>
<p>2．John went to a club. He met Sam.</p>
<p>这里，该语篇由两个句子组成。语篇表述结构可以表述整个文本。为了通过计算来处理DRS，需要将其转换为线性格式。</p>
<p>NLTK中用于提供一阶谓词逻辑实现的模块是<code>nltk.sem.logic</code> ，它的UML图如图9-2所示。</p>
<p><code>nltk.sem.logic</code> 模块用于定义一阶谓词逻辑的表达式。该模块的UML类图包含各种类以及它们的方法，这些是在一阶谓词逻辑中表示对象所需要的，所包含的方法如下。</p>
<ul>
<li><code>substitute_bindings(bindings)</code> ：这里，binding表示变量到表达式的映射，它用一个特定的值替换了表达式中的变量。</li>
<li><code>variables()</code> ：该函数包含了所有需要被替换的变量集，此集合由常量以及自由变量组成。</li>
</ul>
<p><img src="Image00013.gif" alt></p>
<p>图9-2</p>
<ul>
<li><code>replace(variable, expression, replace_bound)</code> ：该函数用于替换变量实例的表达式，<code>replace_bound</code> 用于指定是否需要替换绑定的变量。</li>
<li><code>normalize()</code> ：该函数用于重命名自动生成的唯一变量。</li>
<li><code>visit(self,function,combinatory,default)</code> ：该函数用于访问调用函数的子表达式，然后结果被传递到以一个默认值开始的组合子，最终返回组合的结果。</li>
<li><code>free(indvar_only)</code> ：该函数用于返回对象的所有自由变量集。如果将<code>indvar_ only</code> 设置为<code>True</code> ，则返回独立的变量。</li>
<li><code>simplify()</code> ：用于简化表示对象的表达式。</li>
</ul>
<p>NLTK中为语篇表述理论提供了基础的模块是<code>nltk.sem.drt</code> ，它基于<code>nltk.sem. logic</code> 模块而构建，其UML类图由继承自<code>nltk.sem.logic</code> 模块的类而组成。以下是该模块中所描述的方法：</p>
<ul>
<li><code>get_refs(recursive)</code> ：该方法用于获得当前语篇的指称对象。</li>
<li><code>fol()</code> ：该方法用于将DRS转换为一阶谓词逻辑。</li>
<li><code>draw()</code> ：该方法在Tkinter图表库的帮助下用于绘制DRS。</li>
</ul>
<p>让我们来看看<code>nltk.sem.drt</code> 模块的UML类图如图9-3所示。</p>
<p><img src="Image00014.gif" alt></p>
<p>图9-3</p>
<p>NLTK中可用于访问WordNet 3.0的模块是<code>nltk.corpus.reader.wordnet</code> 。</p>
<p>线性格式由语篇指称对象和DRS条件组成，例如： _( [x], [John(x), Went(x)] )_ 。</p>
<p>让我们来看看如下NLTK中的代码，其可用于实现DRS：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; expr_read = nltk.sem.DrtExpression.from string</span><br><span class="line">&gt;&gt;&gt; expr1 = expr_read(&apos;([x], [John(x), Went(x)])&apos;)</span><br><span class="line">&gt;&gt;&gt; print(expr1)</span><br><span class="line">([x],[John(x), Went(x)])</span><br><span class="line">&gt;&gt;&gt; expr1.draw()</span><br><span class="line">&gt;&gt;&gt; print(expr1.fol())</span><br><span class="line">exists x.(John(x) &amp; Went(x))</span><br></pre></td></tr></table></figure>

</details>




<p>以上NLTK的代码将绘出如图9-4所示的图像。</p>
<p><img src="Image00015.jpg" alt></p>
<p>图9-4</p>
<p>这里，通过使用方法<code>fol()</code> ，表达式被转换为FOPL。</p>
<p>让我们来看看如下NLTK中有关另一个表达式的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; expr_read = nltk.sem.DrtExpression.fromstring</span><br><span class="line">&gt;&gt;&gt; expr2 = expr_read(&apos;([x,y], [John(x), Went(x),Sam(y),Meet(x,y)])&apos;)</span><br><span class="line">&gt;&gt;&gt; print(expr2)</span><br><span class="line">([x,y],[John(x), Went(x), Sam(y), Meet(x,y)])</span><br><span class="line">&gt;&gt;&gt; expr2.draw()</span><br><span class="line">&gt;&gt;&gt; print(expr2.fol())</span><br><span class="line">exists x y.(John(x) &amp; Went(x) &amp; Sam(y) &amp; Meet(x,y))</span><br></pre></td></tr></table></figure>

</details>




<p><code>fol()</code> 函数用于获取表达式的一阶谓词逻辑等价物。以上代码将显示如图9-5所示的图像。</p>
<p><img src="Image00016.jpg" alt></p>
<p>图9-5</p>
<p>通过使用DRS级联运算符（+），我们可以执行两个DRS的级联。让我们来看看如下NLTK中的代码，其可用于执行两个DRS的级联：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; expr_read = nltk.sem.DrtExpression.fromstring</span><br><span class="line">&gt;&gt;&gt; expr3 = expr_read(&apos;([x], [John(x), eats(x)])+</span><br><span class="line">([y],[Sam(y),eats(y)])&apos;)</span><br><span class="line">&gt;&gt;&gt; print(expr3)</span><br><span class="line">(([x],[John(x), eats(x)]) + ([y],[Sam(y), eats(y)]))</span><br><span class="line">&gt;&gt;&gt; print(expr3.simplify())</span><br><span class="line">([x,y],[John(x), eats(x), Sam(y), eats(y)])</span><br><span class="line">&gt;&gt;&gt; expr3.draw()</span><br></pre></td></tr></table></figure>

</details>




<p>以上代码绘出了如图9-6所示的图像。</p>
<p><img src="Image00017.jpg" alt></p>
<p>图9-6</p>
<p>这里，<code>simplify()</code> 函数用于简化表达式。</p>
<p>让我们来看看如下NLTK中的代码，其可用于将一个DRS嵌入到另一个当中：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; expr_read = nltk.sem.DrtExpression.fromstring</span><br><span class="line">&gt;&gt;&gt; expr4 = expr_read(&apos;([],[(([x],[student(x)])-</span><br><span class="line">&gt;([y],[book(y),read(x,y)]))])&apos;)</span><br><span class="line">&gt;&gt;&gt; print(expr4.fol())</span><br><span class="line">all x.(student(x) -&gt; exists y.(book(y) &amp; read(x,y)))</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看另一个例子，其可用于组合两个句子。这里使用了<code>PRO，resolve_ anaphora()</code> 函数用于执行AR：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; expr_read = nltk.sem.DrtExpression.fromstring</span><br><span class="line">&gt;&gt;&gt; expr5 = expr_read(&apos;([x,y],[ram(x),food(y),eats(x,y)])&apos;)</span><br><span class="line">&gt;&gt;&gt; expr6 = expr_read(&apos;([u,z],[PRO(u),coffee(z),drinks(u,z)])&apos;)</span><br><span class="line">&gt;&gt;&gt; expr7=expr5+expr6</span><br><span class="line">&gt;&gt;&gt; print(expr7.simplify())</span><br><span class="line">([u,x,y,z],[ram(x), food(y), eats(x,y), PRO(u), coffee(z),</span><br><span class="line">drinks(u,z)])</span><br><span class="line">&gt;&gt;&gt; print(expr7.simplify().resolve_anaphora())</span><br><span class="line">([u,x,y,z],[ram(x), food(y), eats(x,y), (u = [x,y,z]), coffee(z),</span><br><span class="line">drinks(u,z)])</span><br></pre></td></tr></table></figure>

</details>




<h3 id="9-1-1-使用中心理论执行语篇分析"><a href="#9-1-1-使用中心理论执行语篇分析" class="headerlink" title="9.1.1 使用中心理论执行语篇分析"></a>9.1.1 使用中心理论执行语篇分析</h3><p>使用中心理论执行语篇分析是进行语料库注解的第一步，它还包含执行AR的任务。在中心理论中，为了实现分析，我们需要执行将语篇分割成各种单元的任务。</p>
<p>中心理论包含以下内容：</p>
<ul>
<li>语篇参与者的目的或意图与语篇之间的互动。</li>
<li>参与者的注意。</li>
<li>语篇结构。</li>
</ul>
<p>中心与参与者的关注点以及局部和整体结构如何影响语篇的表达和连贯性有关。</p>
<h3 id="9-1-2-指代消解"><a href="#9-1-2-指代消解" class="headerlink" title="9.1.2 指代消解"></a>9.1.2 指代消解</h3><p>AR可以定义为这样的一个过程：通过AR可以解析句中所使用的代词或名词短语，并且基于语境信息来指代特定的实体。</p>
<p>例如：</p>
<pre><code>John helped Sara. He was kind.
</code></pre><p>这里，He指代的是John。</p>
<p>AR有三种类型，即：</p>
<ul>
<li><strong>代名词（Pronominal）</strong> ：这里，通过代词来指代指称对象。例如：Sam found the love of his life。这里，his指代Sam。</li>
<li><strong>有定名词短语（Definite noun phrase）</strong> ：这里，可以通过<code>&lt;the&gt; &lt;noun phrase&gt;</code> 形式来指代先行语。例如：<code>The relationship could not last long</code> 。这里，<code>The relationship</code> 指代的是上一句中的<code>the love</code> 。</li>
<li><strong>量词/序数</strong> ：量词（如one）和序数（如first）也是AR的例子。例如：<code>He began a new one</code> 。这里，<code>one</code> 指代的是<code>the relationship</code> 。</li>
</ul>
<p>在预指中，指称对象在先行语之前。例如：<code>After his class,Sam will go home</code> 。这里，<code>his</code> 指代的是<code>Sam</code> 。</p>
<p>为了在NLTK架构中集成某些扩展，人们基于现有的模块<code>nltk.sem.logic</code> 和<code>nltk.sem.drt</code> 开发出了一个新模块，新模块就像是<code>nltk.sem.drt</code> 模块的一个替代，用增强的类替换了所有的类。</p>
<p>来自类<code>AbstractDRS</code> 的一个叫作<code>resolve()</code> 的方法可以被间接和直接地调用，然后该方法可以提供一个列表，该列表由一个特定对象的已解析副本所组成，需要被解析的对象必须重写方法<code>readings()</code> 。通过使用<code>traverse()</code> 函数，<code>resolve()</code> 方法可用于生成读数，<code>traverse()</code> 函数可用于对操作列表执行排序。优先级顺序列表包括以下内容：</p>
<ul>
<li>绑定操作。</li>
<li>局部纳入操作。</li>
<li>中级纳入操作。</li>
<li>全局纳入操作。</li>
</ul>
<p>让我们来看看以下有关<code>traverse()</code> 函数执行的流程图如图9-7所示。</p>
<p><img src="Image00018.gif" alt></p>
<p>图9-7</p>
<p>当生成了操作的优先级顺序之后，将会发生如下操作：</p>
<ul>
<li>在<code>deepcopy()</code> 方法的帮助下从操作生成了readings，当前的操作将作为参数。</li>
<li>当运行<code>readings()</code> 函数时，执行了一系列的操作。</li>
<li>到操作列表不为空时，运行这些操作。</li>
<li>如果没有剩余的操作要执行，则将对最后的reading运行可接受性检查; 如果检查成功了，它将被存储。</li>
</ul>
<p>在<code>AbstractDrs</code> 类中定义了<code>resolve()</code> 方法，该方法定义如下： _def resolve(self, verbose=False)_ 。</p>
<p><code>PresuppositionDRS</code> 类包含以下方法：</p>
<ul>
<li><code>find_bindings(drs_list, collect_event_data)</code> ：通过使用<code>is_possible_ binding</code> 方法，可以从DRS实例列表中找出绑定项。如果将<code>collect_event_data</code> 设置为<code>True</code> ，那么就完成了参与信息的收集。</li>
<li><code>is_possible_binding(cond)</code> ：该函数用于找出条件是否为一个绑定候选项，并确保它是具有匹配触发条件的特征的一元谓词。</li>
<li><code>is_presupposition.cond(cond)</code> ：该方法用于在所有条件中标识出触发条件。</li>
<li><code>presupposition_readings(trail)</code> ：它类似于<code>PresuppositionDRS</code> 子类中的readings函数。</li>
</ul>
<p>让我们来看看继承自<code>AbstractDrs</code> 的类，如图9-8所示。</p>
<p><img src="Image00019.gif" alt></p>
<p>图9-8</p>
<p>让我们来看看继承自<code>DrtAbstractVariableExpression</code> 的类，如图9-9所示。</p>
<p><img src="Image00020.gif" alt></p>
<p>图9-9</p>
<p>让我们来看看继承自<code>DrtBooleanExpression</code> 的类，如图9-10所示。</p>
<p><img src="Image00021.gif" alt></p>
<p>图9-10</p>
<p>让我们来看看继承自<code>DrtApplicationExpression</code> 的类，如图9-11所示。</p>
<p><img src="Image00022.gif" alt></p>
<p>图9-11</p>
<p>让我们来看看继承自DRS的类，如图9-12所示。</p>
<p><img src="Image00023.gif" alt></p>
<p>图9-12</p>
<h2 id="9-2-小结"><a href="#9-2-小结" class="headerlink" title="9.2 小结"></a>9.2 小结</h2><p>在本章中，我们讨论了语篇分析、使用中心理论执行的语篇分析以及指代消解。我们也讨论了使用一阶谓词逻辑构建的语篇表述结构，通过UML图，我们还讨论了NLTK是如何用于实现一阶谓词逻辑的。</p>
<p>在下一章中，我们将讨论NLP工具的评估。我们还将讨论用于错误识别、词法匹配、语法匹配以及浅层语义匹配的各种指标。</p>
<h1 id="第10章-NLP系统评估：性能分析"><a href="#第10章-NLP系统评估：性能分析" class="headerlink" title="第10章 NLP系统评估：性能分析"></a>第10章 NLP系统评估：性能分析</h1><p>对NLP系统执行评估，以便我们可以分析一个给定的NLP系统是否产生了预期的结果以及是否实现了预期的性能。通过使用预定义的指标可以自动地执行评估，或者通过将人类的输出与NLP系统的输出进行对比来手工地执行评估。</p>
<p>本章将包含以下主题：</p>
<ul>
<li>NLP系统评估要点。</li>
<li>NLP工具评估（词性标注器、词干提取器及形态分析器）。</li>
<li>使用黄金数据执行解析器评估。</li>
<li>IR系统的评估。</li>
<li>错误识别指标。</li>
<li>基于词汇搭配的指标。</li>
<li>基于句法匹配的指标。</li>
<li>使用浅层语义匹配的指标。</li>
</ul>
<h2 id="10-1-NLP系统评估要点"><a href="#10-1-NLP系统评估要点" class="headerlink" title="10.1 NLP系统评估要点"></a>10.1 NLP系统评估要点</h2><p>对NLP系统执行评估是为了分析NLP系统的输出结果是否与人类的某个输出结果相似。如果能够在早期阶段就识别出模块中的错误，那么将在很大程度上降低NLP系统的校正成本。</p>
<p>假设我们想要评估一个标注器，我们可以通过对比标注器与人类的输出结果来完成该操作。很多时候，我们都无法找到一个公正或者可以被称为专家的人，因此我们可以构建一个黄金标准测试数据来执行标注器的评估。它是一个手工标注过的语料库，并被认为是一个可用于标注器评估的标准语料库。如果标注器给出的标记形式的输出与黄金标准测试数据提供的输出相同，我们则认为标注器是正确的。</p>
<p>创建黄金标准注释语料库是一项主要的任务，而且其实现成本也是非常昂贵的。它通过手工标注给定的测试数据来完成该操作。以这种方式筛选的标记被视为标准标记，其可用于表示大范围的信息。</p>
<h3 id="10-1-1-NLP工具的评估（词性标注器、词干提取器及形态分析器）"><a href="#10-1-1-NLP工具的评估（词性标注器、词干提取器及形态分析器）" class="headerlink" title="10.1.1 NLP工具的评估（词性标注器、词干提取器及形态分析器）"></a>10.1.1 NLP工具的评估（词性标注器、词干提取器及形态分析器）</h3><p>我们可以对各种NLP系统执行评估，例如词性标注器、词干提取器、形态分析器、基于NER的系统以及机器翻译器等。考虑如下NLTK中的代码，其可用于训练一个一元语法标注器。执行完句子标注后，对其进行评估，以便验证标注器给出的输出是否与黄金标准测试数据给出的输出相同：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; sentences=brown.tagged_sents(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; sent=brown.sents(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; unigram_sent=nltk.UnigramTagger(sentences)</span><br><span class="line">&gt;&gt;&gt; unigram_sent.tag(sent[2008])</span><br><span class="line">[(&apos;Others&apos;, &apos;NNS&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;which&apos;, &apos;WDT&apos;), (&apos;are&apos;, &apos;BER&apos;),</span><br><span class="line">(&apos;reached&apos;, &apos;VBN&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;walking&apos;, &apos;VBG&apos;), (&apos;up&apos;, &apos;RP&apos;),</span><br><span class="line">(&apos;a&apos;, &apos;AT&apos;), (&apos;single&apos;, &apos;AP&apos;), (&apos;flight&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;),</span><br><span class="line">(&apos;stairs&apos;, &apos;NNS&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;have&apos;, &apos;HV&apos;), (&apos;balconies&apos;, &apos;NNS&apos;),</span><br><span class="line">(&apos;.&apos;, &apos;.&apos;)]</span><br><span class="line">&gt;&gt;&gt; unigram_sent.evaluate(sentences)</span><br><span class="line">0.9349006503968017</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中的代码，其用分离的数据对一元语法标注器执行了训练和测试。给定的数据被分为80%的训练数据和20%的测试数据：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; sentences=brown.tagged_sents(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; sz=int(len(sentences)*0.8)</span><br><span class="line">&gt;&gt;&gt; sz</span><br><span class="line">3698</span><br><span class="line">&gt;&gt;&gt; training_sents = sentences[:sz]</span><br><span class="line">&gt;&gt;&gt; testing_sents=sentences[sz:]</span><br><span class="line">&gt;&gt;&gt; unigram_tagger=nltk.UnigramTagger(training_sents)</span><br><span class="line">&gt;&gt;&gt; unigram_tagger.evaluate(testing_sents)</span><br><span class="line">0.8028325063827737</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中的代码，其展示了N-Gram（N元语法）标注器的使用。这里，训练语料库是由标注过的数据组成的。另外，在下面的代码示例中，我们使用了N- Gram标注器的一个特例，即bigram（二元语法）标注器：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; sentences=brown.tagged_sents(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; sz=int(len(sentences)*0.8)</span><br><span class="line">&gt;&gt;&gt; training_sents = sentences[:sz]</span><br><span class="line">&gt;&gt;&gt; testing_sents=sentences[sz:]</span><br><span class="line">&gt;&gt;&gt; bigram_tagger=nltk.UnigramTagger(training_sents)</span><br><span class="line">&gt;&gt;&gt; bigram_tagger=nltk.BigramTagger(training_sents)</span><br><span class="line">&gt;&gt;&gt; bigram_tagger.tag(sentences[2008])</span><br><span class="line">[((&apos;Others&apos;, &apos;NNS&apos;), None), ((&apos;,&apos;, &apos;,&apos;), None), ((&apos;which&apos;, &apos;WDT&apos;),</span><br><span class="line">None), ((&apos;are&apos;, &apos;BER&apos;), None), ((&apos;reached&apos;, &apos;VBN&apos;), None), ((&apos;by&apos;,</span><br><span class="line">&apos;IN&apos;), None), ((&apos;walking&apos;, &apos;VBG&apos;), None), ((&apos;up&apos;, &apos;IN&apos;), None), ((&apos;a&apos;,</span><br><span class="line">&apos;AT&apos;), None), ((&apos;single&apos;, &apos;AP&apos;), None), ((&apos;flight&apos;, &apos;NN&apos;), None),</span><br><span class="line">((&apos;of&apos;, &apos;IN&apos;), None), ((&apos;stairs&apos;, &apos;NNS&apos;), None), ((&apos;,&apos;, &apos;,&apos;), None),</span><br><span class="line">((&apos;have&apos;, &apos;HV&apos;), None), ((&apos;balconies&apos;, &apos;NNS&apos;), None), ((&apos;.&apos;, &apos;.&apos;),</span><br><span class="line">None)]</span><br><span class="line">&gt;&gt;&gt; un_sent=sentences[4203]</span><br><span class="line">&gt;&gt;&gt; bigram_tagger.tag(un_sent)</span><br><span class="line">[((&apos;The&apos;, &apos;AT&apos;), None), ((&apos;population&apos;, &apos;NN&apos;), None), ((&apos;of&apos;, &apos;IN&apos;),</span><br><span class="line">None), ((&apos;the&apos;, &apos;AT&apos;), None), ((&apos;Congo&apos;, &apos;NP&apos;), None), ((&apos;is&apos;, &apos;BEZ&apos;),</span><br><span class="line">None), ((&apos;13.5&apos;, &apos;CD&apos;), None), ((&apos;million&apos;, &apos;CD&apos;), None), ((&apos;,&apos;,</span><br><span class="line">&apos;,&apos;), None), ((&apos;divided&apos;, &apos;VBN&apos;), None), ((&apos;into&apos;, &apos;IN&apos;), None),</span><br><span class="line">((&apos;at&apos;, &apos;IN&apos;), None), ((&apos;least&apos;, &apos;AP&apos;), None), ((&apos;seven&apos;, &apos;CD&apos;),</span><br><span class="line">None), ((&apos;major&apos;, &apos;JJ&apos;), None), ((&apos;``&apos;, &apos;``&apos;), None), ((&apos;culture&apos;,</span><br><span class="line">&apos;NN&apos;), None), ((&apos;clusters&apos;, &apos;NNS&apos;), None), ((&quot;&apos;&apos;&quot;, &quot;&apos;&apos;&quot;), None),</span><br><span class="line">((&apos;and&apos;, &apos;CC&apos;), None), ((&apos;innumerable&apos;, &apos;JJ&apos;), None), ((&apos;tribes&apos;,</span><br><span class="line">&apos;NNS&apos;), None), ((&apos;speaking&apos;, &apos;VBG&apos;), None), ((&apos;400&apos;, &apos;CD&apos;), None),</span><br><span class="line">((&apos;separate&apos;, &apos;JJ&apos;), None), ((&apos;dialects&apos;, &apos;NNS&apos;), None), ((&apos;.&apos;, &apos;.&apos;),</span><br><span class="line">None)]</span><br><span class="line">&gt;&gt;&gt; bigram_tagger.evaluate(testing_sents)</span><br><span class="line">0.09181559805385615</span><br></pre></td></tr></table></figure>

</details>




<p>另一种标注方式可以通过不同的抽样方法来实现。在这种方法中，可以通过bigram标注器来执行词性标注。如果一个标记在bigram标注器中找不到，则可以使用unigram标注器的回退方法。当然，如果一个标记在unigram标注器中找不到，则可以使用默认标注器的回退方法。</p>
<p>让我们来看看如下NLTK中实现了组合标注器的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; sentences=brown.tagged_sents(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; sz=int(len(sentences)*0.8)</span><br><span class="line">&gt;&gt;&gt; training_sents = sentences[:sz]</span><br><span class="line">&gt;&gt;&gt; testing_sents=sentences[sz:]</span><br><span class="line">&gt;&gt;&gt; s0=nltk.DefaultTagger(&apos;NNP&apos;)</span><br><span class="line">&gt;&gt;&gt; s1=nltk.UnigramTagger(training_sents,backoff=s0)</span><br><span class="line">&gt;&gt;&gt; s2=nltk.BigramTagger(training_sents,backoff=s1)</span><br><span class="line">&gt;&gt;&gt; s2.evaluate(testing_sents)</span><br><span class="line">0.8122260224480948</span><br></pre></td></tr></table></figure>

</details>




<p>语言学家使用以下线索来确定一个单词的类别：</p>
<ul>
<li>形态线索。</li>
<li>句法线索。</li>
<li>语义线索。</li>
</ul>
<p>形态线索指的是用于确定单词类别的前缀、后缀、中缀和词缀等信息。例如，ment是与动词结合形成名词的后缀，比如establish + ment = establishment 和achieve + ment = achievement。</p>
<p>句法线索在确定一个单词的类别时很有用。例如，假设我们已经知道了名词，那么现在就可以确定形容词了。在一个句子中，形容词可以出现在一个名词或者一个单词之后，例如very。</p>
<p>语义信息也可以用于确定单词的类别。如果已经知道了一个单词的含义，那么我们就能够很容易地知道它的类别。</p>
<p>让我们来看看如下NLTK中的代码，其可用于语块解析器的评估：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; chunkparser = nltk.RegexpParser(&quot;&quot;)</span><br><span class="line">&gt;&gt;&gt; print(nltk.chunk.accuracy(chunkparser, nltk.corpus.conll2000.</span><br><span class="line">chunked_sents(&apos;train.txt&apos;, chunk_types=(&apos;NP&apos;,))))</span><br><span class="line">0.44084599507856814</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看另一段NLTK中的代码，其可用于朴素语块解析器的评估，该解析器可用于寻找诸如CD、JJ等标记：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; grammar = r&quot;NP: &#123;&lt;[CDJNP].*&gt;+&#125;&quot;</span><br><span class="line">&gt;&gt;&gt; cp = nltk.RegexpParser(grammar)</span><br><span class="line">&gt;&gt;&gt; print(nltk.chunk.accuracy(cp, nltk.corpus.conll2000.chunked_</span><br><span class="line">sents(&apos;train.txt&apos;, chunk_types=(&apos;NP&apos;,))))</span><br><span class="line">0.8744798726662164</span><br></pre></td></tr></table></figure>

</details>




<p>下面的NLTK代码用于计算分块数据的条件频率分布：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def chunk_tags(train):</span><br><span class="line">    &quot;&quot;&quot;Generate a following tags list that appears inside chunks&quot;&quot;&quot;</span><br><span class="line">    cfreqdist = nltk.ConditionalFreqDist()</span><br><span class="line">    for t in train:</span><br><span class="line">        for word, tag, chunktag in nltk.chunk.tree2conlltags(t):</span><br><span class="line">            if chtag == &quot;O&quot;:</span><br><span class="line">                cfreqdist[tag].inc(False)</span><br><span class="line">            else:</span><br><span class="line">                cfreqdist[tag].inc(True)</span><br><span class="line">    return [tag for tag in cfreqdist.conditions() if cfreqdist[tag].</span><br><span class="line">max() == True]</span><br><span class="line">&gt;&gt;&gt; training_sents = nltk.corpus.conll2000.chunked_sents(&apos;train.txt&apos;,</span><br><span class="line">chunk_types=(&apos;NP&apos;,))</span><br><span class="line">&gt;&gt;&gt; print chunked_tags(train_sents)</span><br><span class="line">[&apos;PRP$&apos;, &apos;WDT&apos;, &apos;JJ&apos;, &apos;WP&apos;, &apos;DT&apos;, &apos;#&apos;, &apos;$&apos;, &apos;NN&apos;, &apos;FW&apos;, &apos;POS&apos;,</span><br><span class="line">&apos;PRP&apos;, &apos;NNS&apos;, &apos;NNP&apos;, &apos;PDT&apos;, &apos;RBS&apos;, &apos;EX&apos;, &apos;WP$&apos;, &apos;CD&apos;, &apos;NNPS&apos;, &apos;JJS&apos;,</span><br><span class="line">&apos;JJR&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下NLTK中用于执行<code>chunker</code> 评估的代码。这里，使用了两种称为guessed和correct的实体。guessed实体是由语块解析器返回的那些实体，correct实体是那些在测试语料库中定义的语块集：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; correct = nltk.chunk.tagstr2tree(</span><br><span class="line">&quot;[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]&quot;)</span><br><span class="line">&gt;&gt;&gt; print(correct.flatten())</span><br><span class="line">(S the/DT little/JJ cat/NN sat/VBD on/IN the/DT mat/NN)</span><br><span class="line">&gt;&gt;&gt; grammar = r&quot;NP: &#123;&lt;[CDJNP].*&gt;+&#125;&quot;</span><br><span class="line">&gt;&gt;&gt; cp = nltk.RegexpParser(grammar)</span><br><span class="line">&gt;&gt;&gt; grammar = r&quot;NP: &#123;&lt;PRP|DT|POS|JJ|CD|N.*&gt;+&#125;&quot;</span><br><span class="line">&gt;&gt;&gt; chunk_parser = nltk.RegexpParser(grammar)</span><br><span class="line">&gt;&gt;&gt; tagged_tok = [(&quot;the&quot;, &quot;DT&quot;), (&quot;little&quot;, &quot;JJ&quot;), (&quot;cat&quot;,</span><br><span class="line">&quot;NN&quot;),(&quot;sat&quot;, &quot;VBD&quot;), (&quot;on&quot;, &quot;IN&quot;), (&quot;the&quot;, &quot;DT&quot;), (&quot;mat&quot;, &quot;NN&quot;)]</span><br><span class="line">&gt;&gt;&gt; chunkscore = nltk.chunk.ChunkScore()</span><br><span class="line">&gt;&gt;&gt; guessed = cp.parse(correct.flatten())</span><br><span class="line">&gt;&gt;&gt; chunkscore.score(correct, guessed)</span><br><span class="line">&gt;&gt;&gt; print(chunkscore)</span><br><span class="line">ChunkParse score:</span><br><span class="line">    IOB Accuracy: 100.0%</span><br><span class="line">    Precision:    100.0%</span><br><span class="line">    Recall:       100.0%</span><br><span class="line">    F-Measure:    100.0%</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下NLTK中的代码，其可用于评估一元语法<code>chunker</code> 和二元语法 <code>chunker</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; chunker_data = [[(t,c) for w,t,c in nltk.chunk.</span><br><span class="line">tree2conlltags(chtree)]</span><br><span class="line">&gt;&gt;&gt; for chtree in nltk.corpus.conll2000.chunked_</span><br><span class="line">sents(&apos;train.txt&apos;)]</span><br><span class="line">&gt;&gt;&gt; unigram_chunk = nltk.UnigramTagger(chunker_data)</span><br><span class="line">&gt;&gt;&gt; print nltk.tag.accuracy(unigram_chunk, chunker_data)</span><br><span class="line">0.781378851068</span><br><span class="line">&gt;&gt;&gt; bigram_chunk = nltk.BigramTagger(chunker_data, backoff=unigram_</span><br><span class="line">chunker)</span><br><span class="line">&gt;&gt;&gt; print nltk.tag.accuracy(bigram_chunk, chunker_data)</span><br><span class="line">0.893220987404</span><br></pre></td></tr></table></figure>

</details>




<p>考虑下面的代码，其中单词的后缀用于确定词性标记，因此需要训练一个分类器用于提供一个有益的后缀列表。我们还使用了一个特征提取函数来检测给定的单词中所呈现的后缀：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; suffix_freqdist = nltk.FreqDist()</span><br><span class="line">&gt;&gt;&gt; for wrd in brown.words():</span><br><span class="line">...     wrd = wrd.lower()</span><br><span class="line">...     suffix_freqdist[wrd[-1:]] += 1</span><br><span class="line">...     suffix_fdist[wrd[-2:]] += 1</span><br><span class="line">...     suffix_fdist[wrd[-3:]] += 1</span><br><span class="line">&gt;&gt;&gt; common_suffixes = [suffix for (suffix, count) in suffix_freqdist.</span><br><span class="line">most_common(100)]</span><br><span class="line">&gt;&gt;&gt; print(common_suffixes)</span><br><span class="line">[&apos;e&apos;, &apos;,&apos;, &apos;.&apos;, &apos;s&apos;, &apos;d&apos;, &apos;t&apos;, &apos;he&apos;, &apos;n&apos;, &apos;a&apos;, &apos;of&apos;, &apos;the&apos;,</span><br><span class="line"> &apos;y&apos;, &apos;r&apos;, &apos;to&apos;, &apos;in&apos;, &apos;f&apos;, &apos;o&apos;, &apos;ed&apos;, &apos;nd&apos;, &apos;is&apos;, &apos;on&apos;, &apos;l&apos;,</span><br><span class="line"> &apos;g&apos;, &apos;and&apos;, &apos;ng&apos;, &apos;er&apos;, &apos;as&apos;, &apos;ing&apos;, &apos;h&apos;, &apos;at&apos;, &apos;es&apos;, &apos;or&apos;,</span><br><span class="line"> &apos;re&apos;, &apos;it&apos;, &apos;``&apos;, &apos;an&apos;, &quot;&apos;&apos;&quot;, &apos;m&apos;, &apos;;&apos;, &apos;i&apos;, &apos;ly&apos;, &apos;ion&apos;, ...]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; def pos_feature(wrd):</span><br><span class="line">...     feature = &#123;&#125;</span><br><span class="line">...     for suffix in common_suffixes:</span><br><span class="line">...         feature[&apos;endswith(&#123;&#125;)&apos;.format(suffix)] = wrd.lower().</span><br><span class="line">endswith(suffix)</span><br><span class="line">...     return feature</span><br><span class="line">&gt;&gt;&gt; tagged_wrds = brown.tagged_wrds(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; featureset = [(pos_feature(n), g) for (n,g) in tagged_wrds]</span><br><span class="line">&gt;&gt;&gt; size = int(len(featureset) * 0.1)</span><br><span class="line">&gt;&gt;&gt; train_set, test_set = featureset[size:], featureset[:size]</span><br><span class="line">&gt;&gt;&gt; classifier1 = nltk.DecisionTreeClassifier.train(train_set)</span><br><span class="line">&gt;&gt;&gt; nltk.classify.accuracy(classifier1, test_set)</span><br><span class="line">0.62705121829935351</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; classifier.classify(pos_features(&apos;cats&apos;))</span><br><span class="line">&apos;NNS&apos;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(classifier.pseudocode(depth=4))</span><br><span class="line">if endswith(,) == True: return &apos;,&apos;</span><br><span class="line">if endswith(,) == False:</span><br><span class="line">  if endswith(the) == True: return &apos;AT&apos;</span><br><span class="line">  if endswith(the) == False:</span><br><span class="line">    if endswith(s) == True:</span><br><span class="line">      if endswith(is) == True: return &apos;BEZ&apos;</span><br><span class="line">      if endswith(is) == False: return &apos;VBZ&apos;</span><br><span class="line">    if endswith(s) == False:</span><br><span class="line">      if endswith(.) == True: return &apos;.&apos;</span><br><span class="line">      if endswith(.) == False: return &apos;NN&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中的代码，其用于构建一个正则表达式标注器。这里，基于匹配模式进行了标记的分配：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; sentences = brown.tagged_sents(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; sent = brown.sents(categories=&apos;news&apos;)</span><br><span class="line">&gt;&gt;&gt; pattern = [</span><br><span class="line">(r&apos;.*ing$&apos;, &apos;VBG&apos;),              # for gerunds</span><br><span class="line">(r&apos;.*ed$&apos;, &apos;VBD&apos;),               # for simple past</span><br><span class="line">(r&apos;.*es$&apos;, &apos;VBZ&apos;),               # for 3rd singular present</span><br><span class="line">(r&apos;.*ould$&apos;, &apos;MD&apos;),              # for modals</span><br><span class="line">(r&apos;.*\&apos;s$&apos;, &apos;NN$&apos;),              # for possessive nouns</span><br><span class="line">(r&apos;.*s$&apos;, &apos;NNS&apos;),                # for plural nouns</span><br><span class="line">(r&apos;^-?[0-9]+(.[0-9]+)?$&apos;, &apos;CD&apos;), # for cardinal numbers</span><br><span class="line">(r&apos;.*&apos;, &apos;NN&apos;)                    # for nouns (default)</span><br><span class="line">]</span><br><span class="line">&gt;&gt;&gt; regexpr_tagger = nltk.RegexpTagger(pattern)</span><br><span class="line">&gt;&gt;&gt; regexpr_tagger.tag(sent[3])</span><br><span class="line"> [(&apos;``&apos;, &apos;NN&apos;), (&apos;Only&apos;, &apos;NN&apos;), (&apos;a&apos;, &apos;NN&apos;), (&apos;relative&apos;, &apos;NN&apos;),</span><br><span class="line">(&apos;handful&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;NN&apos;), (&apos;such&apos;, &apos;NN&apos;), (&apos;reports&apos;, &apos;NNS&apos;),</span><br><span class="line">(&apos;was&apos;, &apos;NNS&apos;), (&apos;received&apos;, &apos;VBD&apos;), (&quot;&apos;&apos;&quot;, &apos;NN&apos;), (&apos;,&apos;, &apos;NN&apos;),</span><br><span class="line">(&apos;the&apos;, &apos;NN&apos;), (&apos;jury&apos;, &apos;NN&apos;), (&apos;said&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;NN&apos;), (&apos;``&apos;,</span><br><span class="line">&apos;NN&apos;), (&apos;considering&apos;, &apos;VBG&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;widespread&apos;, &apos;NN&apos;),</span><br><span class="line">(&apos;interest&apos;, &apos;NN&apos;), (&apos;in&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;election&apos;, &apos;NN&apos;),</span><br><span class="line">(&apos;,&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;number&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;NN&apos;), (&apos;voters&apos;,</span><br><span class="line">&apos;NNS&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;size&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;NN&apos;),</span><br><span class="line">(&apos;this&apos;, &apos;NNS&apos;), (&apos;city&apos;, &apos;NN&apos;), (&quot;&apos;&apos;&quot;, &apos;NN&apos;), (&apos;.&apos;, &apos;NN&apos;)]</span><br><span class="line">&gt;&gt;&gt; regexp_tagger.evaluate(sentences)</span><br><span class="line">0.20326391789486245</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下用于构建查找标注器的代码。在构建查找标注器的过程中，需要维护一个常用单词及其对应的标记信息的列表。因为一些单词不在最常用的单词列表中，所以它们被分配了<code>None</code> 标记：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.corpus import brown</span><br><span class="line">&gt;&gt;&gt; freqd = nltk.FreqDist(brown.words(categories=&apos;news&apos;))</span><br><span class="line">&gt;&gt;&gt; cfreqd = nltk.ConditionalFreqDist(brown.tagged_</span><br><span class="line">words(categories=&apos;news&apos;))</span><br><span class="line">&gt;&gt;&gt; mostfreq_words = freqd.most_common(100)</span><br><span class="line">&gt;&gt;&gt; likelytags = dict((word, cfreqd[word].max()) for (word, _) in</span><br><span class="line">mostfreq_words)</span><br><span class="line">&gt;&gt;&gt; baselinetagger = nltk.UnigramTagger(model=likelytags)</span><br><span class="line">&gt;&gt;&gt; baselinetagger.evaluate(brown_tagged_sents)</span><br><span class="line">0.45578495136941344</span><br><span class="line">&gt;&gt;&gt; sent = brown.sents(categories=&apos;news&apos;)[3]</span><br><span class="line">&gt;&gt;&gt; baselinetagger.tag(sent)</span><br><span class="line">[(&apos;``&apos;, &apos;``&apos;), (&apos;Only&apos;, None), (&apos;a&apos;, &apos;AT&apos;), (&apos;relative&apos;, None),</span><br><span class="line">(&apos;handful&apos;, None), (&apos;of&apos;, &apos;IN&apos;), (&apos;such&apos;, None), (&apos;reports&apos;, None),</span><br><span class="line">(&apos;was&apos;, &apos;BEDZ&apos;), (&apos;received&apos;, None), (&quot;&apos;&apos;&quot;, &quot;&apos;&apos;&quot;), (&apos;,&apos;, &apos;,&apos;),</span><br><span class="line">(&apos;the&apos;, &apos;AT&apos;), (&apos;jury&apos;, None), (&apos;said&apos;, &apos;VBD&apos;), (&apos;,&apos;, &apos;,&apos;),</span><br><span class="line">(&apos;``&apos;, &apos;``&apos;), (&apos;considering&apos;, None), (&apos;the&apos;, &apos;AT&apos;), (&apos;widespread&apos;,</span><br><span class="line">None),</span><br><span class="line">(&apos;interest&apos;, None), (&apos;in&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;election&apos;, None),</span><br><span class="line">(&apos;,&apos;, &apos;,&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;number&apos;, None), (&apos;of&apos;, &apos;IN&apos;),</span><br><span class="line">(&apos;voters&apos;, None), (&apos;and&apos;, &apos;CC&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;size&apos;, None),</span><br><span class="line">(&apos;of&apos;, &apos;IN&apos;), (&apos;this&apos;, &apos;DT&apos;), (&apos;city&apos;, None), (&quot;&apos;&apos;&quot;, &quot;&apos;&apos;&quot;), (&apos;.&apos;,</span><br><span class="line">&apos;.&apos;)]</span><br><span class="line">&gt;&gt;&gt; baselinetagger = nltk.UnigramTagger(model=likely_tags,</span><br><span class="line">...                                        backoff=nltk.</span><br><span class="line">DefaultTagger(&apos;NN&apos;))</span><br><span class="line">def performance(cfreqd, wordlist):</span><br><span class="line">    lt = dict((word, cfreqd[word].max()) for word in wordlist)</span><br><span class="line">    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.</span><br><span class="line">DefaultTagger(&apos;NN&apos;))</span><br><span class="line">    return baseline_tagger.evaluate(brown.tagged_</span><br><span class="line">sents(categories=&apos;news&apos;))</span><br><span class="line"></span><br><span class="line">def display():</span><br><span class="line">    import pylab</span><br><span class="line">    word_freqs = nltk.FreqDist(brown.words(categories=&apos;news&apos;)).most_</span><br><span class="line">common()</span><br><span class="line">    words_by_freq = [w for (w, _) in word_freqs]</span><br><span class="line">    cfd = nltk.ConditionalFreqDist(brown.tagged_</span><br><span class="line">words(categories=&apos;news&apos;))</span><br><span class="line">    sizes = 2 ** pylab.arange(15)</span><br><span class="line">    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]</span><br><span class="line">    pylab.plot(sizes, perfs, &apos;-bo&apos;)</span><br><span class="line">    pylab.title(&apos;Lookup Tagger Performance with Varying Model Size&apos;)</span><br><span class="line">    pylab.xlabel(&apos;Model Size&apos;)</span><br><span class="line">    pylab.ylabel(&apos;Performance&apos;)</span><br><span class="line">    pylab.show()</span><br><span class="line">display()</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来看看如下NLTK中使用了<code>lancasterstemmer</code> 进行词干提取的代码。通过使用黄金测试数据，我们可以完成这样一个<code>stemmer</code> 的评估：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.stem.lancaster import LancasterStemmer</span><br><span class="line">&gt;&gt;&gt; stri=LancasterStemmer()</span><br><span class="line">&gt;&gt;&gt; stri.stem(&apos;achievement&apos;)</span><br><span class="line">&apos;achiev&apos;</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中的代码，其可用于设计一个基于分类的<code>chunker</code> 。它使用了最大熵分类器：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class ConseNPChunkTagger(nltk.TaggerI):</span><br><span class="line"></span><br><span class="line">    def __init__(self, train_sents):</span><br><span class="line">        train_set = []</span><br><span class="line">        for tagsent in train_sents:</span><br><span class="line">            untagsent = nltk.tag.untag(tagsent)</span><br><span class="line">            history = []</span><br><span class="line">            for i, (word, tag) in enumerate(tagsent):</span><br><span class="line">                featureset = npchunk_features(untagsent, i, history)</span><br><span class="line">                train_set.append( (featureset, tag) )</span><br><span class="line">                history.append(tag)</span><br><span class="line">        self.classifier = nltk.MaxentClassifier.train(</span><br><span class="line">            train_set, algorithm=&apos;megam&apos;, trace=0)</span><br><span class="line"></span><br><span class="line">    def tag(self, sentence):</span><br><span class="line">        history = []</span><br><span class="line">        for i, word in enumerate(sentence):</span><br><span class="line">            featureset = npchunk_features(sentence, i, history)</span><br><span class="line">            tag = self.classifier.classify(featureset)</span><br><span class="line">            history.append(tag)</span><br><span class="line">        return zip(sentence, history)</span><br><span class="line"></span><br><span class="line">class ConseNPChunker(nltk.ChunkParserI): [4]</span><br><span class="line">    def __init__(self, train_sents):</span><br><span class="line">        tagsent = [[((w,t),c) for (w,t,c) in</span><br><span class="line">                         nltk.chunk.tree2conlltags(sent)]</span><br><span class="line">                        for sent in train_sents]</span><br><span class="line">        self.tagger = ConseNPChunkTagger(tagsent)</span><br><span class="line"></span><br><span class="line">    def parse(self, sentence):</span><br><span class="line">        tagsent = self.tagger.tag(sentence)</span><br><span class="line">        conlltags = [(w,t,c) for ((w,t),c) in tagsent]</span><br><span class="line">        return nltk.chunk.conlltags2tree(conlltags)</span><br></pre></td></tr></table></figure>

</details>




<p>下面的代码，通过使用一个特征提取器执行了<code>chunker</code> 的评估。该<code>chunker</code> 的评估结果类似于一元语法<code>chunker</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def npchunk_features(sentence, i, history):</span><br><span class="line">...     word, pos = sentence[i]</span><br><span class="line">...     return &#123;&quot;pos&quot;: pos&#125;</span><br><span class="line">&gt;&gt;&gt; chunker = ConseNPChunker(train_sents)</span><br><span class="line">&gt;&gt;&gt; print(chunker.evaluate(test_sents))</span><br><span class="line">ChunkParse score:</span><br><span class="line">    IOB Accuracy:    92.9%</span><br><span class="line">    Precision:       79.9%</span><br><span class="line">    Recall:          86.7%</span><br><span class="line">    F-Measure:       83.2%</span><br></pre></td></tr></table></figure>

</details>




<p>在下面的代码中，前一个词性标记的特征也被添加了。这包括了标记之间的互动，所以该<code>chunker</code> 的评估结果类似于二元语法<code>chunker</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def npchunk_features(sentence, i, history):</span><br><span class="line">...     word, pos = sentence[i]</span><br><span class="line">...     if i == 0:</span><br><span class="line">...        previword, previpos = &quot;&lt;START&gt;&quot;, &quot;&lt;START&gt;&quot;</span><br><span class="line">...    else:</span><br><span class="line">...        previword, previpos = sentence[i-1]</span><br><span class="line">...    return &#123;&quot;pos&quot;: pos, &quot;previpos&quot;: previpos&#125;</span><br><span class="line">&gt;&gt;&gt; chunker = ConseNPChunker(train_sents)</span><br><span class="line">&gt;&gt;&gt; print(chunker.evaluate(test_sents))</span><br><span class="line">ChunkParse score:</span><br><span class="line">    IOB Accuracy:    93.6%</span><br><span class="line">    Precision:       81.9%</span><br><span class="line">    Recall:          87.2%</span><br><span class="line">    F-Measure:       84.5%</span><br></pre></td></tr></table></figure>

</details>




<p>考虑以下有关<code>chunker</code> 的代码，其中添加了当前单词的特征以便提高<code>chunker</code> 的性能：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def npchunk_features(sentence, i, history):</span><br><span class="line">...     word, pos = sentence[i]</span><br><span class="line">...     if i == 0:</span><br><span class="line">...         previword, previpos = &quot;&lt;START&gt;&quot;, &quot;&lt;START&gt;&quot;</span><br><span class="line">...     else:</span><br><span class="line">...         previword, previpos = sentence[i-1]</span><br><span class="line">...     return &#123;&quot;pos&quot;: pos, &quot;word&quot;: word, &quot;previpos&quot;: previpos&#125;</span><br><span class="line">&gt;&gt;&gt; chunker = ConseNPChunker(train_sents)</span><br><span class="line">&gt;&gt;&gt; print(chunker.evaluate(test_sents))</span><br><span class="line">ChunkParse score:</span><br><span class="line">    IOB Accuracy: 94.5%</span><br><span class="line">    Precision: 84.2%</span><br><span class="line">    Recall: 89.4%</span><br><span class="line">    F-Measure: 86.7%</span><br></pre></td></tr></table></figure>

</details>




<p>让我们来考虑如下NLTK中的代码，其中添加了特征集，例如配对特征、前瞻特征、复杂上下文特征等，以便增强<code>chunker</code> 的性能：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def npchunk_features(sentence, i, history):</span><br><span class="line">...     word, pos = sentence[i]</span><br><span class="line">...     if i == 0:</span><br><span class="line">...          previword, previpos = &quot;&lt;START&gt;&quot;, &quot;&lt;START&gt;&quot;</span><br><span class="line">...     else:</span><br><span class="line">...         previword, previpos = sentence[i-1]</span><br><span class="line">...     if i == len(sentence)-1:</span><br><span class="line">...         nextword, nextpos = &quot;&lt;END&gt;&quot;, &quot;&lt;END&gt;&quot;</span><br><span class="line">...     else:</span><br><span class="line">...         nextword, nextpos = sentence[i+1]</span><br><span class="line">...     return &#123;&quot;pos&quot;: pos,</span><br><span class="line">...             &quot;word&quot;: word,</span><br><span class="line">...             &quot;previpos&quot;: previpos,</span><br><span class="line">...             &quot;nextpos&quot;: nextpos,</span><br><span class="line">...             &quot;previpos+pos&quot;: &quot;%s+%s&quot; % (previpos, pos),</span><br><span class="line">...             &quot;pos+nextpos&quot;: &quot;%s+%s&quot; % (pos, nextpos),</span><br><span class="line">...             &quot;tags-since-dt&quot;: tags_since_dt(sentence, i)&#125;</span><br><span class="line">&gt;&gt;&gt; def tags_since_dt(sentence, i):</span><br><span class="line">...     tags = set()</span><br><span class="line">...     for word, pos in sentence[:i]:</span><br><span class="line">...         if pos == &apos;DT&apos;:</span><br><span class="line">...             tags = set()</span><br><span class="line">...         else:</span><br><span class="line">...             tags.add(pos)</span><br><span class="line">...     return &apos;+&apos;.join(sorted(tags))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; chunker = ConsecutiveNPChunker(train_sents)</span><br><span class="line">&gt;&gt;&gt; print(chunker.evaluate(test_sents))</span><br><span class="line">ChunkParse score:</span><br><span class="line">    IOB Accuracy:  96.0%</span><br><span class="line">    Precision:     88.6%</span><br><span class="line">    Recall:        91.0%</span><br><span class="line">    F-Measure:     89.8%</span><br></pre></td></tr></table></figure>

</details>




<p>通过使用黄金数据，我们也可以执行形态分析器的评估。人类预期的输出已经被存储用于形成一个黄金数据集合，然后将形态分析器的输出与黄金数据进行比较。</p>
<h3 id="10-1-2-使用黄金数据执行解析器评估"><a href="#10-1-2-使用黄金数据执行解析器评估" class="headerlink" title="10.1.2 使用黄金数据执行解析器评估"></a>10.1.2 使用黄金数据执行解析器评估</h3><p>通过使用黄金数据或解析器的输出所对应的标准数据，我们可以执行解析器的评估。</p>
<p>首先，在训练数据上执行解析器模型的训练，然后在不可见数据或测试数据上执行解析。</p>
<p>以下两个手段可用于解析器性能的评估：</p>
<ul>
<li>标记的依恋评分（Labelled Attachment Score，LAS）。</li>
<li>标记的精确匹配（Labelled Exact Match，LEM）。</li>
</ul>
<p>在以上两种情况下，解析器的输出均与测试数据进行了比较。一个好的解析算法是可以给出最高LAS和LEM评分的算法。我们用于解析的训练数据和测试数据可以由黄金标准标记词性标记来组成，因为它们已经被手工地分配了。通过使用以下指标可以执行解析器的评估，例如召回率（Recall）、精确率（Precision）和F值（F-Measure）。</p>
<p>这里，精确率可以被定义为由解析器产生的正确实体的数量除以解析器产生的实体的总数。</p>
<p>召回率可以被定义为由解析器产生的正确实体的数量除以黄金标准解析树中的实体的总数。</p>
<p>F值可以认为是召回率和精确率的调和平均值。</p>
<h2 id="10-2-IR系统的评估"><a href="#10-2-IR系统的评估" class="headerlink" title="10.2 IR系统的评估"></a>10.2 IR系统的评估</h2><p>IR也是自然语言处理的应用之一。</p>
<p>以下是在执行IR系统的评估时需要考虑的几个方面：</p>
<ul>
<li>所需资源。</li>
<li>文档的表述。</li>
<li>市场评估或用户黏性。</li>
<li>检索速度。</li>
<li>构建查询时的协助。</li>
<li>查找所需文档的能力。</li>
</ul>
<p>我们通常将一个系统与另一个系统进行比较来执行评估。</p>
<p>IR系统可以基于文档集、查询集以及所使用的技术等进行比较。用于性能评估的指标有精确率（Precision）、召回率（Recall）和F值（F-Measure）。让我们来进一步了解它们：</p>
<ul>
<li><p>精确率：被定义为相关检索集的比例。</p>
<p>_Precision = |relevant ∩ retrieved| ÷ |retrieved| = P_ ( _relevant | retrieved_ )</p>
</li>
<li><p>召回率：被定义为包括在检索集中的所有相关文档集的比例。</p>
<p>_Recall = |relevant ∩ retrieved| ÷ |relevant| = P_ ( _retrieved | relevant_ )</p>
</li>
<li><p>F值：其可以通过使用精确率和召回率来获取，具体如下：</p>
<p>_F-Measure =_ (2 _<em>Precision</em>Recall_ ) _/_ ( _Precision + Recall_ )</p>
</li>
</ul>
<h2 id="10-3-错误识别指标"><a href="#10-3-错误识别指标" class="headerlink" title="10.3 错误识别指标"></a>10.3 错误识别指标</h2><p>错误识别是一个非常重要的可影响NLP系统性能的方面。搜索任务可能涉及以下术语：</p>
<ul>
<li><strong>真正</strong> （ <strong>True Positive</strong> ， <strong>TP</strong> ）：它可以被定义为被正确识别为相关文档的相关文档集。</li>
<li><strong>真负</strong> （ <strong>True Negative</strong> ， <strong>TN</strong> ）：它可以被定义为被正确识别为无关文档的无关文档集。</li>
<li><strong>假正</strong> （ <strong>False Positive</strong> ， <strong>FP</strong> ）：它也被称为错误类型I，是被错误地识别为相关文档的无关文档集。</li>
<li><strong>假负</strong> （ <strong>False Negative</strong> ， <strong>FN</strong> ）：它也被称为错误类型II，是被错误地识别为无关文档的相关文档集。</li>
</ul>
<p>基于前面所提到的术语，我们得到了如下指标：</p>
<ul>
<li><strong>精确率</strong> _(P)_ ： _TP/(TP+FP)_ 。</li>
<li><strong>召回率</strong> _(R)_ ： _TP/(TP+FN)_ 。</li>
<li><strong>F值：2</strong> _<em>P</em>R/(P+R)_ 。</li>
</ul>
<h2 id="10-4-基于词汇搭配的指标"><a href="#10-4-基于词汇搭配的指标" class="headerlink" title="10.4 基于词汇搭配的指标"></a>10.4 基于词汇搭配的指标</h2><p>我们还可以基于单词或词汇层面来执行性能分析。</p>
<p>考虑如下NLTK中的代码，其中已经采用了影评，并将其标记为积极的或消极的。为了检测一个给定的单词是否存在于文档中，我们构建了一个特征提取器：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.corpus import movie_reviews</span><br><span class="line">&gt;&gt;&gt; docs = [(list(movie_reviews.words(fileid)), category)</span><br><span class="line">...              for category in movie_reviews.categories()</span><br><span class="line">...              for fileid in movie_reviews.fileids(category)]</span><br><span class="line">&gt;&gt;&gt; random.shuffle(docs)</span><br><span class="line">all_wrds = nltk.FreqDist(w.lower() for w in movie_reviews.words())</span><br><span class="line">word_features = list(all_wrds)[:2000]</span><br><span class="line"></span><br><span class="line">def doc_features(doc):</span><br><span class="line">    doc_words = set(doc)</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    for word in word_features:</span><br><span class="line">        features[&apos;contains(&#123;&#125;)&apos;.format(word)] = (word in doc_words)</span><br><span class="line">    return features</span><br><span class="line">&gt;&gt;&gt; print(doc_features(movie_reviews.words(&apos;pos/cv957_8737.txt&apos;)))</span><br><span class="line">&#123;&apos;contains(waste)&apos;: False, &apos;contains(lot)&apos;: False, ...&#125;</span><br><span class="line">featuresets = [(doc_features(d), c) for (d,c) in docs]</span><br><span class="line">train_set, test_set = featuresets[100:], featuresets[:100]</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line">&gt;&gt;&gt; print(nltk.classify.accuracy(classifier, test_set))</span><br><span class="line">0.81</span><br><span class="line">&gt;&gt;&gt; classifier.show_most_informative_features(5)</span><br><span class="line">Most Informative Features</span><br><span class="line">    contains(outstanding) = True pos : neg = 11.1 :</span><br><span class="line">1.0</span><br><span class="line">        contains(seagal) = True neg : pos = 7.7 :</span><br><span class="line">1.0</span><br><span class="line">    contains(wonderfully) = True pos : neg = 6.8 :</span><br><span class="line">1.0</span><br><span class="line">        contains(damon) = True pos : neg = 5.9 :</span><br><span class="line">1.0</span><br><span class="line">        contains(wasted) = True neg : pos = 5.8 :</span><br><span class="line">1.0</span><br></pre></td></tr></table></figure>

</details>




<p>考虑如下NLTK中用于描述<code>nltk.metrics.distance</code> 模块的代码，它提供了用于确定给定的输出与预期的输出是否相同的指标：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">from __future__ import division</span><br><span class="line">def _edit_dist_init(len1, len2):</span><br><span class="line">    lev = []</span><br><span class="line">    for i in range(len1):</span><br><span class="line">        lev.append([0] * len2) # initialization of 2D array to zero</span><br><span class="line">    for i in range(len1):</span><br><span class="line">        lev[i][0] = i # column 0: 0,1,2,3,4,...</span><br><span class="line">    for j in range(len2):</span><br><span class="line">        lev[0][j] = j # row 0: 0,1,2,3,4,...</span><br><span class="line">    return lev</span><br><span class="line"></span><br><span class="line">def _edit_dist_step(lev, i, j, s1, s2, transpositions=False):</span><br><span class="line">    c1 = s1[i - 1]</span><br><span class="line">    c2 = s2[j - 1]</span><br><span class="line"></span><br><span class="line">    # skipping a character in s1</span><br><span class="line">    a = lev[i - 1][j] + 1</span><br><span class="line">    # skipping a character in s2</span><br><span class="line">    b = lev[i][j - 1] + 1</span><br><span class="line">    # substitution</span><br><span class="line">    c = lev[i - 1][j - 1] + (c1 != c2)</span><br><span class="line"></span><br><span class="line">    # transposition</span><br><span class="line">    d = c + 1 # never picked by default</span><br><span class="line">    if transpositions and i &gt; 1 and j &gt; 1:</span><br><span class="line">        if s1[i - 2] == c2 and s2[j - 2] == c1:</span><br><span class="line">            d = lev[i - 2][j - 2] + 1</span><br><span class="line"></span><br><span class="line">    # pick the cheapest</span><br><span class="line">    lev[i][j] = min(a, b, c, d)</span><br><span class="line"></span><br><span class="line">def edit_distance(s1, s2, transpositions=False):</span><br><span class="line"></span><br><span class="line">    # set up a 2-D array</span><br><span class="line">    len1 = len(s1)</span><br><span class="line">    len2 = len(s2)</span><br><span class="line">    lev = _edit_dist_init(len1 + 1, len2 + 1)</span><br><span class="line"></span><br><span class="line">    # iterate over the array</span><br><span class="line">    for i in range(len1):</span><br><span class="line">        for j in range(len2):</span><br><span class="line">            _edit_dist_step(lev, i + 1, j + 1, s1, s2,</span><br><span class="line">transpositions=transpositions)</span><br><span class="line">    return lev[len1][len2]</span><br><span class="line"></span><br><span class="line">def binary_distance(label1, label2):</span><br><span class="line">    &quot;&quot;&quot;Simple equality test.</span><br><span class="line"></span><br><span class="line">    0.0    if the labels are identical, 1.0 if they are different.</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; from nltk.metrics import binary_distance</span><br><span class="line">&gt;&gt;&gt; binary_distance(1,1)</span><br><span class="line">    0.0</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; binary_distance(1,3)</span><br><span class="line">    1.0</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    return 0.0 if label1 == label2 else 1.0</span><br><span class="line"></span><br><span class="line">def jaccard_distance(label1, label2):</span><br><span class="line">    &quot;&quot;&quot;Distance metric comparing set-similarity.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return (len(label1.union(label2)) - len(label1.</span><br><span class="line">intersection(label2)))/len(label1.union(label2))</span><br><span class="line"></span><br><span class="line">def masi_distance(label1, label2)</span><br><span class="line"></span><br><span class="line">    len_intersection = len(label1.intersection(label2))</span><br><span class="line">    len_union = len(label1.union(label2))</span><br><span class="line">    len_label1 = len(label1)</span><br><span class="line">    len_label2 = len(label2)</span><br><span class="line">    if len_label1 == len_label2 and len_label1 == len_intersection:</span><br><span class="line">        m = 1</span><br><span class="line">    elif len_intersection == min(len_label1, len_label2):</span><br><span class="line">        m = 0.67</span><br><span class="line">    elif len_intersection &gt; 0:</span><br><span class="line">        m = 0.33</span><br><span class="line">    else:</span><br><span class="line">        m = 0</span><br><span class="line">    return 1 - (len_intersection / len_union) * m</span><br><span class="line"></span><br><span class="line">def interval_distance(label1,label2):</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        return pow(label1 - label2, 2)</span><br><span class="line">#        return pow(list(label1)[0]-list(label2)[0],2)</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;non-numeric labels not supported with interval</span><br><span class="line">distance&quot;)</span><br><span class="line"></span><br><span class="line">def presence(label):</span><br><span class="line"></span><br><span class="line">    return lambda x, y: 1.0 * ((label in x) == (label in y))</span><br><span class="line"></span><br><span class="line">def fractional_presence(label):</span><br><span class="line">    return lambda x, y:\</span><br><span class="line">        abs(((1.0 / len(x)) - (1.0 / len(y)))) * (label in x and label</span><br><span class="line">in y) \</span><br><span class="line">        or 0.0 * (label not in x and label not in y) \</span><br><span class="line">        or abs((1.0 / len(x))) * (label in x and label not in y) \</span><br><span class="line">        or ((1.0 / len(y))) * (label not in x and label in y)</span><br><span class="line"></span><br><span class="line">def custom_distance(file):</span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    with open(file, &apos;r&apos;) as infile:</span><br><span class="line">        for l in infile:</span><br><span class="line">            labelA, labelB, dist = l.strip().split(&quot;\t&quot;)</span><br><span class="line">            labelA = frozenset([labelA])</span><br><span class="line">            labelB = frozenset([labelB])</span><br><span class="line">            data[frozenset([labelA,labelB])] = float(dist)</span><br><span class="line">    return lambda x,y:data[frozenset([x,y])]</span><br><span class="line"></span><br><span class="line">def demo():</span><br><span class="line">    edit_distance_examples = [</span><br><span class="line">        (&quot;rain&quot;, &quot;shine&quot;), (&quot;abcdef&quot;, &quot;acbdef&quot;), (&quot;language&quot;,</span><br><span class="line">&quot;lnaguaeg&quot;),</span><br><span class="line">        (&quot;language&quot;, &quot;lnaugage&quot;), (&quot;language&quot;, &quot;lngauage&quot;)]</span><br><span class="line">    for s1, s2 in edit_distance_examples:</span><br><span class="line"></span><br><span class="line">        print(&quot;Edit distance between &apos;%s&apos; and &apos;%s&apos;:&quot; % (s1, s2), edit_</span><br><span class="line">distance(s1, s2))</span><br><span class="line">    for s1, s2 in edit_distance_examples:</span><br><span class="line">        print(&quot;Edit distance with transpositions between &apos;%s&apos; and</span><br><span class="line">&apos;%s&apos;:&quot; % (s1, s2), edit_distance(s1, s2, transpositions=True))</span><br><span class="line"></span><br><span class="line">    s1 = set([1, 2, 3, 4])</span><br><span class="line">    s2 = set([3, 4, 5])</span><br><span class="line">    print(&quot;s1:&quot;, s1)</span><br><span class="line">    print(&quot;s2:&quot;, s2)</span><br><span class="line">    print(&quot;Binary distance:&quot;, binary_distance(s1, s2))</span><br><span class="line">    print(&quot;Jaccard distance:&quot;, jaccard_distance(s1, s2))</span><br><span class="line">    print(&quot;MASI distance:&quot;, masi_distance(s1, s2))</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    demo()</span><br></pre></td></tr></table></figure>

</details>




<h2 id="10-5-基于句法匹配的指标"><a href="#10-5-基于句法匹配的指标" class="headerlink" title="10.5 基于句法匹配的指标"></a>10.5 基于句法匹配的指标</h2><p>句法匹配可以通过执行分块任务来完成。NLTK中提供了一个叫作<code>nltk.chunk.api</code> 的模块，其有助于识别语块并为给定的语块序列返回一个解析树。</p>
<p>名为<code>nltk.chunk.named_entity</code> 的模块用于识别一个命名实体列表并生成一个解析结构。考虑如下NLTK中基于句法匹配的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import nltk</span><br><span class="line">&gt;&gt;&gt; from nltk.tree import Tree</span><br><span class="line">&gt;&gt;&gt; print(Tree(1,[2,Tree(3,[4]),5]))</span><br><span class="line">(1 2 (3 4) 5)</span><br><span class="line">&gt;&gt;&gt; ct=Tree(&apos;VP&apos;,[Tree(&apos;V&apos;,[&apos;gave&apos;]),Tree(&apos;NP&apos;,[&apos;her&apos;])])</span><br><span class="line">&gt;&gt;&gt; sent=Tree(&apos;S&apos;,[Tree(&apos;NP&apos;,[&apos;I&apos;]),ct])</span><br><span class="line">&gt;&gt;&gt; print(sent)</span><br><span class="line">(S (NP I) (VP (V gave) (NP her)))</span><br><span class="line">&gt;&gt;&gt; print(sent[1])</span><br><span class="line">(VP (V gave) (NP her))</span><br><span class="line">&gt;&gt;&gt; print(sent[1,1])</span><br><span class="line">(NP her)</span><br><span class="line">&gt;&gt;&gt; t1=Tree.from string(&quot;(S(NP I) (VP (V gave) (NP her)))&quot;)</span><br><span class="line">&gt;&gt;&gt; sent==t1</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; t1[1][1].set_label(&apos;X&apos;)</span><br><span class="line">&gt;&gt;&gt; t1[1][1].label()</span><br><span class="line">&apos;X&apos;</span><br><span class="line">&gt;&gt;&gt; print(t1)</span><br><span class="line">(S (NP I) (VP (V gave) (X her)))</span><br><span class="line">&gt;&gt;&gt; t1[0],t1[1,1]=t1[1,1],t1[0]</span><br><span class="line">&gt;&gt;&gt; print(t1)</span><br><span class="line">(S (X her) (VP (V gave) (NP I)))</span><br><span class="line">&gt;&gt;&gt; len(t1)</span><br><span class="line">2</span><br></pre></td></tr></table></figure>

</details>




<h2 id="10-6-使用浅层语义匹配的指标"><a href="#10-6-使用浅层语义匹配的指标" class="headerlink" title="10.6 使用浅层语义匹配的指标"></a>10.6 使用浅层语义匹配的指标</h2><p>WordNet相似度用于执行语义匹配，在此过程中，可以计算出给定文本与其相应假设的相似度。可使用自然语言工具包来计算文本中的单词与其相应假设的相似度：path distance、Leacock-Chodorow Similarity、Wu-Palmer Similarity、Resnik Similarity、Jiang-Conrath Similarity以及 Lin Similarity。在这些指标中，我们比较的不是单词的相似度，而是词义的相似度。</p>
<p>在浅层语义分析的过程中，也同时执行了NER和共指消解任务。</p>
<p>考虑如下NLTK中用于计算<code>wordnet</code> 相似度的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; wordnet.N[&apos;dog&apos;][0].path_similarity(wordnet.N[&apos;cat&apos;][0])</span><br><span class="line">0.20000000000000001</span><br><span class="line">&gt;&gt;&gt; wordnet.V[&apos;run&apos;][0].path_similarity(wordnet.V[&apos;walk&apos;][0])</span><br><span class="line">0.25</span><br></pre></td></tr></table></figure>

</details>




<h2 id="10-7-小结"><a href="#10-7-小结" class="headerlink" title="10.7 小结"></a>10.7 小结</h2><p>在本章中，我们讨论了各种NLP系统（词性标注器、词干提取器和形态分析器）的评估。你已经学习了分别基于错误识别、词汇搭配、句法匹配、浅层语义匹配的各种指标，它们可用于执行NLP系统的评估。我们还讨论了使用黄金数据来执行的解析器评估，可以使用三个指标来执行评估，即精确率、召回率和F值。此外，你还学习了IR系统的评估。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      <div>    
       
       
      <ul class="post-copyright">
        <li class="post-copyright-author">
            <strong>本文作者：</strong>hac_lang
        </li>
        <li class="post-copyright-link">
          <strong>本文链接：</strong>
          <a href="/2017/11/06/book-《精通Python自然语言处理》/" title="book_《精通Python自然语言处理》">2017/11/06/book-《精通Python自然语言处理》/</a>
        </li>
        <li class="post-copyright-license">
          <strong>版权声明： </strong>
          许可协议，请勿用于商业，转载注明出处！
        </li>
      </ul>
      
      </div>
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/计算机科学/" rel="tag"># 计算机科学</a>
          
            <a href="/tags/计算机/" rel="tag"># 计算机</a>
          
            <a href="/tags/自评/" rel="tag"># 自评</a>
          
            <a href="/tags/books/" rel="tag"># books</a>
          
            <a href="/tags/更毕/" rel="tag"># 更毕</a>
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
            <a href="/tags/码农/" rel="tag"># 码农</a>
          
            <a href="/tags/AI/" rel="tag"># AI</a>
          
            <a href="/tags/nobutdunbuy/" rel="tag"># nobutdunbuy</a>
          
            <a href="/tags/豆瓣4/" rel="tag"># 豆瓣4</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/06/book-《白话深度学习与TensorFlow》/" rel="next" title="book_《白话深度学习与TensorFlow》">
                <i class="fa fa-chevron-left"></i> book_《白话深度学习与TensorFlow》
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/18/book-《深度学习》-Goodfellow_花书/" rel="prev" title="book_《深度学习》_Goodfellow_花书">
                book_《深度学习》_Goodfellow_花书 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
    <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="hac_lang">
  
  <p class="site-author-name" itemprop="name">hac_lang</p>
  <div class="site-description motion-element" itemprop="description">小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理</div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    

    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>







  <div class="links-of-author motion-element">
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://github.com/HACLANG" title="GitHub &rarr; https://github.com/HACLANG" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://stackoverflow.com/yourname" title="StackOverflow &rarr; https://stackoverflow.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://gitter.im" title="Gitter &rarr; https://gitter.im" rel="noopener" target="_blank"><i class="fa fa-fw fa-github-alt"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.jianshu.com/u/442ddccf3f32" title="简书 &rarr; https://www.jianshu.com/u/442ddccf3f32" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="Quora &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://plus.google.com/yourname" title="Google &rarr; https://plus.google.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="mailto:haclang.org@gmail.com" title="E-Mail &rarr; mailto:haclang.org@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="skype:haclang?call|chat" title="Skype &rarr; skype:haclang?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://twitter.com/haclang2" title="Twitter &rarr; https://twitter.com/haclang2" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="FaceBook &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
    
  </div>








          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#内容提要"><span class="nav-text">内容提要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#作者简介"><span class="nav-text">作者简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#审阅者简介"><span class="nav-text">审阅者简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#译者简介"><span class="nav-text">译者简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-text">前言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#本书涵盖内容"><span class="nav-text">本书涵盖内容</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#本书的阅读前提"><span class="nav-text">本书的阅读前提</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#本书的目标读者"><span class="nav-text">本书的目标读者</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#排版约定"><span class="nav-text">排版约定</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#读者反馈"><span class="nav-text">读者反馈</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#客户支持"><span class="nav-text">客户支持</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#下载示例代码"><span class="nav-text">下载示例代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#勘误"><span class="nav-text">勘误</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反盗版"><span class="nav-text">反盗版</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#问题"><span class="nav-text">问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-字符串操作"><span class="nav-text">第1章 字符串操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-切分"><span class="nav-text">1.1 切分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-1-将文本切分为语句"><span class="nav-text">1.1.1 将文本切分为语句</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-其他语言文本的切分"><span class="nav-text">1.1.2 其他语言文本的切分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-3-将句子切分为单词"><span class="nav-text">1.1.3 将句子切分为单词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-4-使用TreebankWordTokenizer执行切分"><span class="nav-text">1.1.4 使用TreebankWordTokenizer执行切分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-5-使用正则表达式实现切分"><span class="nav-text">1.1.5 使用正则表达式实现切分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-标准化"><span class="nav-text">1.2 标准化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-消除标点符号"><span class="nav-text">1.2.1 消除标点符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-文本的大小写转换"><span class="nav-text">1.2.2 文本的大小写转换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-3-处理停止词"><span class="nav-text">1.2.3 处理停止词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-4-计算英语中的停止词"><span class="nav-text">1.2.4 计算英语中的停止词</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-替换和校正标识符"><span class="nav-text">1.3 替换和校正标识符</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-使用正则表达式替换单词"><span class="nav-text">1.3.1 使用正则表达式替换单词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-用其他文本替换文本的示例"><span class="nav-text">1.3.2 用其他文本替换文本的示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-在执行切分前先执行替换操作"><span class="nav-text">1.3.3 在执行切分前先执行替换操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-处理重复字符"><span class="nav-text">1.3.4 处理重复字符</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-5-去除重复字符的示例"><span class="nav-text">1.3.5 去除重复字符的示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-6-用单词的同义词替换"><span class="nav-text">1.3.6 用单词的同义词替换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-7-用单词的同义词替换的示例"><span class="nav-text">1.3.7 用单词的同义词替换的示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-在文本上应用Zipf定律"><span class="nav-text">1.4 在文本上应用Zipf定律</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-相似性度量"><span class="nav-text">1.5 相似性度量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-1-使用编辑距离算法执行相似性度量"><span class="nav-text">1.5.1 使用编辑距离算法执行相似性度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-2-使用Jaccard系数执行相似性度量"><span class="nav-text">1.5.2 使用Jaccard系数执行相似性度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-3-使用Smith-Waterman距离算法执行相似性度量"><span class="nav-text">1.5.3 使用Smith Waterman距离算法执行相似性度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-4-其他字符串相似性度量"><span class="nav-text">1.5.4 其他字符串相似性度量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-小结"><span class="nav-text">1.6 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第2章-统计语言建模"><span class="nav-text">第2章 统计语言建模</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-理解单词频率"><span class="nav-text">2.1 理解单词频率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-为给定的文本开发MLE"><span class="nav-text">2.1.1 为给定的文本开发MLE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-隐马尔科夫模型估计"><span class="nav-text">2.1.2 隐马尔科夫模型估计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-在MLE模型上应用平滑"><span class="nav-text">2.2 在MLE模型上应用平滑</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-加法平滑"><span class="nav-text">2.2.1 加法平滑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-Good-Turing平滑"><span class="nav-text">2.2.2 Good Turing平滑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-Kneser-Ney平滑"><span class="nav-text">2.2.3 Kneser Ney平滑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-Witten-Bell平滑"><span class="nav-text">2.2.4 Witten Bell平滑</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-为MLE开发一个回退机制"><span class="nav-text">2.3 为MLE开发一个回退机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-应用数据的插值以便获取混合搭配"><span class="nav-text">2.4 应用数据的插值以便获取混合搭配</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-通过复杂度来评估语言模型"><span class="nav-text">2.5 通过复杂度来评估语言模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-在语言建模中应用Metropolis-Hastings算法"><span class="nav-text">2.6 在语言建模中应用Metropolis-Hastings算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-在语言处理中应用Gibbs采样法"><span class="nav-text">2.7 在语言处理中应用Gibbs采样法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8-小结"><span class="nav-text">2.8 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第3章-形态学：在实践中学习"><span class="nav-text">第3章 形态学：在实践中学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-形态学简介"><span class="nav-text">3.1 形态学简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-理解词干提取器"><span class="nav-text">3.2 理解词干提取器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-理解词形还原"><span class="nav-text">3.3 理解词形还原</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-为非英文语言开发词干提取器"><span class="nav-text">3.4 为非英文语言开发词干提取器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-形态分析器"><span class="nav-text">3.5 形态分析器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-形态生成器"><span class="nav-text">3.6 形态生成器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-7-搜索引擎"><span class="nav-text">3.7 搜索引擎</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-8-小结"><span class="nav-text">3.8 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第4章-词性标注：单词识别"><span class="nav-text">第4章 词性标注：单词识别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-词性标注简介"><span class="nav-text">4.1 词性标注简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-默认标注"><span class="nav-text">4.1.1 默认标注</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-创建词性标注语料库"><span class="nav-text">4.2 创建词性标注语料库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-选择一种机器学习算法"><span class="nav-text">4.3 选择一种机器学习算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-涉及n-gram的统计建模"><span class="nav-text">4.4 涉及n-gram的统计建模</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-使用词性标注语料库开发分块器"><span class="nav-text">4.5 使用词性标注语料库开发分块器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-小结"><span class="nav-text">4.6 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第5章-语法解析：分析训练资料"><span class="nav-text">第5章 语法解析：分析训练资料</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-语法解析简介"><span class="nav-text">5.1 语法解析简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Treebank建设"><span class="nav-text">5.2 Treebank建设</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-从Treebank提取上下文无关文法规则"><span class="nav-text">5.3 从Treebank提取上下文无关文法规则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-从CFG创建概率上下文无关文法"><span class="nav-text">5.4 从CFG创建概率上下文无关文法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-CYK线图解析算法"><span class="nav-text">5.5 CYK线图解析算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-6-Earley线图解析算法"><span class="nav-text">5.6 Earley线图解析算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-7-小结"><span class="nav-text">5.7 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第6章-语义分析：意义很重要"><span class="nav-text">第6章 语义分析：意义很重要</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-语义分析简介"><span class="nav-text">6.1 语义分析简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-1-NER简介"><span class="nav-text">6.1.1 NER简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-2-使用隐马尔科夫模型的NER系统"><span class="nav-text">6.1.2 使用隐马尔科夫模型的NER系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-3-使用机器学习工具包训练NER"><span class="nav-text">6.1.3 使用机器学习工具包训练NER</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-4-使用词性标注执行NER"><span class="nav-text">6.1.4 使用词性标注执行NER</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-使用Wordnet生成同义词集id"><span class="nav-text">6.2 使用Wordnet生成同义词集id</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-使用Wordnet进行词义消歧"><span class="nav-text">6.3 使用Wordnet进行词义消歧</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-小结"><span class="nav-text">6.4 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第7章-情感分析：我很快乐"><span class="nav-text">第7章 情感分析：我很快乐</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-情感分析简介"><span class="nav-text">7.1 情感分析简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-1-使用NER执行情感分析"><span class="nav-text">7.1.1 使用NER执行情感分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-2-使用机器学习执行情感分析"><span class="nav-text">7.1.2 使用机器学习执行情感分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-3-NER系统的评估"><span class="nav-text">7.1.3 NER系统的评估</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-小结"><span class="nav-text">7.2 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第8章-信息检索：访问信息"><span class="nav-text">第8章 信息检索：访问信息</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1-信息检索简介"><span class="nav-text">8.1 信息检索简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-1-停止词删除"><span class="nav-text">8.1.1 停止词删除</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-2-使用向量空间模型进行信息检索"><span class="nav-text">8.1.2 使用向量空间模型进行信息检索</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-向量空间评分及查询操作符关联"><span class="nav-text">8.2 向量空间评分及查询操作符关联</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-3-使用隐性语义索引开发IR系统"><span class="nav-text">8.3 使用隐性语义索引开发IR系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-4-文本摘要"><span class="nav-text">8.4 文本摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-5-问答系统"><span class="nav-text">8.5 问答系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-6-小结"><span class="nav-text">8.6 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第9章-语篇分析：理解才是可信的"><span class="nav-text">第9章 语篇分析：理解才是可信的</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-1-语篇分析简介"><span class="nav-text">9.1 语篇分析简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-1-使用中心理论执行语篇分析"><span class="nav-text">9.1.1 使用中心理论执行语篇分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-2-指代消解"><span class="nav-text">9.1.2 指代消解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2-小结"><span class="nav-text">9.2 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第10章-NLP系统评估：性能分析"><span class="nav-text">第10章 NLP系统评估：性能分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#10-1-NLP系统评估要点"><span class="nav-text">10.1 NLP系统评估要点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-1-NLP工具的评估（词性标注器、词干提取器及形态分析器）"><span class="nav-text">10.1.1 NLP工具的评估（词性标注器、词干提取器及形态分析器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-2-使用黄金数据执行解析器评估"><span class="nav-text">10.1.2 使用黄金数据执行解析器评估</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-2-IR系统的评估"><span class="nav-text">10.2 IR系统的评估</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-3-错误识别指标"><span class="nav-text">10.3 错误识别指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-4-基于词汇搭配的指标"><span class="nav-text">10.4 基于词汇搭配的指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-5-基于句法匹配的指标"><span class="nav-text">10.5 基于句法匹配的指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-6-使用浅层语义匹配的指标"><span class="nav-text">10.6 使用浅层语义匹配的指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-7-小结"><span class="nav-text">10.7 小结</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hac_lang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  





  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="true"></script>









  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  


  


  




  




  




  



<script>
// GET RESPONSIVE HEIGHT PASSED FROM IFRAME

window.addEventListener("message", function(e) {
  var data = e.data;
  if ((typeof data === 'string') && (data.indexOf('ciu_embed') > -1)) {
    var featureID = data.split(':')[1];
    var height = data.split(':')[2];
    $(`iframe[data-feature=${featureID}]`).height(parseInt(height) + 30);
  }
}, false);
</script>


  

  

  


  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":250,"height":500},"mobile":{"show":false,"scale":0.5},"react":{"opacity":0.7},"log":false,"tagMode":false});</script></body>
</html>
