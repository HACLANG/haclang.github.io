<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">






<link rel="stylesheet" href="/css/main.css?v=7.2.0">






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="作者: [美] Henrik Brink出版社: 机械工业出版社原作名: Real-World Machine Learning译者: 程继洪出版年: 2017-6丛书: 计算机科学先进技术译丛ISBN: 9787111569220 内容提要大数据时代为机器学习的应用提供了广阔的空间，各行各业涉及数据分析的工作都需要使用机器学习算法。本书围绕实际数据分析的流程展开，着重介绍数据探索、数据预处理和常">
<meta name="keywords" content="机器学习,编程,计算机,自评,books,更毕,豆瓣7,数据分析,MachineLearning,算法策略">
<meta property="og:type" content="article">
<meta property="og:title" content="book_《实用机器学习》">
<meta property="og:url" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/index.html">
<meta property="og:site_name" content="Hac_lang">
<meta property="og:description" content="作者: [美] Henrik Brink出版社: 机械工业出版社原作名: Real-World Machine Learning译者: 程继洪出版年: 2017-6丛书: 计算机科学先进技术译丛ISBN: 9787111569220 内容提要大数据时代为机器学习的应用提供了广阔的空间，各行各业涉及数据分析的工作都需要使用机器学习算法。本书围绕实际数据分析的流程展开，着重介绍数据探索、数据预处理和常">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00000.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00001.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00002.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00003.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00004.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00005.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00006.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00007.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00008.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00009.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00010.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00011.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00012.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00013.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00014.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00015.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00016.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00017.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00018.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00019.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00020.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00021.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00022.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00023.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00024.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00025.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00026.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00027.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00028.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00029.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00031.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00032.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00034.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00035.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00036.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00037.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00031.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00038.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00039.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00040.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00041.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00042.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00043.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00044.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00040.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00045.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00046.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00047.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00048.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00049.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00050.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00051.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00052.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00053.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00054.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00055.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00056.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00057.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00058.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00059.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00060.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00053.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00061.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00062.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00065.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00066.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00066.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00067.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00068.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00065.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00069.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00070.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00071.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00006.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00072.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00073.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00074.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00075.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00076.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00077.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00078.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00079.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00080.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00081.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00082.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00083.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00084.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00081.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00087.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00088.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00089.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00090.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00091.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00081.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00092.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00093.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00094.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00080.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00095.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00096.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00097.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00098.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00099.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00099.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00100.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00101.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00100.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00102.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00103.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00104.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00103.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00105.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00106.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00065.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00103.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00107.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00108.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00065.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00103.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00109.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00110.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00111.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00112.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00113.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00114.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00115.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00117.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00118.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00119.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00120.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00121.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00122.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00123.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00124.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00125.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00125.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00126.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00127.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00128.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00129.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00130.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00131.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00132.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00133.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00134.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00135.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00136.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00137.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00138.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00139.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00140.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00141.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00142.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00143.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00144.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00145.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00146.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00147.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00135.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00148.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00149.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00150.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00139.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00151.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00152.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00153.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00142.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00154.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00155.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00156.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00157.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00158.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00159.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00160.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00161.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00160.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00164.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00165.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00080.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00166.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00167.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00168.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00080.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00169.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00170.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00171.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00172.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00173.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00174.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00120.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00175.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00176.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00177.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00178.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00179.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00180.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00181.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00174.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00182.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00174.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00183.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00184.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00185.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00186.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00187.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00188.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00123.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00189.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00190.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00191.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00192.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00174.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00193.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00194.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00195.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00196.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00197.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00198.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00197.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00199.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00200.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00120.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00201.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00203.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00204.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00205.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00206.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00207.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00208.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00209.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00210.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00211.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00212.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00213.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00214.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00215.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00213.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00216.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00217.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00218.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00219.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00220.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00221.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00222.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00223.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00224.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00225.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00226.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00227.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00228.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00227.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00229.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00230.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00231.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00232.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00233.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00234.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00235.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00236.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00237.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00236.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00238.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00239.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00240.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00241.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00242.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00243.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00244.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00244.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00245.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00241.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00246.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00247.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00248.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00245.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00221.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00249.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00250.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00251.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00252.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00253.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00254.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00255.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00256.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00221.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00257.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00221.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00258.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00259.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00260.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00261.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00262.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00263.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00264.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00265.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00248.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00236.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00266.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00267.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00268.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00269.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00270.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00271.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00272.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00271.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00273.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00274.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00275.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00272.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00276.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00277.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00278.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00225.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00279.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00280.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00281.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00282.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00283.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00284.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00285.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00286.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00287.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00288.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00289.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00290.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00291.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00292.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00293.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00293.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00290.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00295.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00297.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00298.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00290.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00290.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00248.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00299.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00300.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00301.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00302.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00303.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00304.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00305.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00306.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00307.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00308.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00309.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00310.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00309.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00302.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00311.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00312.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00313.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00314.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00315.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00248.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00316.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00280.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00317.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00318.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00319.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00320.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00322.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00323.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00324.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00325.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00326.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00254.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00327.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00328.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00329.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00330.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00331.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00332.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00333.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00334.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00335.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00336.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00337.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00338.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00339.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00340.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00341.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00342.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00343.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00344.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00345.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00346.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00348.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00349.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00350.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00351.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00352.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00353.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00354.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00355.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00356.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00357.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00358.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00359.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00360.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00361.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00360.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00361.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00362.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00363.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00364.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00365.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00366.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00367.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00368.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00369.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00370.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00371.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00372.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00373.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00374.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00375.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00376.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00377.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00378.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00379.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00380.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00382.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00383.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00384.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00385.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00386.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00387.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00388.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00389.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00390.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00391.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00392.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00393.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00394.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00395.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00396.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00397.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00398.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00399.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00400.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00401.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00402.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00403.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00404.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00405.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00406.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00407.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00408.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00409.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00384.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00410.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00411.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00412.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00413.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00414.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00415.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00418.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00419.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00420.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00421.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00422.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00423.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00424.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00425.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00426.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00420.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00427.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00428.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00429.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00430.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00431.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00433.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00434.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00435.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00436.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00437.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00438.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00439.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00440.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00441.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00442.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00443.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00444.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00445.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00446.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00447.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00448.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00449.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00450.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00451.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00452.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00453.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00454.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00451.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00452.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00451.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00452.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00455.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00456.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00452.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00451.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00452.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00457.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00351.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00458.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00352.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00459.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00460.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00351.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00352.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00351.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00352.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00461.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00462.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00463.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00464.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00465.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00466.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00467.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00468.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00469.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00464.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00470.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00471.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00351.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00352.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00472.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00471.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00473.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00471.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00123.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00474.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00475.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00123.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00476.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00477.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00479.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00480.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00481.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00482.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00483.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00484.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00485.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00486.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00487.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00488.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00489.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00490.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00491.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00492.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00493.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00494.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00495.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00496.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00497.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00498.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00499.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00500.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00501.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00502.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00503.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00505.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00507.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00508.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00509.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00510.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00509.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00511.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00512.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00513.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00514.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00515.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00516.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00517.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00518.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00519.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00520.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00521.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00522.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00513.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00524.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00525.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00526.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00527.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00526.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00528.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00529.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00530.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00531.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00532.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00533.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00534.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00535.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00536.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00537.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00538.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00539.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00540.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00541.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00542.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00544.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00545.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00546.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00547.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00548.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00549.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00551.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00552.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00553.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00554.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00555.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00547.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00557.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00558.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00559.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00560.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00561.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00531.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00562.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00562.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00530.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00530.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00530.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00563.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00562.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00564.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00565.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00566.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00568.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00569.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00570.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00571.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00572.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00574.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00575.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00576.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00577.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00578.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00531.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00579.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00580.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00581.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00582.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00584.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00585.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00586.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00587.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00588.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00588.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00589.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00590.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00591.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00592.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00593.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00587.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00594.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00595.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00586.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00596.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00597.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00598.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00599.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00531.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00599.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00600.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00600.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00601.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00602.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00603.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00604.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00600.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00605.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00601.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00606.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00607.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00608.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00609.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00610.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00611.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00612.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00613.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00614.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00615.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00616.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00617.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00618.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00619.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00610.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00620.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00621.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00620.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00622.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00623.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00620.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00624.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00625.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00620.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00617.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00626.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00627.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00628.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00629.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00630.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00631.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00632.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00628.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00622.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00628.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00633.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00628.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00634.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00635.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00636.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00637.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00635.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00638.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00639.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00640.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00641.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00642.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00643.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00644.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00645.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00562.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00646.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00648.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00648.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00649.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00650.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00651.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00508.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00509.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00652.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00652.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00653.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00654.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00655.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00656.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00501.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00657.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00658.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00659.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00659.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00659.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00659.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00659.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00659.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00660.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00661.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00662.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00663.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00664.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00665.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00666.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00661.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00661.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00661.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00661.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00667.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00668.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00669.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00670.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00671.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00672.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00673.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00674.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00675.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00676.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00677.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00678.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00679.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00680.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00681.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00682.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00683.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00684.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00685.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00677.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00677.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00677.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00686.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00687.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00688.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00689.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00690.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00691.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00692.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00693.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00694.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00695.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00696.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00697.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00698.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00700.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00693.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00701.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00702.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00703.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00704.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00705.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00706.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00707.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00708.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00709.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00710.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00711.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00712.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00713.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00714.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00715.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00712.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00716.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00717.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00718.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00719.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00720.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00721.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00717.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00722.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00723.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00724.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00725.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00727.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00728.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00534.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00729.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00731.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00540.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00732.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00733.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00734.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00735.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00736.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00737.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00738.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00739.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00740.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00741.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00742.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00743.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00744.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00745.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00746.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00747.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00748.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00749.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00750.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00751.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00753.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00754.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00755.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00756.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00756.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00757.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00758.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00759.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00760.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00302.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00302.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00761.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00762.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00763.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00764.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00765.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00762.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00762.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00766.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00767.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00768.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00769.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00770.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00771.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00772.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00773.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00774.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00775.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00776.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00777.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00778.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00779.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00780.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00781.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00587.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00587.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00782.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00648.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00713.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00713.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00783.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00713.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00713.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00784.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00785.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00786.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00787.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00788.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00789.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00790.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00791.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00790.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00792.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00793.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00794.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00795.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00796.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00797.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00796.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00797.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00793.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00798.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00799.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00800.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00801.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00802.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00803.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00804.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00805.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00805.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00806.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00807.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00808.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00809.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00809.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00810.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00811.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00810.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00812.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00813.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00814.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00815.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00816.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00817.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00805.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00818.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00819.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00789.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00802.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00820.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00822.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00824.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00825.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00822.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00826.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00827.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00824.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00828.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00828.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00829.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00830.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00831.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00828.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00800.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00832.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00833.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00834.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00822.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00832.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00834.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00835.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00836.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00837.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00838.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00800.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00834.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00839.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00802.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00840.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00841.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00842.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00843.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00844.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00845.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00845.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00846.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00847.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00848.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00849.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00850.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00851.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00517.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00852.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00853.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00854.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00855.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00856.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00857.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00858.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00859.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00860.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00861.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00862.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00863.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00864.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00865.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00866.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00867.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00868.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00869.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00870.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00871.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00872.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00873.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00874.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00875.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00876.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00877.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00878.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00878.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00821.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00879.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00880.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00881.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00805.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00882.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00883.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00881.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00884.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00885.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00886.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00886.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00887.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00888.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00889.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00890.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00891.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00865.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00892.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00893.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00894.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00895.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00896.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00897.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00896.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00898.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00899.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00900.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00901.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00902.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00903.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00901.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00902.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00904.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00901.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00902.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00905.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00906.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00907.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00066.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00908.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00909.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00910.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00911.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00912.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00879.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00913.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00914.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00915.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00916.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00879.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00917.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00727.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00919.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00919.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00920.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00921.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00727.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00541.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00922.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00923.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00924.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00922.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00925.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00926.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00927.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00928.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00929.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00930.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00931.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00932.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00931.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00933.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00934.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00800.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00935.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00936.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00802.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00937.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00938.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00939.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00940.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00941.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00942.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00925.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00943.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00944.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00945.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00946.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00947.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00948.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00949.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00950.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00951.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00918.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00925.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00952.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00540.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00816.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00953.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00954.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00955.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00956.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00957.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00957.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00958.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00959.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00960.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00961.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00962.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00962.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00963.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00963.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00964.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00965.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00675.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00966.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00967.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00675.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00966.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00968.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00969.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00970.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00971.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00972.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00973.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00972.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00971.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00974.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00975.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00976.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00977.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00978.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00979.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00980.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00981.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00982.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00983.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00984.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00985.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00986.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00987.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00988.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00989.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00990.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00991.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00992.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00993.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00994.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00995.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00996.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00997.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00996.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00997.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00996.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00997.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00998.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00999.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00999.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01000.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01001.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01002.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01003.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01004.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01005.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01002.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01006.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01007.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01008.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01009.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01010.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01011.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01012.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01013.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01014.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01015.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01016.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01010.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01017.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01018.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01019.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01020.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01021.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01022.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01023.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01021.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01023.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01024.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01025.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01026.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01027.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01028.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01029.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01028.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01031.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01032.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01034.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01035.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01036.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01037.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01038.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01040.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01041.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01042.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01043.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01044.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01045.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01031.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01046.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01047.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01048.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01049.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01050.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01051.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01052.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01053.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01046.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01054.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01055.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01046.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01056.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01057.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01057.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01058.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01059.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01060.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01061.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01062.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01065.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01066.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01067.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01068.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01069.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01070.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01071.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01072.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01073.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01074.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01075.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01076.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01077.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01078.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01079.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01079.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01080.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01081.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01082.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01083.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01084.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01066.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01087.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01088.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01089.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01090.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01091.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01092.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01093.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01094.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01095.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01093.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01095.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01096.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01097.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01098.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01098.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01097.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01087.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01087.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01087.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01099.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01100.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01101.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01102.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01103.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01104.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01105.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01106.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01107.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01108.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01109.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01110.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01111.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00290.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01112.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01113.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01114.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01115.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01116.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01117.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01118.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01119.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01120.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01121.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01122.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01123.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01124.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01125.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01126.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01127.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01057.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01057.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01128.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01129.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01130.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01131.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01132.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01081.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01133.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01130.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01080.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01134.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01135.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01136.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01137.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01138.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01139.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01140.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01141.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01142.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01143.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01144.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00781.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01145.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01146.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01130.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01147.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01148.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01147.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01149.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01150.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01151.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01152.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01153.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01154.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01155.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01156.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01157.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01158.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01159.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01160.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01161.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01162.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01163.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01081.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01133.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01164.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01165.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01166.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01167.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01168.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01169.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01170.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01171.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01172.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01173.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01174.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01174.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01171.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01175.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01176.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01177.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01178.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01179.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01180.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01174.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01178.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01179.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01180.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01181.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01182.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01183.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01064.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01184.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01185.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01186.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01187.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01188.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01189.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01190.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01191.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01192.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01193.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01193.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01194.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01194.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01195.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01196.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01197.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01198.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01199.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01200.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01198.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01201.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01194.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01063.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01203.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01204.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01205.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01206.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01207.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01208.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01209.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01210.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01211.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01212.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01039.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01213.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01214.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01215.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01216.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01216.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01216.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01217.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01217.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01218.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01218.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01219.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01220.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01221.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01222.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01223.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01224.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01225.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01226.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01227.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01228.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01229.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01230.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01231.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01232.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01233.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01085.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01086.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01234.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01235.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01236.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01237.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01238.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01239.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01240.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01241.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01242.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01243.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01244.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01245.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01246.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01247.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01030.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01240.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01248.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01249.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01041.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01250.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01251.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01249.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01250.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01252.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01206.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01250.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01253.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01254.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01224.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01255.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01224.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01202.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01256.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01257.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01258.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01259.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01259.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01260.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01261.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01041.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01033.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01259.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01259.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01031.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01032.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01262.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01263.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01264.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00351.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00352.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01265.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01265.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01265.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01266.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01267.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01268.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01268.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01265.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01269.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01270.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01271.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01272.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01273.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01274.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01275.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01276.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01277.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01278.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01279.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01280.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01281.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01282.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01283.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01284.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01285.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01286.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01287.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01288.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01289.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01290.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01291.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01292.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01292.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01292.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01293.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01295.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01293.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01297.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01298.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01299.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01300.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01301.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01302.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01303.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01304.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01301.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01305.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01306.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01301.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01307.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01304.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01303.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01308.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01309.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01310.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01311.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01302.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01312.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01313.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01314.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01315.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01316.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01317.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01318.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01319.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01320.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01322.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01323.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01324.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01325.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01326.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01327.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01328.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01329.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01330.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01331.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01329.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01332.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00665.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01333.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01334.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01329.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01330.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01331.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01329.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01335.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01336.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01337.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01295.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01338.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01325.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01339.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01339.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01339.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01340.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01341.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01342.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01339.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01343.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01344.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01345.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01346.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01344.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01348.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01349.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01350.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01351.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01352.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01326.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01353.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01354.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01350.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01355.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01355.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01356.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01357.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01358.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01359.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01360.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01361.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01362.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01360.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01363.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01364.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01365.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01364.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01366.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01367.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01368.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01369.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01368.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01368.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01370.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01371.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01372.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01371.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01369.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01373.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01374.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01369.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01375.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01376.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01377.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01378.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01379.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01380.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00437.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01382.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01383.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01384.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01385.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01386.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01387.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01388.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01385.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01385.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01384.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01385.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01384.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01389.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01390.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01293.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01293.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01322.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01323.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01391.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01293.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01392.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01393.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01394.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01395.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00540.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01396.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01321.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01397.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01398.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01399.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01400.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01401.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01402.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01403.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01404.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01405.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01406.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01406.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01407.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01408.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01409.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01410.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01411.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01412.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01412.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01413.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01414.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01338.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01415.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01415.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01418.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01419.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01420.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01421.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01422.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01423.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01424.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01425.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01418.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01426.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01427.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01428.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01421.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01429.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01430.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01426.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01427.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01431.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01406.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01334.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01324.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01421.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01433.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01434.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01433.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01435.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01436.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01415.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01437.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01438.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01419.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01437.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01419.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01439.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01440.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01441.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01442.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01443.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01441.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01444.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01415.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01445.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01446.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01447.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01448.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00213.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01449.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01450.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01445.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01451.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01452.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01453.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01454.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01455.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01456.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01457.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01458.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01459.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01460.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01461.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01462.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01442.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01463.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01464.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01442.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01465.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01466.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01466.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01443.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01467.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01466.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01467.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01465.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01468.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01469.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01470.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01471.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01465.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01468.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01464.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01472.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01473.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01474.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01475.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01462.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01476.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01477.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01479.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01480.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01481.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01482.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01483.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01484.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01474.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01485.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01486.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01487.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01450.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01325.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01485.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01488.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01467.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01489.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01490.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01491.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01421.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01492.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01474.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01493.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01494.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01478.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01495.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01496.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01495.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01497.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01498.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01499.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01497.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01500.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01482.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01501.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01502.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01503.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01505.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01502.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01507.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01450.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01508.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01509.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01510.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01511.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01512.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01432.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01513.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01514.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01515.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01442.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01516.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01517.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01518.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01519.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01520.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01521.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01514.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01442.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01522.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01502.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01524.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01494.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01525.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01526.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01527.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01515.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01526.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01528.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01526.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01528.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01529.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01442.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01463.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01530.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01531.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01532.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01533.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01534.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01440.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01535.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01536.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01537.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01538.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01539.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01429.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01540.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01541.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01429.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01542.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01542.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01544.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01528.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01545.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01539.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01546.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01429.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01547.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01548.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01549.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01295.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01294.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01337.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01551.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01552.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01338.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01553.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01554.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01553.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01555.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01556.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01427.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01557.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01558.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00474.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01559.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01560.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01482.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01561.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01562.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01416.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01417.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01563.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01564.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01563.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01563.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01563.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01565.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01566.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01338.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00816.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01568.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01569.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01570.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01571.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01364.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01572.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01573.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01364.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01570.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01571.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01574.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01557.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01347.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01575.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01575.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01575.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01364.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01576.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01577.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01578.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01579.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01580.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01581.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01557.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01582.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01338.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01324.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01324.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01584.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01584.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01585.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01584.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01586.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01586.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01587.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01588.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01589.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01590.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01591.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01592.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01593.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01594.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01595.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01596.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01597.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01583.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01584.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01598.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01599.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01296.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01600.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01601.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01584.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01602.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01338.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01550.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00823.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01603.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00501.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00930.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01604.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01605.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01606.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01607.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01608.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01609.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01610.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01611.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01612.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01612.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01613.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01614.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01615.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01616.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01612.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01617.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01617.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01618.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01616.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01616.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01619.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01620.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01621.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00474.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01622.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01623.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01624.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01625.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00635.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01626.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01627.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01628.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01629.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01630.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01631.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01632.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01633.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01634.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01635.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01636.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01637.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01638.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01639.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01639.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01612.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01612.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01640.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01641.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01641.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01642.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01643.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01644.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01645.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01612.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01646.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01646.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01640.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01646.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01646.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01648.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01648.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01646.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01648.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01649.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01640.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01648.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01650.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01650.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01650.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01647.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01650.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01651.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01652.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01653.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01654.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01381.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01655.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01656.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01657.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01658.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01659.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01660.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01661.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01662.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01663.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01664.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01665.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01665.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01665.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01666.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01667.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01668.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00541.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01660.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01669.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01670.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01669.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01670.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01671.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01672.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01673.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01674.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01675.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01676.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01677.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01660.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01678.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01679.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00930.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01680.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00914.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01681.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00541.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01682.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00541.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00925.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01683.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00925.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00925.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01684.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01683.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01685.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01686.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01687.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01688.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01689.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01690.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01691.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01692.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01693.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01694.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01695.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01696.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01697.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01692.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01698.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01700.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01701.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01702.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01703.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01704.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01705.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01706.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01707.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01708.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01709.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01710.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01710.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01711.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01712.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01713.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01714.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01697.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01715.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01716.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01717.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01714.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01718.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01719.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00567.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01720.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01721.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01722.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01723.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01724.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01706.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01705.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01704.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01703.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01725.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01727.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01728.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01729.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01731.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01732.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01732.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01705.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01703.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01733.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01726.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01734.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01734.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01735.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01736.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01737.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01736.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00879.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01738.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00506.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01739.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01740.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01741.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01742.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01743.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01744.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01744.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01686.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01745.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01745.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01746.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01747.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01748.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01749.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01750.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01749.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01751.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01752.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01753.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01749.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01749.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01754.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01755.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01756.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00504.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00541.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01692.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01757.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01758.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01759.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01759.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01760.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01761.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01762.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01763.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01764.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01765.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01766.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01767.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01768.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01769.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01764.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01770.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01771.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01772.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01773.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01774.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01775.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01776.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01777.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01699.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01778.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01779.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01780.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01781.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01782.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01783.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01784.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01785.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01714.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01786.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01760.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01761.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01787.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01788.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01640.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01640.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01780.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01779.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01780.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01789.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01790.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01767.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01791.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00915.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00540.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01792.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01793.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01794.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01795.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01615.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01796.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01613.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01797.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01798.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00523.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01799.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01800.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01801.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01802.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01803.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01804.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01805.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01806.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01807.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01795.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01613.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01613.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01808.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01809.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01806.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01810.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01806.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01810.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01811.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01812.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01795.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01813.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01814.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00543.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00730.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01795.gif">
<meta property="og:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image01815.gif">
<meta property="og:updated_time" content="2020-08-16T07:32:00.186Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="book_《实用机器学习》">
<meta name="twitter:description" content="作者: [美] Henrik Brink出版社: 机械工业出版社原作名: Real-World Machine Learning译者: 程继洪出版年: 2017-6丛书: 计算机科学先进技术译丛ISBN: 9787111569220 内容提要大数据时代为机器学习的应用提供了广阔的空间，各行各业涉及数据分析的工作都需要使用机器学习算法。本书围绕实际数据分析的流程展开，着重介绍数据探索、数据预处理和常">
<meta name="twitter:image" content="https://haclang.github.io/2017/07/30/book-《实用机器学习》/Image00000.jpg">





  
  
  <link rel="canonical" href="https://haclang.github.io/2017/07/30/book-《实用机器学习》/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>book_《实用机器学习》 | Hac_lang</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hac_lang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">小白hac_lang的笔记，涉及内容包含但不限于：人工智能   基因工程    信息安全   软件工程   嵌入式   天文物理</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-news">

    
    
      
    

    

    <a href="/news/" rel="section"><i class="menu-item-icon fa fa-fw fa-rss"></i> <br>news</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    
  
  

  

  <a href="https://github.com/HACLANG" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://haclang.github.io/2017/07/30/book-《实用机器学习》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hac_lang">
      <meta itemprop="description" content="小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hac_lang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">book_《实用机器学习》

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-07-30 22:06:06" itemprop="dateCreated datePublished" datetime="2017-07-30T22:06:06+08:00">2017-07-30</time>
            </span>
          

          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>作者: [美] Henrik Brink<br>出版社: 机械工业出版社<br>原作名: Real-World Machine Learning<br>译者: 程继洪<br>出版年: 2017-6<br>丛书: 计算机科学先进技术译丛<br>ISBN: 9787111569220</p>
<h1 id="内容提要"><a href="#内容提要" class="headerlink" title="内容提要"></a>内容提要</h1><p>大数据时代为机器学习的应用提供了广阔的空间，各行各业涉及数据分析的工作都需要使用机器学习算法。本书围绕实际数据分析的流程展开，着重介绍数据探索、数据预处理和常用的机器学习算法模型。本书从解决实际问题的角度出发，介绍回归算法、分类算法、推荐算法、排序算法和集成学习算法。在介绍每种机器学习算法模型时，书中不但阐述基本原理，而且讨论模型的评价与选择。为方便读者学习各种算法，本书介绍了R语言中相应的软件包并给出了示例程序。</p>
<p>本书的最大特色就是贴近工程实践。首先，本书仅侧重介绍当前工业界最常用的机器学习算法，而不追求知识内容的覆盖面；其次，本书在介绍每类机器学习算法时，力求通俗易懂地阐述算法思想，而不追求理论的深度，让读者借助代码获得直观的体验。</p>
<p>本书适合需要应用机器学习算法解决实际问题的工程技术人员阅读，也可作为相关专业高年级本科生或研究生的入门教材或课外读物。</p>
<h1 id="序一"><a href="#序一" class="headerlink" title="序一"></a>序一</h1><p>机器学习是人工智能领域最成功、发展最迅速的分支之一。然而多年来，除了人机对弈之外，成功的机器学习应用对普通百姓来说似乎始终显得遥不可及。大数据时代的来临对机器学习提出了新的挑战，同时也为机器学习打开了一扇通向实用化舞台的大门。近年来，源于实际需求的海量数据集不断出现，一方面为技术交流和算法验证提供了有力的基础数据支撑，另一方面也有助于促进相关科学研究更贴近我们的日常生活。例如，在滴滴，我们正致力于结合机器学习技术和大数据技术来预测各地用户需求，并进行实时的运力调度和订单分配，不断为广大司机和用户提供更满意的服务。可以说，机器学习技术和大数据技术的结合正在显著地拉近科研人员和普通百姓的距离，正在显著地改变我们的生产方式和生活方式。</p>
<p>2016年8月，我在中国人工智能大会上担任“机器学习的明天”专题论坛联席主席时曾指出，人类很多难题的解决都离不开大数据和人工智能的结合。今天，我欣喜地看到，本书两位作者的合作恰恰体现了实用机器学习的这一重要发展趋势。两位作者长期在学术界和工业界工作，其中孙亮博士是我的学生，他从2006年开始就一直从事降维算法、稀疏学习、数值最优化算法等方向的机器学习研究，其快速降维算法研究曾获得机器学习领域顶级会议SIGKDD的最佳论文提名奖；黄倩博士是高文院士的学生，他从2004年开始就一直从事视频大数据的压缩和增强显示研究，其研究成果曾在视频处理领域旗舰期刊IEEE T-CSVT发表并在工业界获得实际应用。两位作者博士毕业后均有在万人以上规模企业的相关研发经历，接触过各类实际问题和数据，对机器学习如何在实际中应用有着深刻的理解和成功的经验。</p>
<p>这本书不厚，但却覆盖了用机器学习技术解决实际问题的主要步骤和常用算法。两位作者从实际应用出发，介绍了数据探索、数据预处理、算法应用、性能评价等具体内容，并深入浅出地介绍了模型复杂度、损失函数等机器学习领域的基本概念。由于集成学习在实际中应用较为广泛，因此本书专列一章加以讨论。考虑到实践中大家更关注的是如何选择和使用算法，两位作者还使用R语言软件包来引导读者实际操作。与市面上对机器学习作一般性介绍的书籍相比，本书介绍的算法稍稍复杂一些，但也更加实用，书中讨论的内容正是实际应用机器学习解决问题时所需要掌握的内容。对于广大业界爱好者和相关专业研究生来说，这是一本理想的入门读物和参考书，因此我非常乐意向大家推荐本书。</p>
<p>叶杰平</p>
<p>滴滴研究院副院长，密歇根大学终身教授</p>
<p>2016年12月</p>
<h1 id="序二"><a href="#序二" class="headerlink" title="序二"></a>序二</h1><p>2016年10月3日，我从宿州匆匆赶回上海，试图从连日飨宴中恢复精力，等待5日凌晨谷歌公司的Pixel发布会。业界预测这将是一个划时代的发布会，标志着“Mobile First”向“AI First”的转型。这一场发布会，让我难掩激动地在朋友圈中留下了洋洋洒洒但未必成熟的千字技术点评。</p>
<p>我为什么会如此期待这场盛会，要从十多年前说起。那时我还是一个学生，学着和IT八竿子打不着的天文学。在本科结束后，响应高中读《时间简史》时内心深处的召唤，我继续学了天文学。经历短暂的欢愉之后，我终于意识到这条路的艰辛，我是不可能做这行了。我开始思考，有没有一条路能让我既接近最前沿的理论，又有足够好的实践环境呢？我在看了《AI》（《人工智能》）这部电影之后被震撼得天旋地转——人工智能可以创造出如此的美好，人工智能可以引发无穷的深思。</p>
<p>我开始去选修《机器人》的课程，开始去听张学工老师的《模式识别》课，开始把自己的知识往这个方向靠。所幸因为玻尔兹曼，这两个专业还是有些联系的。在一次选假期小班课程的时候，我选了一个SVM的讲座，后来当孙亮和我说他也听过这堂课的时候，我才意识到原来高人就在身边，只是我一直不曾留意。我曾看到过孙亮床位下一箱一箱的计算机书，大多与机器学习有关，只是人的神经系统是很奇妙的，当你不关注的时候，这些事情会自动在你的世界被屏蔽；当你关注的时候，这个概念会在极短的时间以各种方式在你面前展现。不久后我认识了同样以机器学习见长的黄倩，那时在周末大家都会找时间一起聚聚，当然话题大多是关于计算机的发展方向和算法的，然后我就作为旁听者被他们带进了门。</p>
<p>孙亮硕士毕业后去美国继续读机器学习的博士，最后到微软公司从事他擅长的机器学习和数据科学的研究；黄倩在高文院士指导下硕博毕业，最终回到高校任教，带领学子探索最前沿的算法。这些年他们一直躬耕一线，从未中断，我也在辗转中断断续续地做着数据相关的工作，最终转到Hadoop/Spark/TensorFlow开源平台。这几年大数据如火如荼，机器学习热浪汹涌，AlphaGo的成功进一步激励了业界，TensorFlow的开源、“AI First”的概念终于在坎坎坷坷10年之后开始盛行，高校的学子和一线的工程师开始被这个全新的世界吸引。同事和实习生不断地要我给他们推荐一些大数据和机器学习方面的书，我也曾给同事买过一些我看过的书。遗憾的是，这些书大多要么是纯理论的“屠龙术”，离实际应用还有一段距离，要么就是针对算法模块搞些例子，使学习者只知其然不知其所以然，还有的就是与目前业界普遍应用的算法不吻合。</p>
<p>对于一线的IT从业者和想要实践算法的学生来说，一本理论与实践相结合、能展现目前业界通用算法使用技巧的书，应该是大家最喜闻乐见的。这要求作者要有极深厚的理论功底，能够把算法娓娓道来，还需要有足够的实践经验，娴熟业界各领域现在通用的算法。《实用机器学习》正是这样的一部佳作。本书详细讨论了机器学习中回归、分类、推荐、排序4类经典问题，详述了每一类问题中常用算法的理论来源，以及在R环境中如何去使用、评测和可视化展现。作为一线的实践者，书中对数据预处理也做了独立讲解，就理论过渡、全面性、实用性、易读性来说，本书都做了充分的考量。R语言作为一个机器学习的平台工具，具有使用简单、分析方便、可用库完备以及可视化容易等特点，为进行问题分析和寻找原因提供了足够的便捷性。即使部分算法没有分布式的解决方案，本书讲授的实用机器学习算法，也极易在Mahout或Spark MLLib上平移对应的接口。在算法和实践平台上，本书倡导的方法和环境，几乎都可以和工业界做到无缝对接。</p>
<p>人工智能的世界来了，浩浩汤汤，开启了IT业者的另外一个世界。或许，我们只是需要一个开始，而《实用机器学习》来得适逢其时，你的一小步将来难说不是人类机器学习世界的一大步！</p>
<p>Fantasy（裴少芳）</p>
<p>威比网络科技（上海）有限公司大数据部总监</p>
<p>2016年12月于上海</p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本书侧重于数据分析和机器学习的实践，涉及从原始数据搜集到建立模型解决问题再到算法性能评估的全过程。书中主要介绍实践中最常用的4类算法，包括回归算法、分类算法、推荐算法和排序算法。此外，书中还会介绍集成学习。集成学习是一类通过综合多个模型取长补短以取得更好效果的方法，对于回归、分类、推荐和排序问题都适用。在实践中，充分掌握这4类算法和集成学习即可解决相当多的实际问题。由于篇幅所限，聚类分析、关联规则等其他相关内容书中并没有一一介绍。</p>
<p>对于每种算法，本书首先介绍算法的原理。在理解算法原理和算法优缺点的基础上，读者在实践中就可以根据数据的特点和问题的具体需求选用合适的算法。为了突出算法的实践性，本书使用R语言中的软件包来介绍机器学习算法，特别是介绍了如何使用各种算法。R语言是一种开源和免费的解释型语言，其最大的优点是提供了各种软件包，实现了各种不同的算法。机器学习中很多强大的算法在R中都有相应的程序包。我们在讲解各种机器学习算法时，都介绍了R中相应的软件包，并提供了相应的R程序来帮助读者学习这些软件包的使用。这样读者就可以通过R来直接使用相应的算法，获得数据分析的第一手建模经验。</p>
<p>除了介绍这4类机器学习算法之外，本书涵盖了使用机器学习解决实际问题的整个流程，包括数据探索、数据预处理、使用机器学习算法所构建的模型的评价和选择等。在实际使用机器学习处理数据的过程中，数据的探索和预处理是非常重要的步骤，在很多场合甚至比建立模型本身更加重要，从原始数据中提取出一个好的特征在很多时候能够显著地提高模型的性能。得到构建的模型后，我们还需要评价和选择模型。本书还会介绍不同类型算法对应的评价标准以及如何进行模型选择，并介绍R中的相关工具（如caret包），以帮助读者直接上手。</p>
<p>我们尽量使用简单通俗的语言来介绍机器学习中的基本概念和各种常用算法，并通过介绍R中对应的软件包来帮助读者迅速了解和掌握各种算法的使用。为了准确地介绍各类算法，不可避免地要用到一些数学知识，本书在第3章特别介绍了一些相关的数学知识。</p>
<p>本书的所有R代码（包括生成书中图的大部分R代码）都可以从人民邮电出版社异步社区（www.epubit.com.cn）网站上获得。</p>
<p>本书的出版得到了国家自然科学基金（61300122、61502145）的支持，得到了人民邮电出版社编辑杨海玲女士的支持和帮助，在此表示诚挚的谢意。成稿的关键时期适逢我们各自的女儿降生，在此衷心感谢双方家人的理解与支持。因水平和时间所限，书中难免有错误或不当之处，恳请广大读者不吝指正。读者若有任何问题或建议，可发送电子邮件至sun.liang@outlook.com或huangqian@gmail.com。</p>
<p>孙 亮 黄 倩</p>
<p>2016年12月分别于华盛顿雷德蒙和南京</p>
<h1 id="第1章-引论"><a href="#第1章-引论" class="headerlink" title="第1章 引论"></a>第1章 引论</h1><p>随着计算机和互联网越来越深入到生活中的方方面面，人们搜集到的数据也呈指数级的增长。在这种情况下，大数据 （big data）应运而生。大数据通常体量特别大，而且数据比较复杂，使得无法直接使用传统的数据库工具对其进行存储和管理。大数据带来了很多挑战，如数据的搜集、整理、存储、共享、分析和可视化等。广义的大数据处理涵盖了上述所有领域；狭义的大数据更多是指如何使用机器学习 来分析大数据，从海量的数据中分析出有用的信息。</p>
<p>大数据分析的核心是机器学习算法。很多时候，我们有足够的数据，但是对如何利用这些数据缺乏理解。同时，实际问题往往比较复杂，并不能直接套用机器学习算法，我们需要对实际问题进行一些转化，使得机器学习算法可以应用。虽然实际问题表现形式各异，但是在将它们转化为机器学习能够处理的问题时，一般转化为如下4类问题：（1）回归问题；（2）分类问题；（3）推荐问题；（4）排序问题。这4类问题是实际应用中最主要的类型，覆盖了大部分实际问题。在1.3节，我们将详细介绍每类问题的具体例子。</p>
<h2 id="1-1-什么是机器学习"><a href="#1-1-什么是机器学习" class="headerlink" title="1.1 什么是机器学习"></a>1.1 什么是机器学习</h2><p>机器学习 （machine learning）是计算机科学的一个分支，也可以认为是模式识别 （pattern recognition）、人工智能<br>（artificial intelligence）、统计学 （statistics）、数据挖掘 （data mining）等多个学科的交叉学科。机器学习与数值优化 （numerical optimization）也有很高的重合度。</p>
<p>机器学习研究如何从数据中学习出有效的模型，进而能对未来作出预测。例如，如果商店能够预测某一件商品在未来一段时间的销售量，就可以提前预订相应数量的商品，这样既可以避免缺货，又可以避免进太多货而造成积压。与传统的决策算法不同的是，机器学习算法依赖于数据。在前面的例子中，我们要从历史数据中学习出相应的模型以对未来进行预测。这样做有两个好处：第一，由于算法依赖于数据，可以使用新的数据来不停地更新模型，使得模型能够自适应地处理新的数据；第二，对人的介入要求少。在使用机器学习的过程中，虽然也会尽量利用人的经验，但更多地强调如何利用人的经验知识从数据中训练得到更好的模型。</p>
<p>目前，机器学习已成为研究和应用的热点之一。一些能够使用机器学习解决的实际问题包括：</p>
<ul>
<li>根据信用卡交易的历史数据，判定哪些交易是欺诈交易；</li>
<li>从字母、数字或者汉字图像中有效地识别出相应的字符；</li>
<li>根据用户以往的购物历史来给用户推荐新的商品；</li>
<li>根据用户当前的查询和以往的消费历史向其推荐适合的网页、商品等；</li>
<li>根据汽车的发动机排量、年份、类型、重量等信息估计汽车的耗油量。</li>
</ul>
<p>虽然这些问题的具体形式不同，但是均可转化成机器学习可以解答的问题形式。</p>
<p>从概念上讲，在机器学习中，我们的目标是从给定的数据集中学习出一个模型 _f_ ，使得它能够有效地从输入数据中预测我们感兴趣的量。根据问题的不同，我们感兴趣的量（或者叫目标值）可以有不同的形式。例如，在分类问题中，目标值就是若干类别之一；在排序问题中，目标值就是关于文档的一个序列。</p>
<p>在机器学习中，通常我们解决问题的流程如下：</p>
<p>（1）搜集足够多的数据；</p>
<p>（2）通过分析问题本身或者分析数据，我们认为模型 _f_ 是可以从数据中学习出来的；</p>
<p>（3）选择合适的模型和算法，从数据中学习出模型 _f_ ；</p>
<p>（4）评价模型 _f_ ，并将其利用在实际中处理新的数据。</p>
<p>在实际中，还需要根据应用的实际情况及时更新模型 _f_ 。例如，若数据发生了显著变化，则需要更新模型 _f_ 。因此，在实际部署机器学习模型时，上面的第3步和第4步是一个循环反复的过程。</p>
<p>一个经常与机器学习同时提起的相关领域是数据挖掘 （data mining）。数据挖掘和机器学习在很多时候都被（不严格地）混用，因为这两者有很多重叠的地方。传统意义上，机器学习更加注重于算法和理论方面，而数据挖掘更加注重实践方面。数据挖掘中的很多算法都来自于机器学习或者相关领域，少数来自于数据挖掘领域，如关联规则<br>（association rule）。</p>
<p>另一个与机器学习关联很深的领域是统计学。在统计学中，我们学习了很多传统的处理数据的方法，包括数据统计量的计算、模型的参数估计、假设检验等。但在实际问题中，很多情况下我们并不能直接使用统计学中的方法来解决问题。一方面，随着数据规模的扩大，统计学中很多传统的数据分析方法需要通过大量的计算才能得到结果，时效性不高；另一方面，传统的统计学方法更多地考虑了算法在数学上的性质，而忽略了如何在实际中更好地应用这些算法。</p>
<h2 id="1-2-机器学习算法的分类"><a href="#1-2-机器学习算法的分类" class="headerlink" title="1.2 机器学习算法的分类"></a>1.2 机器学习算法的分类</h2><p>在机器学习中，常用的算法可以分为监督型学习 （supervised learning）和非监督型学习 （unsupervised learning） ① 。</p>
<ul>
<li>在监督型学习中，除了输入数据 _x_ 外，我们还知道对应的输出 _y_ 。我们的目标是构建一个函数 _f_ ( _x_ )，使得 _f_ ( _x_ )能够预测输出 _y_ 。</li>
<li>在非监督型学习中，我们只有输入数据 _x_ ，没有对应的输出 _y_ 。我们的目标是从数据中学习数据本身存在的模式 （pattern）。例如，聚类分析 （cluster analysis）就是一个非监督型学习的典型例子，它通过分析样本之间的相似度来将样本划分为几个不同的聚类。</li>
</ul>
<p>在监督型学习中，输出 _y_ 一般称为目标变量 （target variable）或者因变量 （dependent variable），而输入 _x_ 称为解释变量 （explanatory variable）或者自变量 （independent variable）。</p>
<p>在实际中，在条件允许的情况下，我们偏好监督型学习。因为我们知道相应的目标变量的值，所以能够更加准确地构建模型，取得更好的效果。对于非监督型学习，在实际中，我们可以直接将其结果作为输出，但更多地是将其结果作为新的特征，再应用到监督型学习的算法中。例如，对于一组数据，可以先使用 _k_ 均值算法对数据进行聚类分析，然后将聚类分析的结果作为新的特征。本书将主要讨论监督型学习。</p>
<p>在监督型学习中，一般将整个数据集分为训练集 （training set）和测试集 （test set）。利用训练集中的数据，可以构建相应的模型<br>（model）或者学习器 （learner）。利用测试集，可以估计所构建模型的性能高低。在数据集中，我们使用样本 （sample）、数据点 （data point）或实例 （instance）来称呼其中的每个点。监督型学习可以进一步分为回归问题、分类问题等。我们将在1.3节利用具体的例子来介绍监督型学习。</p>
<h2 id="1-3-实际应用"><a href="#1-3-实际应用" class="headerlink" title="1.3 实际应用"></a>1.3 实际应用</h2><p>在本节中，我们将会介绍一些可用机器学习解决的实际问题，包括病人住院时间预测、信用分数估计、Netflix上的影片推荐和酒店推荐。每个例子都对应一类不同的机器学习问题。通过这些不同类型的机器学习问题，读者对机器学习可以有更多直观的感受。</p>
<h3 id="1-3-1-病人住院时间预测"><a href="#1-3-1-病人住院时间预测" class="headerlink" title="1.3.1 病人住院时间预测"></a>1.3.1 病人住院时间预测</h3><p>机器学习在医疗行业有着广泛的应用。我们以Heritage Health Prize ② 竞赛作为例子以说明如何使用机器学习来预测病人未来的住院时间。</p>
<p>在美国每年都有超过7000万人次住院。根据相关统计，2006年在护理病人住院上所花的无关费用就已经超过了300亿美元。如果我们能够根据病人的病历提前预测病人将来的住院时间，那么就可以根据病人的具体情况提前做好相关准备从而减少那些无谓的开销。同时，医院可以提前向病人发出预警，这样就能在降低医疗成本的同时提高服务质量。在从2011年开始的Heritage Health Prize竞赛（HHP） 中，竞争者成功地使用机器学习的方法，由病人的历史记录预测了病人在未来一年的住院时间。图1-1显示了竞赛中使用的病历数据的一部分样本。</p>
<p><img src="Image00000.jpg" alt></p>
<p>图1-1 病历数据示例</p>
<h3 id="1-3-2-信用分数估计"><a href="#1-3-2-信用分数估计" class="headerlink" title="1.3.2 信用分数估计"></a>1.3.2 信用分数估计</h3><p>在现实生活中，向银行申请贷款是比较常见的，如房屋贷款、汽车贷款等。银行在办理个人贷款业务时，会根据申请人的经济情况来估计申请人的还款能力，并根据不同还款能力确定安全的借款金额和相应的条款（如不同的利率）。在美国，每个成年人都有相应的信用分数<br>（credit score），用来衡量和评估借款者的还款能力和风险。</p>
<p>在估计申请者的还款能力时，需要搜集用户的多个方面的信息，包括：</p>
<ul>
<li>收入情况；</li>
<li>年龄、性别；</li>
<li>职业；</li>
<li>家庭情况，如子女数量等；</li>
<li>还款历史，包括未按时还款的记录、还款金额等；</li>
<li>现有的各种贷款和欠款情况等。</li>
</ul>
<p>如何将这些因素综合考虑从而决定借贷者的信用分数呢？直观地讲，可以使用一些简单的规则来确定信用分数。例如，某申请者的当前借款金额很高但收入一定，则进一步借款的风险很高，信用分数将会较低；又如，某申请者的某张信用卡在过去经常没有按时还款，则其信用分数也会较低。虽然使用简单的规则能够大致解决信用分数估计的问题，但是这个办法最大的问题是不能自适应地处理大量数据。随着时间的变化，申请者不还款的风险模型可能会发生变化，因此，相应的规则也需要修改。</p>
<p>银行通常可以得到海量的申请者数据和对应的历史数据。利用机器学习的方法，我们希望可以从这些申请者过去的还款记录中自适应地学习出相应的模型，从而能够“智能”地计算申请者的信用分数以了解贷款的风险。具体地讲，在机器学习模型中，将申请者的信息作为输入，我们可以计算申请者在未来能够按时还款的概率。作为一个典型的例子，FICO分数<br>③ 就是美国FICO公司利用机器学习模型开发出来的一个信用分数模型。</p>
<h3 id="1-3-3-Netflix上的影片推荐"><a href="#1-3-3-Netflix上的影片推荐" class="headerlink" title="1.3.3 Netflix上的影片推荐"></a>1.3.3 Netflix上的影片推荐</h3><p>Netflix是美国的一家网络视频点播公司，成立于1997年，到2015年该公司已经有了近7000万的订阅者，并且在世界上超过40个国家或地区提供服务。Netflix上的一项很重要的功能是根据用户的历史观看信息和喜好推荐相应的影片，如图1-2所示。2006年10月至2009年9月，Netflix公司举办了Netflix Prize ④ 比赛，要求参赛者根据用户对于一些电影的评价（1星～5星），推测用户对另外一些没有看过电影的评价。如果能够准确地预测用户对于那些没有看过的电影的评价，就可以相应地向这些用户推荐他们感兴趣的电影，从而显著提高推荐系统的性能和Netflix公司的盈利水平。</p>
<p><img src="Image00001.jpg" alt></p>
<p>图1-2 Netflix上的电影推荐</p>
<p>在Netflix Prize比赛中，获胜的标准是将Netflix现有推荐系统的性能提高10%。在2009年，BellKor’s Pragmatic Chaos队赢得了比赛。其主要方法是基于矩阵分解的推荐算法，并使用集成学习的方法综合了多种模型。Netflix Prize比赛显著地推动了推荐算法的研究，特别是基于矩阵分解的推荐算法的研究。在本书中，我们也将详细介绍这些推荐算法。</p>
<h3 id="1-3-4-酒店推荐"><a href="#1-3-4-酒店推荐" class="headerlink" title="1.3.4 酒店推荐"></a>1.3.4 酒店推荐</h3><p>Expedia是目前世界上最大的在线旅行代理（online travel agency，OTA）之一。它的一项很重要的业务是向用户提供酒店预订，作为用户和大量酒店之间的桥梁。对于用户的每个查询，Expedia需要根据用户的喜好，提供最优的排序结果，这样用户能够方便地从中选出最合适的酒店。</p>
<p>Expedia于2013年年底与国际数据挖掘大会 （International Conference on Data Mining，ICDM）联合举办了酒店推荐比赛。在该项比赛中，Expedia提供了实际数据，包括用户的查询以及其对所推荐结果点击或者购买的记录。在进行酒店推荐时，Expedia考虑了如下因素：</p>
<ul>
<li>用户的位置和酒店的位置；</li>
<li>酒店的特征，如酒店的价格、星级、位置吸引程度等；</li>
<li>用户过去预订酒店的历史，包括价格、酒店类型、酒店星级；</li>
<li>其他竞争对手的信息。</li>
</ul>
<p>根据用户的查询及用户的背景信息，Expedia返回推荐的酒店序列。在Expedia.com上，典型的酒店搜索界面如图1-3所示。根据返回的推荐结果，用户有3种选择：（1）付款预定推荐的酒店；（2）点击推荐的酒店但没有预订；（3）既没有点击也没有预订。显然，根据用户的反应，我们希望在理想的酒店推荐结果中，对应于第一种选择的酒店能够排在最前面，并且对应于第二种选择的酒店排在对应于第三种选择的酒店前面。</p>
<p><img src="Image00002.jpg" alt></p>
<p>图1-3 在Expedia.com上搜索酒店</p>
<h3 id="1-3-5-讨论"><a href="#1-3-5-讨论" class="headerlink" title="1.3.5 讨论"></a>1.3.5 讨论</h3><p>上文中的4个例子分别对应于机器学习中的4类典型问题：</p>
<ul>
<li>回归 （regression）；</li>
<li>分类 （classification）；</li>
<li>推荐 （recommendation）；</li>
<li>排序 （ranking）。</li>
</ul>
<p>在第一类问题中，首先需要为每个病人构建一个特征向量 _ <strong>x</strong> _ ，然后构建一个函数 _f_ ，使得可以用 _f_ ( _ <strong>x</strong> _ )来预测病人的住院时间 _y_ 。注意，这里要预测的量（病人的住院时间 _y_<br>）的范围是0～365（或者366），我们可以将其转化为回归问题。在回归问题中，目标变量是一个连续值。</p>
<p>在第二类问题中，需要为每个申请者构建一个特征向量 _ <strong>x</strong> _ ，而输出 _y_ 是0或者1，代表批准贷款或者不批准贷款。事实上，输出 _y_ 也可以是批准的概率。这是机器学习中典型的分类问题。在分类问题中，目标变量 _y_ 是一个离散变量。与回归问题类似，我们的目标是构建一个函数 _f_ ，使得 _f_ ( _ <strong>x</strong> _ )可以预测真实的 _y_ 。在典型的两类分类 （binary classification）问题中，目标变量的取值为0或者1（有时是−1或者1）。在多类分类 （multi-class classification）问题中，我们有多个类，而目标变量的取值是其中之一。</p>
<p>在第三类问题中，需要根据用户过去的历史为每个用户推荐相应的商品，这是一个典型的推荐问题。与回归和分类问题相比，我们需要为每个用户返回一个感兴趣的商品序列。</p>
<p>在第四类问题中，需要根据用户的输入（在上文的例子中是用户对于酒店的查询），从一系列对象（在这个例子中是酒店）中根据用户的需要返回一个对象的序列，使得该序列最前面的对象是用户最想要的。这类问题称为排序<br>（ranking）问题。同前面的回归问题和分类问题相比，排序问题需要考虑整个返回序列。与前面的影片推荐例子相比，在排序问题中我们需要明确的用户输入，而在影片推荐中我们只是根据用户过去的历史信息来进行推荐，用户没有进行明确的输入。</p>
<p>在实际应用中，机器学习的应用远远超出上面的几个例子。例如，近期非常热门的AlphaGo，谷歌公司在其中使用了深度学习 （deep learning）来学习围棋对弈；德国的蒂森克虏伯（ThyssenKrupp）集团作为电梯的主要制造商之一，应用机器学习来预测电梯发生故障的时间从而提前维修，降低电梯的综合运营成本；美国的很多大型零售商在开设新店时，都要搜集各个地区的各种信息和历史销售数据，通过建立机器学习模型的形式选择最优的店址。</p>
<h2 id="1-4-本书概述"><a href="#1-4-本书概述" class="headerlink" title="1.4 本书概述"></a>1.4 本书概述</h2><p>本书主要从解决实际问题的角度来介绍常用的机器学习算法。在1.3节中我们讨论了机器学习中常见的4类典型问题，基本上覆盖了目前实际中可以使用机器学习算法来解决的主要问题类型。在本书中，我们将主要讨论对应的4类算法，包括：</p>
<ul>
<li>回归算法；</li>
<li>分类算法；</li>
<li>推荐算法；</li>
<li>排序算法。</li>
</ul>
<p>其中回归算法和分类算法是两类最常用的算法，也是其他很多算法的基础，因此我们首先予以介绍。推荐系统在目前有了越来越多的应用，而排序算法在搜索引擎等领域也获得了广泛的应用，因此我们也会对常用的推荐算法和排序算法进行介绍。</p>
<p>在上面的4个例子中，我们可以构建多个不同的模型，希望它们之间能够取长补短，使得综合它们之后的模型的性能能够进一步提升。集成学习 （ensemble learning）是一类通过综合多个模型以得到更好性能的方法，对于回归问题、分类问题、推荐问题、排序问题都适用，因此我们会专门用一章来介绍集成学习。</p>
<p>本书的目标是尽量介绍实用的算法。读者在掌握我们讨论的机器学习算法后就可以实际使用R中的软件包来解决实际问题了。对于每种算法，我们首先介绍其基本原理。理解算法的基本原理非常重要，它是我们实际使用算法的基础。只有理解了不同算法的特点，才能在实际中根据不同数据的特点合理选择处理的算法。在本书中，我们使用R语言来介绍这些算法的实际使用。机器学习中的各种常用算法在R中都有一种甚至多种实现，而这些都是免费和开放的。用户可以直接使用R中对应的软件包来处理数据，构建相应的机器学习模型。实际使用算法是学习机器学习算法最有效的方式之一。我们希望读者阅读完算法的理论部分后，尽可能地使用R去实际处理数据和建立模型，以获得用机器学习解决问题的第一手经验。</p>
<p>这里我们需要强调，很多实用、有效的算法并不简单，甚至比较复杂。以决策树为例，其原理简单，在很多关于机器学习的书籍中都是予以重点介绍的，但是，由于决策树对于噪声比较敏感，在实际中很容易出现过拟合的现象，因此很少直接使用。然而，基于决策树的随机森林和提升树在实际中应用极为广泛。在本书中，我们也会介绍决策树，但主要目的是作为介绍随机森林和提升树的基础。</p>
<p>由于篇幅有限，本书将主要集中介绍那些最实用的算法。例如，我们没有介绍聚类分析 （cluster analysis）和关联规则 （association rule）。此外，我们也省略了一些常用算法。例如，在分类算法中，朴素贝叶斯分类器 （Naïve Bayesian Classifier）就没有介绍。感兴趣的读者可以参考相关读物了解有关内容。</p>
<p>算法的介绍只是本书的一部分。在使用机器学习算法处理实际问题之前，还需要进行如下步骤：</p>
<p>（1）数据探索 （data exploration）；</p>
<p>（2）数据预处理 （data preprocessing）；</p>
<p>（3）从原始数据中构建相应的特征，即特征工程 （feature engineering）。</p>
<p>在实际使用机器学习处理数据的过程中，数据探索和数据预处理是非常重要的步骤。通过数据探索，我们可以了解数据的特性，选用合理的预处理方法，如处理缺失数据等。此外，在很多情况下直接对原始数据使用机器学习算法并不能取得良好的效果，我们需要对数据进行一些变换以生成新的特征，这个过程称为特征工程。在实际使用机器学习算法解决问题时，特征工程是非常重要的一步。良好的特征是成功应用机器学习的关键点之一。</p>
<p>在得到算法构建的模型后，还需要评价和选择模型，包括：</p>
<ul>
<li>不同模型的评价标准；</li>
<li>从多个模型中选择最优模型的方法。</li>
</ul>
<p>注意，不同的算法有不同的评价标准，如分类算法和回归算法的评价标准就不同。因此，我们在介绍一类算法时，通常会介绍此类算法的评价标准。例如，介绍分类算法时，我们介绍了准确率、AUC等评价分类算法的标准以及在R中的计算方法，同时也介绍了交叉检验和R中的<code>caret</code> 包以帮助用户在R中进行模型选择。</p>
<p>此外，我们还介绍了R语言和必要的数学基础。本书广泛使用R语言介绍如何使用算法。R语言具有免费、开放、简单易学的特点，在工业界的使用越来越广泛；同时，R语言有大量免费的软件包可以使用，基本涵盖了机器学习的各个领域，本书所介绍的各个领域都能找到相应的R软件包。本书还介绍了常用的数学基础知识，包括概率统计、矩阵计算等，这样读者能够更加容易地理解和掌握算法的原理。</p>
<h3 id="1-4-1-本书结构"><a href="#1-4-1-本书结构" class="headerlink" title="1.4.1 本书结构"></a>1.4.1 本书结构</h3><p>本书大致可以分为两部分。前半部分介绍一些相关的基础知识，后半部分着重介绍各类算法。</p>
<p>由于我们在全书中都使用R来介绍如何使用各种机器学习算法，因此首先在第2章介绍R语言的基础知识。在第3章中，我们介绍相关的数学基础知识，包括概率统计的基础知识和矩阵计算的基础知识等。</p>
<p>第4章介绍数据的探索和预处理，包括数据类型、数据探索、数据预处理及数据可视化。</p>
<p>从第5章开始，我们着重介绍各类算法。我们首先从最基本的回归算法和分类算法开始讨论，然后介绍推荐算法和排序算法，最后介绍如何使用集成学习来综合多个模型以进一步提高模型的性能。</p>
<p>第5章介绍回归分析，包括常用的回归算法以及回归算法的评价和选取。我们从最基本的线性回归和最小二乘法开始讨论，然后讨论更加复杂的回归算法，包括岭回归、Lasso和Elastic Net。在介绍完多种回归算法之后，我们讨论如何评价和选取不同的回归算法。另外，我们讨论偏差-方差权衡 （bias-variance tradeoff），并讨论模型复杂度 （model complexity）的概念。最后，我们使用实际的案例分析来说明如何在R中使用和选取回归算法。</p>
<p>第6章讨论基本的分类算法，包括决策树、支持向量机和逻辑回归。我们引入了不同的损失函数 （loss function），并讨论不同的分类算法如何对应不同的损失函数，以及如何使用正则化项 （regularization）来控制模型的复杂度 。与回归算法的讨论类似，我们也会讨论如何评价和选取不同的分类算法。为了解决实际中更复杂的分类问题，我们将会详细讨论如何解决不平衡分类 （imbalanced classification）问题。本章还会介绍R中对应的软件包，这样读者可以直接尝试使用各种分类算法。另外，我们会着重介绍<code>caret</code> 包，这样读者能够简单地使用交叉检验 （cross validation）来为各种算法选取最优参数。</p>
<p>第7章介绍常用的推荐算法，主要包括基于相似度的推荐算法和基于矩阵分解的算法。我们还会介绍推荐算法的评价和选取，以帮助用户合理地选取算法。其中，基于相似度的算法包括基于内容的算法和基于邻域的算法；基于矩阵分解的算法包括：</p>
<ul>
<li>无矩阵分解的基准方法；</li>
<li>基于奇异值分解（SVD）的推荐算法；</li>
<li>基于SVD推荐算法的变体，有AFM模型、翻转的AFM模型、ASVD模型（或者SVD++模型）、翻转的ASVD模型、引入时间信息的模型。</li>
</ul>
<p>基于内容的推荐算法不是第7章的重点。对于基于矩阵分解的诸算法，我们推导了对应的随机梯度下降 （stochastic gradient descent）算法。对于基于邻域的推荐算法，我们将讨论基于用户的邻域推荐算法和基于商品的邻域推荐算法，并详细讨论基于邻域的推荐算法的核心部分：如何计算相似度和邻域。</p>
<p>第8章介绍排序算法，包括逐点排序 （pointwise ranking）算法、逐对排序 （pairwise ranking）算法和逐列排序<br>（listwise ranking）算法。我们将着重介绍较实用的LambdaMART算法。</p>
<p>集成学习 可以显著地提升多种算法的性能。在本书的最后一章，我们将介绍集成学习，包括基本思想和3类不同的集成学习方法：</p>
<ul>
<li>bagging的基本思想及典型例子随机森林；</li>
<li>boosting的基本思想及典型例子提升决策树；</li>
<li>stacking的基本思想及应用。</li>
</ul>
<p>同时，我们还会介绍R实现中流行的软件包，包括<code>randomForest</code> 和<code>gbm</code> 。</p>
<h3 id="1-4-2-阅读材料及其他资源"><a href="#1-4-2-阅读材料及其他资源" class="headerlink" title="1.4.2 阅读材料及其他资源"></a>1.4.2 阅读材料及其他资源</h3><p>这里我们介绍机器学习及相关领域的一些教材和相关资源。首先介绍一下目前流行的多种关于机器学习的教材。</p>
<p>参考文献[1]是一本早期的关于机器学习的教材。该书是一本入门读物，介绍了机器学习的很多基本概念和算法。参考文献[2]是机器学习领域影响很大的一本教材，讨论了很多比较实用的算法，但该书对于读者的数学基础要求较高。参考文献[3]是参考文献[2]的简化版，内容和讲解上都更基础一些。参考文献[4]和参考文献[5]是两本从贝叶斯统计角度讨论机器学习的流行教材，阅读的难度稍大。参考文献[5]介绍的内容稍微前沿一些，很多都是从近年的论文中直接总结的。</p>
<p>参考文献[6]从使用R进行实际建模的角度介绍了机器学习，覆盖了R中的很多软件包和具体用法，讨论了实际建模中的各个步骤。但该书对很多算法没有具体讲解原理和步骤，仅适用于各种模型的入门。参考文献[7]也是近期出版的一本使用R来介绍机器学习的著作，该书以应用为主，所介绍的算法过于基础。</p>
<p>关于模式识别方面的教材，我们推荐参考文献[8]。该书是关于模式识别的经典教材，难度适中。</p>
<p>关于数据挖掘方面的教材，我们推荐参考文献[9]。该书讲解了基本的分类算法、关联规则和聚类算法。其他具有代表性的教材包括参考文献[10]和参考文献[11]。参考文献[10]是一本较早从数据库角度讨论数据挖掘的教材，其中介绍的算法比较基础。参考文献[11]则是一本以WEKA<br>⑤ 为核心介绍数据挖掘的教材。此外，参考文献[12]的优点是贴近应用实际，使用实际中的一些应用例子来介绍算法。</p>
<p>在掌握机器学习算法时，要注意深度和广度的问题。作为一名机器学习的实践者，熟知多种机器学习算法是必需的；同时，对于那些最常用的算法，要知道其基本思想和底层实现。本书将讨论机器学习中的一些常用算法，但限于篇幅，很难做到面面俱到。读者可以根据自己的实际需要选择不同教材的相关章节阅读。</p>
<p>最前沿的关于机器学习的研究文章通常会在机器学习领域的顶级会议和期刊上发布或发表。此类顶级会议主要包括：</p>
<ul>
<li>Annual Conference on Neural Information Processing Systems（NIPS）</li>
<li>International Conference on Machine Learning（ICML）</li>
<li>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining（SIGKDD）</li>
</ul>
<p>机器学习领域的顶级期刊主要包括：</p>
<ul>
<li>《IEEE Transactions on Pattern Analysis and Machine Intelligence》（TPAMI）</li>
<li>《Journal of Machine Learning Research》（JMLR）</li>
<li>《Machine Learning Journal》</li>
</ul>
<p>这里我们只列出了部分顶级的会议和期刊。感兴趣的读者可以在互联网上找到更多的此类会议和期刊。注意，中国计算机学会在其官网上给出了各领域的推荐会议和期刊目录 ⑥ ，我们给出的列表主要涉及其中的人工智能、数据挖掘两个领域。</p>
<p>对于机器学习算法的广大应用者来说，最重要的问题是如何利用机器学习算法来解决实际问题。例如，如何将实际问题转化为一个能够直接应用机器学习算法的问题。网站kaggle.com有很多关于机器学习的竞赛，是一个提高这方面能力的优秀媒介。例如，前面讨论的Heritage Health Prize就是由kaggle.com组织的。kaggle.com还提供了过去竞赛的很多获胜方案。在解决具体实际问题时，如果问题和kaggle.com中的某个竞赛类似，则可以借鉴获胜方案。互联网上也有关于kaggle.com竞赛的介绍，例如，SlideShare网站<br>⑦ 就给出了一个很好的关于kaggle.com的介绍。</p>
<p>关于机器学习的实际应用工具，除了R之外，比较常用的还有Python中的scikit-learn ⑧ 。该软件包涵盖了机器学习中的很多实用算法，包括分类、回归、聚类、数据降维和预处理等。该软件包的文档极为完备。如果读者经常使用Python，scikit-learn是一个极好的机器学习库。WEKA也是数据挖据领域使用较多的一个软件包，该软件包集成了很多常用的机器学习算法，同时也提供了调用的API。当数据规模较大时，很多时候我们需要使用Hadoop平台上的Mahout库<br>⑨ 和Spark平台上的MLlib库 ⑩ 。在本书中，为了方便读者简单地使用各种算法，我们以R为基础来介绍各种算法的使用。</p>
<hr>
<p>① 在很多资料中还有第三类称为强化学习（reinforcement learning），近年来还有半监督型学习（semi-supervised learning）提出。本书主要涉及监督型学习和非监督型学习，不讨论强化学习和半监督型学习。</p>
<p>② <a href="http://www.heritagehealthprize.com/c/hhp" target="_blank" rel="noopener">http://www.heritagehealthprize.com/c/hhp</a></p>
<p>③ <a href="http://www.myfico.com/CreditEducation/WhatsInYourScore.aspx" target="_blank" rel="noopener">http://www.myfico.com/CreditEducation/WhatsInYourScore.aspx</a></p>
<p>④ <a href="http://www.netflixprize.com/" target="_blank" rel="noopener">http://www.netflixprize.com/</a></p>
<p>⑤ <a href="http://www.cs.waikato.ac.nz/ml/weka" target="_blank" rel="noopener">http://www.cs.waikato.ac.nz/ml/weka</a></p>
<p>⑥ <a href="http://www.ccf.org.cn/sites/ccf/paiming.jsp" target="_blank" rel="noopener">http://www.ccf.org.cn/sites/ccf/paiming.jsp</a></p>
<p>⑦ <a href="http://www.slideshare.net/ksankar/oscon-kaggle20" target="_blank" rel="noopener">http://www.slideshare.net/ksankar/oscon-kaggle20</a></p>
<p>⑧ <a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">http://scikit-learn.org/stable/</a></p>
<p>⑨ <a href="http://mahout.apache.org/" target="_blank" rel="noopener">http://mahout.apache.org/</a></p>
<p>⑩ <a href="https://spark.apache.org/docs/1.1.0/mllib-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/1.1.0/mllib-guide.html</a></p>
<h1 id="第2章-R语言"><a href="#第2章-R语言" class="headerlink" title="第2章 R语言"></a>第2章 R语言</h1><h2 id="2-1-R的简单介绍"><a href="#2-1-R的简单介绍" class="headerlink" title="2.1 R的简单介绍"></a>2.1 R的简单介绍</h2><p>R是一种自由、免费、开源的解释型编程语言，支持Unix、Windows、Mac等系统平台。与Perl、Python、JavaScript等其他解释型语言相比，R具有强大的数据分析工具和图形工具，支持多种设备上的分析和展示。</p>
<p>R语言的历史可以追溯到1976年贝尔实验室开发的一种用于数据探索、统计分析、作图的解释型编程语言——S语言。S系统由Richard A. Becker、John M. Chambers等人实现，它无须用户关心内存分配与数据结构细节，具有良好的可移植性与可扩展性[13] 。20世纪80年代末期，Richard A. Becker、John M. Chambers、Allan R. Wilks等人对S语言进行了功能方面的更新，参见参考文献[14]。1998年，为了表彰S系统在分析、可视化和操作数据方面的贡献，ACM授予其主要设计者John M. Chambers软件系统奖 ① 。在S语言的基础上，MathSoft公司 ② 研发了商业软件S-Plus，其特点有二：一是可以交互式地挖掘数据中的信息，并轻松实现新的统计方法；二是可以直接使用Excel、Lotus、Access、SAS、SPSS等软件的数据，具有极好的兼容性。有关S-Plus的数据结构、函数、作图以及面向对象编程，参见参考文献[15]。</p>
<p>1975年，MIT人工智能实验室为Lisp类语言研发了解释器Scheme[16] 。奥克兰大学的Ross Ihaka、Robert Gentleman等人综合了Scheme与S语言的优点研发出了R语言[17] 。R语言与S语言非常类似，但其底层实现和语法都来自Scheme。具体实现方面，Ross Ihaka、Robert Gentleman等人首先为Scheme的一个子集编写了解释器，然后分如下3个阶段向S语言靠拢：一是替换语言的语法解析器，使得表面上看来语法与S类似，但底层语法仍来自Scheme；二是用S的向量数据类型替换原有的标量数据类型，这一步改动较大；三是引入S语言中对参数延迟求值的思想。最终获得的R语言在功能上弱于S-PLUS，但同样具备很强的实用性——根据参考文献[17]，R在可移植性、计算效率、内存管理等方面都具有优势。</p>
<p>R语言的得名主要有两方面的原因，一方面是为了感谢S语言的正面影响，另一方面是因为两位主要研发者名字的首字母都为R[17] 。有关R的文档资源可以从<a href="https://cran.rstudio.com/manuals.html" target="_blank" rel="noopener">https://cran.rstudio.com/ manuals.html</a> 获取，各种操作系统环境下的常见问题可以到如下页面寻求解答：<a href="https://cran.rstudio.com/faqs.html" target="_blank" rel="noopener">https://cran.rstudio.com/ faqs.html</a> 。这里CRAN是Comprehensive R Archive Network的简称，意为R语言的综合网络资源所在。</p>
<p>由于其免费、开放及实用的特性，R在统计学、机器学习和数据科学 （data science）中越来越流行。很多算法都以第三方软件包的形式提供。在CRAN上，2016年4月时已有超过8000个软件包 ③ ，涵盖了统计学、机器学习、生物统计学等众多领域的大量算法。本书中介绍的大量机器学习算法在R中都有成熟的实现，因此本书的实例均基于R语言给出。在本章中，主要介绍R的下载、安装、基本语法以及软件包的使用。</p>
<h2 id="2-2-R的初步体验"><a href="#2-2-R的初步体验" class="headerlink" title="2.2 R的初步体验"></a>2.2 R的初步体验</h2><p>首先访问<a href="https://www.r-project.org/" target="_blank" rel="noopener">https://www.r-project.org/</a> ，下载安装3.0或更高版本的R。以Windows下的3.2.3版本为例，R图形界面启动后会给出如图2-1所示的命令行控制台，首先介绍R的版本、版权、贡献者及简单使用方法，最后给出默认的命令提示符“&gt;”。</p>
<p><img src="Image00003.jpg" alt></p>
<p>图2-1 R图形界面及命令行控制台</p>
<p>接下来，可以像使用MS-DOS系统一样，在控制台中编写R语言程序。例如，图2-1给出了获取工作目录的命令<code>getwd()</code> 、创建文件夹的命令<code>dir.create()</code> 及修改工作目录的命令<code>setwd()</code> 。但这样逐条输入命令毕竟比较麻烦，可以通过“文件”菜单下的“新建程序脚本”来编写R程序，通过“打开程序脚本”来读入已保存的R程序。</p>
<p>为了更方便地编写和调试R程序，可以从<a href="https://www.rstudio.com/products/RStudio/" target="_blank" rel="noopener">https://www.rstudio.com/products/RStudio/</a> 下载免费、开源的R语言集成开发环境——RStudio。RStudio有两个版本可供选择：单机版RStudio Desktop和服务器版RStudio Server，我们选择RStudio Desktop。图2-2给出了Windows下0.99.491版本的RStudio运行界面，包括菜单、内容区、命令行控制台、工作区环境、展示区等。其中，内容区不仅可以显示源代码，还可以给出指定数据结构的内容；命令行控制台可以提示程序中的错误或输出中间结果；工作区环境简要地对当前运行环境中的数据、变量、函数等进行描述；展示区列出文件目录、输出图表等信息。</p>
<p><img src="Image00004.jpg" alt></p>
<p>图2-2 RStudio界面</p>
<p>在图2-2中，我们通过几行简单的代码，读取了Data/Test-Chapter2.csv文件的Year、Number两列数据，并通过二维折线图进行了展示。接下来，本章将依次介绍R语言的基本语法、常用数据结构、使用技巧和软件包。</p>
<h2 id="2-3-基本语法"><a href="#2-3-基本语法" class="headerlink" title="2.3 基本语法"></a>2.3 基本语法</h2><p>从图2-2的例子可以看出，R语言允许直接使用变量，不需要预先定义数据类型。本节主要介绍R的语句和函数。</p>
<h3 id="2-3-1-语句"><a href="#2-3-1-语句" class="headerlink" title="2.3.1 语句"></a>2.3.1 语句</h3><p>本节主要介绍R语言的注释语句 、表达式语句 、函数调用 语句 和控制语句 。</p>
<h4 id="1．注释语句"><a href="#1．注释语句" class="headerlink" title="1．注释语句"></a>1．注释语句</h4><p>以符号<code>#</code> 开头的语句称为注释语句 。注释内容占用多行时，每行前面都需要加<code>#</code> 。在RStudio中，可以使用快捷键Ctrl+Shift+C来注释一块程序。对于已经注释的块程序，也可以使用快捷键Ctrl+Shift+C来去除注释。</p>
<h4 id="2．表达式语句"><a href="#2．表达式语句" class="headerlink" title="2．表达式语句"></a>2．表达式语句</h4><p>在R中，所有的变量、数据及函数都以对象 （object）的形式保存在内存中。对象的名字必须以字母开头（大小写皆可），中间可以包含数字、点（<code>.</code><br>）及下划线（<code>_</code> ）。例如，<code>a3.b</code> 、<code>a3_b</code> 都是合理的对象名，而<code>_a3</code> 则不是。此外，R是对大小写敏感的，因此<code>a</code> 和<code>A</code> 是不同的对象。</p>
<p>赋值功能可以用<code>=</code> 、<code>&lt;-</code> 或<code>-&gt;</code> 实现。例如，下面3条语句分别将变量<code>x</code> 、<code>y</code> 和<code>z</code> 赋值为<code>3</code> 、<code>4</code> 和<code>3.4</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x&lt;-3</span><br><span class="line">4-&gt;y</span><br><span class="line">z=3.4</span><br></pre></td></tr></table></figure>

</details>


<p>加、减、乘、除、乘方、取模、整数除法等算术运算分别用<code>+</code> 、<code>-</code> 、<code>*</code> 、<code>/</code> 、<code>^</code> 、<code>%%</code> 和<code>%/%</code> 实现。例如，在以上赋值的基础上，下面的语句执行之后，<code>x0</code> 、<code>x1</code> 、<code>x2</code> 、<code>x3</code> 、<code>x4</code> 、<code>x5</code> 、<code>x6</code> 的值分别为<code>6.4</code> 、<code>0.6</code> 、<code>12</code> 、<code>0.75</code> 、<code>81</code> 、<code>3</code> 和<code>0</code> 。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x0&lt;-x+z</span><br><span class="line">x1&lt;-y-z</span><br><span class="line">x2&lt;-x*y</span><br><span class="line">x3&lt;-x/y</span><br><span class="line">x4&lt;-x^y</span><br><span class="line">x5=x%%y</span><br><span class="line">x6=x%/%y</span><br></pre></td></tr></table></figure>

</details>


<p>注意，在R语言中，<code>+</code> 号可以用作续行符，因此，虽然<code>++x</code> 这样的写法语法正确，但如下赋值之后，<code>x7</code> 和<code>x</code> 的值都仍是<code>3</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x7=++x</span><br></pre></td></tr></table></figure>

</details>


<p>大于、小于、大于等于、小于等于、等于、不等于这些比较运算分别用<code>&gt;</code> 、<code>&lt;</code> 、<code>&gt;=</code> 、<code>&lt;=</code> 、<code>==</code> 和<code>!=</code> 表示。比较运算的结果为<code>TRUE</code> （<code>1</code> ）或<code>FALSE</code> （<code>0</code> ）。例如，字符型数据可以用单引号或者双引号分别表示为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  s1 = &apos;hello world&apos;</span><br><span class="line">s2 = &quot;hello world&quot;</span><br></pre></td></tr></table></figure>
<p>这里<code>s1</code> 和<code>s2</code> 是一样的，因此<code>s1==s2</code> 返回<code>TRUE</code> 。</p>
<p>与、或、非、异或这些逻辑运算分别用<code>&amp;</code> 、<code>|</code> 、<code>!</code> 和<code>xor</code> 实现。例如，<code>xor(x,y)</code> 在<code>x</code> 和<code>y</code> 的逻辑值不同时计算得到<code>TRUE</code> ，在<code>x</code> 和<code>y</code> 的逻辑值相同时计算得到<code>FALSE</code> 。对于后面将要介绍的向量数据类型，<code>x&amp;&amp;y</code> 只对向量<code>x</code> 和向量<code>y</code> 的第一个元素进行逻辑“与”运算，<code>x||y</code> 只对向量<code>x</code> 和向量<code>y</code> 的第一个元素进行逻辑“或”运算。假定向量<code>x</code> 和<code>y</code> 长度一致，则<code>x&amp;y</code> 对向量<code>x</code> 和向量<code>y</code> 的每个元素进行逻辑“与”运算，结果是一个相同长度的向量；类似地，<code>x|y</code> 对向量<code>x</code> 和向量<code>y</code> 的每个元素进行逻辑“或”运算。此外，<code>!x</code> 和<code>xor(x,y)</code> 也都返回一个向量。</p>
<p>此外，使用<code>a %in% b</code> ，可判断<code>a</code> 是否在<code>b</code> 中间。例如，<code>1 %in% 1:5</code> 是<code>TRUE</code> ，<code>1 %in% 2:5</code> 是<code>FALSE</code> 。</p>
<h4 id="3．函数调用语句"><a href="#3．函数调用语句" class="headerlink" title="3．函数调用语句"></a>3．函数调用语句</h4><p>函数调用语句 的一般形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">函数名(实际参数1, 实际参数2,...)</span><br></pre></td></tr></table></figure>
<p>与其他程序设计语言类似，R语言中调用函数的过程也是一个把实际参数赋给函数定义中的形式参数，然后执行函数体并返回函数值的过程。图2-1和图2-2中给出的<code>getwd()</code> 、<code>dir.create()</code> 、<code>setwd()</code> 、<code>rm()</code> 和<code>plot()</code> 都是函数调用语句；<code>D&lt;-read.csv()</code> 是函数调用语句和赋值语句组成的复合语句。</p>
<h4 id="4．控制语句"><a href="#4．控制语句" class="headerlink" title="4．控制语句"></a>4．控制语句</h4><p>控制语句 用于R程序的执行流程，主要包括条件判断语句、循环语句、跳转语句等。本节讨论的条件判断语句包括<code>if</code> 、<code>if…else</code> 等，循环语句包括<code>for</code> 、<code>while</code> 、<code>repeat</code> 等，跳转语句包括<code>break</code> 等。在R中，最常用的循环语句是<code>for</code> 和<code>while</code> ，下面给出示例代码。</p>
<p>示例1：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n = 10</span><br><span class="line">for(i in 1:n)&#123;</span><br><span class="line">    if (i%%2)</span><br><span class="line">        k = 1</span><br><span class="line">    else</span><br><span class="line">        k = 0</span><br><span class="line">    cat(&quot;k=&quot;, k)</span><br><span class="line">    cat(&quot;\n&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">k= 1</span><br><span class="line">k= 0</span><br><span class="line">k= 1</span><br><span class="line">k= 0</span><br><span class="line">k= 1</span><br><span class="line">k= 0</span><br><span class="line">k= 1</span><br><span class="line">k= 0</span><br><span class="line">k= 1</span><br><span class="line">k= 0</span><br></pre></td></tr></table></figure>

</details>


<p>示例2：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n = 10</span><br><span class="line">s = 0</span><br><span class="line">while(n&gt;0)&#123;</span><br><span class="line">    s &lt;-s + n</span><br><span class="line">    n &lt;-n-1</span><br><span class="line">    if(n==3)</span><br><span class="line">        break</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>


<p>当循环结束时，<code>s</code> =49。</p>
<h3 id="2-3-2-函数"><a href="#2-3-2-函数" class="headerlink" title="2.3.2 函数"></a>2.3.2 函数</h3><p>本节介绍一些常用的函数。在后面的章节中，我们会根据需要介绍更多的函数。</p>
<h4 id="1．基本函数"><a href="#1．基本函数" class="headerlink" title="1．基本函数"></a>1．基本函数</h4><p>我们把安装R之后即可使用的函数称为基本函数 ，如图2-1中的<code>getwd()</code> 、<code>dir.create()</code> 、<code>setwd()</code> 等。本节介绍部分基本函数的常见用法。</p>
<p>（1）<code>ls()</code> 函数 。函数<code>ls()</code> 的功能是列出内存中的所有对象名，包括数据、函数等。例如，对于2.3.1节的示例1，函数<code>ls()</code> 的调用结果为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;i&quot; &quot;k&quot; &quot;n&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>当对象较多时，可以增加正则表达式模式参数<code>pat</code> 以实现更好的匹配。例如，可以用<code>ls(pat=&quot;u&quot;)</code> 来指定名称中包含字符<code>u</code> 的对象，或者用<code>ls(pat=&quot;^s&quot;)</code> 来指定名称以字母<code>s</code> 开头的对象。若想获取对象的类型、值等信息，可调用函数<code>ls.str()</code> ，此时上述<code>pat</code> 参数同样适用。</p>
<p>（2）<code>help()</code> 函数 。读者可以调用帮助函数<code>help()</code> 获取关于函数的进一步介绍信息。例如，若想获得函数<code>plot</code> 的帮助信息，可调用<code>?plot</code> 、<code>help(plot)</code> 或<code>help(&quot;plot&quot;)</code> 。</p>
<p>（3）<code>rm()/remove()</code> 函数 。我们在图2-2中调用的第一个函数就是<code>rm(list=ls())</code> ，其功能是删除内存中的所有对象。我们可以借助上述<code>pat</code> 参数实现选择性删除，也可以通过<code>rm(k)</code> 、<code>rm(i,n)</code> 、<code>rm(i,k,n,sum)</code> 等调用对内存中的一个或多个对象进行点名删除。<code>remove()</code> 函数的使用与<code>rm()</code> 函数完全相同。</p>
<p>（4）<code>paste()</code> 函数 和<code>paste0()</code> 函数 。我们通过一个例子展示一下<code>paste()</code> 函数的功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paste(&apos;hello&apos;, &apos;world&apos;, sep=&apos;-&apos;)</span><br></pre></td></tr></table></figure>
<p>对应的输出是<code>&quot;hello-world&quot;</code> 。<code>paste()</code> 函数的作用就是将多个字符型对象串起来，其中<code>sep</code> 参数指定串起来时的分隔符。在<code>paste</code> 函数中，<code>sep</code> 的默认值为<code>&quot; &quot;</code> （空格字符）。使用<code>paste()</code> 函数时，输入的字符串对象可以是多个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paste(&apos;hello&apos;, &apos;world&apos;, &apos;from&apos;, &apos;R&apos;, sep=&apos;-&apos;)</span><br></pre></td></tr></table></figure>
<p>返回的结果是<code>&quot;hello-world-from-R&quot;</code> 。下面给出一个更复杂的关于<code>paste()</code> 函数的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paste(c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),1:7,sep=&quot;-&quot;)</span><br></pre></td></tr></table></figure>
<p>这里<code>paste()</code> 函数的第一个参数只有3个元素，因此会循环补足以便与第二个参数中的1～7相匹配。注意，<code>1:7</code> 并不是字符型对象，<code>paste()</code> 函数会将其转换为字符型对象。所得到的结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;x-1&quot; &quot;y-2&quot; &quot;z-3&quot; &quot;x-4&quot; &quot;y-5&quot; &quot;z-6&quot; &quot;x-7&quot;</span><br></pre></td></tr></table></figure>
<p>如果我们将<code>sep</code> 设为<code>&quot;&quot;</code> （空字符），<code>paste(c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;),1:7,sep=&quot;&quot;)</code> 对应的输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;x1&quot; &quot;y2&quot; &quot;z3&quot; &quot;x4&quot; &quot;y5&quot; &quot;z6&quot; &quot;x7&quot;</span><br></pre></td></tr></table></figure>
<p>很多时候，我们希望分隔字符为空，这时推荐使用<code>paste0</code> 函数。例如，<code>paste0(c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;),1:7)</code> 的输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;x1&quot; &quot;y2&quot; &quot;z3&quot; &quot;x4&quot; &quot;y5&quot; &quot;z6&quot; &quot;x7&quot;</span><br></pre></td></tr></table></figure>
<p>可以看出，上述调用与在<code>paste</code> 函数中将<code>sep</code> 设为空字符的效果是一样的。当然，在<code>paste0</code> 函数中，我们也可以显式地指定<code>sep</code> 。</p>
<p>（5）<code>cat()</code> 函数 。以用户自定义的形式输出结果。例如，前面我们在输出中间结果时，用</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat(&quot;k=&quot;,k)</span><br><span class="line">cat(&quot;\n&quot;)</span><br></pre></td></tr></table></figure>

</details>


<p>实现了拼接输出和换行，获得了比<code>print()</code> 函数更好的输出效果。</p>
<p>（6）<code>plot()</code> 函数 。使用函数<code>plot(x,y)</code> 可以作图画出<code>y</code> 相对于<code>x</code> 变化的情况。参数<code>type</code> 可以用来指定图中点的形状。假设我们使用向量<code>Year</code> 记录年份，使用向量<code>Number</code> 记录每年对应的数据，则使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(Year, Number, type=&apos;o&apos;)</span><br></pre></td></tr></table></figure>
<p>可以获得与图2-2右下角类似的折线图。注意，如果生成绘图后继续调用<code>plot</code> 函数，通常会覆盖之前的绘图。</p>
<p>（7）<code>c()</code> 函数 。该函数的功能是把若干个参数组合成一个向量或者列表，函数的返回值即为组合的结果。例如，下面的语句把4个值组合为向量<code>x</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &lt;-c(1, 0, 2, 9)</span><br></pre></td></tr></table></figure>
<p>利用<code>c()</code> 函数，也能将多个<code>x</code> 连接起来。例如：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x1 &lt;-c(1,0,2,9)</span><br><span class="line">x2 &lt;-c(4,3)</span><br><span class="line">x3 &lt;-c(x1, x2)</span><br></pre></td></tr></table></figure>

</details>


<p>则<code>x3</code> 为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 1 0 2 9 4 3</span><br></pre></td></tr></table></figure>

</details>


<h4 id="2．自定义函数"><a href="#2．自定义函数" class="headerlink" title="2．自定义函数"></a>2．自定义函数</h4><p>R语言允许以如下的形式自定义函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  函数名 &lt;-function(形式参数1,形式参数2,...)&#123;</span><br><span class="line">    函数体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>函数调用的方式见2.3.1节。</p>
<p>例如，下面的代码首先定义了一个求和函数，然后以3、4作为实际参数调用该函数：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">aml_sum &lt;-function(a, b) &#123;</span><br><span class="line">  r &lt;-a+b</span><br><span class="line">  return (r)</span><br><span class="line">&#125;</span><br><span class="line">x&lt;-3</span><br><span class="line">y&lt;-4</span><br><span class="line">z&lt;-aml_sum(x,y)</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

</details>


<p>程序的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 7</span><br></pre></td></tr></table></figure>

</details>


<p>注意，上面对<code>r</code> 的赋值仅在函数定义的内部可见，在程序的其他位置<code>r</code> 是没有定义的。采用如下的超级赋值 （super assignment）运算符可以使<code>r</code> 从局部变量变为全局变量：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r &lt;&lt;-a+b</span><br></pre></td></tr></table></figure>

</details>


<p>在前面的函数定义中，<code>{</code> 和<code>}</code> 分别表示函数体的开始与结束。<code>{</code> 写在第一行的行末仅是一种书写风格，亦可单独成行书写为如下形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  函数名 &lt;-function(形式参数1, 形式参数2, ...)</span><br><span class="line">&#123;</span><br><span class="line">    函数体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果函数体只有一条语句，大括号可以省略。</p>
<h2 id="2-4-常用数据结构"><a href="#2-4-常用数据结构" class="headerlink" title="2.4 常用数据结构"></a>2.4 常用数据结构</h2><p>数据结构指的是程序设计语言所支持的数据组织方式。前面提到，R语言中把操作的实体称为“对象”，因此本书所讨论的数据结构，实际上就是“对象”的数据组织方式，主要包括向量<br>（vector）、因子 （factor）、矩阵 （matrix）、数组 （array）、数据框 （data frame）和列表<br>（list）等。其中向量与列表的概念相似，但向量要求其组成元素的类型相同，而列表对此不作要求，可视为“泛化的向量”。数组是一个 _k_ 维的数据表格，而矩阵是数组的二维特例。在实际数据处理中，我们一般使用矩阵表示数据即可。本节着重介绍向量、因子、矩阵、数据框和列表。</p>
<h3 id="2-4-1-向量"><a href="#2-4-1-向量" class="headerlink" title="2.4.1 向量"></a>2.4.1 向量</h3><h4 id="1．向量和基本数据类型"><a href="#1．向量和基本数据类型" class="headerlink" title="1．向量和基本数据类型"></a>1．向量和基本数据类型</h4><p>首先，R中的基本类型 （base type）是向量 （vector），而不是标量<br>（scalar）。在R中，向量由同一类型的若干元素组成，而且我们可以使用下标系统来访问每个元素。注意，在R中，下标都是从1开始的。根据存储元素类型的不同，向量可以分为如下几类：</p>
<ul>
<li><p>整数型 （integer）；</p>
</li>
<li><p>数值型 （numeric）；</p>
</li>
<li><p>字符型 （character）；</p>
</li>
<li><p>逻辑型 （logical）；</p>
</li>
<li><p>复数型 （complex）。</p>
</li>
</ul>
<p>严格地讲，整数型是数值型的一个特例。在实际数据处理中，复数型很少遇到，因此我们主要讨论前面的4种类型。</p>
<p>下面4条语句分别得到逻辑型向量<code>w</code> 、整数型向量<code>x</code> 、数值型向量<code>y</code> 和字符型向量<code>z</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w &lt;-c(TRUE, FALSE, TRUE)</span><br><span class="line">x = 1:9</span><br><span class="line">assign(&quot;y&quot;, c(2.5, 4.2, 1.8))</span><br><span class="line">z &lt;-c(&quot;m1&quot;,&quot;a2&quot;,&quot;c3&quot;,&quot;h4&quot;,&quot;i5&quot;,&quot;n6&quot;,&quot;e7&quot;)</span><br></pre></td></tr></table></figure>

</details>


<p>这里<code>assign</code> 函数可替换为<code>&lt;-</code> 。注意，在同一向量中不能包含不同类型的元素。这里可以使用<code>class()</code> 函数来检查生成的这4个向量的类型：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; class(w)</span><br><span class="line">[1] &quot;logical&quot;</span><br><span class="line">&gt; class(x)</span><br><span class="line">[1] &quot;integer&quot;</span><br><span class="line">&gt; class(y)</span><br><span class="line">[1] &quot;numeric&quot;</span><br><span class="line">&gt; class(z)</span><br><span class="line">[1] &quot;character&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>在R中，逻辑值的表示为<code>TRUE</code> 和<code>FALSE</code> ，也可以简写为<code>T</code> 和<code>F</code> 。下面两条语句的效果是完全一样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  w1 &lt;-TRUE</span><br><span class="line">w1 &lt;-T</span><br></pre></td></tr></table></figure>
<p>可以直接使用下标来访问向量中的一个或者多个元素。如果要访问向量<code>y</code> 中的第2个元素，可直接使用<code>y[2]</code> 。例如，语句</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y[2])</span><br></pre></td></tr></table></figure>

</details>


<p>将输出</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 4.2</span><br></pre></td></tr></table></figure>

</details>


<p>使用<code>y[1:2]</code> 可以访问<code>y</code> 中的前两个元素。<code>print(y[1:2])</code> 的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 2.5 4.2</span><br></pre></td></tr></table></figure>

</details>


<p>此外，使用前面介绍的<code>c()</code> 函数可以将若干个向量组成一个新的向量。例如：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p &lt;-c(1/y,y)</span><br><span class="line">print(p)</span><br></pre></td></tr></table></figure>

</details>


<p>将输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 0.4000000 0.2380952 0.5555556 2.5000000 4.2000000 1.8000000</span><br></pre></td></tr></table></figure>
<p>前面在介绍控制语句时，我们用过如下语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for(i in 1:n)&#123;...&#125;</span><br></pre></td></tr></table></figure>
<p>这里<code>1:n</code> 就是从<code>1</code> 到<code>n</code> 步长为1的整数向量。如果改为<code>n:1</code> 的形式，则循环体中<code>i</code> 从<code>n</code> 开始依次递减到1，即从<code>n</code> 到<code>1</code> 步长为−1的整数向量。具体来说，<code>v1:v2</code> 生成如下序列：</p>
<ul>
<li><p>如果<code>v1&gt;v2</code> ，则步长为−1，生成的向量为<code>v1，v1-1，v1-2，…，v2</code> ；</p>
</li>
<li><p>如果<code>v1&lt;v2</code> ，则步长为1，生成的向量为<code>v1，v1+1，v1+2，…，v2</code> 。</p>
</li>
</ul>
<p>这里我们简单讨论一下生成向量的最后一个元素。当步长为正时，最后一个元素是小于等于<code>v2</code> 的最大值；当步长为负时，最后一个元素是大于等于<code>v2</code> 的最小值。例如，<code>1.5:4</code> 生成的向量为<code>c(1.5,2.5,3.5)</code> ，<code>4.5:1</code> 生成的向量为<code>c(4.5,3.5,2.5,1.5)</code> 。</p>
<p>更一般的序列可以使用<code>seq()</code> 函数生成。例如，我们可以使用<code>seq(1,9,by=2)</code> 生成1～9的奇数，这里第一个参数是起点，第二个参数是终点，参数<code>by</code> 指定了步长。</p>
<p>在实际中，很多时候需要先生成一定长度的向量，再逐步修改其中的元素。在这种情况下，我们可以使用<code>rep</code> 函数生成一个所有元素都相同的向量。下面的代码产生了3个长度为5的向量（但类型不同），其中<code>v_ch</code> 中每个元素都是空字符，<code>v_num</code> 中每个元素都是<code>0</code> ，<code>v_logic</code> 中每个元素都是<code>FALSE</code> 。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_ch &lt;-rep(&apos;&apos;, 5)</span><br><span class="line">v_num &lt;-rep(0, 5)</span><br><span class="line">v_logic &lt;-rep(FALSE, 5)</span><br></pre></td></tr></table></figure>

</details>


<p>其对应的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; v_ch</span><br><span class="line">[1] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot;</span><br><span class="line">&gt; v_num</span><br><span class="line">[1] 0 0 0 0 0</span><br><span class="line">&gt; v_logic</span><br><span class="line">[1] FALSE FALSE FALSE FALSE FALSE</span><br></pre></td></tr></table></figure>

</details>


<p>上面的<code>v_ch</code> 、<code>v_num</code> 和<code>v_logic</code> 可以用下面3个函数得到，效果完全一样：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_ch &lt;-character(5)</span><br><span class="line">v_num &lt;-numeric(5)</span><br><span class="line">v_logic &lt;-logical(5)</span><br></pre></td></tr></table></figure>

</details>


<p>这3个函数的输入参数表示生成向量的长度。</p>
<p>在实际中，很多时候还需要显式地进行类型转换。常用的类型转换函数有：</p>
<ul>
<li><p><code>as.integer</code></p>
</li>
<li><p><code>as.numeric</code></p>
</li>
<li><p><code>as.character</code></p>
</li>
<li><p><code>as.logical</code></p>
</li>
</ul>
<p>如<code>as.numeric(&#39;3.4&#39;)</code> 返回<code>3.4，as.logical(1)</code> 返回<code>TRUE</code> ，<code>as.logical(0)</code> 返回<code>FALSE</code> ，<code>as.character(3.4)</code> 返回<code>&quot;3.4&quot;</code> ，<code>as.integer(&#39;3.4&#39;)</code> 返回<code>3</code> 。</p>
<p>在R中，无论数据的类型如何，缺失值都使用<code>NA</code> 表示。对<code>NA</code> 进行任何操作所得的结果都是<code>NA</code> 。例如，<code>NA+1</code> 、<code>NA&gt;0</code> 等的结果都是<code>NA</code> 。对于<code>NA</code> 值，我们一般使用函数<code>is.na()</code> 来处理。下面是一个简单的例子。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; w &lt;-NA</span><br><span class="line">&gt; is.na(w)</span><br><span class="line">[1] TRUE</span><br></pre></td></tr></table></figure>

</details>


<p>由于<code>w</code> 是<code>NA</code> ，因此<code>is.na(w)</code> 输出值为<code>TRUE</code> 。</p>
<h4 id="2．向量的运算"><a href="#2．向量的运算" class="headerlink" title="2．向量的运算"></a>2．向量的运算</h4><p>（1）基本运算。向量的基本运算直接施加在其组成元素上。例如，语句</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x &lt;-c(16, 33, 45, 88)</span><br><span class="line">y &lt;-x/2</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

</details>


<p>将得到如下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1]  8.0 16.5 22.5 44.0</span><br></pre></td></tr></table></figure>
<p>在上述算术赋值的基础上，赋值语句</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">large &lt;-x&gt;17</span><br></pre></td></tr></table></figure>

</details>


<p>可以得到值为<code>(False, TRUE, TRUE, TRUE)</code> 的逻辑向量<code>large</code> 。</p>
<p>（2）常用函数。对于给定的向量<code>x</code> ，我们列出R中提供的常用函数：</p>
<ul>
<li><p><code>length(x)</code> 返回其元素个数；</p>
</li>
<li><p><code>sum(x)</code> 计算其元素的和；</p>
</li>
<li><p><code>prod(x)</code> 计算其元素的乘积；</p>
</li>
<li><p><code>max(x)</code> 和<code>min(x)</code> 分别计算其中元素的最大值与最小值；</p>
</li>
<li><p><code>mean(x)</code> 计算<code>x</code> 中元素的平均值；</p>
</li>
<li><p><code>median(x)</code> 计算<code>x</code> 中元素的中位数；</p>
</li>
<li><p><code>which.min(x)</code> 返回<code>x</code> 中最小元素所对应的下标；</p>
</li>
<li><p><code>which.max(x)</code> 返回<code>x</code> 中最大元素所对应的下标；</p>
</li>
<li><p><code>which(x&gt;a)</code> 返回<code>x</code> 中大于<code>a</code> 的元素所对应的下标。在<code>which</code> 函数中，用户可以设定不同的条件得到不同结果；</p>
</li>
<li><p><code>unique(x)</code> 返回<code>x</code> 中所有不同的取值；</p>
</li>
<li><p><code>range(x)</code> 返回<code>x</code> 中的最小值和最大值，等同于<code>c(min(x), max(x))</code> ；</p>
</li>
<li><p><code>rev(x)</code> 将<code>x</code> 中所有元素的顺序反转，如<code>rev(1:9)</code> 的结果是<code>9:1</code> ；</p>
</li>
<li><p><code>sort(x)</code> 将<code>x</code> 中元素按照升序排列。<code>sort(x, decreasing=TRUE)</code> 可将<code>x</code> 中元素按照降序排列。</p>
</li>
</ul>
<p>除此之外，还可以使用<code>log</code> 、<code>exp</code> 、<code>sin</code> 、<code>cos</code> 、<code>tan</code> 、<code>sqrt</code> 、<code>var</code> 等数学函数。例如，如下语句首先对向量中的元素由大到小排序，然后计算每个元素的平方根：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &lt;-c(36, 16, 49, 81)</span><br><span class="line">sqrt(sort(x, TRUE))</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 9 7 6 4</span><br></pre></td></tr></table></figure>

</details>


<h3 id="2-4-2-因子"><a href="#2-4-2-因子" class="headerlink" title="2.4.2 因子"></a>2.4.2 因子</h3><p>在R中，因子 是用来表示分类变量 （categorical variable）的一种有效方法。所谓分类变量就是取值来自一个集合的变量。顾名思义，分类变量的每一个取值表示该变量的一个状态（或者分类）。一个典型的例子就是分类问题中的类别。用R处理分类问题时，样本的类别信息通常以因子的形式保存。下面我们用一个分类变量的例子来解释因子数据。假设有一个向量<code>gender_v</code> 用来描述一组对象的性别：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gender_v &lt;-c(&apos;Male&apos;, &apos;Female&apos;, &apos;Female&apos;, &apos;Male&apos;)</span><br></pre></td></tr></table></figure>
<p>如果将每个元素都存储为字符型数据，则会占用比较多的存储空间。为节省存储空间，可以用正整数的形式保存每个元素，并保存正整数到各个取值之间的映射关系。这样得到的数据就是因子数据，而所有不同的取值则称为水平<br>（level）。在实际中，水平值一般是比较长的字符，而我们只保存整数则可以显著地节省保存空间。</p>
<p>下面我们使用具体的例子来说明因子数据。首先将<code>gender_v</code> 转化为因子数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f_gender &lt;-factor(gender_v)</span><br></pre></td></tr></table></figure>
<p>在R中打印<code>f_gender</code> 和<code>gender_v</code> ，可以直接看出它们的不同：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; gender_v</span><br><span class="line">[1] &quot;Male&quot;   &quot;Female&quot; &quot;Female&quot; &quot;Male&quot;  </span><br><span class="line">&gt; f_gender</span><br><span class="line">[1] Male   Female Female Male  </span><br><span class="line">Levels: Female Male</span><br></pre></td></tr></table></figure>

</details>


<p>注意，<code>f_gender</code> 保存的并不是字符型数据（输出中没有引号）。此外，<code>Levels</code> 还列出了所有不同的水平值。</p>
<p>通过使用函数<code>levels</code> ，可以得到<code>f_gender</code> 对应的所有水平值，下面是对应的输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; levels(f_gender)</span><br><span class="line">[1] &quot;Female&quot; &quot;Male&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>还可以直接将<code>f_gender</code> 转化为数值型数据，对应的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; f_gender_num &lt;-as.numeric(f_gender)</span><br><span class="line">&gt; f_gender_num</span><br><span class="line">[1] 2 1 1 2</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出<code>f_gender_num[1]</code> 为2，表示其值对应于<code>levels(f_gender)[2]</code> ，也就是<code>&#39;Male&#39;</code> 。我们可以调用<code>as.character()</code> 函数直接得到<code>f_gender</code> 中每个元素对应的字符：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; f_gender_ch &lt;-as.character(f_gender)</span><br><span class="line">&gt; f_gender_ch</span><br><span class="line">[1] &quot;Male&quot;   &quot;Female&quot; &quot;Female&quot; &quot;Male&quot;</span><br></pre></td></tr></table></figure>

</details>


<h3 id="2-4-3-矩阵"><a href="#2-4-3-矩阵" class="headerlink" title="2.4.3 矩阵"></a>2.4.3 矩阵</h3><h4 id="1．矩阵的定义"><a href="#1．矩阵的定义" class="headerlink" title="1．矩阵的定义"></a>1．矩阵的定义</h4><p>在介绍矩阵之前，首先引入“维度”的概念。对于R语言中的对象，可以通过在<code>[]</code> 中指定下标的方式访问其中的某个元素。例如，在下面的例子中，可以通过<code>x[2]</code> 访问向量<code>x</code> 的第2个元素。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &lt;-c(1, 2, 3, 4, 5, 6, 7, 8)</span><br></pre></td></tr></table></figure>
<p>如果<code>[]</code> 中只能指定一个下标，则称该对象是一维的；如果<code>[]</code> 中最多只能指定两个下标，则称该对象是二维的，依此类推。在上面的例子中，我们不能直接将<code>x</code> 视为二维向量，通过<code>x[2,4]</code> 或<code>x[4,2]</code> 访问其第8个元素。</p>
<p>R语言允许用函数<code>dim</code> 来设定对象的维度。例如，语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  dim(x) &lt;-c(2,4)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<p>将产生如下输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">      [,1] [,2] [,3] [,4]</span><br><span class="line">[1,]    1    3    5    7</span><br><span class="line">[2,]    2    4    6    8</span><br></pre></td></tr></table></figure>

</details>


<p>从而我们可以将<code>x</code> 看作二维向量，并分别用<code>x[2,3]</code> 、<code>x[1,4]</code> 访问元素<code>6</code> 和<code>7</code> 。本书把数值或复数型的二维向量称为矩阵 ④ 。</p>
<p>除上述方法外，还可以利用<code>array</code> 函数或<code>matrix</code> 函数来得到矩阵。例如，上述矩阵可通过如下两种函数调用之一来构造：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array(1:8, dim=c(2,4))</span><br><span class="line">matrix(1:8, nrow=2, byrow=FALSE)</span><br></pre></td></tr></table></figure>

</details>


<p>这里可以使用<code>array</code> 函数来创建更高维的数组。例如，下面的代码产生了一个三维数组：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(1:27, dim=c(3,3,3))</span><br></pre></td></tr></table></figure>
<p>因为本书主要讨论矩阵，所以我们主要使用<code>matrix</code> 函数。例如，创建一个4×3的零矩阵可以使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M &lt;-matrix(0, nrow=4, ncol=3)</span><br></pre></td></tr></table></figure>
<p>或者简写为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M &lt;-matrix(0, 4, 3)</span><br></pre></td></tr></table></figure>
<p>在R中，矩阵的每行或者每列都可以有相应的名字。使用<code>colnames(M)</code> 可以获取和修改<code>M</code> 的列名，使用<code>rownames(M)</code> 可以获取和修改<code>M</code> 的行名：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  &gt; colnames(M)</span><br><span class="line">NULL</span><br><span class="line">&gt; rownames(M)</span><br><span class="line">NULL</span><br></pre></td></tr></table></figure>
<p>这里由于没有预先设置，因此列名和行名都为空。下列语句将<code>M</code> 的列名分别改为<code>&#39;F1&#39;</code> 、<code>&#39;F2&#39;和&#39;F3&#39;</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; colnames(M) &lt;-c(&apos;F1&apos;, &apos;F2&apos;, &apos;F3&apos;)</span><br></pre></td></tr></table></figure>
<p>这样<code>M</code> 就有了列名：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; M</span><br><span class="line">     F1 F2 F3</span><br><span class="line">[1,]  0  0  0</span><br><span class="line">[2,]  0  0  0</span><br><span class="line">[3,]  0  0  0</span><br><span class="line">[4,]  0  0  0</span><br></pre></td></tr></table></figure>

</details>


<p>在实际使用矩阵或稍后讨论的数据框来处理数据时，基本上每行对应一个样本，每列对应一个变量，为每列赋予一个列名有助于更好地处理数据。</p>
<h4 id="2．矩阵的运算"><a href="#2．矩阵的运算" class="headerlink" title="2．矩阵的运算"></a>2．矩阵的运算</h4><p>（1）基本运算。对于两个矩阵<code>A</code> 和<code>B</code> ，其矩阵乘积在R中用<code>A %*% B</code> 表示。例如，下面的代码将2行4列的矩阵<code>x</code> 与4行2列的矩阵<code>y</code> 相乘：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &lt;-array(c(1, 2, 3, 4, 5, 6, 7, 8), c(2,4))</span><br><span class="line">y &lt;-array(c(1, 2, 3, 4, 5, 6, 7, 8), c(4,2))</span><br><span class="line">z &lt;-x %*% y</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

</details>


<p>获得的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">     [,1] [,2]</span><br><span class="line">[1,]   50  114</span><br><span class="line">[2,]   60  140</span><br></pre></td></tr></table></figure>

</details>


<p>此外，可以分别使用<code>A+B</code> 、<code>A-B</code> 来实现加法和减法。</p>
<p>在上面的例子中，如果已知<code>x</code> 和<code>z</code> ，则求解<code>y</code> 的过程是求矩阵积的逆过程，可通过如下函数调用实现：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &lt;-solve(x, z)</span><br></pre></td></tr></table></figure>

</details>


<p>注意，直接调用<code>solve(x)</code> 将得到<code>x</code> 的逆矩阵<code>x</code> −1 ，但不推荐用如下方式计算<code>y</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &lt;-solve(x) %*% z</span><br></pre></td></tr></table></figure>

</details>


<p>（2）常用函数。假设用<code>A</code> 和<code>B</code> 表示两个矩阵，则</p>
<ul>
<li><p><code>t(A)</code> 表示<code>A</code> 的转置矩阵；</p>
</li>
<li><p><code>crossprod(A, B)</code> 计算<code>t(A)</code> 与<code>B</code> 的矩阵积；</p>
</li>
<li><p><code>tcrossprod(A, B)</code> 计算<code>A</code> 与<code>t(B)</code> 的矩阵积；</p>
</li>
<li><p><code>diag(A)</code> 返回矩阵<code>A</code> 的对角元素；</p>
</li>
<li><p><code>cbind(A,B,…)</code> 函数将矩阵（或者向量）<code>A</code> 、<code>B</code> 沿水平方向结合起来，<code>A</code> 和<code>B</code> 中的每一列都是结合之后矩阵的列，函数名中的<code>&#39;c&#39;</code> 表示<code>column</code> ；</p>
</li>
<li><p><code>rbind(A,B,…)</code> 函数矩阵（或者向量）<code>A</code> 、<code>B</code> 沿竖直方向结合起来，<code>A</code> 和<code>B</code> 中的每一行都是结合之后矩阵的行，函数名的的<code>&#39;r&#39;</code> 表示<code>row</code> ；</p>
</li>
<li><p><code>nrow(A)</code> 返回矩阵<code>A</code> 的行数；</p>
</li>
<li><p><code>ncol(A)</code> 返回矩阵<code>A</code> 的列数；</p>
</li>
<li><p><code>dim(A)</code> 同时返回矩阵<code>A</code> 的行数和列数；</p>
</li>
<li><p><code>rowMeans(A)</code> 计算矩阵<code>A</code> 中每一行的均值，返回结果是一个向量；</p>
</li>
<li><p><code>rowSums(A)</code> 计算矩阵<code>A</code> 中每一行的和，返回结果是一个向量；</p>
</li>
<li><p><code>colMeans(A)</code> 计算矩阵<code>A</code> 中每一列的均值，返回结果是一个向量；</p>
</li>
<li><p><code>colSums(A)</code> 计算矩阵<code>A</code> 中每一列的和，返回结果是一个向量；</p>
</li>
<li><p><code>det(A)</code> 计算方阵<code>A</code> 的行列式；</p>
</li>
<li><p><code>eigen(A)</code> 计算方阵<code>A</code> 的特征值与特征向量；</p>
</li>
<li><p><code>head(A,k)</code> 返回矩阵<code>A</code> 的前<code>k</code> 行；</p>
</li>
<li><p><code>tail(A,k)</code> 返回矩阵<code>A</code> 的最后<code>k</code> 行。</p>
</li>
</ul>
<p>相关数学概念参见第3章。</p>
<h3 id="2-4-4-数据框"><a href="#2-4-4-数据框" class="headerlink" title="2.4.4 数据框"></a>2.4.4 数据框</h3><h4 id="1．数据框的定义"><a href="#1．数据框的定义" class="headerlink" title="1．数据框的定义"></a>1．数据框的定义</h4><p>数据框 （data frame）是R语言中最接近SAS和SPSS数据集的数据结构，它在形式上类似于矩阵，但允许各列的数据类型不同，可以很方便地表示实际中的各种数据，因此是R中最常用的数据结构之一。</p>
<p>下面我们通过读取文件和手工创建两种方式来定义数据框。</p>
<p>一方面，我们可以通过读取文件的方式创建数据框：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  D &lt;-read.csv(&apos;Data/Test-Chapter2.csv&apos;)</span><br><span class="line">print(D)</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>获得的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  Year Volume Number</span><br><span class="line">1 2010   3000    307</span><br><span class="line">2 2011   3500    350</span><br><span class="line">3 2012   4000    480</span><br><span class="line">4 2013   4500    550</span><br><span class="line">5 2014   5000    700</span><br><span class="line">6 2015   5500    800</span><br><span class="line">7 2016   6000   1000</span><br></pre></td></tr></table></figure>

</details>


<p>另一方面，我们也可以手工创建数据框：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Year &lt;-2010:2016</span><br><span class="line">Volume &lt;-c(3000,3500,4000,4500,5000,5500,6000)</span><br><span class="line">Number &lt;-c(307, 350, 480, 550, 700, 800, 1000)</span><br><span class="line">df2 &lt;-data.frame(Year, Volume, Number)</span><br><span class="line">print(df2)</span><br></pre></td></tr></table></figure>

</details>


<p>获得的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  Year Volume Number</span><br><span class="line">1 2010   3000    307</span><br><span class="line">2 2011   3500    350</span><br><span class="line">3 2012   4000    480</span><br><span class="line">4 2013   4500    550</span><br><span class="line">5 2014   5000    700</span><br><span class="line">6 2015   5500    800</span><br><span class="line">7 2016   6000   1000</span><br></pre></td></tr></table></figure>

</details>


<h4 id="2．数据框的操作"><a href="#2．数据框的操作" class="headerlink" title="2．数据框的操作"></a>2．数据框的操作</h4><p>（1）基本操作。数据框中的元素可以按列名或按坐标访问。例如，对于前面定义的数据框<code>df2</code> ，我们可以通过<code>df2[1:3,2:3]</code> 访问其第2～3列的前3行，也可以通过<code>df2$Volume</code> 、<code>df2[[2]]</code> 或<code>df2[,&#39;Volume&#39;]</code> 访问其第2列。2.4.5节将进行更加详细的讨论。</p>
<p>（2）常用函数。对于给定的数据框<code>dfa</code> 和<code>dfb，rbind(dfa,dfb)</code> 表示按<code>dfa</code> 中的列顺序依次捆绑<code>dfa</code> 和<code>dfb</code> 的行，生成一个新的数据框。下面我们使用具体的例子来说明<code>rbind</code> 的使用。首先我们产生两个数据框<code>df3</code> 和<code>df4</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">No. &lt;-c(4001:4005)</span><br><span class="line">Score &lt;-c(82,77,90,63,85)</span><br><span class="line">Age &lt;-c(20, 19, 21, 22, 20)</span><br><span class="line">df3 &lt;-data.frame(No., Score, Age)</span><br><span class="line">No. &lt;-c(4006:4007)</span><br><span class="line">Score &lt;-c(92,65)</span><br><span class="line">Age&lt;-c(20, 19)</span><br><span class="line">df4 &lt;-data.frame(Score, Age, No.)</span><br></pre></td></tr></table></figure>

</details>


<p>使用<code>rbind(df3,df4)</code> 得到的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   No. Score  Age</span><br><span class="line">1 4001    82  20</span><br><span class="line">2 4002    77  19</span><br><span class="line">3 4003    90  21</span><br><span class="line">4 4004    63  22</span><br><span class="line">5 4005    85  20</span><br><span class="line">6 4006    92  20</span><br><span class="line">7 4007    65  19</span><br></pre></td></tr></table></figure>

</details>


<p>注意，上述代码中<code>df4</code> 与<code>df3</code> 的列顺序不同。类似地，<code>cbind(dfa,dfb)</code> 表示按<code>dfa</code> 中的行顺序依次捆绑<code>dfa</code> 和<code>dfb</code> 的列，生成一个新的数据框。例如，在上述代码的基础上，代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  Grade &lt;-c(&apos;B&apos;,&apos;C&apos;,&apos;A&apos;,&apos;D&apos;,&apos;B&apos;)</span><br><span class="line">df5 &lt;-data.frame(Grade)</span><br><span class="line">cbind(df3,df5)</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>得到的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   No. Score Age Grade</span><br><span class="line">1 4001    82  20     B</span><br><span class="line">2 4002    77  19     C</span><br><span class="line">3 4003    90  21     A</span><br><span class="line">4 4004    63  22     D</span><br><span class="line">5 4005    85  20     B</span><br></pre></td></tr></table></figure>

</details>


<p>可见，<code>df3</code> 的值并没有因为之前的<code>rbind(df3,df4)</code> 调用而改变。此外，调用<code>cbind</code> 函数时，也可以直接以<code>Grade</code> 作为第二个参数，结果一样。</p>
<p>利用<code>colnames</code> 和<code>rownames</code> 可以访问或者改变数据框的列名和行名。利用<code>head</code> 和<code>tail</code> 函数，可以探索数据框的前面和最后若干行。这些函数的使用可参见矩阵部分的相应讨论。</p>
<p>利用<code>expand.grid</code> 函数，可以将参数的不同组合保存在一个数据框中。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  age_v &lt;-6:8</span><br><span class="line">gender_v &lt;-c(&apos;Male&apos;, &apos;Female&apos;)</span><br><span class="line">D &lt;-expand.grid(age=age_v, gender=gender_v)</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>得到的D为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  age gender </span><br><span class="line">1   6   Male</span><br><span class="line">2   7   Male</span><br><span class="line">3   8   Male</span><br><span class="line">4   6 Female</span><br><span class="line">5   7 Female</span><br><span class="line">6   8 Female</span><br></pre></td></tr></table></figure>

</details>


<p><code>expand.grid()</code> 函数在机器学习中，特别是交叉检验中为模型选定参数时很实用。</p>
<p>在R的实际使用中，对于很多数据，通常都是通过数据框的形式从硬盘上导入、导出。在R中，常用的文件读取函数包括<code>read.table()</code> 、<code>read.csv()</code> 、<code>read.delim()</code> 等。例如，对于Data/Test-Chapter2.csv，可以通过以下代码将其导入内存，并保存在数据框<code>D</code> 中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D &lt;-read.csv(&apos;Data/Test-Chapter2.csv&apos;)</span><br></pre></td></tr></table></figure>
<p>注意，无论是Windows系统还是Unix系统，R中的文件路径永远使用分隔符<code>/</code> 。读取<code>D</code> 后，获得如下输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  Year Volume  Number</span><br><span class="line">1 2010   3000    307</span><br><span class="line">2 2011   3500    350</span><br><span class="line">3 2012   4000    480</span><br><span class="line">4 2013   4500    550</span><br><span class="line">5 2014   5000    700</span><br><span class="line">6 2015   5500    800</span><br><span class="line">7 2016   6000   1000</span><br></pre></td></tr></table></figure>

</details>


<p>对于csv文件 （comma separated values file，即以逗号作为分隔符的文件），在R中最常用的读取函数就是<code>read.csv()</code><br>；而<code>read.table()</code> 函数比<code>read.csv()</code> 函数更加通用一些。在<code>read.csv()</code> 函数中，常用的控制参数有：</p>
<ul>
<li><p><code>header</code> ，一个逻辑型值，表示文件是否有文件头；</p>
</li>
<li><p><code>sep</code> ，一个字符变量，表示文件的分隔符。默认值是<code>&quot;,&quot;</code> 。对于一般的csv文件，不需设置。对于其他文件，如由制表符分隔的文件，可将<code>sep</code> 设为<code>&quot;\t&quot;。</code></p>
</li>
</ul>
<p>同样，我们也可以将R中的数据写到文件中。与前面导入函数对应的导出函数是<code>write.table()</code> 和<code>write.csv()</code><br>（注意没有<code>write.delim()</code> 函数）。利用<code>write.table()</code> 和<code>write.csv()</code> 函数，我们可以将数据框写到文本文件Data/Test-Chapter2-rewrite.csv中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">write.csv(D, &apos;Data/Test-Chapter2-rewrite.csv&apos;)</span><br></pre></td></tr></table></figure>
<p>更多函数的用法可以直接使用<code>help</code> 函数来获取。</p>
<h3 id="2-4-5-列表"><a href="#2-4-5-列表" class="headerlink" title="2.4.5 列表"></a>2.4.5 列表</h3><p>R的列表 （list）是一个由对象的有序集合构成的对象。列表中包含的对象又称为它的分量<br>（component）。列表中的分量可以为任何类型，包括列表。下面给出列表的几个具体例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &lt;-list(1:4, &apos;b&apos;, c(&apos;ca&apos;, &apos;cb&apos;, &apos;cc&apos;), c(FALSE, TRUE, TRUE), c(1.9, -1.7))</span><br><span class="line">y &lt;-list(c1=1:4, c2=c(&apos;ca&apos;, &apos;cb&apos;, &apos;cc&apos;), c3=c(FALSE, TRUE, TRUE))</span><br></pre></td></tr></table></figure>

</details>


<p>这里的两个列表<code>x</code> 和<code>y</code> 中都包含了多种不同类型的数据。列表在R中使用广泛，后面讨论的很多机器学习算法返回的模型就是以列表形式保存的。注意，与向量不同的是，在R中我们需要调用<code>list()</code> 函数而不是<code>c()</code> 函数来构建列表。构建完列表后，可以使用<code>length()</code> 函数得到其中分量的数目，使用<code>is.list()</code> 函数来判定一个对象是否是列表对象，使用<code>c()</code> 函数合并多个列表。</p>
<p>对于列表，我们可以同时使用“<code>[[]]</code> ”和“<code>[]</code> ”来操作列表。使用“<code>[[]]</code> ”可以访问和修改列表中的单个成员；使用“<code>[]</code><br>”可以得到列表的一个子列表，其结果仍然是列表。对于列表要特别注意“<code>[[]]</code> ”和“<code>[]</code> ”的区别。下面用具体的例子加以详细说明，假设<code>x</code> 是上面定义的列表。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; x1 &lt;-x[1]</span><br><span class="line">&gt; x1</span><br><span class="line">[[1]]</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line"></span><br><span class="line">&gt; is.list(x1)</span><br><span class="line">[1] TRUE</span><br><span class="line">&gt; x2 &lt;-x[1:2]</span><br><span class="line">&gt; x2</span><br><span class="line">[[1]]</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line"></span><br><span class="line">[[2]]</span><br><span class="line">[1] &quot;b&quot;</span><br><span class="line"></span><br><span class="line">&gt; is.list(x2)</span><br><span class="line">[1] TRUE</span><br><span class="line">&gt; x3 &lt;-x[[1]]</span><br><span class="line">&gt; x3</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line">&gt; is.list(x3)</span><br><span class="line">[1] FALSE</span><br></pre></td></tr></table></figure>

</details>


<p>这里<code>x[1]</code> 和<code>x[1:2]</code> 都是列表，而<code>x[[1]]</code> 则是<code>x</code> 中的第一个分量，不是列表类型。</p>
<p>我们可以进一步修改列表x中的数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  x[[2]] &lt;-c(&apos;b&apos;, &apos;c&apos;)</span><br><span class="line">x[[3]][1] &lt;-&apos;ca_new&apos;</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>这里我们将其第二个成员改为一个向量，其第三个成员是向量，我们将该向量中的第一个元素改为<code>&#39;ca_new&#39;</code> 。修改之后的<code>x</code> 如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; x</span><br><span class="line">[[1]]</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line"></span><br><span class="line">[[2]]</span><br><span class="line">[1] &quot;b&quot; &quot;c&quot;</span><br><span class="line"></span><br><span class="line">[[3]]</span><br><span class="line">[1] &quot;ca_new&quot; &quot;cb&quot;     &quot;cc&quot;    </span><br><span class="line"></span><br><span class="line">[[4]]</span><br><span class="line">[1] FALSE  TRUE  TRUE</span><br><span class="line"></span><br><span class="line">[[5]]</span><br><span class="line">[1]  1.9 -1.7</span><br></pre></td></tr></table></figure>

</details>


<p>列表一个很方便使用的特点是，可以给列表中的每个成员赋予一个名字，这样就可以使用名字存取数据了。使用<code>names()</code> 函数可以得到列表中每个分量对应的名字。同时也可以使用<code>names()</code> 函数来修改各个分量的名字。在前面的例子中，列表<code>x</code> 的各个分量没有名字：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  &gt; names(x)</span><br><span class="line">NULL</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>使用如下语句可以给各分量命名：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; names(x) &lt;-c(&apos;c_int&apos;, &apos;c_ch1&apos;, &apos;c_ch2&apos;, &apos;c_logical&apos;, &apos;c_numeric&apos;)</span><br><span class="line">&gt; names(x)</span><br><span class="line">[1] &quot;c_int&quot;     &quot;c_ch1&quot;     &quot;c_ch2&quot;     &quot;c_logical&quot; &quot;c_numeric&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>在列表中，可以用“<code>[[]]</code> ”或者“<code>$</code> ”来存取和修改分量，如下面的例子所示：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; x$c_int</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line">&gt; x[[&quot;c_int&quot;]]</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line">&gt; x$c_i</span><br><span class="line">[1] 1 2 3 4</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出<code>x$c_int</code> 等价于<code>x[[&quot;c_int&quot;]]</code> 。注意<code>x$c_int</code> 和<code>x$c_i</code> 都对应<code>x</code> 中的第一个成员，这是因为列表中分量名字可以简写，只要能很好地区分各个分量就行。也可以使用“[]”和分量的名字来获取和修改子序列：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; x[c(&apos;c_int&apos;, &apos;c_numeric&apos;)]</span><br><span class="line">$c_int</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line"></span><br><span class="line">$c_numeric</span><br><span class="line">[1]  1.9 -1.7</span><br></pre></td></tr></table></figure>

</details>


<p>当成员的名字保存在另一个变量中时，使用“<code>[[]]</code> ”和成员名字的做法特别有效。例如，可以用如下语句将<code>x$c_int</code> 和<code>x$c_numeric</code> 中的所有元素都乘以<code>2</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name_list &lt;-c(&apos;c_int&apos;, &apos;c_numeric&apos;)</span><br><span class="line">for (n in name_list) </span><br><span class="line">  x[[n]] &lt;-2 * x[[n]]</span><br></pre></td></tr></table></figure>

</details>


<p>当然，也可以在使用<code>list()</code> 函数构建列表时直接给每个分量赋予相应的名字:</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; y &lt;-list(c1=1:4, c2=c(&apos;ca&apos;, &apos;cb&apos;, &apos;cc&apos;), c3=c(FALSE, TRUE, TRUE))</span><br><span class="line">&gt; y</span><br><span class="line">$c1</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line"></span><br><span class="line">$c2</span><br><span class="line">[1] &quot;ca&quot; &quot;cb&quot; &quot;cc&quot;</span><br><span class="line"></span><br><span class="line">$c3</span><br><span class="line">[1] FALSE  TRUE  TRUE</span><br></pre></td></tr></table></figure>

</details>


<p>此外，列表也是可以扩充的。我们可以往上面的<code>y</code> 中再增加一个或者多个分量：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; y[[4]] &lt;-c(4,1)</span><br><span class="line">&gt; y[[&apos;c_num&apos;]] &lt;-c(1.9, -1.7)</span><br><span class="line">&gt; y</span><br><span class="line">$c1</span><br><span class="line">[1] 1 2 3 4</span><br><span class="line"></span><br><span class="line">$c2</span><br><span class="line">[1] &quot;ca&quot; &quot;cb&quot; &quot;cc&quot;</span><br><span class="line"></span><br><span class="line">$c3</span><br><span class="line">[1] FALSE  TRUE  TRUE</span><br><span class="line"></span><br><span class="line">[[4]]</span><br><span class="line">[1] 4 1</span><br><span class="line"></span><br><span class="line">$c_num</span><br><span class="line">[1]  1.9 -1.7</span><br></pre></td></tr></table></figure>

</details>


<p>当我们使用<code>y[[4]]</code> 增加第四个分量时没有给定相应的名字，所以其名字为空。这里可以使用<code>names()</code> 函数来检查<code>y</code> 中各分量的名字：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  &gt; names(y)</span><br><span class="line">[1] &quot;c1&quot;    &quot;c2&quot;    &quot;c3&quot;    &quot;&quot;      &quot;c_num&quot;</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>函数<code>c()</code> 也适用于列表，其功能是将多个列表合并成一个列表。下面是一个简单的例子：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&gt; L1 &lt;-list(c1=c(1,4), c2=c(&apos;aa&apos;, &apos;dd&apos;))</span><br><span class="line">&gt; L2 &lt;-list(c3=c(TRUE, FALSE), c4=c(4.3, 1.5))</span><br><span class="line">&gt; LC &lt;-c(L1, L2)</span><br><span class="line">&gt; L1</span><br><span class="line">$c1</span><br><span class="line">[1] 1 4</span><br><span class="line"></span><br><span class="line">$c2</span><br><span class="line">[1] &quot;aa&quot; &quot;dd&quot;</span><br><span class="line"></span><br><span class="line">&gt; L2</span><br><span class="line">$c3</span><br><span class="line">[1]  TRUE FALSE</span><br><span class="line"></span><br><span class="line">$c4</span><br><span class="line">[1] 4.3 1.5</span><br><span class="line"></span><br><span class="line">&gt; LC</span><br><span class="line">$c1</span><br><span class="line">[1] 1 4</span><br><span class="line"></span><br><span class="line">$c2</span><br><span class="line">[1] &quot;aa&quot; &quot;dd&quot;</span><br><span class="line"></span><br><span class="line">$c3</span><br><span class="line">[1]  TRUE FALSE</span><br><span class="line"></span><br><span class="line">$c4</span><br><span class="line">[1] 4.3 1.5</span><br></pre></td></tr></table></figure>

</details>


<h3 id="2-4-6-下标系统"><a href="#2-4-6-下标系统" class="headerlink" title="2.4.6 下标系统"></a>2.4.6 下标系统</h3><p>利用R中的下标系统 ，可以方便地访问R中很多对象的元素。在R中，下标可以是数值型的或者逻辑型的。前面已经讲解了一些下标使用的例子，这里再深入讨论一下。</p>
<p>首先，在R中使用下标必须使用中括号，而小括号则是用来指定函数参数的。例如，<code>x[1]</code> 表示<code>x</code> 中的第一个元素，而<code>x(1)</code> 则指调用函数<code>x</code> ，且指定输入参数为<code>1</code> 。其次，下标值从1开始。再次，下标可以根据元素类型分为数值型和逻辑型，也可以根据维度分为一维、二维等。</p>
<p>数值型的下标是最简单的下标。对于向量<code>x=1:10</code> ，可以使用<code>x[3]</code> 访问或者改变<code>x</code> 中的第3个元素：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[3] &lt;-0</span><br></pre></td></tr></table></figure>

</details>


<p>下标也可以是一个数值型的向量。下面的代码将<code>x</code> 中的第1个和第3个元素置为0：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[c(1,3)] &lt;-0</span><br></pre></td></tr></table></figure>

</details>


<p>当然也可以使用序列来指定<code>x</code> 中的多个元素：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[3:6] &lt;-0</span><br></pre></td></tr></table></figure>

</details>


<p>上述语句表示将<code>x</code> 中的第3个～第6个元素置为0。</p>
<p>此外，负号也可以用在下标中，表示排除对应下标的意思。例如，<code>x[-3]</code> 表示除第3个元素外的所有元素，<code>x[-c(1,3)]</code> 表示除第1个和第3个元素之外的所有元素，<code>x[-(3:6)]</code> 表示除第3～6个元素之外的所有元素。下面的代码将<code>x</code> 中的第1个和第3个元素置为0，其他元素置为1：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  x &lt;-1:10</span><br><span class="line">x[c(1,3)] &lt;-0</span><br><span class="line">x[-c(1,3)] &lt;-1</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>执行完代码之后<code>x</code> 为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  &gt; x</span><br><span class="line">[1] 0 1 0 1 1 1 1 1 1 1</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>第二类下标是逻辑型下标。使用逻辑型下标时，下标是逻辑型值，一般是某些判定条件。我们仍以向量为例进行说明。<code>x[x&lt;5] &lt;-0</code> 表示将<code>x</code> 中小于5的元素全部置为<code>0</code> ，<code>x[x%%2==0]=0</code> 表示将<code>x</code> 中的偶数全部置为<code>0</code> 。</p>
<p>对于矩阵或者数据框<code>M</code> ，可以使用二维的下标系统来访问，如<code>M[i, j]</code> 访问<code>M</code> 中第<code>i</code> 行第<code>j</code> 列的元素。此外，还可以通过下标系统来访问一行或者一列，如<code>M[i,]</code> 访问第<code>i</code> 行，<code>M[,j]</code> 访问第<code>j</code> 列。</p>
<p>可以使用<code>data()</code> 函数来载入R或者R中软件包提供的数据。例如，可以用<code>data(iris)</code> 载入R中提供的<code>iris</code> 数据，之后得到一个变量名和数据名相同的数据框。下面的代码载入了<code>iris</code> 数据并显示了前面几行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  data(iris)</span><br><span class="line">head(iris)</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; data(iris)</span><br><span class="line">&gt; head(iris)</span><br><span class="line">   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species</span><br><span class="line">1          5.1         3.5          1.4         0.2  setosa</span><br><span class="line">2          4.9         3.0          1.4         0.2  setosa</span><br><span class="line">3          4.7         3.2          1.3         0.2  setosa</span><br><span class="line">4          4.6         3.1          1.5         0.2  setosa</span><br><span class="line">5          5.0         3.6          1.4         0.2  setosa</span><br><span class="line">6          5.4         3.9          1.7         0.4  setosa</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，<code>iris</code> 数据框中含有5个列：<code>Sepal.Length</code> 、<code>Sepal.Width</code> 、<code>Petal.Length</code> 、<code>Petal.Width</code> 和<code>Species</code> 。在R中可以使用列名来访问和改变数据框中的元素。具体来说，我们可以使用<code>iris$Sepal.Width</code> 来访问<code>Sepal.Width</code> 列。利用列名，还可以进一步访问其中的元素，例如：</p>
<ul>
<li><p>访问<code>iris</code> 中<code>Sepal.Width</code> 列中的第2个元素，即<code>iris$Sepal.Width[2]</code> 、<code>iris[2,2]</code> 或<code>iris[2, &#39;Sepal.Width&#39;]</code> ；</p>
</li>
<li><p>访问<code>iris</code> 中<code>Sepal.Width</code> 列中的第1～10个元素，即<code>iris$Sepal.Width[1:10]</code> 、<code>iris[1:10,2]</code> 或<code>iris[1:10, &#39;Sepal.Width&#39;]。</code></p>
</li>
</ul>
<h2 id="2-5-公式对象和apply函数"><a href="#2-5-公式对象和apply函数" class="headerlink" title="2.5 公式对象和apply函数"></a>2.5 公式对象和apply函数</h2><p>本节介绍R中的一些使用技巧，包括R中的公式对象及<code>apply</code> 函数族 。</p>
<p>在R中，利用公式对象 （formula object），可以描述建模时各个变量之间的关系。假设要构建一个线性回归模型，我们希望用变量 _x_ 1、 _x_ 2、 _x_ 3来预测变量 _y_ 。从数学上讲，这几个变量之间的关系可以使用下面的公式来描述：</p>
<p>_y_ = _a_ × _x_ 1+ _b_ × _x_ 2+ _c_ × _x_ 3+ _d_</p>
<p>在R中，可以利用公式 _y_ ~ _x_ 1+ _x_ 2+ _x_ 3来描述它们之间的关系。可以看出，公式中加号的含义与R中加号通常的含义不同。</p>
<p>公式的典型形式是 _y_ ~ _model_ ，其中 _y_ 是响应变量， _model_ 是一些元素项的集合，而~表示 _y_ 是 _model_ 的一个函数。特别地，公式 _y_ ~.表示 _y_ 是因变量，而其他变量是自变量。</p>
<p>在下面的代码中，我们使用<code>mtcars</code> 来构建一个简单的线性回归模型，R提供了该数据集，我们只需要使用<code>data(mtcars)</code> 就可以载入数据。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data(mtcars)</span><br><span class="line">formula_demo &lt;-as.formula(mpg ~ cyl + disp + hp + gear)</span><br><span class="line">class(formula_demo)</span><br><span class="line">M_lm &lt;-lm(formula_demo, mtcars)</span><br><span class="line">class(M_lm)</span><br><span class="line">print(M_lm)</span><br><span class="line">summary(M_lm)</span><br></pre></td></tr></table></figure>

</details>


<p>这里想用<code>cyl</code> 、<code>disp</code> 、<code>hp</code> 、<code>gear这</code> 4个变量来预测<code>mpg</code> 变量，因此，首先构建了一个<code>formula_demo</code> 公式对象。我们使用<code>class(formula_demo)</code> 检查<code>formula_demo</code> 的类别，其对应的输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  &gt; class(formula_demo)</span><br><span class="line">[1] &quot;formula&quot;</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>可以看到我们创建了一个公式对象。接下来，我们使用该公式和数据框<code>mtcars</code> 构建一个线性回归模型<code>M_lm</code> ，并使用<code>class(M_lm)</code> 来检查<code>M_lm</code> 的类别，最后使用<code>print</code> 和<code>summary</code> 函数来输出<code>M_lm</code> 的相关信息，其输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&gt; class(M_lm)</span><br><span class="line">[1] &quot;lm&quot;</span><br><span class="line">&gt; print(M_lm)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = formula_demo, data = mtcars)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)          cyl         disp           hp         gear  </span><br><span class="line">    27.4234      -0.8662      -0.0119      -0.0305       1.4231  </span><br><span class="line"></span><br><span class="line">&gt; summary(M_lm)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = formula_demo, data = mtcars)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max </span><br><span class="line">-4.3716 -2.3319 -0.8279  1.3156  7.0782 </span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">            Estimate Std. Error t value Pr(&gt;|t|)</span><br><span class="line">(Intercept) 27.42342    6.30108   4.352 0.000173    ***</span><br><span class="line">cyl         -0.86624    0.84941  -1.020 0.316869</span><br><span class="line">disp        -0.01190    0.01190  -1.000 0.325996</span><br><span class="line">hp          -0.03050    0.01982  -1.539 0.135498</span><br><span class="line">gear         1.42306    1.21053   1.176 0.250030</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">Signif. codes: 0 &apos;***&apos; 0.001 &apos;**&apos; 0.01 &apos;*&apos; 0.05 &apos;. &apos; 0.1 &apos; &apos; 1</span><br><span class="line"></span><br><span class="line">Residual standard error: 3.035 on 27 degrees of freedom</span><br><span class="line">Multiple R-squared:  0.7792,    Adjusted R-squared:  0.7465 </span><br><span class="line">F-statistic: 23.82 on 4 and 27 DF,  p-value: 1.606e-08</span><br></pre></td></tr></table></figure>

</details>


<p>R中的统计模型通常返回一个类名与函数名相同的对象。例如，在上面的例子中，<code>lm</code> 函数返回一个<code>lm</code> 的对象。我们可以使用<code>print</code> 和<code>summary</code> 函数来打印<code>lm</code> 对象<code>M_lm</code> 的相关信息。一般来讲，对于R中的模型，<code>print</code> 函数通常输出简短的汇总信息，而<code>summary</code> 通常给出更为详细的汇总信息。注意，也可以直接使用<code>print</code> 和<code>summary</code> 函数来处理其他对象。例如，我们可以使用<code>glmnet</code> 函数得到一个<code>glmnet</code> 对象，也可以使用这两个函数来处理。与C++中的函数类似，这些函数称为泛型 （generic）。</p>
<p>在R中，可以使用<code>for</code> 、<code>while</code> 等语句实现循环。由于R中的基本类型就是向量，因此很多情况下循环语句可以避免。此外，在R中可以使用向量化来更高效地实现循环。例如，对两个同样长度的向量<code>u</code> 和<code>v</code> 求和，可以直接使用</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w &lt;-u +ｖ</span><br></pre></td></tr></table></figure>

</details>


<p>而不必使用循环</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  w &lt;-rep(0, length(u))</span><br><span class="line">for (i in 1:length(u)) w[i] = u[i] + v[i]</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>特别地，我们介绍一下<code>apply</code> 函数。<code>apply</code> 函数作用于矩阵和数据框的行或者列。假设<code>M</code> 是一个矩阵或者数据框，<code>func</code> 表示一个函数，<code>apply</code> 的常用方法包括：</p>
<ul>
<li><p><code>apply(M, 1, func, args)</code> ——对于<code>M</code> 的每行执行函数<code>func</code> ，且函数的参数为<code>args</code> ；</p>
</li>
<li><p><code>apply(M, 2, func, args)</code> ——对于<code>M</code> 的每列执行函数<code>func</code> ，且函数的参数为<code>args。</code></p>
</li>
</ul>
<p>例如，<code>apply(M, 1, mean)</code> 计算每行的平均值，<code>apply(M, 2, sum)</code> 计算每列的和。</p>
<p>在R中，还有其他的<code>apply</code> 函数，如<code>tapply</code> 、<code>sapply</code> 和<code>lapply</code> ，在这里就不一一介绍了。</p>
<h2 id="2-6-R软件包"><a href="#2-6-R软件包" class="headerlink" title="2.6 R软件包"></a>2.6 R软件包</h2><p>所有的R函数和数据集都存放在包<br>（package）中，安装相应的包之后才能使用。本章之所以在前面的讨论中跳过了这一概念，是因为R的安装环境中已经集成了数十个常用的包。例如，在2.3.2节介绍的基本函数中，<code>ls()</code> 、<code>rm()</code> 、<code>paste()</code> 、<code>cat()</code> 、<code>c()</code> 来自<code>base</code> 包，<code>help()</code> 来自<code>utils</code> 包，<code>plot()</code> 来自<code>graphics</code> 包。</p>
<p>R软件包 分为标准包 和贡献包 两类。标准包是R源代码的一部分，R环境安装后即可直接使用；贡献包则由全世界的R语言爱好者自行开发，读者可以根据自身需求选择性下载使用 ⑤ 。截至2016年4月，R标准包包括<code>base</code> 、<code>compiler</code> 、<code>datasets</code> 、<code>grDevices</code> 、<code>graphics</code> 、<code>grid</code> 、<code>methods</code> 、<code>parallel</code> 、<code>splines</code> 、<code>stats</code> 、<code>stats4</code> 、<code>tcltk</code> 、<code>tools</code> 、<code>utils</code> 等14个；R贡献包超过8000个，其中<code>KernSmooth</code> 、<code>MASS</code> 、<code>Matrix</code> 、<code>boot</code> 、<code>class</code> 、<code>cluster</code> 、<code>codetools</code> 、<code>foreign</code> 、<code>lattice</code> 、<code>mgcv</code> 、<code>nlme</code> 、<code>nnet</code> 、<code>rpart</code> 、<code>spatial</code> 、<code>survival</code> 这15个备受推崇的贡献包也已集成到了R安装环境中。本书将介绍机器学习中常用的R软件包。</p>
<h3 id="2-6-1-软件包的安装"><a href="#2-6-1-软件包的安装" class="headerlink" title="2.6.1 软件包的安装"></a>2.6.1 软件包的安装</h3><p>调用<code>search()</code> 函数可以看到，在上面提到的29个随R环境一起安装的软件包中，有7个随着R的运行直接进入了内存：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> [1] &quot;.GlobalEnv&quot;        &quot;tools:rstudio&quot;     &quot;package:stats&quot;    </span><br><span class="line"> [4] &quot;package:graphics&quot;  &quot;package:grDevices&quot; &quot;package:utils&quot;    </span><br><span class="line"> [7] &quot;package:datasets&quot;  &quot;package:methods&quot;   &quot;Autoloads&quot;        </span><br><span class="line">[10] &quot;package:base&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>其他已安装但未调入内存的包可以通过<code>library()</code> 函数加载。例如，如果想加载<code>compiler</code> 包，则可调用<code>library(&#39;compiler&#39;)</code> 或<code>library(compiler)</code> 。如果直接调用<code>library()</code> ，则可以看到当前系统中已安装的所有软件包。</p>
<p>我们可以使用函数<code>install.packages</code> 来安装相应的包。下面的代码安装了<code>glmnet</code> 包以及它所依赖的包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&apos;glmnet&apos;, dependencies=T)</span><br></pre></td></tr></table></figure>
<p>也可以在RStudio提供的GUI界面上安装。单击Tools，选择Install Packages…，RStudio会弹出如图2-3所示的界面。在Packages中输入要安装的包的名字，单击Install按钮就可以了。</p>
<p><img src="Image00005.jpg" alt></p>
<p>图 2-3 RStudio 中安装包的界面</p>
<p>在运行大量的R的代码时，当我们要调用某个包时可能不知道其是否已经安装。一种简单的办法是先检查该包是否已经安装，如果没有安装，就先安装再调用，如果已经安装，则直接调用。下面的代码显示了如何先检查<code>glmnet</code> 的安装情况并调用该包。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">glmnet.installed &lt;-&apos;glmnet&apos; %in% rownames(installed.packages())</span><br><span class="line">if (glmnet.installed) &#123;</span><br><span class="line">  print(&quot;the glmnet package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">  print(&quot;let&apos;s install the glmnet package first...&quot;)</span><br><span class="line">  install.packages(&apos;glmnet&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(&apos;glmnet&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>如果不能自动找到所需的安装包，则可以手工下载软件包，然后通过RStudio的Tools→Install Packages…菜单命令或R GUI的“程序包”→“从本地zip文件安装程序包”菜单命令安装。</p>
<p>对于已经安装过的包，可以定期调用<code>update.packages()</code> 加以更新。</p>
<h3 id="2-6-2-软件包的使用"><a href="#2-6-2-软件包的使用" class="headerlink" title="2.6.2 软件包的使用"></a>2.6.2 软件包的使用</h3><p>软件包安装后，我们即可使用其中的函数。以<code>utils</code> 包为例，下列调用将列出该包中的函数名称和简要功能说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">library(help = utils)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，这一列表对同类函数进行了合并，因此并不完整。例如，<code>tail()</code> 函数也归类在<code>head()</code> 函数下面。如果需要详细了解某个函数，可参考2.3.2节中<code>help()</code> 函数的用法。</p>
<h3 id="2-6-3-软件包的开发"><a href="#2-6-3-软件包的开发" class="headerlink" title="2.6.3 软件包的开发"></a>2.6.3 软件包的开发</h3><p>参考文献[20]详细介绍了如何开发软件包和撰写R帮助文件，感兴趣的读者可以到<a href="https://cran.rstudio.com/manuals.html#R-exts" target="_blank" rel="noopener">https://cran.rstudio.com/manuals.html#R-exts</a> 免费下载。</p>
<h2 id="2-7-网络资源"><a href="#2-7-网络资源" class="headerlink" title="2.7 网络资源"></a>2.7 网络资源</h2><p>互联网上有很多关于R的资源，在这里我们简要列出一些最主要的网络资源。</p>
<ul>
<li><p>Comprehensive R Archive Network（CRAN，<a href="https://cran.r-project.org/" target="_blank" rel="noopener">https://cran.r-project.org/</a> ）是一个关于R的各种资源的网站。用户可以从这里下载R的安装程序。此外，该网站包含了各种包的下载和相关信息。其中<a href="https://cran.r-project.org/web/views/MachineLearning.html" target="_blank" rel="noopener">https://cran.r-project.org/web/views/MachineLearning.html</a> 上给出了常用的机器学习方面的包。</p>
</li>
<li><p><a href="http://www.r-tutor.com/r-introduction" target="_blank" rel="noopener">http://www.r-tutor.com/r-introduction</a> 是一个很好的关于R入门的网站。</p>
</li>
<li><p><a href="http://www.r-bloggers.com/" target="_blank" rel="noopener">http://www.r-bloggers.com/</a> 则是一个关于R的最新进展的网站，上面常常有一些介绍R的很多包用法的文章。</p>
</li>
</ul>
<p>关于R的入门，我们还推荐Emmanuel Paradis所著的《R for Beginners》。该手册有中译本，感兴趣的读者可以参考阅读。</p>
<hr>
<p>① <a href="http://awards.acm.org/award_winners/chambers_6640862.cfm" target="_blank" rel="noopener">http://awards.acm.org/award_winners/chambers_6640862.cfm</a></p>
<p>② 2001年更名为Insightful公司，2008年被TIBCO收购，后者于2014年被Vista Equity Partners收购。</p>
<p>③ <a href="https://cran.r-project.org/web/packages/" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/</a></p>
<p>④ 有些资料（如参考文献[18 ]和[19]）引入“数组”来定义矩阵，但R语言并未严格区分向量、数组和矩阵。</p>
<p>⑤ 下载地址<a href="https://cran.rstudio.com/web/packages/" target="_blank" rel="noopener">https://cran.rstudio.com/web/packages/</a> 。</p>
<h1 id="第3章-数学基础"><a href="#第3章-数学基础" class="headerlink" title="第3章 数学基础"></a>第3章 数学基础</h1><p>本章介绍机器学习算法中经常用到的概率、统计及矩阵计算方面的基础知识，它们是后继章节讨论具体机器学习算法的基础。数学基础较好的读者可以直接跳过本章。</p>
<h2 id="3-1-概率"><a href="#3-1-概率" class="headerlink" title="3.1 概率"></a>3.1 概率</h2><h3 id="3-1-1-基本概念"><a href="#3-1-1-基本概念" class="headerlink" title="3.1.1 基本概念"></a>3.1.1 基本概念</h3><p>我们在日常生活中遇到的事件可分为确定性事件和随机事件两类。对于确定性事件，我们可以在一定的条件下预先知道其是否会发生，如“明天早上9点开会”。对于随机事件，我们无法预先知道其是否会发生，但可以研究其发生的可能性，如“从1～10中随机取出的两个数为一奇一偶”。本章把随机事件 _A_ 发生的可能性大小称为 _A_ 发生的概率，记为 _P_ ( _A_ )。</p>
<p>在给出概率的数学定义之前，首先引入随机试验、样本、样本空间和随机变量的概念。</p>
<p>（1）随机试验 。为了研究某个随机事件而进行的具有如下特点的科学观测：</p>
<ul>
<li>在相同条件下可重复进行；</li>
<li>结果的范围明确可知，且结果不止一种；</li>
<li>每次试验恰好获得一种结果，但试验之前不知道会是哪一种。</li>
</ul>
<p>（2）样本 。随机试验的每种可能结果，称为一个样本。</p>
<p>（3）样本空间 。所有样本组成的集合，称为样本空间。</p>
<p>（4）随机变量 。设随机试验的样本空间为<img src="Image00006.gif" alt> ，若存在函数 _X_ ()，使得<img src="Image00007.gif" alt> ，都存在一个对应的实数 _X_ ( _w_ )，则称 _X_<br>()为随机变量。不难看出，随机事件可以用随机变量来表示。以前面的“从1～10中随机取出的两个数为一奇一偶”事件为例，若把从1～10中随机取出的两数之和记为 _X_ ，则该事件可等价描述为“ _X_ 为奇数”。</p>
<p>若随机变量 _X_ 的取值范围是可列的，则称 _X_ 为离散型随机变量。这里讲的“可列”指可以按确定的顺序线性地列出，即对于任何一个取值，可以明确指出它的前一个取值、后一个取值分别是什么。具体包括如下两种情形之一：</p>
<ul>
<li>取值范围有限，如{3,9,5,12.5}；</li>
<li>取值范围无限，如正整数倒数的集合<img src="Image00008.gif" alt> 。注意，实数集之类的集合是不可列的，证明从略。</li>
</ul>
<p>对于离散型随机变量 _X_ ，假设 _X_ 的所有可能取值集合为<img src="Image00009.gif" alt> ，各取值出现的概率分别为<img src="Image00010.gif" alt> ，而 _X_ 所描述的随机事件 _A_ 对应的样本集合为<img src="Image00011.gif" alt> ，则 _A_ 发生的概率为：</p>
<p><img src="Image00012.gif" alt></p>
<p>（3-1）</p>
<p>特别地，当取值范围有限且各取值出现的概率均等时，随机事件 _A_ 发生的概率为：</p>
<p><img src="Image00013.gif" alt></p>
<p>（3-2）</p>
<p>其中| |表示集合中元素的数量。式（3-2）就是著名的古典概率定义。</p>
<p>对于随机变量 _X_ ，若存在非负可积的函数 _p_ ( _x_ )，使得</p>
<p><img src="Image00014.gif" alt></p>
<p>（3-3）</p>
<p>则称 _X_ 为连续型随机变量，称函数 _p_ ( _x_ )为 _X_ 的概率密度函数。这种情况下，假设随机事件 _A_ 对应的样本区间为 _R_ ( _A_ )，则 _A_ 发生的概率为：</p>
<p><img src="Image00015.gif" alt></p>
<p>（3-4）</p>
<p>注意， _p_ ( _x_ )不一定是连续函数。</p>
<p>此外，对于一些既非离散型也非连续型的随机变量，可以分段考虑。</p>
<h3 id="3-1-2-基本公式"><a href="#3-1-2-基本公式" class="headerlink" title="3.1.2 基本公式"></a>3.1.2 基本公式</h3><p>概率具有如下基本性质：</p>
<p><img src="Image00016.gif" alt></p>
<p>（3-5）</p>
<p>其中 _Φ_ 对应于空集，表示不可能事件；样本空间<img src="Image00017.gif" alt> 对应于全集，表示必然事件。注意，式（3-5）中第一个等式仅表明不可能事件发生的概率为0；事实上，概率为0的随机事件未必是不可能事件，见3.1.4节。</p>
<p>对于随机事件 _A_ 、 _B_ ，如果用<img src="Image00018.gif" alt> 表示 _A_ 、 _B_ 中至少有一个发生，用<img src="Image00019.gif" alt> 表示 _A_ 、 _B_ 同时发生，则有</p>
<p><img src="Image00020.gif" alt></p>
<p>（3-6）</p>
<p>如果 _A_ 和 _B_ 互斥，则<img src="Image00021.gif" alt> ，代入式（3-6）得</p>
<p><img src="Image00022.gif" alt></p>
<p>（3-7）</p>
<p>一般来说，若 _A_ 1 , _A_ 2 , _A_ 3 ,…为可列无穷个互斥事件，则有</p>
<p><img src="Image00023.gif" alt></p>
<p>（3-8）</p>
<p>若随机事件 _A_ 发生一定会导致随机事件 _B_ 发生，则有</p>
<p><img src="Image00024.gif" alt></p>
<p>（3-9）</p>
<p>这里，<img src="Image00025.gif" alt> 表示事件 _B_ 发生而事件 _A_ 不发生的概率。</p>
<p>当 _P_ ( _B_ )&gt;0时，称</p>
<p><img src="Image00026.gif" alt></p>
<p>（3-10）</p>
<p>为在已知随机事件 _B_ 发生的前提下，随机事件 _A_ 发生的条件概率。写成乘法形式为</p>
<p><img src="Image00027.gif" alt></p>
<p>（3-11）</p>
<p>如果存在一组互斥事件<img src="Image00028.gif" alt> ，满足</p>
<p><img src="Image00029.gif" alt></p>
<p>（3-12）</p>
<p>则对任意随机事件 _B_ ，有</p>
<p><img src="Image00030.gif" alt></p>
<p>（3-13）</p>
<p>式（3-13）称为全概率公式 。若<img src="Image00031.gif" alt> &gt;0，则根据式（3-10），<img src="Image00032.gif" alt> 有</p>
<p><img src="Image00033.gif" alt></p>
<p>（3-14）</p>
<p>根据式（3-11），将<img src="Image00034.gif" alt> 替换为<img src="Image00035.gif" alt> ，可得如下的贝叶斯公式<br>：</p>
<p><img src="Image00036.gif" alt></p>
<p>（3-15）</p>
<p>如果随机事件 _A_ 、 _B_ 满足条件</p>
<p><img src="Image00037.gif" alt></p>
<p>（3-16）</p>
<p>则称事件 _A_ 、 _B_ 相互独立。对照式（3-11）不难看出，当<img src="Image00031.gif" alt> &gt;0时，式（3-16）等价于</p>
<p><img src="Image00038.gif" alt></p>
<p>（3-17）</p>
<p>也就是说，随机事件 _B_ 是否发生不会影响随机事件 _A_ 发生的概率。</p>
<h3 id="3-1-3-常用分布"><a href="#3-1-3-常用分布" class="headerlink" title="3.1.3 常用分布"></a>3.1.3 常用分布</h3><h4 id="1．常用离散分布"><a href="#1．常用离散分布" class="headerlink" title="1．常用离散分布"></a>1．常用离散分布</h4><p>（1）0-1分布 （0-1 distribution）。</p>
<p>如果随机变量 _X_ 满足</p>
<p><img src="Image00039.jpg" alt></p>
<p>（3-18）</p>
<p>这里<img src="Image00040.gif" alt> ，则称 _X_ 服从0-1分布 ，也称为伯努利分布<br>（Bernoulli distribution）。在实际应用中，若随机试验仅有两种可能的结果，则可以定义服从0-1分布的随机变量加以描述。在机器学习中，由于 _k_ 的取值只能为0或者1，因此我们通常将<img src="Image00041.gif" alt> 写为如下形式：</p>
<p><img src="Image00042.gif" alt></p>
<p>（2）几何分布 （geometric distribution）。</p>
<p>如果随机变量 _X_ 满足</p>
<p><img src="Image00043.gif" alt></p>
<p>（3-19）</p>
<p>其中<img src="Image00044.gif" alt> ，<img src="Image00040.gif" alt> ，则称 _X_ 服从几何分布<img src="Image00045.gif" alt> ，记为<img src="Image00046.gif" alt> 。在实际应用中，若多次随机试验相互独立，则可以定义服从几何分布的随机变量来描述随机事件的“首次出现”。</p>
<p>（3）二项分布 （binomial distribution）。</p>
<p>如果随机变量 _X_ 满足</p>
<p><img src="Image00047.gif" alt></p>
<p>（3-20）</p>
<p>其中<img src="Image00048.gif" alt> ，则称 _X_ 服从二项分布 _B_ ( _n_ , _p_ )，记为 _X~B_ ( _n_ , _p_ )。在实际应用中，若多次随机试验相互独立，则可以定义服从二项分布的随机变量来描述随机事件的出现次数。</p>
<h4 id="2．常用连续分布"><a href="#2．常用连续分布" class="headerlink" title="2．常用连续分布"></a>2．常用连续分布</h4><p>对于连续型随机变量 _X_ ，设其概率密度函数为<img src="Image00049.gif" alt> 。</p>
<p>（1）均匀分布 （uniform distribution）。</p>
<p>假设 _b_ &gt; _a_ ，若 _x_ <img src="Image00050.gif" alt> [ _a_ , _b_ ]时有</p>
<p><img src="Image00051.gif" alt></p>
<p>（3-21）</p>
<p>且<img src="Image00052.gif" alt> 时<img src="Image00053.gif" alt> ，则称 _X_ 服从均匀分布<img src="Image00054.gif" alt> ，记为<img src="Image00055.gif" alt> 。在实际应用中，若随机试验的结果落在区间<img src="Image00056.gif" alt> 内任意一点的可能性都相等，则可以定义服从均匀分布的随机变量加以描述。</p>
<p>（2）指数分布 （exponential distribution）。</p>
<p>设<img src="Image00057.gif" alt> &gt;0为常数，若<img src="Image00058.gif" alt> ，</p>
<p><img src="Image00059.gif" alt></p>
<p>（3-22）</p>
<p>且<img src="Image00060.gif" alt> 时<img src="Image00053.gif" alt> ，则称 _X_ 服从指数分布<img src="Image00061.gif" alt> ，记为<img src="Image00062.gif" alt> 。在实际应用中，若随机试验涉及元器件使用寿命、系统等待时间等，则往往可以定义服从指数分布的随机变量加以描述。</p>
<p>（3）正态分布 （normal distribution）。</p>
<p>若<img src="Image00063.gif" alt> ，</p>
<p><img src="Image00064.gif" alt></p>
<p>（3-23）</p>
<p>其中<img src="Image00065.gif" alt> 、<img src="Image00066.gif" alt> 均为常数且<img src="Image00066.gif" alt> &gt;0，则称 _X_ 服从正态分布<img src="Image00067.gif" alt> ，记为<img src="Image00068.gif" alt> 。在实际应用中，若随机试验受多种独立随机因素的影响，则往往可以定义服从正态分布的随机变量加以描述。特别地，<img src="Image00065.gif" alt> =0、<img src="Image00069.gif" alt> =1时的正态分布称为标准正态分布。图3-1是标准正态分布<img src="Image00070.gif" alt> 的密度函数图像。</p>
<p><img src="Image00071.jpg" alt></p>
<p>图3-1 标准正态分布的密度函数图像</p>
<h3 id="3-1-4-随机向量及其分布"><a href="#3-1-4-随机向量及其分布" class="headerlink" title="3.1.4 随机向量及其分布"></a>3.1.4 随机向量及其分布</h3><h4 id="1．随机向量与分布函数"><a href="#1．随机向量与分布函数" class="headerlink" title="1．随机向量与分布函数"></a>1．随机向量与分布函数</h4><p>前面讨论了单个随机变量及其常见分布，但有时用单个随机变量难以描述随机事件，需要把多个随机变量作为一个整体进行研究。设随机试验的样本空间为<img src="Image00006.gif" alt> ，若存在向量<img src="Image00072.gif" alt> ，使得<img src="Image00073.gif" alt> ，都存在一组对应的实数<img src="Image00074.gif" alt> ，则称<img src="Image00075.gif" alt> 为 _d_ 维随机向量。由前面随机变量的定义可知， _d_ 维随机向量是由 _d_ 个随机变量组成的。特别地，若<img src="Image00076.gif" alt> ，有</p>
<p><img src="Image00077.gif" alt></p>
<p>（3-24）</p>
<p>则称这 _d_ 个随机变量相互独立（<img src="Image00078.gif" alt><br>）。为了讨论方便，我们通常把<img src="Image00079.gif" alt> 称为随机向量<img src="Image00080.gif" alt> 的分布函数，记为<img src="Image00081.gif" alt> 。从而，式（3-24）可等价写为</p>
<p><img src="Image00082.gif" alt></p>
<p>（3-25）</p>
<p>这里<img src="Image00083.gif" alt> 分别是随机变量<img src="Image00084.gif" alt> 的分布函数。</p>
<p>随机向量的分布函数<img src="Image00081.gif" alt> 具有如下性质：</p>
<ul>
<li>对任意给定的<img src="Image00085.gif" alt> (<img src="Image00086.gif" alt> )，若<img src="Image00087.gif" alt> &lt;<img src="Image00088.gif" alt> ，则</li>
</ul>
<p><img src="Image00089.gif" alt></p>
<p>（3-26）</p>
<ul>
<li><img src="Image00090.gif" alt> ，且</li>
</ul>
<p><img src="Image00091.gif" alt></p>
<p>（3-27）</p>
<ul>
<li><p><img src="Image00081.gif" alt> 对<img src="Image00092.gif" alt> 均为右连续；</p>
</li>
<li><p><img src="Image00093.gif" alt> ，有</p>
</li>
</ul>
<p><img src="Image00094.gif" alt></p>
<p>（3-28）</p>
<p>对于<img src="Image00080.gif" alt> 的任意<img src="Image00095.gif" alt> 个分量所组成的随机向量<img src="Image00096.gif" alt> ，设剩余的 _d_ -_k_ 个分量组成的随机向量为<img src="Image00097.gif" alt> ，把</p>
<p><img src="Image00098.gif" alt></p>
<p>（3-29）</p>
<p>称为<img src="Image00099.gif" alt> 的 _k_ 维边缘分布函数。</p>
<p>与随机变量类似，若随机向量<img src="Image00099.gif" alt> 的取值范围是可列的，则称其为 _d_ 维离散型随机向量；若存在非负可积的函数<img src="Image00100.gif" alt> ，使.得</p>
<p><img src="Image00101.gif" alt></p>
<p>（3-30）</p>
<p>则称 <strong>_X_ </strong> 为 _d_ 维连续型随机向量，称函数<img src="Image00100.gif" alt> 为 <strong>_X_ </strong> 的概率密度函数。</p>
<p>不难看出，当 _d_ =1时，随机向量就退化成为随机变量，本节给出的分布函数定义与性质同样适用于随机变量。</p>
<h4 id="2．随机向量的常用分布"><a href="#2．随机向量的常用分布" class="headerlink" title="2．随机向量的常用分布"></a>2．随机向量的常用分布</h4><p>（1）二维均匀分布。</p>
<p>假设 _S_ 为二维平面上的有界区域，其面积为<img src="Image00102.gif" alt> ，随机向量<img src="Image00103.gif" alt> 的概率密度函数为</p>
<p><img src="Image00104.gif" alt></p>
<p>（3-31）</p>
<p>则称<img src="Image00103.gif" alt> 服从均匀分布<img src="Image00105.gif" alt> ，记为<img src="Image00106.gif" alt> 。在图3-2中，设正方形OPQR的面积为<img src="Image00065.gif" alt><br>(OPQR)，如果在该正方形区域内等概率地取点，则<img src="Image00103.gif" alt> 落在三角形OPQ中的概率为</p>
<p><img src="Image00107.jpg" alt></p>
<p>图3-2 二维均匀分布示例</p>
<p><img src="Image00108.jpg" alt></p>
<p>（3-32）</p>
<p>这里<img src="Image00065.gif" alt> (OPQ)为三角形OPQ的面积。显然，<img src="Image00103.gif" alt> 落在三角形OPQ和三角形OQR中的概率都为0.5，而落在线段OQ上的概率为0。这个例子表明，零概率事件是有可能发生的。</p>
<p>（2） _d_ 维正态分布。</p>
<p>若<img src="Image00109.gif" alt> ，其密度函数定义为</p>
<p><img src="Image00110.gif" alt></p>
<p>（3-33）</p>
<p>其中<img src="Image00111.gif" alt> 为常数列向量，<img src="Image00112.gif" alt> ，<img src="Image00113.gif" alt> 为 _d_ 阶正定对称矩阵，<img src="Image00114.gif" alt> 和<img src="Image00115.gif" alt> 分别为<img src="Image00116.gif" alt> 的行列式与逆矩阵，则称 _<strong>X</strong> _ 服从 _d_ 维正态分布 _N_ ( <strong>_μ_ </strong> , <strong>_Σ_ </strong> )，记为 <strong>_X_ </strong> ～ _N_ ( <strong>_μ_ </strong> , <strong>_Σ_ </strong> )。这里的行列式、逆矩阵和正定对称矩阵的定义详见3.3节。</p>
<p>特别地，当 _d_ =1时，Σ&gt;0为常数，式（3-33）等价于</p>
<p><img src="Image00117.gif" alt></p>
<p>（3-34）</p>
<p>令<img src="Image00118.gif" alt> ，则式（3-34）等同于式（3-23）。</p>
<h3 id="3-1-5-随机变量的数字特征"><a href="#3-1-5-随机变量的数字特征" class="headerlink" title="3.1.5 随机变量的数字特征"></a>3.1.5 随机变量的数字特征</h3><h4 id="1．数学期望"><a href="#1．数学期望" class="headerlink" title="1．数学期望"></a>1．数学期望</h4><p>对于离散型随机变量 _X_ ，若样本<img src="Image00119.gif" alt> 出现的概率都相等，则可以定义如下的<img src="Image00120.gif" alt> 来表示这 _n_ 个样本的均值：</p>
<p><img src="Image00121.gif" alt></p>
<p>（3-35）</p>
<p>若各样本出现的概率不同，用<img src="Image00122.gif" alt> 表示<img src="Image00123.gif" alt> 的出现概率（ _i_ =1,2,…, _n_ ），则有</p>
<p><img src="Image00124.gif" alt></p>
<p>（3-36）</p>
<p>若级数<img src="Image00125.gif" alt> 绝对收敛，则称<img src="Image00125.gif" alt> 为 _X_ 的数学期望。</p>
<p>对于连续型随机变量 _X_ ，设其概率密度函数为<img src="Image00126.gif" alt> ，若<img src="Image00127.gif" alt> 绝对收敛，则称<img src="Image00128.gif" alt> 为 _X_ 的数学期望。</p>
<p>本书将随机变量 _X_ 的数学期望记为<img src="Image00129.gif" alt> 。对于常见的随机变量，绝对收敛的条件一般都满足，不需要特别考虑。</p>
<p>数学期望具有如下重要性质：</p>
<ul>
<li>对任意<img src="Image00130.gif" alt> ；</li>
<li>对任意常数 _a_ ，<img src="Image00131.gif" alt> ；</li>
<li>若<img src="Image00132.gif" alt> 相互独立，则<img src="Image00133.gif" alt> 。</li>
</ul>
<p>对于3.1.3节介绍的常用分布，其数学期望见表3-1。</p>
<p>表3-1 常用分布的数学期望</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>常 用 分 布</th>
<th>数 学 期 望</th>
<th>备 注  </th>
</tr>
</thead>
<tbody>
<tr>
<td> 0-1分布</td>
<td><img src="Image00134.gif" alt></td>
<td>离散型  </td>
</tr>
<tr>
<td> 几何分布<img src="Image00135.gif" alt></td>
<td><img src="Image00136.gif" alt></td>
<td>离散型  </td>
</tr>
<tr>
<td> 二项分布 _B_ ( _n_ , _p_ )</td>
<td><img src="Image00137.gif" alt></td>
<td>离散型  </td>
</tr>
<tr>
<td> 均匀分布 _U_ [ _a_ , _b_ ]</td>
<td><img src="Image00138.gif" alt></td>
<td>连续型  </td>
</tr>
<tr>
<td> 指数分布<img src="Image00139.gif" alt></td>
<td><img src="Image00140.gif" alt></td>
<td>连续型  </td>
</tr>
<tr>
<td> 正态分布 _N_ ( _μ_ σ 2 )</td>
<td><img src="Image00141.gif" alt></td>
<td>连续型  </td>
</tr>
</tbody>
</table>
</div>
<h4 id="2．方差"><a href="#2．方差" class="headerlink" title="2．方差"></a>2．方差</h4><p>数学期望体现了随机变量取值的平均程度，方差则描述了数据相对于数学期望的分散程度。本书将随机变量 _X_ 的方差 记为<img src="Image00142.gif" alt> ，则有</p>
<p><img src="Image00143.gif" alt></p>
<p>（3-37）</p>
<p>根据数学期望的性质可得</p>
<p><img src="Image00144.gif" alt></p>
<p>（3-38）</p>
<p>方差的平方根称为标准差 ，记为<img src="Image00145.gif" alt> ：</p>
<p><img src="Image00146.gif" alt></p>
<p>（3-39）</p>
<p>对于3.1.3节介绍的常用分布，其方差见表3-2。</p>
<p>表3-2 常用分布的方差</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>常 用 分 布</th>
<th>方 差</th>
<th>备 注  </th>
</tr>
</thead>
<tbody>
<tr>
<td> 0-1分布</td>
<td><img src="Image00147.gif" alt></td>
<td>离散型  </td>
</tr>
<tr>
<td> 几何分布<img src="Image00135.gif" alt></td>
<td><img src="Image00148.gif" alt></td>
<td>离散型  </td>
</tr>
<tr>
<td> 二项分布 _B_ ( _n_ , _p_ )</td>
<td><img src="Image00149.gif" alt></td>
<td>离散型  </td>
</tr>
<tr>
<td> 均匀分布 _U_ [ _a_ , _b_ ]</td>
<td><img src="Image00150.gif" alt></td>
<td>连续型  </td>
</tr>
<tr>
<td> 指数分布<img src="Image00139.gif" alt></td>
<td><img src="Image00151.gif" alt></td>
<td>连续型  </td>
</tr>
<tr>
<td> 正态分布 _N_ ( _μ_ , _σ_ 2 )</td>
<td><img src="Image00152.gif" alt></td>
<td>连续型  </td>
</tr>
</tbody>
</table>
</div>
<h4 id="3．协方差与相关系数"><a href="#3．协方差与相关系数" class="headerlink" title="3．协方差与相关系数"></a>3．协方差与相关系数</h4><p>对于两个随机变量 _X_ 和 _Y_ ，其协方差定义为</p>
<p><img src="Image00153.gif" alt></p>
<p>（3-40）</p>
<p>若<img src="Image00142.gif" alt> &gt;0、<img src="Image00154.gif" alt> &gt;0均存在，则 _X_ 与 _Y_ 的相关系数<img src="Image00155.gif" alt> 定义为</p>
<p><img src="Image00156.gif" alt></p>
<p>（3-41）</p>
<p>当<img src="Image00157.gif" alt> 时称 _X_ 与 _Y_ 不相关。注意， _X_ 与 _Y_ 不相关表明 _X_ 与 _Y_ 之间不存在线性关系，但可能存在其他关系，因此 _X_ 与 _Y_ 不一定相互独立。</p>
<p>相关系数的一个重要性质是<img src="Image00158.gif" alt> ，即<img src="Image00159.gif" alt> ，证明从略。</p>
<h4 id="4．偏度"><a href="#4．偏度" class="headerlink" title="4．偏度"></a>4．偏度</h4><p>对于随机变量 _X_ ，可以用如下定义的偏度<img src="Image00160.gif" alt> 来衡量样本数据分布的非对称性：</p>
<p><img src="Image00161.gif" alt></p>
<p>（3-42）</p>
<p>当<img src="Image00160.gif" alt> <0时，样本分布的偏度为负，此时直线![](image00162.gif) 左侧的样本数量直观上少于右侧的样本数量；当![](image00163.gif) 时，样本分布的偏度为0，此时直线![](image00162.gif) 左侧的样本数量直观上等于右侧的样本数量；当![](image00160.gif)>0时，样本分布的偏度为正，此时直线<img src="Image00164.gif" alt> 左侧的样本数量直观上多于右侧的样本数量。图3-3显示了3个不同的分布对应的密度函数图像，直观地显示了不同的偏度值。</0时，样本分布的偏度为负，此时直线![](image00162.gif)></p>
<p><img src="Image00165.jpg" alt></p>
<p>图3-3 偏度范例</p>
<h3 id="3-1-6-随机向量的数字特征"><a href="#3-1-6-随机向量的数字特征" class="headerlink" title="3.1.6 随机向量的数字特征"></a>3.1.6 随机向量的数字特征</h3><p>前面讨论了随机变量的数字特征，对于实际问题中更常见的随机向量，相关数字特征的定义是类似的。例如，对于 _n_ 维随机向量<img src="Image00080.gif" alt> ，定义</p>
<p><img src="Image00166.gif" alt></p>
<p>（3-43）</p>
<p>为 <strong>_X_ </strong> 的数学期望向量，定义</p>
<p><img src="Image00167.gif" alt></p>
<p>（3-44）</p>
<p>为 <strong>_X_ </strong> 的方差向量，定义</p>
<p><img src="Image00168.gif" alt></p>
<p>（3-45）</p>
<p>为 <strong>_X_ </strong> 的标准差向量。</p>
<p>定义随机向量的协方差和相关系数时，必须考虑各分量之间的两两关系。因此，随机向量<img src="Image00080.gif" alt> 的协方差矩阵<img src="Image00169.gif" alt> 和相关系数矩阵<img src="Image00170.gif" alt> 分别由式（3-46）和式（3-47）定义：</p>
<p><img src="Image00171.gif" alt></p>
<p>（3-46）</p>
<p>其中 _cov_ ( _X i _ , _X j _ )的定义见式（3-40），</p>
<p><img src="Image00172.gif" alt></p>
<p>（3-47）</p>
<p>其中 _cc_ ( _X i _ , _X j _ ) 的定义见式（3-41）。</p>
<h2 id="3-2-统计"><a href="#3-2-统计" class="headerlink" title="3.2 统计"></a>3.2 统计</h2><p>统计指的是以概率论为基础，对所得的数据进行收集、整理、分析的过程。在这里，我们介绍一些在统计学和机器学习中常用的数据特征，包括均值、方差、标准差、极差、分位数、绝对平均偏差、中位数绝对偏差、四分位极差、偏度、协方差、相关系数等基本概念，并重点介绍极大似然估计。</p>
<h3 id="3-2-1-常用数据特征"><a href="#3-2-1-常用数据特征" class="headerlink" title="3.2.1 常用数据特征"></a>3.2.1 常用数据特征</h3><h4 id="1．一元数据常用数字特征"><a href="#1．一元数据常用数字特征" class="headerlink" title="1．一元数据常用数字特征"></a>1．一元数据常用数字特征</h4><p>在前面的讨论中，我们讨论了服从某个随机分布的随机变量的一些数字特征，如期望和方差。在本节中，我们则从统计的角度来计算数据的数字特征。</p>
<p>在一元数据中，假设我们有样本</p>
<p><img src="Image00173.gif" alt></p>
<p>其中 _n_ 称为样本容量。在某些简单的问题中，这 _n_ 个观测值就是所要研究的对象的全体；更普遍的情况是这 _n_ 个样本是从总体中抽取的样本。在前一种情况中，我们要利用全部的 _n_ 个样本研究数据中包括的信息；在后一种情况中，我们要从有限的 _n_ 个样本中推断总体的信息。</p>
<p>下面我们介绍如何从给定的样本集合中计算数字特征，包括均值、方差、标准差、中位数、极差、分位数、绝对平均偏差、中位数绝对偏差、四分位极差和偏度等。在机器学习中，在很多情况下给定的这组样本就是我们所有的数据，因此从数据中直接计算数字特征对于了解数据是非常重要的。</p>
<p>（1）均值是样本<img src="Image00174.gif" alt> 的平均值，记为<img src="Image00120.gif" alt> ：</p>
<p><img src="Image00175.gif" alt></p>
<p>（3-48）</p>
<p>均值表示了数据的集中位置。</p>
<p>（2）方差则描述了数据的分散程度，它度量了一组数据相对于均值的偏离程度。我们把方差记为<img src="Image00176.gif" alt> ，其定义为：</p>
<p><img src="Image00177.gif" alt></p>
<p>（3-49）</p>
<p>（3）方差的开方称为标准差，我们记为 _s_ ：</p>
<p><img src="Image00178.gif" alt></p>
<p>（3-50）</p>
<p>注意，方差的量纲与数据的量纲不一致，而标准差的量纲与数据的量纲是一致的。</p>
<p>上面讨论的均值、方差、标准差等数字特征适用于来自正态分布的数据。如果数据的分布严重偏向某一边，或者极端值比较多，则上述的这些特征就不适用了。在这种情况下，我们可以计算中位数、分位数、极差等数据特征。在下面的讨论中，我们假设将数据按从小到大的顺序排好，并将排好的样本记为</p>
<p><img src="Image00179.gif" alt></p>
<p>（1）中位数 。当 _n_ 为奇数时，中位数 _M_ 定义为：</p>
<p><img src="Image00180.gif" alt></p>
<p>（3-51）</p>
<p>当 _n_ 为偶数时，中位数 _M_ 定义为：</p>
<p><img src="Image00181.gif" alt></p>
<p>（3-52）</p>
<p>与均值相比，中位数基本上不受极端值的影响。此外，如果数据的分布对称，中位数与均值会比较接近。但是当数据的分布偏向某一边时，均值和中位数的差异会比较大。在实际的数据处理中，中位数是一个非常重要的统计量。</p>
<p>（2）极差 （range）。对于样本数据<img src="Image00174.gif" alt> ，极差的定义为：</p>
<p><img src="Image00182.gif" alt></p>
<p>（3-53）</p>
<p>（3）分位数 。对于0≤ _p_ &lt;1，数据<img src="Image00174.gif" alt> 的 _p_ 分位数为：</p>
<p><img src="Image00183.jpg" alt></p>
<p>（3-54）</p>
<p>这里<img src="Image00184.gif" alt> 表示 _np_ 的整数部分。当<img src="Image00185.gif" alt> 时，我们定义</p>
<p><img src="Image00186.gif" alt></p>
<p>（3-55）</p>
<p>_p_ 分位数又称为100 _p_ 百分位数。常用的 _p_ 分位数包括0.25分位数、0.5分位数和0.75分位数。其中0.5分位数就是中位数。0.25分位数也称为下四分位数，记为：</p>
<p><img src="Image00187.gif" alt></p>
<p>（3-56）</p>
<p>0.75分位数也称为上四分位数，记为：</p>
<p><img src="Image00188.gif" alt></p>
<p>（3-57）</p>
<p>我们注意到均值的计算容易受到离群数据或者极端数据的影响。在计算方差时，我们也需要用到均值。事实上，方差比均值对极端数据更加敏感。因为对于每个点<img src="Image00123.gif" alt> ，首先要计算<img src="Image00189.gif" alt> 再平方。在平方的过程中，极端数据的影响会进一步放大。为了降低极端数据的影响，我们引入如下的统计量来衡量数据散布区间的大小。</p>
<p>（1）绝对平均偏差 （absolute average deviation，AAD）：</p>
<p><img src="Image00190.gif" alt></p>
<p>（3-58）</p>
<p>（2）中位数绝对偏差 （median absolute deviation，MAD）：</p>
<p><img src="Image00191.gif" alt></p>
<p>（3-59）</p>
<p>（3）四分位极差 （inter-quartile range，IQR）：</p>
<p><img src="Image00192.gif" alt></p>
<p>（3-60）</p>
<p>（4）偏度 。对于样本数据<img src="Image00174.gif" alt> ，偏度的计算公式是：</p>
<p><img src="Image00193.gif" alt></p>
<p>（3-61）</p>
<p>事实上，这里讨论的很多样本数字特征是相应的总体数字特征的矩估计。当样本容量 _n_ 足够大，且总体数字特征存在时，样本数字特征趋近于待估参数的真实值。</p>
<h4 id="2．多元数据常用数字特征"><a href="#2．多元数据常用数字特征" class="headerlink" title="2．多元数据常用数字特征"></a>2．多元数据常用数字特征</h4><p>在实际问题中，更多的情况下我们碰到的都是多元数据。在多元数据处理中，除了分析各个分量的数字特征外，我们更关心变量之间的相互关系。</p>
<p>假设我们有 _n_ 个 _d_ 维向量组成的样本：</p>
<p><img src="Image00194.gif" alt></p>
<p>（3-62）</p>
<p>与一元数据分析类似，我们可以定义每个变量的均值和方差。对于第<img src="Image00195.gif" alt> 个变量，其均值<img src="Image00196.gif" alt> 和方差<img src="Image00197.gif" alt> 定义为</p>
<p><img src="Image00198.gif" alt></p>
<p>（3-63）</p>
<p>注意，这里我们使用<img src="Image00197.gif" alt> 来标记第 _j_ 个变量的方差，因为我们要考虑不同变量之间的协方差，例如，使用<img src="Image00199.gif" alt> 来标记第 _j_ 个变量和第 _k_ 个变量的协方差。根据该样本集合，我们定义第 _j_ 个变量和第 _k_ 个变量的协方差为</p>
<p><img src="Image00200.gif" alt></p>
<p>（3-64）</p>
<p>我们可以根据样本数据定义均值向量，记为<img src="Image00120.gif" alt> ：</p>
<p><img src="Image00201.gif" alt></p>
<p>（3-65）</p>
<p>相应地，我们定义 <strong>_S_ </strong> 为样本观测数据的协方差矩阵：</p>
<p><img src="Image00202.gif" alt></p>
<p>（3-66）</p>
<p>如果表示成矩阵的形式，我们有</p>
<p><img src="Image00203.gif" alt></p>
<p>（3-67）</p>
<p>由协方差的定义可知矩阵 <strong>_S_ </strong> 是对称矩阵，即</p>
<p><img src="Image00204.gif" alt></p>
<p>（3-68）</p>
<p>在协方差的基础上，还可以根据样本数据计算相关系数。可以计算第 _j_ 个变量和第 _k_ 个变量的相关系数如下：</p>
<p><img src="Image00205.gif" alt></p>
<p>（3-69）</p>
<p>相关系数衡量了两个变量之间存在线性关系的程度。当<img src="Image00206.gif" alt> 时，这里第 _j_ 个和第 _k_ 个变量之间存在线性关系。类似地，也可以将相关系数表示成矩阵的形式，称为相关矩阵：</p>
<p><img src="Image00207.gif" alt></p>
<p>（3-70）</p>
<p>相关矩阵是多元观测数据最重要的数字特征之一。在进行多元数据探索时，相关矩阵是我们首先要计算的若干关键指标之一。</p>
<h3 id="3-2-2-参数估计"><a href="#3-2-2-参数估计" class="headerlink" title="3.2.2 参数估计"></a>3.2.2 参数估计</h3><p>前面讨论的数字特征是基于整个样本空间 _Ω_ 定义的。在实际应用中，我们往往只能选取部分研究对象进行试验，试验的结果是 _Ω_ 的真子集。本章把全体研究对象所组成的集合称为母体，把所选取的部分研究对象所组成的集合称为子样，把选取子样的过程称为抽样。为了使子样能较好地反映母体特性，我们要求抽样过程具备如下性质。</p>
<ul>
<li>代表性：子样随机向量的每个分量与母体随机向量具有相同的分布。</li>
<li>独立性：子样随机向量的各分量相互独立。</li>
</ul>
<p>本节讨论在母体概率分布类型已知的前提下，通过分析子样来估计母体概率分布中的未知参数的方法。参数估计的方法主要有点估计、区间估计两类，前者可以得到明确的参数近似值，后者则可以指明某个区间以多大概率包含所求参数。在点估计中，常用的方法有矩估计和极大似然估计。在这里我们介绍极大似然估计<br>（maximum likelihood estimation）。</p>
<p>对于连续型随机变量 _X_ ，假设其密度函数为 _p_ ( _x_ , _θ_ )，其中 _θ_ 为待估计的参数。若{ _X_ 1 , _X_ 2 ,…, _X n _ }是来自母体的一个子样，则称</p>
<p><img src="Image00208.gif" alt></p>
<p>（3-71）</p>
<p>为该子样的似然函数 （likelihood function）。</p>
<p>对于离散型随机变量 _X_ ，假设对于任意 _i_ ， _X_ 取值 _x i _ 的概率为 _P_ ( _X_ = _x i _ , _θ_ )，其中为待估计的参数。若{ _X_ 1 , _X_ 2 ,…, _X n _ }是来自母体的一个子样，则称</p>
<p><img src="Image00209.gif" alt></p>
<p>（3-72）</p>
<p>为该子样的似然函数。</p>
<p>极大似然估计法的基本思想是通过最大化似然函数来求出参数 _θ_ 的估计量。下面通过一个例子加以说明。</p>
<p><strong>例3-1</strong> 假设{ _X_ 1 , _X_ 2 ,…, _X n _ }是母体的一个子样，求概率密度函数</p>
<p><img src="Image00210.gif" alt></p>
<p>（3-73）</p>
<p>的极大似然估计量。这里 _θ_ &gt;−1是未知参数。</p>
<p><strong>解</strong> 似然函数为</p>
<p><img src="Image00211.gif" alt></p>
<p>（3-74）</p>
<p>两边取对数，有</p>
<p><img src="Image00212.gif" alt></p>
<p>（3-75）</p>
<p>两边对<img src="Image00213.gif" alt> 求导，令<img src="Image00214.gif" alt> ，得</p>
<p><img src="Image00215.gif" alt></p>
<p>（3-76）</p>
<p>于是<img src="Image00213.gif" alt> 的极大似然估计量为</p>
<p><img src="Image00216.gif" alt></p>
<p>（3-77）</p>
<h2 id="3-3-矩阵"><a href="#3-3-矩阵" class="headerlink" title="3.3 矩阵"></a>3.3 矩阵</h2><p>在机器学习中，矩阵是一个非常常用的工具。因为在很多实际问题中，数据可以很容易地用矩阵来表示。当我们用矩阵来表示数据时，数据的很多操作可以直接用矩阵的相应操作和分解来进行。在本节，我们介绍矩阵的基本概念、矩阵的基本运算、特征值与特征向量，以及矩阵的一些常用分解，如基于特征值与特征向量的分解、奇异值分解和主成分分析。</p>
<h3 id="3-3-1-基本概念"><a href="#3-3-1-基本概念" class="headerlink" title="3.3.1 基本概念"></a>3.3.1 基本概念</h3><h4 id="1．矩阵的定义-1"><a href="#1．矩阵的定义-1" class="headerlink" title="1．矩阵的定义"></a>1．矩阵的定义</h4><p>_m_ × _n_ 个数<img src="Image00217.gif" alt> 构成的如下排列称为 _m_ 行 _n_ 列的矩阵，简称 _m_ × _n_ 矩阵：</p>
<p><img src="Image00218.gif" alt></p>
<p>（3-78）</p>
<p>矩阵一般用大写字母表示。例如，上述矩阵可以简记为<img src="Image00219.gif" alt> 。特别地，当 _m=n_ 时，矩阵<img src="Image00220.gif" alt> 称为 _m_ 阶方阵。</p>
<p>对于矩阵<img src="Image00221.gif" alt> ，把矩阵<img src="Image00222.gif" alt> 称为<br><strong>_A_ </strong> 的转置矩阵，记为<img src="Image00223.gif" alt><br>：</p>
<p><img src="Image00224.gif" alt></p>
<p>（3-79）</p>
<h4 id="2．特殊矩阵"><a href="#2．特殊矩阵" class="headerlink" title="2．特殊矩阵"></a>2．特殊矩阵</h4><p>方阵<img src="Image00225.gif" alt> 有如下几种常见的特殊类型。</p>
<ul>
<li>对称矩阵 ：若<img src="Image00226.gif" alt> ，则称 <strong>_A_ </strong> 为对称矩阵；</li>
<li>对角矩阵 ：若<img src="Image00227.gif" alt> ，则称 <strong>_A_ </strong> 为对角矩阵：</li>
</ul>
<p><img src="Image00228.gif" alt></p>
<p>（3-80）</p>
<ul>
<li>单位矩阵 ：若<img src="Image00227.gif" alt> 且<img src="Image00229.gif" alt> ，则称 <strong>_A_ </strong> 为单位矩阵，通常记为 <strong>_A_ </strong> = <strong>_I_ </strong> 或 <strong>_A_ </strong> = <strong>_E_ </strong> ：</li>
</ul>
<p><img src="Image00230.gif" alt></p>
<p>（3-81）</p>
<ul>
<li>正交矩阵 ：若<img src="Image00231.gif" alt> ，则称 <strong>_A_ </strong> 为正交矩阵；将矩阵 <strong>_A_ </strong> 记为<img src="Image00232.gif" alt> ，这里<img src="Image00233.gif" alt> 是 <strong>_A_ </strong> 的第 _i_ 列，则有</li>
</ul>
<p><img src="Image00234.jpg" alt></p>
<p>（3-82）</p>
<p>类似地，我们利用<img src="Image00235.gif" alt> 也可证明 <strong>_A_ </strong> 的每行也具有类似的性质。</p>
<ul>
<li>正定矩阵 ：若对任意列向量<img src="Image00236.gif" alt> ，都有<img src="Image00237.gif" alt> &gt;0，则称 <strong>_A_ </strong> 为正定矩阵。</li>
<li>半正定矩阵 ：若对任意列向量<img src="Image00236.gif" alt> ，都有<img src="Image00238.gif" alt> ，则称 <strong>_A_ </strong> 为半正定矩阵。</li>
<li>可逆矩阵 ：存在方阵 <strong>_B_ </strong> ，使得<img src="Image00239.gif" alt> ，则称 <strong>_A_ </strong> 为可逆矩阵，称<img src="Image00240.gif" alt> 为 <strong>_A_ </strong> 的逆矩阵。如果矩阵 <strong>_A_ </strong> 不可逆，则称 <strong>_A_ </strong> 为奇异矩阵。</li>
</ul>
<h4 id="3．矩阵的行列式"><a href="#3．矩阵的行列式" class="headerlink" title="3．矩阵的行列式"></a>3．矩阵的行列式</h4><p>在介绍行列式之前，首先引入排列的奇偶性概念：假设在1,2…, _n_ 的某种排列<img src="Image00241.gif" alt> 中，<img src="Image00242.gif" alt> ，有<img src="Image00243.gif" alt> 个比<img src="Image00244.gif" alt> 小的数排在<img src="Image00244.gif" alt> 之后，则用<img src="Image00245.gif" alt> 的奇偶性来描述排列<img src="Image00241.gif" alt> 的奇偶性。例如，对于排列1,3,2,7,5，<img src="Image00246.gif" alt> ，则该排列为偶排列。</p>
<p>设1, 2, …, _n_ 的所有排列集合为 _Ω_ ，称<img src="Image00247.gif" alt> 为方阵<img src="Image00248.gif" alt> 的行列式，这里<img src="Image00245.gif" alt> 的含义同上。</p>
<h4 id="4．秩"><a href="#4．秩" class="headerlink" title="4．秩"></a>4．秩</h4><p>若将矩阵<img src="Image00221.gif" alt> 的每一行、每一列都看做一个向量，则线性无关的行向量最大个数称为 <strong>_A_ </strong> 的行秩 ，线性无关的列向量最大个数称为 <strong>_A_ </strong> 的列秩 。矩阵<br><strong>_A_ </strong> 的行秩等于列秩。</p>
<h4 id="5．向量的范数"><a href="#5．向量的范数" class="headerlink" title="5．向量的范数"></a>5．向量的范数</h4><p>对于一个 _d_ 维向量<img src="Image00249.gif" alt> ，其<img src="Image00250.gif" alt> 范数定义为</p>
<p><img src="Image00251.gif" alt></p>
<p>（3-83）</p>
<p>根据该定义，可以得到如下常用的1-范数（也称为L1 范数）：</p>
<p><img src="Image00252.gif" alt></p>
<p>（3-84）</p>
<p>相应地，2-范数（也称为L2 范数）定义为</p>
<p><img src="Image00253.gif" alt></p>
<p>（3-85）</p>
<p>范数的概念也可以引申到矩阵中。感兴趣的读者可以阅读矩阵计算的相关教材[21]。在本书中，我们只介绍向量的范数。</p>
<h3 id="3-3-2-基本运算"><a href="#3-3-2-基本运算" class="headerlink" title="3.3.2 基本运算"></a>3.3.2 基本运算</h3><p>本节介绍矩阵的和/差、数乘、乘法、幂等基本运算。</p>
<h4 id="1．和-差运算"><a href="#1．和-差运算" class="headerlink" title="1．和/差运算"></a>1．和/差运算</h4><p>对任意矩阵<img src="Image00254.gif" alt> 和<img src="Image00255.gif" alt> ，有</p>
<p><img src="Image00256.gif" alt></p>
<p>（3-86）</p>
<p>不难证明，矩阵的和运算满足交换律和结合律。</p>
<h4 id="2．数乘运算"><a href="#2．数乘运算" class="headerlink" title="2．数乘运算"></a>2．数乘运算</h4><p>对任意矩阵<img src="Image00221.gif" alt> 和常数 _k_ ，有</p>
<p><img src="Image00257.gif" alt></p>
<p>（3-87）</p>
<p>不难证明，矩阵的数乘运算满足分配律。</p>
<h4 id="3．乘法运算"><a href="#3．乘法运算" class="headerlink" title="3．乘法运算"></a>3．乘法运算</h4><p>对任意矩阵<img src="Image00221.gif" alt> 和<img src="Image00258.gif" alt> ，<br><strong>_A_ </strong> 与 <strong>_B_ </strong> 的乘积 <strong>_C_ </strong> 是一个 _m_ × _p_ 矩阵，且<img src="Image00259.gif" alt> {1,2,…, _m_ }，有<img src="Image00260.gif" alt> ：</p>
<p><img src="Image00261.gif" alt></p>
<p><img src="Image00262.gif" alt></p>
<p>（3-88）</p>
<p>不难证明，乘法运算满足结合律和分配律。</p>
<h4 id="4．幂运算"><a href="#4．幂运算" class="headerlink" title="4．幂运算"></a>4．幂运算</h4><p>对任意方阵<img src="Image00263.gif" alt> 和正整数 _k_ ， <strong>_A_ </strong> 的 _k_ 次幂定义为</p>
<p><img src="Image00264.jpg" alt></p>
<p>（3-89）</p>
<p>对任意正整数 _k_ 、 _m_ ，矩阵 <strong>_A_ </strong> 的幂运算满足：</p>
<p><img src="Image00265.gif" alt></p>
<p>（3-90）</p>
<h3 id="3-3-3-特征值与特征向量"><a href="#3-3-3-特征值与特征向量" class="headerlink" title="3.3.3 特征值与特征向量"></a>3.3.3 特征值与特征向量</h3><p>对于方阵<img src="Image00248.gif" alt> ，若存在 _n_ 维列向量<img src="Image00236.gif" alt> ，使得：</p>
<p><img src="Image00266.gif" alt></p>
<p>（3-91）</p>
<p>则称 <strong>_v_ </strong> 为 <strong>_A_ </strong> 的特征向量 ，称 _λ_ 为对应的特征值 。本节介绍求解特征值与特征向量的方程法与幂方法。</p>
<h4 id="1．方程法求解"><a href="#1．方程法求解" class="headerlink" title="1．方程法求解"></a>1．方程法求解</h4><p>求解特征值与特征向量时，可以把式（3-91）等价写为</p>
<p><img src="Image00267.gif" alt></p>
<p>（3-92）</p>
<p>可以直接求解该方程；或者通过求解<img src="Image00268.gif" alt> 来求解特征值 _λ_ ，进而代入式（3-92）求得对应的特征向量 <strong>_v_ </strong> 。下面通过一个例子加以说明。</p>
<p><strong>例3-2</strong> 求矩阵</p>
<p><img src="Image00269.gif" alt></p>
<p>（3-93）</p>
<p>的特征值与特征向量。</p>
<p><strong>解</strong> 设所求特征值与特征向量分别为 _λ_ 和 <strong>_v_ </strong> ，则有</p>
<p><img src="Image00270.gif" alt></p>
<p>（3-94）</p>
<p>解得<img src="Image00271.gif" alt> 或<img src="Image00272.gif" alt> 。</p>
<p>当<img src="Image00271.gif" alt> 时，方程<img src="Image00273.gif" alt> 变为</p>
<p><img src="Image00274.gif" alt></p>
<p>（3-95）</p>
<p>解得<img src="Image00275.gif" alt> 。</p>
<p>当<img src="Image00272.gif" alt> 时，方程<img src="Image00276.gif" alt> 变为</p>
<p><img src="Image00277.gif" alt></p>
<p>（3-96）</p>
<p>解得<img src="Image00278.gif" alt> 。</p>
<p>由方程法可以看出，在复数域上，方阵 <strong>_A_ </strong> 的特征值个数一定等于方阵的阶数。</p>
<h4 id="2．幂方法求解"><a href="#2．幂方法求解" class="headerlink" title="2．幂方法求解"></a>2．幂方法求解</h4><p>对于方阵<img src="Image00225.gif" alt> ，假设其 _n_ 个特征值按模由大到小的顺序依次排列为<img src="Image00279.gif" alt> ，对应的特征向量分别为<img src="Image00280.gif" alt> 。任取特征向量的线性组合<img src="Image00281.gif" alt> ，然后构造序列<img src="Image00282.gif" alt> ，不难看出：</p>
<p><img src="Image00283.gif" alt></p>
<p>（3-97）</p>
<p>式（3-97）可改写为</p>
<p><img src="Image00284.gif" alt></p>
<p>（3-98）</p>
<p>若<img src="Image00285.gif" alt> ，即<img src="Image00286.gif" alt> 严格大于<img src="Image00287.gif" alt> ，则有</p>
<p><img src="Image00288.gif" alt></p>
<p>（3-99）</p>
<p>从而<img src="Image00289.gif" alt> 。由于<img src="Image00290.gif" alt> 是方阵 <strong>_A_ </strong> 模最大的特征值，因此称其为方阵 <strong>_A_ </strong> 的主特征值 （principal eigenvalue），把对应的特征向量<img src="Image00291.gif" alt> 称为方阵 <strong>_A_
</strong> 的主特征向量 （principal eigenvector）。由式（3-99）可得</p>
<p><img src="Image00292.gif" alt></p>
<p>（3-100）</p>
<p>这里 _c_ 为常数。因此，主特征值与主特征向量均可以通过迭代的方法近似求得。</p>
<p>从这里的分析我们可以看出，幂方法收敛速度依赖于<img src="Image00293.gif" alt> 。<img src="Image00293.gif" alt> 越小，幂方法收敛的速度越快。当前两个特征值<img src="Image00290.gif" alt> 和<img src="Image00294.gif" alt> 满足<img src="Image00295.gif" alt> 时，我们可以分3种情况讨论：</p>
<ul>
<li><img src="Image00296.gif" alt> ，上面讨论的算法仍然收敛；</li>
<li><img src="Image00297.gif" alt> ，只需要对矩阵<img src="Image00298.gif" alt> 使用幂方法即可；</li>
<li>当<img src="Image00290.gif" alt> 、<img src="Image00294.gif" alt> 是共轭复数时，幂方法不收敛。</li>
</ul>
<p>在这里我们只是简单讨论幂方法的基本原理。当<img src="Image00290.gif" alt> 、<img src="Image00294.gif" alt> 是共轭复数时，我们可以使用幂方法更加复杂的变体来求解。</p>
<p>在具体迭代时，对于方阵<img src="Image00248.gif" alt> ，任取初始特征向量<img src="Image00299.gif" alt> 并指定一个小正数 _ε_ ，然后<img src="Image00300.gif" alt> 可通过下面的式（3-101）迭代得到最小的 _k_ 值：</p>
<p><img src="Image00301.gif" alt></p>
<p>（3-101）</p>
<p>这样得到的向量<img src="Image00302.gif" alt> 称为方阵 <strong>_A_ </strong> 的（近似）主特征向量，相应的特征值称为（近似）主特征值。这里仍以例3-2为例，若我们选取初始特征向量<img src="Image00303.gif" alt> 并指定小正数<img src="Image00304.gif" alt> ，则有</p>
<p><img src="Image00305.gif" alt></p>
<p><img src="Image00306.gif" alt></p>
<p><img src="Image00307.gif" alt></p>
<p><img src="Image00308.gif" alt></p>
<p>计算到<img src="Image00309.gif" alt> 时我们发现，<img src="Image00310.gif" alt> &lt;0.05，满足条件，于是<img src="Image00309.gif" alt> 为方阵 <strong>_A_ </strong> 的（近似）主特征向量。根据式（3-101），特征向量<img src="Image00302.gif" alt> 恒为单位向量，因此<img src="Image00311.gif" alt> ，于是根据式（3-91）有</p>
<p><img src="Image00312.gif" alt></p>
<p>（3-102）</p>
<p>因此，方阵 <strong>_A_ </strong> 对应于<img src="Image00313.gif" alt> 的（近似）主特征值为</p>
<p><img src="Image00314.gif" alt></p>
<p>这个结果跟例3-2的其中一个结果非常接近。如果要计算另一组特征值和特征向量，可构造方阵<img src="Image00315.gif" alt> ，然后对 <strong>_B_ </strong> 进行迭代计算，过程从略。</p>
<h3 id="3-3-4-矩阵分解"><a href="#3-3-4-矩阵分解" class="headerlink" title="3.3.4 矩阵分解"></a>3.3.4 矩阵分解</h3><h4 id="1．基于特征值与特征向量的分解"><a href="#1．基于特征值与特征向量的分解" class="headerlink" title="1．基于特征值与特征向量的分解"></a>1．基于特征值与特征向量的分解</h4><p>设方阵<img src="Image00248.gif" alt> 的 _n_ 个特征值分别为<img src="Image00316.gif" alt> ，对应的特征向量分别为<img src="Image00280.gif" alt> 。根据特征值和特征向量的定义有</p>
<p><img src="Image00317.gif" alt></p>
<p>（3-103）</p>
<p>我们可以将这 _n_ 个等式写成矩阵的形式：</p>
<p><img src="Image00318.gif" alt></p>
<p>（3-104）</p>
<p>若将<img src="Image00319.gif" alt> 记为 <strong>_V_ </strong> ，将 _n_ × _n_ 的对角矩阵<img src="Image00320.gif" alt> 记为 _<strong>D</strong> _ ：</p>
<p><img src="Image00321.gif" alt></p>
<p>（3-105）</p>
<p>则式（3-104）可写为</p>
<p><img src="Image00322.gif" alt></p>
<p>（3-106）</p>
<p>如果矩阵 <strong>_V_ </strong> 可逆，可以将矩阵 <strong>_A_ </strong> 表示为</p>
<p><img src="Image00323.gif" alt></p>
<p>（3-107）</p>
<p>若矩阵 <strong>_A_ </strong> 是对称矩阵，则矩阵 <strong>_V_ </strong> 是正交矩阵（意味着<img src="Image00324.gif" alt> ，即<img src="Image00325.gif" alt><br>），可以将矩阵 <strong>_A_ </strong> 表示为</p>
<p><img src="Image00326.gif" alt></p>
<p>（3-108）</p>
<h4 id="2．奇异值分解"><a href="#2．奇异值分解" class="headerlink" title="2．奇异值分解"></a>2．奇异值分解</h4><p>在前面的特征值分解中，对于一个实矩阵，所得的特征值和特征向量中可能还有虚数。下面我们介绍奇异值分解 （singular value decomposition, SVD）。奇异值分解在矩阵计算中是一项非常有用的工具，在机器学习特别是降维中有着广泛的应用。在SVD中，我们也把一个实矩阵分解成为3个实矩阵的乘积，且中间的那个矩阵是对角矩阵。</p>
<p>假设矩阵<img src="Image00254.gif" alt> 的秩为 _r_ ，则存在矩阵<img src="Image00327.gif" alt> 、<img src="Image00328.gif" alt> 、<img src="Image00329.gif" alt> ，使得</p>
<p><img src="Image00330.gif" alt></p>
<p>（3-109）</p>
<p>这里<img src="Image00331.gif" alt> 为对角矩阵，<img src="Image00332.gif" alt> 和<img src="Image00333.gif" alt> 满足：</p>
<p><img src="Image00334.gif" alt></p>
<p>（3-110）</p>
<p>我们把式（3-109）称为矩阵 <strong>_A_ </strong> 的奇异值分解，把<img src="Image00335.gif" alt> 称为左奇异向量，把<img src="Image00336.gif" alt> 称为右奇异向量，把<img src="Image00337.gif" alt> 称为奇异值。图3-4显示了SVD的示例。注意，根据秩的定义，我们知道<img src="Image00338.gif" alt> 。</p>
<p><img src="Image00339.jpg" alt></p>
<p>图3-4 SVD示例</p>
<p>式（3-109）可展开为</p>
<p><img src="Image00340.gif" alt></p>
<p>（3-111）</p>
<p>尽管对角矩阵可以任意排列<img src="Image00341.gif" alt> 的次序（调整 <strong>_U_ </strong> 、 <strong>_V_ </strong> 中对应列的顺序即可），但在矩阵分解中，我们一般约定<img src="Image00342.gif" alt> 。</p>
<p>下面我们给出一个SVD的实际例子：</p>
<p><img src="Image00343.gif" alt></p>
<p>可以通过对向量的变换来解释奇异值分解。假设要对向量 <strong>_w_ </strong> 进行线性变换 <strong>_Aw_ </strong> ：</p>
<p><img src="Image00344.gif" alt></p>
<p>（3-112）</p>
<p>我们可以分别考虑每个<img src="Image00345.gif" alt> 。首先利用 <strong>_v_ </strong> i 对 <strong>_w_ </strong> 进行变换，得到 <strong>_w_ </strong> 在 <strong>_v_ </strong> i 方向上的分量<img src="Image00346.gif" alt><br>（这是一个实数值）；然后使用奇异值<img src="Image00347.gif" alt> 来调整该分量的大小，得到<img src="Image00348.gif" alt><br>；最后将该分量投影到由 <strong>_u_ </strong> i 规定的方向上，得到<img src="Image00349.gif" alt> 。将所有的分量累加起来，就得到<img src="Image00350.gif" alt> 。</p>
<p>简单地讲， <strong>_V_ </strong> 的列 <strong>_v_ </strong> i 可以是“输入”空间的基向量， <strong>_U_ </strong> 的列 <strong>_u_ </strong> i 可以认为是输出空间的基向量，而对应的奇异值<img src="Image00347.gif" alt> 可以认为是对输入空间和输出空间的向量长度进行控制的项。</p>
<h3 id="3-3-5-主成分分析"><a href="#3-3-5-主成分分析" class="headerlink" title="3.3.5 主成分分析"></a>3.3.5 主成分分析</h3><p>在很多实际数据中，通常涉及很多的变量。大量的变量不但增加了计算复杂度，而且有些变量有可能是噪声，从而将数据中的主要信息“淹没”。此外，虽然每个变量都提供了相应的信息，但是很多变量可能存在一定的相关性。因此，我们希望能够从数据中提取最主要的信息，用较少的新变量来表达原来数据中的主要信息。在主成分分析<br>（principal component analysis, PCA）中，我们使用旧变量的线性组合构造新的变量。换言之，主成分分析的基本思想是利用线性变换将高维空间中的数据投影到低维空间中，并保留其中最主要的信息。</p>
<p>下面我们使用一组二维数据来说明主成分分析的原理。在图3-5中，我们有一组二维数据。对于这组数据，我们可以看出数据大致沿着图中斜线分布。如果我们将数据投影到这条斜线上，则数据的分散程度最大，能够给我们提供最多的关于数据间差异的信息。严格地讲，投影后数据的分散程度最大，即方差最大。注意，我们这里是将数据投影到一条直线上，如果用<img src="Image00351.gif" alt> 、<img src="Image00352.gif" alt> 分别表示原始数据中的两个变量，则可以使用这两个变量的线性组合来表示所构造的新变量 _v_ ：</p>
<p><img src="Image00353.gif" alt></p>
<p>（3-113）</p>
<p>这里<img src="Image00354.gif" alt> 。我们将使得所得投影后新变量 _v_ 方差最大的 _v_ 称为第一主成分。</p>
<p>在上面的这个二维数据的例子中，我们将数据投影到了一个一维空间，因此仅需要第一主成分。对于更加高维的数据，如果我们要将其投影到多维空间中，那么需要多个主成分。在得到第一主成分之后，我们求解第二主成分时要求其与第一主成分正交，表示要从原始数据中提取第一主成分没有提取到的信息。一般而言，在求解第 _i_ 个主成分时，要求它与前面已知的 _i_ -1个主成分都正交。</p>
<p><img src="Image00355.jpg" alt></p>
<p>图3-5 PCA示例（图中斜线表示一组二维数据的第一主成分的方向）</p>
<p>在下面的讨论中，我们使用严格的数学语言来讨论对于给定的数据，如何求解主成分。</p>
<p>假设待研究的数据可表示为 _n_ × _d_ 的矩阵 <strong>_X_ </strong> ：</p>
<p><img src="Image00356.gif" alt></p>
<p>（3-114）</p>
<p>这里 _n_ 是样本数， _d_ 为维度，矩阵 <strong>_X_ </strong> 的每一行对应一个样本，每一列对应一个变量，而 <strong>_x_ </strong> i 是第 _i_ 个样本所对应的列向量。我们将数据的协方差矩阵记为<img src="Image00357.gif" alt> ：</p>
<p><img src="Image00358.jpg" alt></p>
<p>（3-115）</p>
<p>在主成分分析中，我们使用线性变换将数据投影到低维空间中，希望能够保存原来数据的主要信息。在前面的讨论中，我们知道在主成分分析中希望投影后数据点的分散度更高，即投影后的方差最大。使用线性变换<br><strong>_g_ 1 </strong> 将 <strong>_X_ </strong> 投影到一维空间后，新的数据表示为 <strong>_Xg_ </strong> 1 ，对应的方差为</p>
<p><img src="Image00359.gif" alt></p>
<p>（3-116）</p>
<p>因此，在主成分分析中，我们需要最大化<img src="Image00360.gif" alt> 。注意，我们必须对<img src="Image00361.gif" alt> 加以限制，否则<img src="Image00360.gif" alt> 无界。这里我们假设<img src="Image00361.gif" alt> 具有单位长度，即要求<img src="Image00362.gif" alt> 。因此，我们需要求解如下的优化问题：</p>
<p><img src="Image00363.gif" alt></p>
<p>（3-117）</p>
<p>得到投影向量 <strong>_g_ </strong> 1 后，可以将 <strong>_X_ </strong> 投影得到 <strong>_Xg_ </strong> 1 ，称为 <strong>_X_ </strong> 的第一主成分。</p>
<p>如果第一主成分所反映的信息不足，还要从原始数据 <strong>_X_ </strong> 中进一步提取信息。计算第二主成分时，为了使得到的信息不相互重叠，要求 <strong>_Xg_ </strong> 1 和 <strong>_Xg_ </strong> 2 不相关，即</p>
<p><img src="Image00364.gif" alt></p>
<p>（3-118）</p>
<p>与求解第一主成分类似，我们要从原始数据中最大程度地提取剩余的信息，要最大化<img src="Image00365.gif" alt> 。但是，对于 _<strong>g</strong> _ 2 除了限制条件<img src="Image00366.gif" alt> 外，还要求<img src="Image00367.gif" alt> 。因此，对于<img src="Image00368.gif" alt> ，我们需要求解的优化问题是：</p>
<p><img src="Image00369.gif" alt></p>
<p>（3-119）</p>
<p>一般来说，在求解第 _i_ 个主成分时，我们要最大化<img src="Image00370.gif" alt> ，同时要求<img src="Image00371.gif" alt> 和<img src="Image00372.gif" alt> 都不相关。因此，相应的优化问题是：</p>
<p><img src="Image00373.gif" alt></p>
<p>（3-120）</p>
<p>下面介绍两种方法来求解 <strong>_g_ </strong> i 。注意，协方差矩阵<img src="Image00116.gif" alt> 半正定且对称，因此其所有的特征值都是非负实数。这里我们把<img src="Image00116.gif" alt> 的 _d_ 个特征值记为<img src="Image00374.gif" alt> ，并将其对应的特征向量记为<img src="Image00375.gif" alt> 。这些特征向量也可以写成矩阵的形式<img src="Image00376.gif" alt> 。下面我们严格证明 <strong>_g_ </strong> i 是协方差矩阵<img src="Image00116.gif" alt> 的第 _i_ 大的特征值所对应的特征向量<img src="Image00377.gif" alt> 。</p>
<h4 id="1．拉格朗日函数法"><a href="#1．拉格朗日函数法" class="headerlink" title="1．拉格朗日函数法"></a>1．拉格朗日函数法</h4><p>第一种方法是使用拉格朗日函数来求解条件约束的最优化问题。求解 <strong>_g_ </strong> 1 时，我们可以构造相应的拉格朗日函数 _L_ 1 ：</p>
<p><img src="Image00378.gif" alt></p>
<p>（3-121）</p>
<p>这里<img src="Image00379.gif" alt> 是拉格朗日乘子。对 _L_ 1 求关于<br><strong>_g_ </strong> 1 的导数并置为0 ① ，得到：</p>
<p><img src="Image00380.gif" alt></p>
<p>（3-122）</p>
<p>这个结果表明 <strong>_g_ </strong> 1 是<img src="Image00381.gif" alt> 的一个特征向量，且其对应的特征值是<img src="Image00382.gif" alt> 。重新回到目标函数，我们有：</p>
<p><img src="Image00383.gif" alt></p>
<p>（3-123）</p>
<p>所以，最大化<img src="Image00384.gif" alt> 等价于最大化<img src="Image00385.gif" alt> 。因此<img src="Image00386.gif" alt> ，即<br><strong>_g_ </strong> 1 是<img src="Image00116.gif" alt> 最大的特征值所对应的特征向量。</p>
<p>接下来我们使用类似的技巧来求解 <strong>_g_ </strong> 2 。相应地，可以为 <strong>_g_ </strong> 2 构造拉格朗日函数 _L_ 2 ：</p>
<p><img src="Image00387.gif" alt></p>
<p>（3-124）</p>
<p>这里<img src="Image00388.gif" alt> 是拉格朗日乘子。对 _L_ 2 求关于 <strong>_g_ </strong> 2 的导数并置为0，得到：</p>
<p><img src="Image00389.gif" alt></p>
<p>（3-125）</p>
<p>将式（3-125）左右两边都左乘<img src="Image00390.gif" alt> ，可得：</p>
<p><img src="Image00391.gif" alt></p>
<p>（3-126）</p>
<p>注意到<img src="Image00392.gif" alt> ，这里我们利用了 <strong>_g_ </strong> 1 是<img src="Image00116.gif" alt> 的特征向量的性质，所以可以确定<img src="Image00393.gif" alt> 。因此，在式（3-126）中，第一项和第二项都为0，第三项也只能为0：</p>
<p><img src="Image00394.gif" alt></p>
<p>（3-127）</p>
<p>将<img src="Image00395.gif" alt> 代回到<img src="Image00396.gif" alt> 中，我们得到：</p>
<p><img src="Image00397.gif" alt></p>
<p>（3-128）</p>
<p>所以 <strong>_g_ </strong> 2 也是<img src="Image00116.gif" alt> 的特征向量，其对应的特征值为<img src="Image00398.gif" alt> 。使用类似的技巧，我们可以得到<img src="Image00399.gif" alt> 。因此， <strong>_g_ </strong> 2 是<img src="Image00116.gif" alt> 第二大的特征值所对应的特征向量。</p>
<p>我们可以顺次使用上面的推导，获得一般性的结论： <strong>_g_ </strong> i 是<img src="Image00116.gif" alt> 第<img src="Image00400.gif" alt> 大的特征值所对应的特征向量。</p>
<h4 id="2．特征值分解法"><a href="#2．特征值分解法" class="headerlink" title="2．特征值分解法"></a>2．特征值分解法</h4><p>第二种方法是直接利用协方差矩阵<img src="Image00116.gif" alt> 的特征值分解。我们知道</p>
<p><img src="Image00401.gif" alt></p>
<p>（3-129）</p>
<p>写成矩阵的形式就是：</p>
<p><img src="Image00402.gif" alt></p>
<p>（3-130）</p>
<p>这里<img src="Image00403.gif" alt> 。由于矩阵<img src="Image00116.gif" alt> 是对称矩阵，因此：</p>
<p><img src="Image00404.gif" alt></p>
<p>（3-131）</p>
<p>这里<img src="Image00405.gif" alt> 是<img src="Image00406.gif" alt> 的单位矩阵。利用该性质和式（3-130），我们有</p>
<p><img src="Image00407.gif" alt></p>
<p>（3-132）</p>
<p>注意到 <strong>_V_ </strong> 构成了 _d_ 维空间的一组基，因此可以将 <strong>_g_ </strong> 1 写成：</p>
<p><img src="Image00408.gif" alt></p>
<p>（3-133）</p>
<p>这里 <strong>_w_ </strong> 1 是 _d_ 维向量<img src="Image00409.gif" alt> 。在求解 <strong>_g_ </strong> 1 时，我们要最大化<img src="Image00384.gif" alt> ，它可以进一步表示为：</p>
<p><img src="Image00410.gif" alt></p>
<p>（3-134）</p>
<p>注意到关于向量 <strong>_g_ </strong> 1 有约束条件<img src="Image00411.gif" alt> ，可以表示为：</p>
<p><img src="Image00412.gif" alt></p>
<p>（3-135）</p>
<p>注意到<img src="Image00413.gif" alt> 。我们要在满足<img src="Image00414.gif" alt> 的条件下最大化<img src="Image00415.gif" alt> ，对于最优解必须有：</p>
<p><img src="Image00416.gif" alt></p>
<p>（3-136）</p>
<p>此时有：</p>
<p><img src="Image00417.gif" alt></p>
<p>（3-137）</p>
<p>因此， <strong>_g_ </strong> 1 就是<img src="Image00381.gif" alt> 最大的特征值所对应的特征向量。</p>
<p>接下来考虑 <strong>_g_ </strong> 2 的求解。类似地，可以将 <strong>_g_ </strong> 2 表示为<img src="Image00418.gif" alt> ，这里 <strong>_w_ </strong> 2 是 _d_ 维向量<img src="Image00419.gif" alt> 。在求解 <strong>_g_ </strong> 2 时，我们要最大化<img src="Image00420.gif" alt> ，它也可表示为：</p>
<p><img src="Image00421.gif" alt></p>
<p>（3-138）</p>
<p>但是要注意到关于 <strong>_g_ </strong> 2 还有两个约束条件。与关于 <strong>_g_ </strong> 1 的推导类似，其中一个约束条件可以表示为：</p>
<p><img src="Image00422.gif" alt></p>
<p>（3-139）</p>
<p>这里讨论另一个约束条件<img src="Image00423.gif" alt> ：</p>
<p><img src="Image00424.gif" alt></p>
<p>（3-140）</p>
<p>因此，约束条件<img src="Image00425.gif" alt> 可以表示成：</p>
<p><img src="Image00426.gif" alt></p>
<p>（3-141）</p>
<p>而我们要最大化的目标<img src="Image00420.gif" alt> 也可以表示为：</p>
<p><img src="Image00427.gif" alt></p>
<p>（3-142）</p>
<p>在这种情况下，要取得最大值，只有：</p>
<p><img src="Image00428.gif" alt></p>
<p>（3-143）</p>
<p>此时有：</p>
<p><img src="Image00429.gif" alt></p>
<p>（3-144）</p>
<p>因此， <strong>_g_ </strong> 2 就是<img src="Image00116.gif" alt> 第二大的特征值所对应的特征向量。</p>
<p>类似地，我们可以顺次使用上面的推导，获得一般性的结论： <strong>_g_ </strong> i 是<img src="Image00116.gif" alt> 第<img src="Image00430.gif" alt> 大的特征值所对应的特征向量。</p>
<h4 id="3．贡献率"><a href="#3．贡献率" class="headerlink" title="3．贡献率"></a>3．贡献率</h4><p>根据前面的推导，第 _i_ 个主成分所对应的方差为：</p>
<p><img src="Image00431.gif" alt></p>
<p>（3-145）</p>
<p>假设我们从原始数据中选取了所有 _d_ 个主成分，则所得的主成分的总方差为：</p>
<p><img src="Image00432.gif" alt></p>
<p>（3-146）</p>
<p>我们将<img src="Image00433.gif" alt> 称为第 _i_ 个主成分的贡献率 ，描述了第 _i_ 个主成分提取的信息占所有信息的比例。将<img src="Image00434.gif" alt> 称为前 _i_ 个主成分的累积贡献率 ，描述了前 _i_ 个主成分提取的所有信息的比例。在实际中，对于高维数据，通常要选择提取的主成分的数目。通常当选取的主成分的累积贡献率达到较高的比例（如80%）时即可。</p>
<p><strong>例3-3</strong> 假设有一组三维数据对应的协方差矩阵如下，要求出对应的所有3个主成分：</p>
<p><img src="Image00435.gif" alt></p>
<p>（3-147）</p>
<p><strong>解</strong> 我们将原始数据中的3个变量记为<img src="Image00436.gif" alt> 、<img src="Image00437.gif" alt> 、<img src="Image00438.gif" alt> 。可以求得<img src="Image00116.gif" alt> 的特征值和对应的特征向量如下：</p>
<p><img src="Image00439.gif" alt></p>
<p><img src="Image00440.gif" alt></p>
<p><img src="Image00441.gif" alt></p>
<p>则第一个主成分对应的新变量是：</p>
<p><img src="Image00442.gif" alt></p>
<p>类似地，第二个主成分对应的新变量是：</p>
<p><img src="Image00443.gif" alt></p>
<p>如果我们只选取前面两个主成分，则其对应的累积贡献率为：</p>
<p><img src="Image00444.gif" alt> %</p>
<h3 id="3-3-6-R中矩阵的计算"><a href="#3-3-6-R中矩阵的计算" class="headerlink" title="3.3.6 R中矩阵的计算"></a>3.3.6 R中矩阵的计算</h3><p>在本节，我们将要介绍如何使用R中提供的函数来进行较复杂的矩阵计算和分解，如SVD和PCA。关于在R中进行矩阵计算的基本知识可参见2.4.3 节。在本节中，着重介绍<code>eigen</code> 函数和<code>svd</code> 函数，同时也介绍在矩阵操作中常用的<code>diag</code> 函数。本节所有的矩阵分析相关的程序都在文件matrix_decomp_example.R中，而如何使用PCA的程序在文件PCA_example.R中。</p>
<p>我们介绍matrix_decomp_example.R文件以演示如何在R中进行矩阵分解时，首先使用<code>rnorm</code> 函数产生一系列基于正态分布的随机数。具体来说，<code>rnorm(N)</code> 产生<code>N</code> 个随机数。为了保证读者多次运行程序能够得到一致的结果，我们首先使用<code>set.seed</code> 函数将随机种子设为1（读者可以根据需要设置不同的随机种子）。这样，我们得到了一个4×4的矩阵<code>A</code> 和一个4×3的矩阵<code>B</code> 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  set.seed(1)</span><br><span class="line">A = matrix(rnorm(16), 4, 4)</span><br><span class="line">B = matrix(rnorm(12), 4, 3)</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>然后演示<code>diag</code> 函数的用法。R中<code>diag</code> 函数的主要使用方法如下。</p>
<ul>
<li><code>diag(A)</code> ：如果输入参数是一个矩阵<code>A</code> ，则返回其主对角线上的元素组成的一个向量。</li>
<li><code>diag(v)</code> ：如果输入参数是一个向量，则返回一个方阵，其主对角线上的元素来自向量<code>v</code> ，其他元素为0。</li>
<li><code>diag(k)</code> ：返回一个<code>k</code> ×<code>k</code> 的单位矩阵。</li>
<li><code>diag(a,k)</code> ：返回一个<code>k</code> ×<code>k</code> 的对角矩阵，且对角线的元素都是<code>a</code> 。</li>
</ul>
<p>注意，在R中我们还可以将<code>diag(A)</code> 作为左值，从而修改矩阵<code>A</code> 的对角线上的元素。我们使用如下代码将<code>A</code> 的对角线元素都乘以2，将<code>B</code> 的对角线元素都加5。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  diag(A) &lt;-diag(A) * 2</span><br><span class="line">diag(B) &lt;-diag(B) + 5</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>接着使用<code>eigen</code> 函数计算矩阵<code>A</code> 的特征值分解。该函数的主要用法如下：</p>
<ul>
<li><code>eigen(A)</code> ：计算矩阵<code>A</code> 的特征值和特征向量。</li>
<li><code>eigen(A, only.values=T)</code> ：只计算矩阵<code>A</code> 的特征值。</li>
<li><code>eigen(A, symmetric=T)</code> ：指定矩阵<code>A</code> 为对称矩阵并计算其特征值和特征向量。</li>
</ul>
<p>该函数返回的计算结果是一个列表，其中<code>values</code> 保存特征值，<code>vectors</code> 保留对应的特征向量。具体来说，<code>vectors</code> 部分是一个矩阵，每列对应一个特征向量。</p>
<p>先计算矩阵<code>A</code> 的特征值和特征向量，其代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">E_A=eigen(A)</span><br><span class="line">E_A$values</span><br><span class="line">E_A$vectors</span><br><span class="line"># Verify the decomposition</span><br><span class="line">Delta_A = E_A$vectors %*% diag(E_A$values) %*% solve(E_A$vectors) -A</span><br></pre></td></tr></table></figure>

</details>


<p>计算的特征值如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; E_A$values</span><br><span class="line">[1]　3.038531+0.000000i -0.639326+1.598516i -0.639326-1.598516i</span><br><span class="line">[4] -1.720029+0.000000i</span><br></pre></td></tr></table></figure>

</details>


<p>计算的特征向量如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; E_A$vectors</span><br><span class="line">　　　　　 [,1]　　　　　　　　[,2]　　　　　　　　[,3]</span><br><span class="line">[1,]　0.1005331+0i -0.3117119+0.1413634i -0.3117119-0.1413634i</span><br><span class="line">[2,] -0.1273888+0i -0.7008545+0.0000000i -0.7008545+0.0000000i</span><br><span class="line">[3,]　0.9763473+0i -0.0082256-0.1360593i -0.0082256+0.1360593i</span><br><span class="line">[4,]　0.1428676+0i　0.2922527+0.5363428i　0.2922527-0.5363428i</span><br><span class="line">　　　　　 [,4]</span><br><span class="line">[1,]　0.39879838+0i</span><br><span class="line">[2,] -0.90121557+0i</span><br><span class="line">[3,]　0.16814566+0i</span><br><span class="line">[4,] -0.02230215+0i</span><br></pre></td></tr></table></figure>

</details>


<p>为了验证对应的特征值分解，上面计算了矩阵<code>Delta_A</code> ，这里<code>solve(E_A$vectors)</code> 计算特征向量组成的矩阵的逆矩阵。矩阵<code>Delta_A</code> 为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; Delta_A</span><br><span class="line">　　　　　　　　　　　[,1]　　　　　　　　　　 [,2]</span><br><span class="line">[1,] -1.110223e-15+5.25347e-17i -4.996004e-16+2.94439e-17i</span><br><span class="line">[2,] -1.054712e-15+2.28771e-16i -6.661338e-16+5.16959e-17i</span><br><span class="line">[3,]　1.110223e-16-5.67661e-17i -8.881784e-16-6.22705e-18i</span><br><span class="line">[4,]　1.332268e-15-1.10856e-16i　7.771561e-16-6.94834e-17i</span><br><span class="line">　　　　　　　　　　　[,3]　　　　　　　　　　　[,4]</span><br><span class="line">[1,]　4.440892e-16+2.14516e-17i　2.220446e-16+5.639643e-17i</span><br><span class="line">[2,] -3.885781e-16+2.36165e-17i -4.440892e-16+1.090217e-16i</span><br><span class="line">[3,]　8.881784e-16+1.15951e-17i -2.220446e-16-5.513789e-17i</span><br><span class="line">[4,] -6.106227e-16-5.61287e-17i　4.440892e-16-1.110718e-16i</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出<code>Delta_A</code> 非常接近零矩阵。</p>
<p>使用<code>eigen(A, only.values = T)</code> 时只计算特征值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E_A_only = eigen(A, only.values = T)</span><br></pre></td></tr></table></figure>
<p>在这种情况下，<code>E_A_only$vectors为NULL。</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  &gt; E_A_only$vectors</span><br><span class="line">NULL</span><br></pre></td></tr></table></figure>
<p>&gt;</p>
<p>接下来生成一个对称矩阵，并计算它的特征值和特征向量，同时验证对应的特征值分解。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">AA = A + t(A)</span><br><span class="line">E_AA = eigen(AA, symmetric=T)</span><br><span class="line">E_AA$values</span><br><span class="line">E_AA$vectors</span><br><span class="line">Delta_AA = E_AA$vectors %*% diag(E_AA$values) %*% t(E_AA$vectors) -AA</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt; E_AA$values</span><br><span class="line">[1]　6.398773　0.311260 -2.265899 -4.364432</span><br><span class="line">&gt; E_AA$vectors</span><br><span class="line">　　　　　[,1]　　　 [,2]　　　 [,3]　　　　[,4]</span><br><span class="line">[1,] -0.004510515 -0.2627321　0.85543559　0.44629748</span><br><span class="line">[2,] -0.016587737　0.3347921　0.51450720 -0.78925372</span><br><span class="line">[3,]　0.973683278　0.2099321　0.02524274　0.08504233</span><br><span class="line">[4,]　0.227256191 -0.8802355 -0.05361995 -0.41311613</span><br><span class="line">&gt; Delta_AA</span><br><span class="line">　　　　　[,1]　　　　 [,2]　　　　 [,3]　　　　 [,4]</span><br><span class="line">[1,] -8.881784e-16 -2.220446e-16　1.110223e-16 -3.330669e-16</span><br><span class="line">[2,] -4.440892e-16 -5.329071e-15 -2.886580e-15　5.107026e-15</span><br><span class="line">[3,]　1.110223e-16 -2.886580e-15 -8.881784e-16　3.774758e-15</span><br><span class="line">[4,] -3.330669e-16　5.107026e-15　3.774758e-15 -7.799317e-15</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，对称矩阵<code>AA</code> 的特征值和特征向量都是实数。由于 <code>E_AA$vectors</code> 是正交矩阵，因此，在验证特征值分解时，直接使用了<code>t(E_AA$vectors)</code> 而不是<code>E_AA$vectors</code> 的逆矩阵。</p>
<p>然后，我们可以直接使用<code>svd</code> 函数来计算矩阵的奇异值分解。返回的结果也是一个列表对象，其中<code>u</code> 对应SVD中的 <strong>_U_ </strong> 矩阵、<code>v</code> 对应SVD中的 <strong>_V_ </strong> 矩阵，而<code>d</code> 是一个向量，保存着所有的奇异值。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">S_B=svd(B)</span><br><span class="line">S_B$u</span><br><span class="line">S_B$v</span><br><span class="line">S_B$d</span><br><span class="line">Delta_B = S_B$u %*% diag(S_B$d) %*% t(S_B$v) -B</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt; S_B$u</span><br><span class="line">　　　　 [,1]　　　[,2]　　　　[,3]</span><br><span class="line">[1,] -0.5091763　0.36701422　0.7482381</span><br><span class="line">[2,] -0.7646891 -0.52790019 -0.1667094</span><br><span class="line">[3,] -0.2938966　0.76529501 -0.4904828</span><br><span class="line">[4,]　0.2638465　0.03074893　0.4144568</span><br><span class="line">&gt; S_B$v</span><br><span class="line">　　　　 [,1]　　　[,2]　　　　[,3]</span><br><span class="line">[1,] -0.4980551　0.3829775　0.7779906</span><br><span class="line">[2,] -0.8096673 -0.5265935 -0.2591105</span><br><span class="line">[3,] -0.3104513　0.7589648 -0.5723569</span><br><span class="line">&gt; S_B$d</span><br><span class="line">[1] 6.714185 5.163792 4.389618</span><br><span class="line">&gt; Delta_B</span><br><span class="line">　　　　　 [,1]　　　　 [,2]　　　　 [,3]</span><br><span class="line">[1,]　2.664535e-15 -4.773959e-15 -2.220446e-15</span><br><span class="line">[2,] -9.992007e-16　8.881784e-16 -1.151856e-15</span><br><span class="line">[3,]　0.000000e+00 -8.049117e-16　5.329071e-15</span><br><span class="line">[4,]　9.992007e-16 -6.661338e-16 -1.332268e-15</span><br></pre></td></tr></table></figure>

</details>


<p>接下来我们讨论如何在R中计算PCA。在文件PCA_example.R的例子中，使用了<code>iris</code> 数据。在第6章中我们会更加详细地介绍该数据集，这里只是用该数据集中的前4个变量进行PCA分解。这里我们使用R中的<code>prcomp</code> 函数来进行主成分分析。代码如下；</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data(iris)</span><br><span class="line">D = iris</span><br><span class="line"># show the basic information of　the data </span><br><span class="line">str(D)</span><br><span class="line"></span><br><span class="line"># log transform </span><br><span class="line">D_log &lt;-log(D[, 1:4])</span><br><span class="line"></span><br><span class="line"># apply PCA </span><br><span class="line">M_pca &lt;-prcomp(D_log, center = TRUE, scale. = TRUE) </span><br><span class="line"></span><br><span class="line"># print method</span><br><span class="line">print(M_pca)</span><br><span class="line"># members of M_pca</span><br><span class="line">M_pca$sdev</span><br><span class="line">M_pca$rotation</span><br><span class="line"></span><br><span class="line"># plot method</span><br><span class="line">plot(M_pca, type = &quot;l&quot;)</span><br><span class="line"></span><br><span class="line"># summary method</span><br><span class="line">summary(M_pca)</span><br><span class="line"></span><br><span class="line"># Predict PCs</span><br><span class="line">y_test &lt;-predict(M_pca, newdata= D_log[1:10,])</span><br></pre></td></tr></table></figure>

</details>


<p>由于R中已经提供了<code>iris</code> 数据，因此这里我们首先直接使用<code>data</code> 函数载入该数据，并使用<code>str</code> 函数对该数据集进行简单分析。这里我们只考虑该数据的前面4列，同时将其进行了对数变换，同时将数据保存在数据框<code>D_log</code> 中。</p>
<p>之后我们使用<code>prcomp</code> 函数对输入数据<code>D_log</code> 进行主成分分析。这里我们将<code>center</code> 和<code>scale.</code> 都设为真，表示要将数据进行标准化处理，使得每个变量处理后均值为0、方差为1。如果不进行标准化处理，那么可能有些变量由于值比较大会在计算中占优势，从而使得PCA产生偏差。</p>
<p>得到PCA模型<code>M_pca</code> 后，可以使用<code>print</code> 函数打印该模型的主要信息，其输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; print(M_pca)</span><br><span class="line">Standard deviations:</span><br><span class="line">[1] 1.7124583 0.9523797 0.3647029 0.1656840</span><br><span class="line"></span><br><span class="line">Rotation:</span><br><span class="line">　　　　　　　　 PC1　　　　 PC2　　　　  PC3　　　PC4</span><br><span class="line">Sepal.Length　0.5038236 -0.45499872　0.7088547　0.19147575</span><br><span class="line">Sepal.Width　-0.3023682 -0.88914419 -0.3311628 -0.09125405</span><br><span class="line">Petal.Length　0.5767881 -0.03378802 -0.2192793 -0.78618732</span><br><span class="line">Petal.Width　 0.5674952 -0.03545628 -0.5829003　0.58044745</span><br></pre></td></tr></table></figure>

</details>


<p>这里产生了所有的4个主成分。<code>print</code> 函数打印出主成分的标准差和协方差的特征向量。进一步，利用<code>M_pca$sdev</code> ，可以得到每个主成分的标准差，即协方差矩阵的特征值的平方根。利用<code>M_pca$rotation</code> ，可以得到协方差矩阵对应的特征向量。直接使用<code>M_pca$sdev</code> 和<code>M_pca$rotation</code> 的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; M_pca$sdev</span><br><span class="line">[1] 1.7124583 0.9523797 0.3647029 0.1656840</span><br><span class="line">&gt; M_pca$rotation</span><br><span class="line">　　　　　　　　 PC1　　　　 PC2　　　　PC3　　　　 PC4</span><br><span class="line">Sepal.Length　0.5038236 -0.45499872　0.7088547　0.19147575</span><br><span class="line">Sepal.Width　-0.3023682 -0.88914419 -0.3311628 -0.09125405</span><br><span class="line">Petal.Length　0.5767881 -0.03378802 -0.2192793 -0.78618732</span><br><span class="line">Petal.Width　 0.5674952 -0.03545628 -0.5829003　0.58044745</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，其输出与<code>print</code> 函数类似，但是我们单独输出了每个部分。利用<code>M_pca$rotation</code> ，可以从原始数据得到新的主成分。例如，对于第一主成分<code>PC1</code> ，可以利用下面的公式得到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PC1=0.5038236×Sepal.Length—0.3023682×Sepal.Width+0.5767881×Petal.Length+ 0.5674952×Petal.Width</span><br></pre></td></tr></table></figure>
<p>其中的系数来自于<code>M_pca$rotation</code> 的第一列，这里<code>Sepal.Length</code> 、<code>Sepal.Width</code> 、<code>Petal.Length</code> 和<code>Petal.Width</code> 是4个原始变量。</p>
<p>利用<code>plot</code> 函数，可以得到所有主成分对应的方差。图3-6是模型<code>M_pca</code> 对应的输出，其中x轴是主成分的编号，y轴是对应的方差。也可以使用<code>M_pca$sdev^2</code> 来直接计算各个主成分所对应的方差。使用图3-6，可以简单地估计一下应该保留多少个主成分来做进一步的分析。从图3-6可以看出，对于<code>iris</code> 数据，两个主成分就可以了。</p>
<p><img src="Image00445.jpg" alt></p>
<p>图3-6 PCA中各个主成分所对应的方差</p>
<p>利用<code>summary</code> 函数可以进一步研究每个主成分的重要性。其输出的第一行是每个主成分对应的标准差（平方后为方差），第二行为每个主成分的贡献率，第三行为累积贡献率。从<code>summary</code> 函数的输出可以看出，前两个主成分已经贡献了95.99%的方差，因此，选用前两个主成分对于<code>iris</code> 数据就足够了。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(M_pca)</span><br><span class="line">Importance of components:</span><br><span class="line">　　　　　　　　　　　PC1　　PC2　　 PC3　　 PC4</span><br><span class="line">Standard deviation　　 1.7125 0.9524 0.36470 0.16568</span><br><span class="line">Proportion of Variance 0.7331 0.2268 0.03325 0.00686</span><br><span class="line">Cumulative Proportion　0.7331 0.9599 0.99314 1.00000</span><br></pre></td></tr></table></figure>

</details>


<p>最后，使用<code>predict</code> 函数来对新数据进行主成分分析。在使用过程中，利用<code>newdata</code> 来指定新的数据。在我们的代码中，直接使用了<code>D_log</code> 的前10行。<code>predict</code> 函数的输出是这10个样本所对应的4个主成分，其输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; y_test</span><br><span class="line">　　　 PC1　　　 PC2　　　　PC3　　　　 PC4</span><br><span class="line">1　-2.406639 -0.3969554　0.19396467　0.004779476</span><br><span class="line">2　-2.223539　0.6901804　0.35000151　0.048868378</span><br><span class="line">3　-2.581105　0.4275418　0.01889761　0.049909545</span><br><span class="line">4　-2.450869　0.6860074 -0.06874595 -0.149646465</span><br><span class="line">5　-2.536853 -0.5082516　0.02932259 -0.040048202</span><br><span class="line">6　-1.841495 -1.2899381 -0.25276831　0.163890597</span><br><span class="line">7　-2.479490　0.1011323 -0.49740434　0.122759047</span><br><span class="line">8　-2.348593 -0.1569003　0.13601863 -0.095498156</span><br><span class="line">9　-2.535948　1.2477681 -0.11188119 -0.075468614</span><br><span class="line">10 -2.625580　0.5074073　0.65947371 -0.473259070</span><br></pre></td></tr></table></figure>

</details>


<hr>
<p>① 关于如何对向量或者矩阵求导，推荐参考文献[22]。</p>
<h1 id="第4章-数据探索和预处理"><a href="#第4章-数据探索和预处理" class="headerlink" title="第4章 数据探索和预处理"></a>第4章 数据探索和预处理</h1><p>本章主要讨论数据的基本知识，包括数据类型、数据探索和数据预处理。在实际问题中，得到数据之后，首先要了解数据的类型；然后根据类型计算数据的一些统计量以了解每个特征的大致信息；之后可以使用各种方法来进一步探索数据，并对数据进行一些预处理，如删除一些无关变量，通过变换构建新的特征等，为后面的建模打好基础。数据探索和预处理是解决实际问题过程中一个非常重要的步骤，应在实际中予以高度重视。</p>
<h2 id="4-1-数据类型"><a href="#4-1-数据类型" class="headerlink" title="4.1 数据类型"></a>4.1 数据类型</h2><p>在实际问题中，数据呈现出各种各样的形式。在本书中，我们将数据看成是一组对象 （object）的集合，而每个对象由一定数目的特征 （feature）或者属性<br>（attribute）来描述。根据不同的语境，对象也可称为样本 （sample）、记录 （record）、数据点 （data point）、向量<br>（vector）等；特征也可称为变量 （variable）。每个对象可能有不同数目的特征，但在机器学习中，为了处理的方便，我们一般都假设对象的特征是一致的。</p>
<p>本书用 _n_ 表示对象或者样本的数目， _d_ 表示特征的数目，同时我们把第 _i_ 个样本记为</p>
<p><img src="Image00446.gif" alt></p>
<p>（4-1）</p>
<p>这里 _<strong>x</strong> i _ 是 _d_ 维列向量。在本书中，我们通常将向量写成列向量的形式。</p>
<p>下面我们讨论特征。由于实际问题的复杂性，特征有不同的类型。这里举一个搜集学生体检信息的例子。简单起见，我们记录了表4-1所示的数据。</p>
<p>表4-1 学生体检数据及范例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>学号</th>
<th>姓名</th>
<th>性别</th>
<th>年龄</th>
<th>体重</th>
<th>身高  </th>
</tr>
</thead>
<tbody>
<tr>
<td>201503345</td>
<td>李乐</td>
<td>男</td>
<td>21</td>
<td>65</td>
<td>175  </td>
</tr>
</tbody>
</table>
</div>
<p>在表4-1中，有6个特征，包括学号、姓名、性别、年龄、体重和身高。下面逐一分析各个特征，并介绍特征的不同类型。</p>
<p>首先分析学号。在这个例子中，学号虽然是整数，但是我们并不能执行加法或者减法运算。对于体重，65（kg）加1有着明确的含义，而某个学号加1并不一定有着明确的含义（甚至有可能是空号）。因此，在这个例子中，学号虽然是整数，但是我们并不能将其视为数值变量。</p>
<p>在数据分析中主要有两类变量：分类变量 （categorical variable）和数值变量 （numeric variable）。分类变量的取值来自一个集合，顾名思义，每一个取值表示该变量的一个状态（或者分类）。在这个讨论学生体检的例子中，性别就是一个最明显的例子，其取值集合是{男，女}。在该数据集中，学号也是一个分类变量，其取值集合就是该校所有学号的集合。而数值变量的取值一般是整数域或者实数域，当然，根据数据的实际情况，其取值区间会有具体变化。与分类变量相比，数值变量的最大特征就是支持数值操作，如加减法。因此，通过考虑能否对数据进行加减法操作，可以推断变量的类型。类似地，姓名也是分类变量。</p>
<p>在分类变量中，还可以进一步将分类变量分为顺序变量 （ordinal variable）和名称变量 （nominal variable）。对于顺序变量，可以将其取值按照一定顺序排列起来。例如，我们评价体检的结果是不良、一般、良好，这3个取值可以按照顺序</p>
<p>不良&lt;一般&lt;良好</p>
<p>排列起来。而对于名称变量，则不存在这样的顺序关系。在这个讨论的例子中，学生的性别为男或者女，而且这两种取值之间并不存在顺序关系，因此性别为名称变量。如果分类变量的取值只有两种结果，则该变量也称为二分变量<br>（binary variable）。</p>
<p>对于数值变量，首先变量本身是数值型的，其次能够对该变量进行一些常用的适用于数值变量的操作，如计算平均值和标准差等。在判定一个变量是数值变量还是分类变量时，一个简单的方法就是考虑能否在该变量上进行数值操作。在上面的例子中，学生年龄是数值变量，可以计算学生的平均年龄和年龄的标准差。而对于学号变量来说，虽然在这个例子中该变量都是整数型的，我们也可以计算学号的平均值，但是这个平均值没有任何意义。因此，我们断定学号变量是分类变量，而不是数值变量。</p>
<p>类似地，我们可以看出身高和体重变量都是数值变量。</p>
<p>在很多实际应用中，都有一组对象，而每个对象都可以用一组属性或者变量来表示。如果每个对象的属性一致，数据一般可以表示成矩阵的形式，称为数据矩阵 （data matrix）。在数据矩阵中，每行是一个数据点（或者一个对象），每列是一个变量、属性或特征。例如，在前面讨论的体检数据中，每一行对应一个学生的体检数据，每一列是体检中的一个变量。当然，我们也可以将每一列表示成一个对象，而每一行表示成一个属性。从矩阵操作上讲，只是将矩阵做了一个简单的转置。因此，在以后的讨论中，我们约定每行是一个对象，每列是一个属性。</p>
<p>在实际中，很多这类数据都是用文本文件来保存的。利用关系数据库也能很好地表示数据矩阵。常见的文本文件格式包括csv（comma separated values）和tsv（tab separated values）。在R中，我们可以使用<code>read.csv</code> 或者类似的<code>read.table</code> 函数来直接将数据导入到内存中。我们在下一节中将会详细地介绍如何在R中导入数据和进行基本的数据探索。</p>
<p>在实际应用中，还有各种各样的其他数据。例如，社交网络就以图 （graph）的形式保存。在本书中，我们主要讨论以数据矩阵保存的数据。</p>
<h2 id="4-2-数据探索"><a href="#4-2-数据探索" class="headerlink" title="4.2 数据探索"></a>4.2 数据探索</h2><p>在实际中，数据探索 （data exploration）是了解数据的一个很重要的步骤。我们经常说要根据数据的具体特征选择不同的方法，而数据探索就是了解数据具体特征的重要步骤。在数据探索中，我们主要计算数据的一些统计量，并通过图和表的形式进行总结。在4.4节，我们将详细讨论多种以图的形式表现数据主要特征的方法。本节主要介绍在数据探索中常用的统计量和R中的计算方法。</p>
<h3 id="4-2-1-常用统计量"><a href="#4-2-1-常用统计量" class="headerlink" title="4.2.1 常用统计量"></a>4.2.1 常用统计量</h3><p>一般来讲，得到数据后要首先检查数据质量 （data quality）。例如，每个变量的取值是否都是合乎数据定义的。通常，我们可以通过计算每个变量的一些统计量来检查数据是否存在问题。另一个问题是数据中通常存在缺失值<br>（missing value）。缺失值可能由很多原因引起，如在数据收集过程中，有些数据可能因难以获得而被标识为缺失值。因此，在进行数据探索时要计算每个变量是否存在缺失值，以及缺失值的比例等。</p>
<p>计算统计量是探索数据的一种非常直接且有效的方法。统计量包括两方面：</p>
<p>（1）单个变量的统计量，如数值变量的均值、极值，分类变量的所有不同取值等；</p>
<p>（2）变量之间的统计量，如每两个变量之间的相关系数。</p>
<p>此外，对于不同类型的变量，我们需要计算不同的统计量。在一般的数据分析中，对于分类变量，我们通常感兴趣的有：</p>
<ul>
<li>有多少个不同的取值；</li>
<li>每个取值的频率；</li>
<li>最常见的取值。</li>
</ul>
<p>对于数值变量，我们可以计算各种不同的统计量，包括</p>
<ul>
<li>均值；</li>
<li>方差和标准差；</li>
<li>中位数；</li>
<li>下四分位数；</li>
<li>上四分位数；</li>
<li>最小值；</li>
<li>最大值；</li>
<li>偏度；</li>
<li>数据的具体分布等。</li>
</ul>
<p>这些统计量在数学基础的相关章节中已经作了详细讨论。接下来，我们介绍如何在R中利用相关的函数来探索数据。</p>
<h3 id="4-2-2-使用R实际探索数据"><a href="#4-2-2-使用R实际探索数据" class="headerlink" title="4.2.2 使用R实际探索数据"></a>4.2.2 使用R实际探索数据</h3><p>下面我们利用具体的例子讨论如何在R中导入文本数据，并通过计算相关的统计量来探索数据。本节讨论的R程序都在文件R_data_load_example.R中。</p>
<p>我们前面讨论的数据一般都可以保存在文本文件中，其中应用最广泛的是csv文件 。在csv文件中，文件的每一行对应于数据矩阵的每一行，每列之间以逗号（,）分隔。csv文件也有一些变体，如tsv文件，其主要区别在于使用制表符Tab键（\t）来分隔各列。</p>
<p>R中提供了<code>read.csv</code> 函数和<code>read.table</code> 函数，可以直接将数据从csv文件导入为数据框。下面是一段示例R程序。这段程序从Data/iris.data文件导入数据，并进行了简单的探索。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data_file = &apos;Data/iris.data&apos;</span><br><span class="line">D = read.csv(data_file, header=F)</span><br><span class="line"># Get key statistics for this data frame</span><br><span class="line"># show some samples of the data frame D</span><br><span class="line">print(&apos;the first 10 rows of D is&apos;)</span><br><span class="line">print(head(D, 10))</span><br><span class="line">print(&apos;the last 10 rows of D&apos;)</span><br><span class="line">print(tail(D, 10))</span><br><span class="line"># Show the summary</span><br><span class="line">print(&apos;summary of D&apos;)</span><br><span class="line">print(summary(D))</span><br><span class="line"># Check the type of the 1st column</span><br><span class="line">c1 = class(D[,1])</span><br><span class="line">msg = paste0(&apos;the type of 1st column is &apos;, c1)</span><br><span class="line">print(msg)</span><br><span class="line">c5 = class(D[,5])</span><br><span class="line">msg = paste0(&apos;the type of the 5th column is &apos;, c5)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># An alternative function to load data</span><br><span class="line">D1 = read.table(data_file, header=F, sep=&apos;,&apos;)</span><br><span class="line">print(&apos;summary of D1&apos;)</span><br><span class="line">print(summary(D1))</span><br></pre></td></tr></table></figure>

</details>


<p>首先我们使用<code>read.csv</code> 导入文件。在使用该函数时，只需要指定文件名即可。注意，这里将<code>header</code> 设为<code>F</code> ，表示该csv文件的第一行就是数据，没有各列的列名信息。将数据读入并保存在数据框D中之后，使用<code>head</code> 函数来输出前10行，使用<code>tail</code> 函数来输出最后10行。在探索数据时，需要直接检查原始数据。虽然在很多时候由于数据规模太大而无法检查所有数据，但是直接检查一部分原始数据永远是必要的。</p>
<p>我们直接使用了R中的<code>summary</code> 函数来计算数据框中每列的统计量。对于数值变量，<code>summary</code> 函数计算如下统计量：</p>
<ul>
<li>最小值；</li>
<li>下四分位数；</li>
<li>中位数；</li>
<li>均值；</li>
<li>上四分位数；</li>
<li>最大值。</li>
</ul>
<p>对于分类变量，<code>summary</code> 函数值列出了所有不同的取值及每个取值出现的频率。</p>
<p>在上面这段程序中，还检查了这个数据框第1列和第5列的类型。在R中，可以使用<code>class</code> 函数来得到变量的类型。从输出可以看出，第1列是数值变量，而第5列是factor类型，表示是分类变量。</p>
<p>在R中，<code>read.csv</code> 专门用来导入csv文件。<code>read.table</code> 是更加通用的函数，使用它可以轻松地处理tsv和csv文件。在上面的程序中，我们也示范了如何使用<code>read.table</code> 函数来导入<code>csv</code> 函数。在使用时，我们通过<code>sep=&#39;,&#39;</code> 来指定分隔符是逗号。</p>
<p>下面是这段R程序的输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"> [1] &quot;the first 10 rows of D is&quot;</span><br><span class="line">　　V1　V2　V3　V4　　　　V5</span><br><span class="line">1　5.1 3.5 1.4 0.2 Iris-setosa</span><br><span class="line">2　4.9 3.0 1.4 0.2 Iris-setosa</span><br><span class="line">3　4.7 3.2 1.3 0.2 Iris-setosa</span><br><span class="line">4　4.6 3.1 1.5 0.2 Iris-setosa</span><br><span class="line">5　5.0 3.6 1.4 0.2 Iris-setosa</span><br><span class="line">6　5.4 3.9 1.7 0.4 Iris-setosa</span><br><span class="line">7　4.6 3.4 1.4 0.3 Iris-setosa</span><br><span class="line">8　5.0 3.4 1.5 0.2 Iris-setosa</span><br><span class="line">9　4.4 2.9 1.4 0.2 Iris-setosa</span><br><span class="line">10 4.9 3.1 1.5 0.1 Iris-setosa</span><br><span class="line">[1] &quot;the last 10 rows of D&quot;</span><br><span class="line">　　 V1　V2　V3　V4　　　　　 V5</span><br><span class="line">141 6.7 3.1 5.6 2.4 Iris-virginica</span><br><span class="line">142 6.9 3.1 5.1 2.3 Iris-virginica</span><br><span class="line">143 5.8 2.7 5.1 1.9 Iris-virginica</span><br><span class="line">144 6.8 3.2 5.9 2.3 Iris-virginica</span><br><span class="line">145 6.7 3.3 5.7 2.5 Iris-virginica</span><br><span class="line">146 6.7 3.0 5.2 2.3 Iris-virginica</span><br><span class="line">147 6.3 2.5 5.0 1.9 Iris-virginica</span><br><span class="line">148 6.5 3.0 5.2 2.0 Iris-virginica</span><br><span class="line">149 6.2 3.4 5.4 2.3 Iris-virginica</span><br><span class="line">150 5.9 3.0 5.1 1.8 Iris-virginica</span><br><span class="line">[1] &quot;summary of D&quot;</span><br><span class="line">　　　V1　　　　　　 V2　　　　　　 V3　　　　　　　V4　　　　　　　　　　 V5 </span><br><span class="line">Min.　 :4.300　Min.　 :2.000　Min.　 :1.000　 Min.　 :0.100　Iris-setosa　　:50</span><br><span class="line">1st Qu.:5.100　1st Qu.:2.800　1st Qu.:1.600　 1st Qu.:0.300　Iris-versicolor:50</span><br><span class="line">Median :5.800　Median :3.000　Median :4.350　 Median :1.300　Iris-virginica :50</span><br><span class="line">Mean　 :5.843　Mean　 :3.054　Mean　 :3.759　 Mean　 :1.199　　　　　</span><br><span class="line">3rd Qu.:6.400　3rd Qu.:3.300　3rd Qu. :5.100　 3rd Qu.:1.800　　　　　　</span><br><span class="line">Max.　 :7.900　Max.　 :4.400　Max.　 :6.900　 Max.　 :2.500　　　</span><br><span class="line">[1] &quot;the type of 1st column is numeric&quot;</span><br><span class="line">[1] &quot;the type of the 5th column is factor&quot;</span><br><span class="line">[1] &quot;summary of D1&quot;</span><br><span class="line">　　　V1　　　　　　 V2　　　　　　 V3　　　　　　　V4　　　　　　　　　　 V5 </span><br><span class="line">Min.　 :4.300　Min.　 :2.000　Min.　 :1.000　 Min.　 :0.100　Iris-setosa　　:50</span><br><span class="line">1st Qu.:5.100　1st Qu.:2.800　1st Qu.:1.600　 1st Qu.:0.300　Iris-versicolor:50</span><br><span class="line">Median :5.800　Median :3.000　Median :4.350　 Median :1.300　Iris-virginica :50</span><br><span class="line">Mean　 :5.843　Mean　 :3.054　Mean　 :3.759　 Mean　 :1.199　　　　　</span><br><span class="line">3rd Qu.:6.400　3rd Qu.:3.300　3rd Qu. :5.100　 3rd Qu.:1.800　　　　　　</span><br><span class="line">Max.　 :7.900　Max.　 :4.400　Max.　 :6.900　 Max.　 :2.500</span><br></pre></td></tr></table></figure>

</details>


<p>我们也可以直接从互联网上读取csv数据。R的一大优点是有非常多的第三方包，提供了丰富的功能和算法。在这里，我们可以使用<code>RCurl</code> 包从网络直接读取数据。在下面的示例程序中，首先检查<code>RCurl</code> 包有没有安装，如果没有安装，则使用<code>install.packages</code> 函数安装该包。安装完该包之后，使用<code>library</code> 函数载入该包。<code>f_URL</code> 指定了我们所要读取的文件的URL。我们首先使用<code>RCurl</code> 包中的<code>getURL</code> 函数来读取该文件，并保存在一个字符串<code>f_text</code> 中；然后使用<code>read.csv</code> 函数将字符串转换为数据框。注意，这次在使用<code>read.csv</code> 函数时，我们必须显式指定输入数据是<code>text</code> 类型。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Step 2. Load data from a URL directly</span><br><span class="line"># We need to install RCurl package first. </span><br><span class="line"># Before we install it, we need to check if the RCurl package is already installed.</span><br><span class="line">RCurl.installed &lt;-&apos;RCurl&apos; %in% rownames(installed.packages())</span><br><span class="line">if (RCurl.installed) &#123;</span><br><span class="line">　print(&quot;the RCurl package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the RCurl package first...&quot;)</span><br><span class="line">　install.packages(&apos;RCurl&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(&apos;RCurl&apos;)</span><br><span class="line"># Use RCurl to read the file specified by the URL f_URL</span><br><span class="line">f_URL = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;</span><br><span class="line"># f_text is the string we download for this URL</span><br><span class="line">f_text = getURL(f_URL)</span><br><span class="line">D2 = read.csv(text=f_text, header=F)</span><br><span class="line">print(&apos;summary of D2&apos;)</span><br><span class="line">print(summary(D2))</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;the RCurl package is already installed, let&apos;s load it...&quot;</span><br><span class="line">[1] &quot;summary of D2&quot;</span><br><span class="line">　　　V1　　　　　　 V2　　　　　　 V3　　　　　　　V4　　　　　　　　　　 V5 </span><br><span class="line">Min.　 :4.300　Min.　 :2.000　Min.　 :1.000　 Min.　 :0.100　Iris-setosa　　:50</span><br><span class="line">1st Qu.:5.100　1st Qu.:2.800　1st Qu.:1.600　 1st Qu.:0.300　Iris-versicolor:50</span><br><span class="line">Median :5.800　Median :3.000　Median :4.350　 Median :1.300　Iris-virginica :50</span><br><span class="line">Mean　 :5.843　Mean　 :3.054　Mean　 :3.759　 Mean　 :1.199　　　　　</span><br><span class="line">3rd Qu.:6.400　3rd Qu.:3.300　3rd Qu. :5.100　 3rd Qu.:1.800　　　　　　</span><br><span class="line">Max.　 :7.900　Max.　 :4.400　Max.　 :6.900　 Max.　 :2.500</span><br></pre></td></tr></table></figure>

</details>


<p>R中也自带了一些数据。对于这些自带的数据，可以使用<code>data</code> 函数直接载入。使用<code>data()</code> 可以列出R中所有的自带数据。指定数据集名，则可载入相应的数据。在下面的程序中，首先使用<code>data()</code> 列出所有的自带数据，然后使用<code>data(&#39;mtcars&#39;)</code> 载入<code>mtcars</code> 数据，最后使用<code>summary</code> 函数计算这个数据集的主要统计量。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Step 3. Load dataset provided by R</span><br><span class="line"># list all data sets avaiable in R</span><br><span class="line">data()</span><br><span class="line"># load the mtcars dataset</span><br><span class="line">data(&apos;mtcars&apos;)</span><br><span class="line">print(&apos;summary of mtcars&apos;)</span><br><span class="line">print(summary(mtcars))</span><br></pre></td></tr></table></figure>

</details>


<p>下面是关于<code>mtcars</code> 数据集的输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;summary of mtcars&quot;</span><br><span class="line">    　　　mpg　　　　　　　 cyl　　　　　　 disp　　　　　　　 hp　　　　　　drat　　　　　　　wt </span><br><span class="line">Min.　  :10.40　 Min.　  :4.000　 Min.　  : 71.1　 Min.　 : 52.0　 Min.　  :2.760　 Min.　 :1.513 </span><br><span class="line">1st Qu. :15.43　 1st Qu. :4.000　 1st Qu. :120.8　 1st Qu. : 96.5　 1st Qu. :3.080　 1st Qu. :2.581 </span><br><span class="line">Median  :19.20　 Median  :6.000　 Median :196.3　 Median :123.0　 Median :3.695　 Median :3.325</span><br><span class="line">Mean　  :20.09　 Mean　  :6.188　 Mean　 :230.7　　Mean　 :146.7　 Mean　 :3.597　 Mean　 :3.217　</span><br><span class="line">3rd Qu. :22.80　3rd Qu.  :8.000　 3rd Qu. :326.0　 3rd Qu.:180.0　 3rd Qu. :3.920　 3rd Qu. :3.610</span><br><span class="line">Max.　  :33.90　 Max.　  :8.000　 Max.　  :472.0　 Max.　 :335.0　 Max.　 :4.930　 Max.　 :5.424</span><br><span class="line">    　　　qsec　　　　　　　　vs　　　　　　　　　am　　　　　　　　gear　　　　　　 carb　　　</span><br><span class="line">Min. :14.50　 Min.　 :0.0000　 Min.　　:0.0000　 Min.　 :3.000　 Min.  :1.000　</span><br><span class="line">1st Qu. :16.89　 1st Qu. :0.0000　 1st Qu.:0.0000　 1st Qu. :3.000　 1st Qu.:2.000　</span><br><span class="line">Median  :17.71　 Median :0.0000　 Median :0.0000　 Median :4.000　 Median :2.000　</span><br><span class="line">Mean　  :17.85　 Mean　 :0.4375　 Mean　　:0.4062　 Mean　 :3.688　 Mean  :2.812　</span><br><span class="line">3rd Qu. :18.90　 3rd Qu. :1.0000　 3rd Qu.:1.0000　 3rd Qu. :4.000　 3rd Qu.:4.000　</span><br><span class="line">Max.　  :22.90　 Max.　 :1.0000　 Max.　　:1.0000　 Max.　 :5.000　 Max.  :8.000</span><br></pre></td></tr></table></figure>

</details>


<p>虽然<code>summary</code> 函数可以一次计算多个统计量，但是在一些场合我们只需要计算某一个特定统计量。表4-2列出了常用的统计函数及它们对应的功能。</p>
<p>表4-2 R中常用的统计函数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>函 数 名</th>
<th>描 述  </th>
</tr>
</thead>
<tbody>
<tr>
<td> <code>min</code></td>
<td>计算最小值  </td>
</tr>
<tr>
<td> <code>max</code></td>
<td>计算最大值  </td>
</tr>
<tr>
<td> <code>mean</code></td>
<td>计算均值  </td>
</tr>
<tr>
<td> <code>median</code></td>
<td>计算中位数  </td>
</tr>
<tr>
<td> <code>quantile</code></td>
<td>计算四分位数  </td>
</tr>
<tr>
<td> <code>skewness</code></td>
<td>计算偏度  </td>
</tr>
<tr>
<td> <code>table</code></td>
<td>计算每种取值（单变量时）或每种取值组合（多变量时）出现的频率  </td>
</tr>
</tbody>
</table>
</div>
<p>在表4-2中，除<code>skewness</code> 函数外，其他函数在基本的R中直接提供了，因此不需要安装第三方包。为了使用<code>skewness</code> 函数计算偏度，我们需要载入<code>moments</code> 包。</p>
<p>下面的程序将会示范如何使用这些函数计算第1列数据对应的统计量。此外，我们还会利用第5列数据示范如何使用<code>table</code> 函数。与前面安装<code>RCurl</code> 包类似，这里将安装<code>moments</code> 包以使用<code>skewness</code> 函数。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># Step 4. Instead of using the summary function, we compute these statistics</span><br><span class="line"># using separate functions</span><br><span class="line">m1 = min(D[,1])</span><br><span class="line">msg = paste0(&apos;min = &apos;, m1)</span><br><span class="line">print(msg)</span><br><span class="line">m3 = max(D[,1])</span><br><span class="line">msg = paste0(&apos;max = &apos;, m3)</span><br><span class="line">print(msg)</span><br><span class="line">m2 = mean(D[,1])</span><br><span class="line">msg = paste0(&apos;mean = &apos;, m2)</span><br><span class="line">print(msg)</span><br><span class="line">md = median(D[,1])</span><br><span class="line">msg = paste0(&apos;median = &apos;, md)</span><br><span class="line">print(msg)</span><br><span class="line">q1 = quantile(D[,1], 0.25)</span><br><span class="line">msg = paste0(&apos;25% quantile = &apos;, q1)</span><br><span class="line">print(msg)</span><br><span class="line">q2 = quantile(D[,1], 0.5)</span><br><span class="line">msg = paste0(&apos;50% quantile = &apos;, q2)</span><br><span class="line">print(msg)</span><br><span class="line">q3 = quantile(D[,1], 0.75)</span><br><span class="line">msg = paste0(&apos;75% quantile = &apos;, q3)</span><br><span class="line">print(msg)</span><br><span class="line"># Check the skewness of a single column in R</span><br><span class="line"># We need to install moments package first. </span><br><span class="line"># Before we install it, we need to check if the moments package is already installed.</span><br><span class="line">moments.installed &lt;-&apos;moments&apos; %in% rownames(installed.packages())</span><br><span class="line">if (moments.installed) &#123;</span><br><span class="line">　print(&quot;the moments package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the moments package first...&quot;)</span><br><span class="line">　install.packages(&apos;moments&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(&apos;moments&apos;)</span><br><span class="line">s1 = skewness((D[,1]))</span><br><span class="line">msg = paste0(&apos;the skewness of 1st column is &apos;, s1)</span><br><span class="line">print(s1)</span><br><span class="line">print(&apos;the distribution of the 5th column&apos;)</span><br><span class="line">t5 = table(D[,5])</span><br><span class="line">print(t5)</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;min = 4.3&quot;</span><br><span class="line">[1] &quot;max = 7.9&quot;</span><br><span class="line">[1] &quot;mean = 5.84333333333333&quot;</span><br><span class="line">[1] &quot;median = 5.8&quot;</span><br><span class="line">[1] &quot;25% quantile = 5.1&quot;</span><br><span class="line">[1] &quot;50% quantile = 5.8&quot;</span><br><span class="line">[1] &quot;75% quantile = 6.4&quot;</span><br><span class="line">[1] &quot;the moments package is already installed, let&apos;s load it...&quot;</span><br><span class="line">[1] 0.3117531</span><br><span class="line">[1] &quot;the distribution of the 5th column&quot;</span><br><span class="line"></span><br><span class="line">　　Iris-setosa Iris-versicolor　Iris-virginica </span><br><span class="line">　　　　　　 50　　　　　　　50　　　　　　　50</span><br></pre></td></tr></table></figure>

</details>


<h2 id="4-3-数据预处理"><a href="#4-3-数据预处理" class="headerlink" title="4.3 数据预处理"></a>4.3 数据预处理</h2><p>在实际建模中，数据预处理 （data preprocessing）是非常关键的一步，直接影响最终模型结果的好坏。在大多数情况下，原始数据都不宜直接用来建模，需要对数据进行多方面的预处理之后才能进入建模环节。一般而言，数据的预处理包括：</p>
<p>（1）删除部分数据，如可以直接删除冗余或者无关数据；</p>
<p>（2）增加新的数据，从已有数据中，可以构造新的特征；</p>
<p>（3）数据的变换，原有的数据不适合直接建模，需要作一些变换以便模型可以更好地直接利用。</p>
<p>当然，数据的预处理和具体使用哪种模型建模直接相关。好的数据预处理能够显著提高模型的性能，坏的数据预处理能够完全毁掉模型的性能。此外，不同的模型对于数据的要求可能完全不一样。基于树的一些模型对于数据不是特别敏感，但另外一些模型，如线性回归，对于数据则较为敏感。</p>
<p>由于数据的预处理与实际原始数据的特征和所要解决的问题直接相关，因此，在本节中，我们着重讨论一些一般性的技巧。</p>
<h3 id="4-3-1-缺失值的处理"><a href="#4-3-1-缺失值的处理" class="headerlink" title="4.3.1 缺失值的处理"></a>4.3.1 缺失值的处理</h3><p>在实际中，缺失值是很普遍的。有些缺失值可能是因为搜集数据时有遗漏或者无法搜集，有些缺失数据可能是暂时的（当时无法搜集但是过段时间后可能能得到）。在我们后面讨论的算法中，有些算法能够直接处理缺失值，如基于决策树的算法；而有些算法不能直接处理缺失值，如线性回归。</p>
<p>在实际中，对于缺失数据处理的第一步是：明确缺失数据的重要性。如果缺失变量对于目标值的预测不重要，那么可以直接删除该变量；如果缺失变量很重要，无法直接丢弃，那么通常用如下两种方法来处理。第一种方法是采用能够直接处理缺失数据的模型来进行建模，如基于决策树的模型等。如果我们要使用那些不能直接处理缺失数据的算法进行建模，同时也不能直接丢弃缺失变量，那么可以采用第二种方法——进行缺失值填充。</p>
<p>每一个缺失值的填充可以视为一个单独的预测问题：我们可以利用其他没有缺失的变量来估计缺失的变量。这相当于为了解决原始的预测问题，我们引入了一个新的预测问题。这种方法显著地增加了问题的复杂度。事实上，我们最终关心的并不是缺失值的估计是否准确，而是原始问题的预测是否准确。因此，在解决缺失值填充问题时，我们更加倾向于使用简单的方法来估计缺失值。这样做一方面能够降低缺失值估计的计算复杂度，另一方面还能够避免过拟合。</p>
<p>缺失值填充有如下常用方法。</p>
<p>（1）使用平均值或者中位数来进行填充。</p>
<p>（2）使用 _k_ 近邻方法进行填充。具体来说，假设样本 _<strong>x</strong> i _ 的第 _j_ 个变量缺失（记为 _x ij _ ），我们的目标就是要估计 _x ij _ 。首先利用 _<strong>x</strong> i _ 中没有缺失的变量，找到最相似的 _k_ 个样本，并用这 _k_ 个样本的第 _j_ 个变量的平均值作为 _x ij _ 的估计值。在该方法中，主要的控制参数是 _k_ ，即邻域的大小。在实践中人们发现，缺失值填充算法的性能常常对 _k_ 不敏感。</p>
<h3 id="4-3-2-数据的标准化"><a href="#4-3-2-数据的标准化" class="headerlink" title="4.3.2 数据的标准化"></a>4.3.2 数据的标准化</h3><p>对于数值变量，每个变量都有自己的单位。在处理数据时我们需要同时考虑多个变量。例如，我们要计算两个向量<img src="Image00447.gif" alt> 、<img src="Image00448.gif" alt> 的欧几里得距离：</p>
<p><img src="Image00449.gif" alt></p>
<p>（4-2）</p>
<p>如果第一个变量的绝对差比第二个变量的绝对差大很多，那么欧几里得距离<img src="Image00450.gif" alt> 就会被第一个变量所左右，而第二个变量被忽视。</p>
<p>为了解决这个问题，通常可以先进行数据标准化。对于每个变量 _j_ ，我们可以计算其对应的均值<img src="Image00451.gif" alt> 和标准差<img src="Image00452.gif" alt> ，然后进行如下变换：</p>
<p><img src="Image00453.gif" alt></p>
<p>（4-3）</p>
<p>经过数据标准化后，所有的变量对应的均值都是0，标准差都是1。经过标准化处理之后的<img src="Image00454.gif" alt> 也称为Z分值<br>（Z-score），因此，该变换也称为Z分值标准化 （Z-score normalization）。</p>
<p>注意，均值和标准差的计算都受到离群数据的影响。如果某个变量对应的离群数据较多，那么可以将上面变换公式中的均值<img src="Image00451.gif" alt> 和标准差<img src="Image00452.gif" alt> 替换为受离群数据影响较小的统计量。例如，均值<img src="Image00451.gif" alt> 可以直接替换为中位数<br>（median），标准差<img src="Image00452.gif" alt> 可以替换为绝对平均偏差 （absolute average deviation，AAD）：</p>
<p><img src="Image00455.gif" alt></p>
<p>（4-4）</p>
<p>这里我们先不讨论离群数据，本章稍后将予以详细介绍。</p>
<p>在3.2.1节中我们已经介绍过绝对平均值。第3章介绍的其他关于数据分布的集中位置和分散程度的统计量都可以在数据标准化中使用。</p>
<p>在R中，我们可以利用<code>scale</code> 函数来完成对矩阵类数据的标准化。假设我们利用<code>scale</code> 函数来对R中自带的<code>iris</code> 数据集进行处理。该数据集含有150个样本和5个变量，其中前4个变量是数值变量，最后一个变量是每个样本的类别信息。在下面的R程序中，我们首先使用<code>data</code> 函数来导入数据并只考虑前4个数值变量，然后利用数据的前100个样本计算每个变量的均值<img src="Image00456.gif" alt> 和标准差<img src="Image00452.gif" alt> 并标准化，之后再使用计算出的<img src="Image00451.gif" alt> 和<img src="Image00452.gif" alt> 对后面的50个样本进行同样的标准化：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data(iris)</span><br><span class="line">D &lt;-iris[,1:4]</span><br><span class="line">D1 &lt;-D[1:100,]</span><br><span class="line">D2 &lt;-D[101:150,]</span><br><span class="line">D1_normalized &lt;-scale(D1, center = T, scale = T)</span><br><span class="line">D2_normalized &lt;-scale(D2, </span><br><span class="line">　　　　　　　　　　　 center = attr(D1_normalized, &apos;scaled:center&apos;),</span><br><span class="line">　　　　　　　　　　　 scale = attr(D1_normalized, &apos;scaled:scale&apos;))</span><br></pre></td></tr></table></figure>

</details>


<p>在使用R中的<code>scale</code> 函数时，有两个重要参数<code>center</code> 和<code>scale</code> 。当<code>center</code> 和<code>scale</code> 是逻辑型值时，分别表示在进行数据标准化时是否减去均值和除以标准差。当<code>center</code> 和<code>scale</code> 是数值型的向量时，表示在进行标准化时不需要计算数据的均值和标准差，直接调用<code>center</code> 和<code>scale</code> 所赋予的值即可。在上面的例子中，前一个标准化过程将<code>center</code> 和<code>scale</code> 设为逻辑型值，后一个标准化过程使用前面数据的均值和标准差进行数据的标准化。这里的<code>attr（D1_normalized, &#39;scaled:center&#39;）</code> 和<code>attr（D1_normalized, &#39;scaled:scale&#39;）</code> 都是向量，在R中可以直接打印出来：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; attr(D1_normalized, &apos;scaled:center&apos;)</span><br><span class="line">Sepal.Length　Sepal.Width Petal.Length　Petal.Width </span><br><span class="line">　　　 5.471　　　　3.099　　　　2.861　　　　0.786 </span><br><span class="line">&gt; attr(D1_normalized, &apos;scaled:scale&apos;)</span><br><span class="line">Sepal.Length　Sepal.Width Petal.Length　Petal.Width </span><br><span class="line">　 0.6416983　　0.4787389　　1.4495485　　0.5651531</span><br></pre></td></tr></table></figure>

</details>


<p>我们也可以使用<code>apply</code> 函数来手动验证<code>D1_normalized</code> 中的每列都是均值为0、标准差为1。具体代码和结果如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; apply(D1_normalized, 2, sum)</span><br><span class="line"> Sepal.Length　 Sepal.Width　Petal.Length　 Petal.Width </span><br><span class="line">-1.497413e-14 -4.233159e-14 -1.454392e-14 -1.032507e-14 </span><br><span class="line">&gt; apply(D1_normalized, 2, sd)</span><br><span class="line">Sepal.Length　Sepal.Width Petal.Length　Petal.Width </span><br><span class="line">　　　　　 1　　　　　　1　　　　　　1　　　　　　1</span><br></pre></td></tr></table></figure>

</details>


<h3 id="4-3-3-删除已有变量"><a href="#4-3-3-删除已有变量" class="headerlink" title="4.3.3 删除已有变量"></a>4.3.3 删除已有变量</h3><p>在很多实际数据中，经常存在冗余甚至重复数据。在这种情况下，我们需要删除一些冗余和重复的变量。一方面，删除这些变量之后能够降低数据的规模，进而降低算法的计算时间；另一方面，有些算法在存在冗余数据时会导致性能降低。以线性回归为例，在极端的情况下，如有两个变量<img src="Image00457.gif" alt> ，则必须使用正则化项，否则性能会严重降低。关于线性回归的具体讨论参见第5章。</p>
<p>为了删除冗余变量，我们可以采用主成分分析 （principal component analysis，PCA）来进行降维。主成分分析的原理和使用参见3.3.5节和3.3.6节。</p>
<p>使用主成分分析的缺点是新变量是原来变量的线性组合，这样一般难以解释新变量。因此，我们一般采用一些启发式方法 （heuristic methods）来删除那些冗余甚至重复的变量。首先，我们可以计算变量两两之间的相关系数。若相关系数接近1或者−1，则说明对应的这两个变量之间存在线性相关性，我们需要删除其中一个变量。</p>
<p>在实际操作中，为了消除变量之间的线性相关性，我们可以要求任何两个变量之间的相关系数的绝对值低于一个阈值（如0.75）。虽然这种方法只考虑了两两之间的相关系数而忽视了多个变量之间的相互关系，但是在很多情况下这种简单的处理方法也能取得较好的效果。这里我们简单介绍一种启发式方法来控制变量两两之间相关系数的最大值，如算法4-1所示。</p>
<p>算法4-1 通过相关系数删除冗余数据</p>
<blockquote>
<p>（1）计算变量两两之间的相关系数，得到一个 _d_ × _d_ 的矩阵。若该矩阵中所有元素的绝对值都小于规定的阈值 _τ_ ，则退出。 &gt; &gt; （2）从该矩阵中选出相关系数绝对值最大的两个变量(记为 _v_ 1 和 _v_ 2 )。 &gt; &gt; （3）计算变量<img src="Image00351.gif" alt> 和所有其他变量的相关系数的绝对值平均值，记为<img src="Image00458.gif" alt> ；同样，为变量<img src="Image00352.gif" alt> 计算对应的<img src="Image00459.gif" alt> 。 &gt; &gt; （4）如果<img src="Image00460.gif" alt> ，则删除变量<img src="Image00351.gif" alt> ；否则删除变量<img src="Image00352.gif" alt> 。 &gt; &gt; （5）重复步骤（2）～（4），直到剩余变量两两之间相关系数的绝对值都小于规定的阈值。</p>
</blockquote>
<p>此外，我们需要着重指出：如果一个变量在数据集中只有一个值，则意味着该变量没有为模型的建立提供任何有用的信息。一般而言，可以直接删除该变量。</p>
<h3 id="4-3-4-数据的变换"><a href="#4-3-4-数据的变换" class="headerlink" title="4.3.4 数据的变换"></a>4.3.4 数据的变换</h3><p>在很多实际问题中，直接使用原始数据进行建模很难达到令人满意的性能。很多时候，我们需要对数据进行变换，生成若干新的特征。例如，我们可以计算两个变量的比值作为一个新的变量。假设我们的任务是要根据一组数据判定每种材料是塑料还是金属。在原始数据中，我们给定了<img src="Image00351.gif" alt> 是质量、<img src="Image00352.gif" alt> 是体积，则计算比值<img src="Image00461.gif" alt> 是每种材料对应的密度。通过增加新的变量，该分类问题可以得到很好的解决。</p>
<p>在很多与时间相关的问题中，我们也可以从原始数据中计算很多关于时间的新特征。假设我们要预测淘宝上某个商家在11月每天的销售额。我们知道，11月11日一般是淘宝集中促销的日子，消费者在11月的消费模式与11月11日有很大关系：在11月11日之前，有很多的消费者会持币待购，而在11月11日之后则可能会敞开购买。因此，我们可以构建如下变量。</p>
<p>（1）当前日期是否在11月11日之后，若是，则为1，否则为0。</p>
<p>（2）计算当前日期与11月11日所隔天数的绝对值。例如，当前日期是11月9日，则为2；当前日期为11月13日，也为2。</p>
<p>同时我们也要注意到，当前日期是否为周末对于销售量也有较大影响，因此，我们还可以计算以下变量：</p>
<p>（3）检查当前日期是否为周末，若是，则为1，否则为0。</p>
<p>从上面的这两个例子可以看出，如何建立合理的新变量，与所要解决的问题息息相关。此外，不同的模型对于特征的要求也不一样。</p>
<p>下面我们给出一些常用的数据变换函数：</p>
<ul>
<li><img src="Image00462.gif" alt> ，包括<img src="Image00463.gif" alt> 、<img src="Image00464.gif" alt> 等；</li>
<li><img src="Image00465.gif" alt></li>
<li><img src="Image00466.gif" alt></li>
<li><img src="Image00467.gif" alt></li>
<li><img src="Image00468.gif" alt> 和<img src="Image00469.gif" alt></li>
<li><img src="Image00464.gif" alt></li>
</ul>
<p>上面列出的变换函数都是适用于单个原始变量的变化函数。相应地，我们也可以同时对多个原始变量进行变换以得到新的变量，如<img src="Image00470.gif" alt> 等。</p>
<h3 id="4-3-5-构建新的变量：哑变量"><a href="#4-3-5-构建新的变量：哑变量" class="headerlink" title="4.3.5 构建新的变量：哑变量"></a>4.3.5 构建新的变量：哑变量</h3><p>在建模中，不是所有的模型或者算法都能够直接处理分类变量 （categorical variable）。有些模型，如基于决策树的模型，能够较好地处理分类变量。但另外一些模型，如线性回归和逻辑回归，不能直接处理分类变量。在这种情况下，一种通用的方法是将分类变量转化为多个哑变量<br>（dummy variable），从而能够使用那些模型。</p>
<p>哑变量的取值只能为0或者1。在前面的数据集中，学生的性别只能为男或者女，因此，我们可以将分类变量“性别”转化为哑变量“性别=男”。因此，前面的数据可以转化为如表4-3所示的形式。</p>
<p>表4-3 哑变量构建实例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>学号</th>
<th>姓名</th>
<th>性别=男</th>
<th>年龄</th>
<th>体重</th>
<th>身高  </th>
</tr>
</thead>
<tbody>
<tr>
<td>201503345</td>
<td>李乐</td>
<td>1</td>
<td>21</td>
<td>65</td>
<td>175  </td>
</tr>
</tbody>
</table>
</div>
<p>这样，我们在考虑“性别=男”、年龄、体重、身高这4个特征时，可以直接使用线性回归或者逻辑回归等模型。</p>
<p>注意，在这个例子中，我们也可以构建“性别=女”作为新的变量，但没有必要同时构建“性别=男”和“性别=女”这两个哑变量。因为如果“性别=男”为1的话，“性别=女”肯定为0，所以没有必要引入冗余的哑变量。另外，如果在构建哑变量的时候引入冗余信息，在有些模型如线性回归中会导致计算方面的问题，具体的讨论参见本书第5章。</p>
<p>下面再给出一个有关季节变量的实际例子。季节变量<img src="Image00471.gif" alt> 的取值有4个（春、夏、秋、冬），则可引入3个哑变量 _v_ 1 、 _v_ 2 、 _v_ 3 。其中<img src="Image00351.gif" alt> 表示是否为春季，<img src="Image00352.gif" alt> 表示是否为夏季，<img src="Image00472.gif" alt> 表示是否为秋季。如果<img src="Image00471.gif" alt> 是冬季，那么<img src="Image00473.gif" alt> 。表4-4显示了如何将季节变量<img src="Image00471.gif" alt> 转化为3个哑变量 _v_ 1 、 _v_ 2 、 _v_ 3 。</p>
<p>表4-4 季节变量转化为哑变量</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>_v_ 0 </strong></th>
<th><strong>_v_ 1 </strong></th>
<th><strong>_v_ 2 </strong></th>
<th><strong>_v_ 3 </strong>  </th>
</tr>
</thead>
<tbody>
<tr>
<td>春</td>
<td>1</td>
<td>0</td>
<td>0  </td>
</tr>
<tr>
<td>夏</td>
<td>0</td>
<td>1</td>
<td>0  </td>
</tr>
<tr>
<td>秋</td>
<td>0</td>
<td>0</td>
<td>1  </td>
</tr>
<tr>
<td>冬</td>
<td>0</td>
<td>0</td>
<td>0  </td>
</tr>
</tbody>
</table>
</div>
<p>一般来说，如果一个分类变量<img src="Image00123.gif" alt> 有 _k_ 种不同的取值，我们可以建立 _k_<br>-1个新的哑变量来代替。假设我们把<img src="Image00474.gif" alt> 的 _k_ 种不同取值记为<img src="Image00475.gif" alt> ，则可以将每个哑变量定义为“<img src="Image00123.gif" alt> 的取值为<img src="Image00476.gif" alt><br>”，这里<img src="Image00477.gif" alt> 。</p>
<p>如果一个分类变量的不同取值太多，则需要做进一步的处理。如果直接转化为哑变量的话，会生成大量的哑变量，而且大部分哑变量的值为0。事实上，对那些能够直接处理分类变量的模型（如决策树）来说，一个分类变量如果有过多的不同取值，也会影响这个变量在模型中的使用。</p>
<p>在这种情况下，一种方法是将那些取值太多的分类变量进行简化，以减少可能的取值数目。例如，如果该分类变量是顺序变量，那么可以将相邻的几个不同值归约到同一个值。这里我们用一个简单的例子加以说明。在前面的学生体检例子中，假设对于某项指标有负责检查的医生进行打分，按照从差到好的程度给出A、B、C、D、E和F。为了缩小该变量的取值范围，可以按照如下规则进行归约：</p>
<p>_A,B_ <img src="Image00478.gif" alt> 差, _C,D_ <img src="Image00478.gif" alt> 中， _E,F_ <img src="Image00478.gif" alt> 好</p>
<p>这样，这个变量的取值范围从6个值缩小到3个值，从而方便后续处理。</p>
<p>如果分类变量不是顺序变量，则需要其他的方法来缩小该变量的取值范围。通常情况下需要进一步分析该变量的具体含义及所要解决的问题而决定。例如，学生有家庭地址一栏。事实上，基本上每个学生的家庭地址都不一样。那么，“家庭地址”这个分类变量的取值就有很多种，很难在模型中直接应用。实际上，对于家庭地址这个变量，我们可以根据问题和数据的具体特点规约到不同的值。例如，如果体检的对象是小学生，由于小学生都住得比较接近，我们可以将家庭地址归约到街道名称这一级。换言之，我们由“家庭地址”构造新的变量“家庭地址街道名称”。如果体检的对象是大学生，一般而言，大学里面的学生来自于全国各地，我们将家庭地址规约到家庭地址所对应的省或者市即可。具体是省还是市，则需要根据所要解决的问题来进一步决定。</p>
<h3 id="4-3-6-离群数据的处理"><a href="#4-3-6-离群数据的处理" class="headerlink" title="4.3.6 离群数据的处理"></a>4.3.6 离群数据的处理</h3><p>在实际数据中，我们经常会碰到离群数据<br>（outlier）。通常，我们将那些离主流数据很远的数据点定义为离群数据。在合适的假设下，我们可以用严格的统计语言来描述离群数据。但在实际中，很难对离群数据给出严格的定义。更多的时候，我们可以通过图/表等工具来探索数据，并识别出离群数据。同时，在识别离群数据时要非常小心，要仔细辨别离群数据是否来自于错误的观测数据等。</p>
<p>在建模时，有些模型对于离群数据是较为健壮的，如基于决策树的模型。因为在基于决策树的模型中，通常是通过类似于如下的规则来进行分类或者回归的：</p>
<p>（1）变量 _j_ 的值是否大于或者小于某个阈值（对于数值变量）；</p>
<p>（2）变量 _j_ 的值是否等于某个值（对于分类变量）。</p>
<p>换言之，在基于决策树的模型中，我们根本没有考虑对应的变量 _j_ 的值与主流值的差距，而只考虑它在哪一边。因此，基于决策树的模型对于离群数据的健壮性非常强。</p>
<p>但是，如果所采用的模型对于离群数据是较为敏感的，那么我们需要对数据进行一些预处理以消除离群数据对于模型的影响。一种常用的方法是对数据分组<br>（binning），来对变量中的离群数据进行处理。具体而言，可将所有样本变量的取值从小到大排列好，然后分为若干组。这样，对于离群数据，我们可以使用对应组中数据的均值或者中位数等来对它进行修正。在实际中，常用的分组方法有以下两种。</p>
<ul>
<li>等距分组 （equal-interval binning）：将整个数据分布区间分成若干个等长的子区间。</li>
<li>等频分组 （equal-frequency binning）：在划分过程中使得每个区间的样本数一样。</li>
</ul>
<h2 id="4-4-数据可视化"><a href="#4-4-数据可视化" class="headerlink" title="4.4 数据可视化"></a>4.4 数据可视化</h2><p>数据可视化 （data visualization）是研究数据具体分布的有效方法。通过数据可视化，可以获得对数据的直观认识，为下一步的处理打下较好的基础。本节介绍如下方法：</p>
<ul>
<li>直方图；</li>
<li>柱状图；</li>
<li>茎叶图；</li>
<li>箱线图；</li>
<li>散点图。</li>
</ul>
<p>利用这些工具，可以生动直观地了解数据的具体分布，为数据的后续处理和建模工作提供很大的便利。在本节中，除了介绍这些工具的基本原理之外，还介绍R中对应的函数，这样读者可以在实际中方便地作图以分析、探索数据。</p>
<p>在本章中我们主要介绍R中提供的基本的画图函数。在R中有一个非常流行的包叫<code>ggplot2</code> 。本节讨论的所有数据探索方法都可以同时利用R中的基本画图函数和<code>ggplot2</code> [23] 来实现。简单起见，我们主要介绍R中的基本函数。本节所有的程序都在文件R_data_visualization.R中。</p>
<h3 id="4-4-1-直方图"><a href="#4-4-1-直方图" class="headerlink" title="4.4.1 直方图"></a>4.4.1 直方图</h3><p>直方图<br>（histogram）是显示样本分布的有效方法。在直方图中，我们将数据的取值区间划分为若干个子区间，并计算整个样本集合中落入每个子区间的样本数。在作图时，为每个子区间画一个矩形，它的高度与落入子区间的样本数相对应。</p>
<p>在画直方图时，子区间的数目对直方图的最终形态有很大影响。子区间数目太少，直方图所反映的概率密度的形态就不是很准确；子区间数目太多，由于随机性的影响，相邻子区间的数目分布可以差异很大。因此，在画直方图时，我们需要选择合适的子区间数目，使得直方图能够较为真实地反映总体的概率分布。若所划分的子区间间隔相等，则每个子区间的长度称为组距。</p>
<p>在R中，我们可以使用函数<code>hist</code> 来画直方图。在<code>hist</code> 函数中，我们可以使用<code>breaks</code> 参数来指定子区间的数目。在本节给出的关于R的例子中，我们使用<code>iris</code> 数据和<code>mtcars</code> 数据来讲述如何在R中快速地探索数据。由于这两组数据是R中自带的，因此我们可以使用<code>data</code> 函数来直接载入它们：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  # Step 1. Load the data</span><br><span class="line">data(&quot;iris&quot;)</span><br><span class="line">data(&quot;mtcars&quot;)</span><br></pre></td></tr></table></figure>
<p>之后，我们可以直接使用两个数据框：<code>iris</code> 和<code>mtcars</code> 。在下面的代码中，我们使用<code>hist</code> 函数画出了多个直方图。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Step 2. Plot the histogram using mtcars$mpg</span><br><span class="line"># Plot a basic histogram</span><br><span class="line">hist(mtcars$mpg)</span><br><span class="line"># Plot a histogram with 10 bins</span><br><span class="line"># Specify approximate number of bins with breaks</span><br><span class="line">hist(mtcars$mpg, breaks=10)</span><br><span class="line"># Plot a red histogram with 12 bins</span><br><span class="line">hist(mtcars$mpg, breaks=12, col=&quot;red&quot;)</span><br><span class="line"># Add labels</span><br><span class="line">hist(mtcars$mpg, breaks=12, col=&quot;red&quot;, xlab = &quot;Miles Per Gallon&quot;, main =   </span><br><span class="line">&quot;Histogram of mpg&quot;)</span><br></pre></td></tr></table></figure>

</details>


<p>在第一个例子中，我们指定数据<code>mtcars$mpg</code> ，并使用hist函数中的默认参数来画直方图，结果如图4-1所示。在第二个例子中，我们虽然仍使用<code>mtcars$mpg</code> 数据，但是通过设置<code>breaks</code> 参数指定子区间的数目为10，其结果如图4-2所示。注意，这里指定的子区间的数目是一个大概数目，<code>hist</code> 函数会根据数据的具体特征进行适当的调整，在图4-2中我们就得到了12个子区间。在第三个例子中，我们将子区间的数目指定为12，并将直方图的颜色设置为红色，结果如图4-3所示。在最后一个例子中，我们在第三个例子的基础上，将 _x_ 轴的标注指定为<code>Miles Per Gallon</code> ，将整个直方图的标题指定为<code>Histogram of mpg</code> ，结果如图4-4所示。</p>
<p><img src="Image00479.jpg" alt></p>
<p>图4-1 <code>mtcars</code> 数据中<code>mpg</code> 列对应的直方图（子区间数目为5）</p>
<p><img src="Image00480.jpg" alt></p>
<p>图4-2 <code>mtcars</code> 数据中<code>mpg</code> 列对应的直方图（子区间数目为12）</p>
<p><img src="Image00481.jpg" alt></p>
<p>图4-3 <code>mtcars</code> 数据中<code>mpg</code> 列对应的直方图（子区间数目为12，颜色为红色）</p>
<p><img src="Image00482.jpg" alt></p>
<p>图4-4 <code>mtcars</code> 数据中<code>mpg</code> 列对应的直方图（子区间数目为12，红色，并且重新指定 _x_ 轴的标注和标题）</p>
<h3 id="4-4-2-柱状图"><a href="#4-4-2-柱状图" class="headerlink" title="4.4.2 柱状图"></a>4.4.2 柱状图</h3><p>柱状图 （bar chart）也称为长条图 、条状图 ，是一种以长方形的长度表示变量信息的统计图。柱状图通常用来研究分类变量不同取值的分布情况。在柱状图中， _x_ 轴是分类变量所取的不同的值， _y_ 轴是样本数据中该变量取得对应值的数目或者百分比。注意，柱状图也可以横向表示，在下面的例子中我们会介绍如何显示不同的柱状图。</p>
<p>柱状图与直方图非常类似，主要区别在于：直方图主要用来研究数据的分布，而柱状图主要用来比较某个变量不同的值。因此，直方图用于数值变量的研究和探索，而柱状图用于分类变量的探索。</p>
<p>我们在这里考察<code>iris</code> 数据集：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">table_Species = table(iris$Species)</span><br><span class="line">barplot(table_Species)</span><br><span class="line"># plot a horizontal barplot</span><br><span class="line">barplot(table_Species, horiz=T)</span><br></pre></td></tr></table></figure>

</details>


<p>在这个例子中，我们首先使用<code>table</code> 函数来计算<code>iris</code> 数据框中<code>Species</code> 列中有多少个不同的值，以及每个值出现的频率，接着使用<code>barplot</code> 函数来显示对应的柱状图（见图4-5）。根据生成的柱状图显示，<code>Species</code> 列中有3个不同的值：<code>setosa</code> 、<code>versicolor</code> 和<code>virginica</code> ，并且每个值出现的次数都是50。<code>table_Species</code> 的输出如下：</p>
<p><img src="Image00483.jpg" alt></p>
<p>图4-5 <code>iris</code> 数据中<code>Species</code> 列对应的柱状图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; table_Species</span><br><span class="line">    </span><br><span class="line">    　　setosa versicolor　virginica </span><br><span class="line">    　　　　50　　　　 50　　　　 50</span><br></pre></td></tr></table></figure>
<p>在<code>barplot</code> 函数中，将<code>horiz</code> 参数设为真，则可以得到横向的柱状图，如图4-6所示。</p>
<p>我们还可以只考虑<code>iris</code> 数据框中<code>Species</code> 列中的前120行，并输出对应的柱状图。在下面的例子中，我们同时将柱状图的颜色设为蓝色：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  table_Species2 = table(iris[1:120, &quot;Species&quot;])</span><br><span class="line">barplot(table_Species2, col=&apos;blue&apos;)</span><br></pre></td></tr></table></figure>
<p>图4-7是输出的柱状图。</p>
<p><img src="Image00484.jpg" alt></p>
<p>图4-6 <code>iris</code> 数据中<code>Species</code> 列对应的横向柱状图</p>
<p><img src="Image00485.jpg" alt></p>
<p>图4-7 <code>iris</code> 数据中<code>Species</code> 列中前120行对应的柱状图（蓝色）</p>
<h3 id="4-4-3-茎叶图"><a href="#4-4-3-茎叶图" class="headerlink" title="4.4.3 茎叶图"></a>4.4.3 茎叶图</h3><p>与直方图相比，茎叶图 （stem-and-leaf plot）能够更加细致地观察样本数据的分布，因此也是探索数据分布的有效方法之一。</p>
<p>在茎叶图中，我们把每个样本数据用一条竖线划分为两部分，左边的视为“茎”，右边的视为“叶”。我们把所有样本中具有相同茎的数据点排成一横行，并用竖线隔开。竖线左边是共同的茎，竖线右边是所有的叶（去除茎之后的部分）。注意，在茎叶图中，我们将同一行中的叶按照从小到大的顺序排好。</p>
<p>从茎叶图的定义看，它与直方图相比，具有以下特点。</p>
<ul>
<li>茎叶图和直方图一样，都可以直观地观察数据的分布情况。</li>
<li>利用茎叶图，可以很自然地对数据进行排序。因此，可以简单地得到这组数据的次序统计量。</li>
</ul>
<p>在R中，我们可以简单利用<code>stem</code> 函数来得到茎叶图。在下面的例子中，我们使用<code>mtcars</code> 数据框中的<code>disp</code> 列来生成茎叶图。首先使用<code>sort（mtcars$disp）</code> 向读者展示一下数据。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; sort(mtcars$disp)</span><br><span class="line"> [1]　71.1　75.7　78.7　79.0　95.1 108.0 120.1 120.3 121.0 140.8 145.0 146.7</span><br><span class="line"> 160.0 160.0 167.6 167.6 225.0 258.0 275.8 275.8</span><br><span class="line"> [21] 275.8 301.0 304.0 318.0 350.0 351.0 360.0 360.0 400.0 440.0 460.0 472.0</span><br><span class="line">&gt; stem(mtcars$disp)</span><br><span class="line"></span><br><span class="line">　The decimal point is 2 digit(s) to the right of the |</span><br><span class="line"></span><br><span class="line">　0 | 7888</span><br><span class="line">　1 | 012224</span><br><span class="line">　1 | 556677</span><br><span class="line">　2 | 3</span><br><span class="line">　2 | 6888</span><br><span class="line">　3 | 002</span><br><span class="line">　3 | 5566</span><br><span class="line">　4 | 04</span><br><span class="line">　4 | 67</span><br></pre></td></tr></table></figure>

</details>


<p>接下来我们直接使用<code>stem</code> 函数生成茎叶图。注意，R的输出中提示小数点在竖线的右边两位处。因此，我们考察第一行，茎是0，意味着这一行对应的数据是<code>0**.**</code> 的形式。根据<code>sort(mtcars$disp)</code> 的结果，我们知道有71.1、75.7、78.7、79.0、95.1。如果在叶部分只用一位有效数字来表示这几个样本，则有</p>
<p><img src="Image00486.gif" alt></p>
<p><img src="Image00487.gif" alt></p>
<p><img src="Image00488.gif" alt></p>
<p><img src="Image00489.gif" alt></p>
<p><img src="Image00490.gif" alt></p>
<p>注意，四舍五入之后95.1已经不满足要求了，需要将它放入下一行处理。因此，0这一行对应的叶为7、8、8、8，如上面的输出所示。</p>
<p>对于规模比较小的数据，茎叶图能够得到比较好的效果。当数据规模较大时，可以先对数据取样 （sampling），再使用茎叶图研究数据。</p>
<h3 id="4-4-4-箱线图"><a href="#4-4-4-箱线图" class="headerlink" title="4.4.4 箱线图"></a>4.4.4 箱线图</h3><p>箱线图 （box plot）也是一种探索数据的有效工具。与茎叶图相比，箱线图更加简洁直观。在箱线图中，我们利用如下统计量来画图从而反映数据的分布：</p>
<ul>
<li>下四分位数；</li>
<li>上四分位数；</li>
<li>中位数。</li>
</ul>
<p>画箱线图时，其基本原理是先画一个矩形，矩形的两端分别为下四分位数和上四分位数，而在矩形中间有一条线代表中位数；然后，从矩形的两端各画一条虚线到达不是异常值的最大/最小值。</p>
<p>在R中，我们可以直接使用<code>boxplot</code> 函数生成箱线图。以<code>mtcars</code> 数据框为例，我们从<code>boxplot</code> 最简单的用法开始讨论，并介绍涉及多个变量时如何生成箱线图。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Plot a basic boxplot using only 1 variable</span><br><span class="line">boxplot(mtcars$mpg)</span><br><span class="line">boxplot(mtcars$mpg, horizontal=T)</span><br><span class="line"># Plot a basic boxplot for cars whose cyl=4</span><br><span class="line">boxplot(mtcars[mtcars$cyl==4, &apos;mpg&apos;])</span><br><span class="line"># Plot a box plot invovling 2 variables.</span><br><span class="line">boxplot(mpg~cyl,data=mtcars, main=&quot;Car Milage Data&quot;, </span><br><span class="line">　　　　xlab=&quot;Number of Cylinders&quot;, ylab=&quot;Miles Per Gallon&quot;)</span><br><span class="line"># Plot box plot involving 3 variables</span><br><span class="line">boxplot(mpg~cyl*gear, data=mtcars, </span><br><span class="line">　　　　col=(c(&quot;blue&quot;,&quot;red&quot;)),</span><br><span class="line">　　　　main=&quot;Car Milage Data&quot;, xlab=&quot;Cylinder and Gear&quot;)</span><br></pre></td></tr></table></figure>

</details>


<p>在这段R程序中，我们首先直接使用<code>boxplot</code> 函数生成<code>mtcars</code> 数据框中<code>mpg</code> 列对应的箱线图，如图4-8所示。</p>
<p>我们也可以生成水平的箱线图，只需要将参数<code>horizontal</code> 设为真即可。<code>mtcars</code> 数据框中<code>mpg</code> 列对应的横向箱线图如图4-9所示。</p>
<p><img src="Image00491.jpg" alt></p>
<p>图4-8 <code>mtcars</code> 数据中<code>mpg</code> 列对应的箱线图</p>
<p><img src="Image00492.jpg" alt></p>
<p>图4-9 <code>mtcars</code> 数据中<code>mpg</code> 列对应的横向箱线图</p>
<p>接下来我们讨论涉及多个变量的箱线图。首先只考虑<code>mtcars</code> 数据框中当变量<code>cyl</code> 为4时<code>mpg</code> 列的分布。我们可以使用<code>mtcars[mtcars$cyl==4, &#39;mpg&#39;]</code> 得到对应的数据，直接调用<code>boxplot</code> 函数，得到图4-10所示的箱线图。</p>
<p><img src="Image00493.jpg" alt></p>
<p>图4-10 <code>mtcars</code> 数据中<code>mpg</code> 列在<code>cyl</code> 为4时对应的箱线图</p>
<p>可以看出，当<code>cyl</code> 为4时，<code>mpg</code> 高一些。那么我们能否对于<code>cyl</code> 变量的每个不同的值画出相应的箱线图呢？答案是肯定的。在<code>boxplot</code> 函数中，首先我们将第一个变量设为公式<code>mpg～cyl</code> ，表示我们要画<code>mpg</code> 变量的箱线图，但是要单独考虑<code>cyl</code> 变量的每个不同的值；然后使用<code>data=mtcars</code> 告诉<code>boxplot</code> 函数，<code>mpg</code> 和<code>cyl</code> 变量都来自于<code>mtcars</code> 数据框；最后我们将所生成的图的标题设为“Car Milage Data”， _x_ 轴标注为“Number of Cylinders”， _y_ 轴标注为“Miles Per Gallon”。所生成的箱线图如图4-11所示。</p>
<p>从图4-11可以看出，<code>cyl</code> 变量只有3种取值：4、6和8。随着<code>cyl</code> 的增加，对应<code>mpg</code> 变小。</p>
<p>在最后一个例子中，我们同时使用了3个变量。公式<code>mpg～cyl*gear</code> 表示我们要考虑<code>cyl</code> 和<code>gear</code> 变量所有不同值的组合，并为每一组组合画出<code>mpg</code> 变量对应的箱线图。此外，我们还指定了箱线图的颜色。生成的箱线图如图4-12所示，其中轮流使用了蓝色（图中为深灰）和红色（图中为浅灰）。注意 _x_ 轴的取值。这里我们同时考虑了两个变量：<code>cyl</code> 和<code>gear</code> 。 _x_ 轴上面的6.3表示<code>cyl</code> =6、<code>gear</code> =3。注意，当<code>cyl</code> =4、<code>gear</code> =3时，<code>mtcars</code> 中只有一个数据点，因此在图中只有一条黑线。</p>
<p><img src="Image00494.jpg" alt></p>
<p>图4-11 <code>mtcars</code> 数据中<code>mpg</code> 列在<code>cyl</code> 取不同值时对应的箱线图</p>
<p><img src="Image00495.jpg" alt></p>
<p>图4-12 <code>mtcars</code> 数据中<code>mpg</code> 列在<code>cyl</code> 和<code>gear</code> 取不同值时对应的箱线图</p>
<h3 id="4-4-5-散点图"><a href="#4-4-5-散点图" class="headerlink" title="4.4.5 散点图"></a>4.4.5 散点图</h3><p>在前面介绍的画图工具中，我们研究的都是一个变量。这里我们介绍散点图 （scatter plot），用来探索两个变量之间的关系。通常情况下，散点图主要用于数值变量的研究。</p>
<p>在散点图中， _x_ 轴和 _y_ 轴用来表示两个不同的变量，并将样本数据点根据选定的两个变量的值在图中画出。通过观察这些点的分布，可以直观地观察和分析两个变量之间的关系。注意，散点图只是一种初步的分析工具，进一步的分析需要更精确的分析和计算。例如，如果我们发现两个变量之间存在某种关联，那么可以计算相关系数 来精确衡量这两个变量之间存在线性关系的程度。</p>
<p>在R中，使用最基本的plot函数可以直接画出散点图。下面讨论如何生成散点图。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Plot a basic scatter plot</span><br><span class="line">plot(mtcars$wt, mtcars$mpg)</span><br><span class="line"># We add more controls</span><br><span class="line">plot(mtcars$wt, mtcars$mpg, main=&quot;Scatterplot Example&quot;, </span><br><span class="line">　　 xlab=&quot;Car Weight &quot;, ylab=&quot;Miles Per Gallon &quot;, pch=19)</span><br><span class="line"># Advanced scatter plot using the car package</span><br><span class="line"># The scatterplot( ) function in the car package offers many enhanced features,</span><br><span class="line"># including fit lines, marginal box plots, conditioning on a factor, </span><br><span class="line"># and interactive point identification. Each of these features is optional.</span><br><span class="line">car.installed &lt;-&apos;car&apos; %in% rownames(installed.packages())</span><br><span class="line">if (car.installed) &#123;</span><br><span class="line">　print(&quot;the car package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">　library(&apos;car&apos;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the car package first...&quot;)</span><br><span class="line">　install.packages(&apos;car&apos;, dependencies=T)</span><br><span class="line">　library(&apos;car&apos;)</span><br><span class="line">&#125;</span><br><span class="line"># Enhanced Scatterplot of MPG vs. Weight </span><br><span class="line">scatterplot(mpg ~ wt, data=mtcars, smoother=F,</span><br><span class="line">　　　　　　xlab=&quot;Weight of Car&quot;, ylab=&quot;Miles Per Gallon&quot;, </span><br><span class="line">　　　　　　main=&quot;Enhanced Scatter Plot&quot;, </span><br><span class="line">　　　　　　labels=row.names(mtcars))</span><br></pre></td></tr></table></figure>

</details>


<p>在第一个例子中，我们指定 _x_ 轴是<code>mtcars$wt</code> ， _y_ 轴是<code>mtcars$mpg</code> ，直接使用<code>plot</code> 函数即可。生成的散点图如图4-13所示。</p>
<p>在第二个例子中，我们在<code>plot</code> 函数中增加了更多的控制参数。我们使用参数<code>main</code> 指定所生成的图的标题，使用参数<code>xlab</code> 指定 _x_ 轴的标注，使用<code>ylab</code> 指定 _y_ 轴的标注。特别地，我们使用参数<code>pch</code> 控制散点图中生成的每个点的形状。图4-14给出了常用<code>pch</code> 值（0～25的整数）所对应的形状。图4-14中倒数第二行最后两个形状和最后一行所有形状对应的<code>pch</code> 值为<code>&#39;@&#39;</code> 、<code>&#39;+&#39;</code> 、<code>&#39;%&#39;</code> 、<code>&#39;#&#39;</code> 、<code>&#39;A&#39;</code> 、<code>&#39;a&#39;</code> 、<code>&#39;m&#39;</code> 、<code>&#39;?:&#39;</code> 和<code>&#39;*:&#39;</code> 。</p>
<p><img src="Image00496.jpg" alt></p>
<p>图4-13 <code>mtcars</code> 数据中<code>wt</code> 列和<code>mpg</code> 列对应的散点图</p>
<p><img src="Image00497.jpg" alt></p>
<p>图4-14 R中绘图参数<code>pch</code> 常用的形状</p>
<p>所生成的散点图如图4-15所示。</p>
<p><img src="Image00498.jpg" alt></p>
<p>图4-15 <code>mtcars</code> 数据中<code>wt</code> 列和<code>mpg</code> 列对应的散点图（更多控制参数）</p>
<p>我们还可以使用R中提供的其他包来生成更加复杂的散点图。这里我们使用<code>car</code> 包画了一个更加复杂的散点图。首先我们检查<code>car</code> 包是否已经安装，如果没有安装，则使用<code>install.packages</code> 安装包；然后使用<code>library</code> 函数载入<code>car</code> 包；最后调用<code>car</code> 包中的<code>scatterplot</code> 函数生成散点图。我们使用公式<code>mpg~wt</code> 表示 _x_ 轴对应<code>wt</code> ， _y_ 轴对应<code>mpg</code><br>；使用参数<code>data</code> 指定数据源是<code>mtcars</code> 数据框；使用<code>xlab</code> 、<code>ylab</code> 和<code>main</code> 分别指定 _x_ 轴标识、 _y_ 轴标识和图的标题。注意，这里的<code>scatterplot</code> 函数会为散点图自动拟合模型，包括线性回归模型和非参数回归模型等。这里我们将<code>smoother</code> 设为假，表示不选非参数回归模型，而只使用线性回归模型。最后生成的散点图如图4-16所示。注意，我们同时也生成了对应的两个变量的箱线图。</p>
<p><img src="Image00499.jpg" alt></p>
<p>图4-16 使用<code>car</code> 包生成<code>mtcars</code> 数据中<code>wt</code> 列和<code>mpg</code> 列对应的散点图</p>
<p>在实践中，直接使用微软公司的Excel软件画出这些图也是一个很好的选择，这里就不一一介绍了。</p>
<h1 id="第5章-回归分析"><a href="#第5章-回归分析" class="headerlink" title="第5章 回归分析"></a>第5章 回归分析</h1><p>回归问题在实际中有着非常广泛的应用。很多实际问题都可以转化为回归问题的形式，因此回归分析是处理大数据的一种常用且有效的方法。与分类方法相比，回归分析的输出是连续的，而分类方法的输出是离散的。本章介绍回归分析的基本思想、基本方法，以及评价和选取回归算法的标准与方法。特别地，本章形象地解释了Lasso以及其为何能得到稀疏解。本章最后以实际的数据集作为例子，详细介绍如何使用R中的<code>glmnet</code> 包来完成回归分析。</p>
<h2 id="5-1-回归分析的基本思想"><a href="#5-1-回归分析的基本思想" class="headerlink" title="5.1 回归分析的基本思想"></a>5.1 回归分析的基本思想</h2><p>回归 （regression）分析是分析两个或者多个变量之间相互关系的一种方法。回归和分类<br>（classification）的最大区别是，在回归问题中，我们要预测的变量是连续的；而在分类问题中，我们要预测的变量是离散的。而且在分类问题中，类别一般是有限的。例如，在两类分类<br>（binary classification）问题中就只有两个类别。</p>
<p>严格地讲，在回归分析中，我们假设因变量 _y_ 和自变量 _<strong>x</strong> _ 之间存在某种相关关系。在数据分析中，假设有一个数据集合：</p>
<p><img src="Image00500.gif" alt></p>
<p>这里<img src="Image00501.gif" alt> 是<img src="Image00502.gif" alt> 维向量，<img src="Image00503.gif" alt> 。我们的目的是根据该数据集合推断出函数<img src="Image00504.gif" alt> ：</p>
<p><img src="Image00505.gif" alt></p>
<p>这里，我们将 _y_ 称为因变量 ，将 _<strong>x</strong> _ 称为自变量 。函数<img src="Image00506.gif" alt> 称为 _y_ 对 _<strong>x</strong> _ 的回归函数。如果<img src="Image00507.gif" alt> ，则称为一元回归分析，因为只有一个自变量；如果 _d_ &gt;1，则称为多元回归分析。按照函数 _f_ 的类型，回归分析可分为线性回归分析和非线性回归分析。</p>
<p>“回归”一词最早由法兰西斯·高尔顿（Francis Galton）引入。他研究了子女身高和父母身高的关系。当时他发现虽然父母的身高可以影响子女的身高，但是子女的身高却有逐渐“回归”到中等的现象。现在回归一词的含义已经发生了很大变化。在现代，回归主要指的是研究 _y_ 和 _<strong>x</strong> _ 之间的关系，其中 _y_ 是连续型的变量。</p>
<p>回归分析的运用十分广泛。一些常用例子包括：</p>
<ul>
<li>收入水平（ _y_ ）与受教育程度（ _x_ ）之间的关系；</li>
<li>子女身高（ _y_ ）与父亲身高（<img src="Image00508.gif" alt> ）、母亲身高（<img src="Image00509.gif" alt> ）之间的关系；</li>
<li>车辆每公里耗油量（ _y_ ）与发动机排量（<img src="Image00510.gif" alt> ）、发动机马力（<img src="Image00509.gif" alt> ）和车辆重量（<img src="Image00511.gif" alt> ）之间的关系。</li>
</ul>
<p>这里我们分别用 _y_ 和 _x_ 标出对应问题中的因变量和自变量。</p>
<p>线性回归是回归分析中最简单的例子。下面首先从最简单的线性分析开始介绍，并引入机器学习中的一些重要概念，包括损失函数、模型复杂度等。</p>
<h2 id="5-2-线性回归和最小二乘法"><a href="#5-2-线性回归和最小二乘法" class="headerlink" title="5.2 线性回归和最小二乘法"></a>5.2 线性回归和最小二乘法</h2><p>在线性回归 （linear regression）中，我们假设因变量 _y_ 和自变量 _<strong>x</strong> _ 之间存在如下线性关系：</p>
<p><img src="Image00512.gif" alt></p>
<p>（5-1）</p>
<p>这里 _ε_ 是均值为0、方差恒定的随机误差，称为误差项。在这个简单的线性模型 （linear model）中，参数<img src="Image00513.gif" alt> ，<img src="Image00514.gif" alt> 是向量 _<strong>x</strong> _ 的第 _j_ 个分量。在很多时候，我们可以把1作为一个分量放入 _<strong>x</strong> _ 中，这样就不用考虑线性函数 _f_ ( _<strong>x</strong> _ )的截距了。</p>
<p>在机器学习中，我们通常引入训练集和损失函数 （loss function）来求得模型的参数 _<strong>w</strong> _ 。假设我们有训练集<img src="Image00515.gif" alt> ，这里自变量<img src="Image00516.gif" alt> 是 _d_ 维向量，<img src="Image00517.gif" alt> 。在机器学习中，通常先搜集一些已知数据作为训练集，从中归纳出所假设模型的参数。此外，对于同样的数据，使用不同的损失函数，可以得到不同的模型。在线性回归中，我们使用平方和损失函数<br>（sum-of-squares loss function）。对于训练集中的任一点 _<strong>x</strong> i _ ，我们的预测值是<img src="Image00518.gif" alt> ，则最小化如下函数：</p>
<p><img src="Image00519.gif" alt></p>
<p>（5-2）</p>
<p>将式（5-1）代入，则有</p>
<p><img src="Image00520.gif" alt></p>
<p>（5-3）</p>
<p>写成矩阵的形式是</p>
<p><img src="Image00521.gif" alt></p>
<p>（5-4）</p>
<p>这里</p>
<p><img src="Image00522.gif" alt></p>
<p>在最小二乘法 （least squares）中，我们要求出使得损失函数最小的 _<strong>w</strong> _ 。因此，现在的问题就是找到使得式（5-3）最小化的<img src="Image00513.gif" alt> 。事实上式（5-3）是关于<img src="Image00523.gif" alt> 的二次函数，要最小化 _L_ 只需要求得 _L_ 关于<img src="Image00523.gif" alt> 的导数并设为0，即可求出使得 _L_ 最小的<img src="Image00523.gif" alt> 。这里我们给出 _L_ 关于<img src="Image00524.gif" alt> 的导数（这里使用下标 _k_ ）：</p>
<p><img src="Image00525.gif" alt></p>
<p>（5-5）</p>
<p>也可以将<img src="Image00526.gif" alt> 综合起来写成矩阵的形式：</p>
<p><img src="Image00527.gif" alt></p>
<p>（5-6）</p>
<p>令导数<img src="Image00526.gif" alt> 为0，可得方程：</p>
<p><img src="Image00528.gif" alt></p>
<p>（5-7）</p>
<p>写成矩阵的形式是：</p>
<p><img src="Image00529.gif" alt></p>
<p>（5-8）</p>
<p>该方程称为正规方程 （normal equation）。如果矩阵<img src="Image00530.gif" alt> 可逆，则 _<strong>w</strong> _ 的最小二乘估计<img src="Image00531.gif" alt> 为：</p>
<p><img src="Image00532.gif" alt></p>
<p>（5-9）</p>
<p>根据训练集得到的<img src="Image00533.gif" alt> ，对于第 _i_ 个样本<img src="Image00534.gif" alt> ，可以得到相应的预测值：</p>
<p><img src="Image00535.gif" alt></p>
<p>（5-10）</p>
<h3 id="5-2-1-最小二乘法的几何解释"><a href="#5-2-1-最小二乘法的几何解释" class="headerlink" title="5.2.1 最小二乘法的几何解释"></a>5.2.1 最小二乘法的几何解释</h3><p>在线性回归中，我们假设所得的模型<img src="Image00536.gif" alt> 是一个高维空间中的超平面。在图5-1中，假设<img src="Image00537.gif" alt> 是二维向量，那么<img src="Image00504.gif" alt> 就是一个平面。在最小二乘法中，我们要求误差项<img src="Image00538.gif" alt> 的平方和最小。事实上，<img src="Image00539.gif" alt> 就是真实值<img src="Image00540.gif" alt> 到平面上的相应点<img src="Image00541.gif" alt> 之间的距离。</p>
<p><img src="Image00542.jpg" alt></p>
<p>图5-1 最小二乘法的几何解释</p>
<h3 id="5-2-2-线性回归和极大似然估计"><a href="#5-2-2-线性回归和极大似然估计" class="headerlink" title="5.2.2 线性回归和极大似然估计"></a>5.2.2 线性回归和极大似然估计</h3><p>在前面的介绍中，我们从损失函数的角度出发，推导出了正规方程。事实上，我们也可以从统计学中的极大似然估计的角度推导出最小二乘法的基本公式。我们假设因变量<img src="Image00543.gif" alt> 可以表示成如下形式：</p>
<p><img src="Image00544.gif" alt></p>
<p>（5-11）</p>
<p>这里<img src="Image00545.gif" alt> 是噪声，它服从均值为0、方差为<img src="Image00546.gif" alt> 的正态分布<img src="Image00547.gif" alt> 。由于已知<img src="Image00548.gif" alt> ，因此<img src="Image00543.gif" alt> 满足正态分布<img src="Image00549.gif" alt> 。因此，观察到数据<img src="Image00550.gif" alt> 的对数似然函数为：</p>
<p><img src="Image00551.gif" alt></p>
<p>可以看出，前面两项<img src="Image00552.gif" alt> 和<img src="Image00553.gif" alt> 都是恒定值。最大化<img src="Image00554.gif" alt> 等价于最小化<img src="Image00555.gif" alt> 。因此，在噪声<img src="Image00556.gif" alt> 服从正态分布<img src="Image00547.gif" alt> 的情况下，最大化 _<strong>y</strong> _ 的似然函数等价于最小化平方和损失函数。这也从另一个角度解释了为什么我们在回归分析中采用平方和损失函数。</p>
<p>高斯-马尔可夫定理 （Gauss-Markov theorem）是统计学中非常著名的一个发现，它在理论上奠定了最小二乘法的基础。我们在这里只是简单介绍一下定理本身，而定理的具体证明从略，感兴趣的读者可以参考[2]。</p>
<p>在高斯-马尔可夫定理中，我们并不假设<img src="Image00556.gif" alt> 服从正态分布。严格地讲，我们假设<img src="Image00556.gif" alt> 满足：</p>
<p>1）<img src="Image00556.gif" alt> 的期望值为0，也即<img src="Image00557.gif" alt> ；</p>
<p>2）<img src="Image00556.gif" alt> 的方差满足<img src="Image00558.gif" alt> &lt;<img src="Image00559.gif" alt> ；</p>
<p>3）<img src="Image00556.gif" alt> 与<img src="Image00560.gif" alt> 不相关，即<img src="Image00561.gif" alt> 。</p>
<p>则回归系数 _<strong>w</strong> _ 的最佳线性无偏估计就是使用最小二乘法得到的<img src="Image00531.gif" alt> 。</p>
<h2 id="5-3-岭回归和Lasso"><a href="#5-3-岭回归和Lasso" class="headerlink" title="5.3 岭回归和Lasso"></a>5.3 岭回归和Lasso</h2><h3 id="5-3-1-岭回归"><a href="#5-3-1-岭回归" class="headerlink" title="5.3.1 岭回归"></a>5.3.1 岭回归</h3><p>在式（5-9）中，我们给出了计算 _<strong>w</strong> _ 的公式。但是，使用该公式的前提条件是矩阵<img src="Image00562.gif" alt> 可逆。而在实际数据处理中，这个条件可能是不成立的。</p>
<p>下面给出一个简单的例子。我们使用R中的<code>rnorm</code> 函数生成由正态分布产生的1000个三维向量，并计算矩阵<img src="Image00562.gif" alt> 。在该数据集中，有3个变量<code>x1</code> 、<code>x2</code> 和<code>x3</code> ，并假设<code>x3=0.6x1+0.4x2</code> 。相应的R代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set.seed(10)</span><br><span class="line">n = 1000</span><br><span class="line">x1 = rnorm(1000)</span><br><span class="line">x2 = rnorm(1000)</span><br><span class="line">x3 = 0.6*x1 + 0.4*x2</span><br><span class="line">x_matrix = cbind(x1, x2, x3)</span><br><span class="line">x_cov = t(x_matrix)%*%x_matrix</span><br><span class="line">delta = 0.6 * x_cov[,1] + 0.4 * x_cov[,2] -x_cov[,3]</span><br></pre></td></tr></table></figure>

</details>


<p>注意，在R中矩阵乘法使用<code>%*%</code> ，而<code>x_matrix</code> 的转置矩阵是使用函数<code>t</code> 得到的。这里我们使用<code>set.seed</code> 函数来确定随机数生成器的种子以便于重复实验结果。在我们的模拟中可得</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; delta</span><br><span class="line">　　　　　 x1　　　　　　x2　　　　　　x3 </span><br><span class="line">-5.684342e-13 -1.705303e-13　5.684342e-13</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，<img src="Image00530.gif" alt> 的列是线性相关的（这里<code>delta</code> 在数值上已经非常小了），因此这是一个奇异矩阵。由于矩阵<img src="Image00530.gif" alt> 是不可逆的，导致我们不能直接使用式（5-9）。在实际中，<img src="Image00530.gif" alt> 可能是非奇异矩阵，但其实非常接近奇异矩阵。在实际计算中，如果一个矩阵越接近奇异矩阵，那么计算它的逆矩阵的误差就越大 ① 。</p>
<p>为了保证计算的精确度，同时降低模型的复杂度，很多时候人们使用岭回归 （ridge regression）。在岭回归中，我们计算<img src="Image00563.gif" alt> 的逆矩阵，这里 _λ_ &gt;0是控制参数， _<strong>I</strong> _ 是单位矩阵。简而言之，我们就在矩阵<img src="Image00562.gif" alt> 的所有对角元素上都加上了 _λ_ ，从而使得计算逆矩阵<img src="Image00564.gif" alt> 的精度更高。</p>
<p>以上是从计算的角度来解释岭回归。我们还可以从损失函数和贝叶斯统计的角度来解释岭回归。从贝叶斯统计的角度对岭回归的解释可参见参考文献[4] ② 。下面我们再从损失函数的角度来解释岭回归。</p>
<p>严格地讲，岭回归最小化如下的损失函数：</p>
<p><img src="Image00565.gif" alt></p>
<p>（5-12）</p>
<p>写成矩阵的形式是：</p>
<p><img src="Image00566.gif" alt></p>
<p>（5-13）</p>
<p>这里 _λ_ &gt;0称为复杂度参数 （complexity parameter），控制着模型的复杂度。复杂度参数在机器学习中有着广泛的应用。我们在本书中将会广泛使用复杂度参数。在岭回归中，它直接控制着<img src="Image00567.gif" alt> 的大小。 _λ_ 越大，越多的<img src="Image00567.gif" alt> 接近于0。图5-2反映了随着 _λ_ 的变化，我们使用岭回归处理Boston housing data数据 ③ 时 _<strong>w</strong> _ 中各个分量<img src="Image00567.gif" alt> 的变化。在图5-2中，横坐标是<img src="Image00568.gif" alt> ，纵坐标是<img src="Image00567.gif" alt> 的大小，每条曲线代表一个<img src="Image00567.gif" alt> 的值在各个 _λ_ 值下的变化。在这组数据中， _<strong>X</strong> _ 的维数 _d_ 是13，因此有13条曲线。从图5-2中可以看出，当 _λ_ 值较大时，<img src="Image00567.gif" alt> 都趋近于0，而当 _λ_ 较小时，<img src="Image00567.gif" alt> 所受的影响较小。从式（5-12）中可以看出，当 _λ_ 较大时，损失函数中的第二项<img src="Image00569.gif" alt> 影响更大，使得<img src="Image00567.gif" alt> 的值都趋向于0；而当 _λ_ 较小时，损失函数的第一项影响较大，则<img src="Image00567.gif" alt> 的值受第二项的影响较小。</p>
<p><img src="Image00570.jpg" alt></p>
<p>图5-2 岭回归中 _w j _ 随着 _λ_ 的变化情况</p>
<p>最小化式（5-12）中的损失函数等价于最小化如下条件约束问题：</p>
<p><img src="Image00571.gif" alt></p>
<p>（5-14）</p>
<p><img src="Image00572.gif" alt></p>
<p>式（5-12）中的 _λ_ 和式（5-14）中的 _γ_ 存在一一对应关系。该等价关系可以由拉格朗日定理推得，在这里省略具体的证明过程。</p>
<p>类似地，通过最小化新的损失函数得到 _<strong>w</strong> _ 的最优解。由于式（5-12）中的函数仍然是关于<img src="Image00567.gif" alt> 的二次函数，求得<img src="Image00573.gif" alt> 关于<img src="Image00567.gif" alt> 的导数并设为0即可找出使得<img src="Image00573.gif" alt> 最小的<img src="Image00567.gif" alt> 。这里我们给出<img src="Image00573.gif" alt> 关于<img src="Image00567.gif" alt> 的导数：</p>
<p><img src="Image00574.gif" alt></p>
<p>（5-15）</p>
<p>也可以写成矩阵的形式：</p>
<p><img src="Image00575.gif" alt></p>
<p>（5-16）</p>
<p>令导数<img src="Image00576.gif" alt> 为0，可得方程：</p>
<p><img src="Image00577.gif" alt></p>
<p>（5-17）</p>
<p>该方程是岭回归下的正规方程。由于矩阵<img src="Image00578.gif" alt> 可逆，则在岭回归下 _<strong>w</strong> _ 的最优估计<img src="Image00531.gif" alt> 为</p>
<p><img src="Image00579.gif" alt></p>
<p>（5-18）</p>
<p>如果数据中有相关的变量，则岭回归是一个非常有效的工具。在线性回归中，数据是否标准化（normalization）对于最终结果 _<strong>w</strong> _ 的求解没有影响，但在岭回归中则不然。数据是否标准化对于最终的 _<strong>w</strong> _ 有着明显的影响。一般而言，我们推荐在使用岭回归之前首先标准化原始数据。</p>
<h3 id="5-3-2-Lasso与稀疏解"><a href="#5-3-2-Lasso与稀疏解" class="headerlink" title="5.3.2 Lasso与稀疏解"></a>5.3.2 Lasso与稀疏解</h3><p>使用回归算法分析实际数据时，很多时候很多变量对于因变量 _y_ 的预测其实是没有用处的，或者很多数据是冗余的。在这里，一个重要问题是如何从众多的变量中选取出最有效的子集。在本节，我们讨论Lasso ④ 。Lasso与岭回归很相似，它们都在损失函数中添加正规化项<br>（regularization）。在岭回归中，我们使用的是<img src="Image00580.gif" alt> ，在Lasso中，我们使用的则是<img src="Image00581.gif" alt> 。严格地讲，在Lasso中，我们最小化如下的损失函数：</p>
<p><img src="Image00582.gif" alt></p>
<p>（5-19）</p>
<p>这里 _λ_ &gt;0是对应的复杂度参数。可以将<img src="Image00583.gif" alt> 写成矩阵的形式：</p>
<p><img src="Image00584.gif" alt></p>
<p>（5-20）</p>
<p>事实上，<img src="Image00585.gif" alt> 也是一个凸函数（因为<img src="Image00586.gif" alt> 和<img src="Image00587.gif" alt> 都是关于 _<strong>w</strong> _ 的凸函数），因此，<img src="Image00583.gif" alt> 的局部最优解也是它的全局最优解。但是这里最小化<img src="Image00583.gif" alt> 的难点在于<img src="Image00583.gif" alt> 在很多点是不可导的，这样就不能使用前面求导数并使之为0的方法来找出最小化<img src="Image00583.gif" alt> 的最优解了。这里简单解释一下为什么<img src="Image00583.gif" alt> 在有些点是不可导的。图5-3显示了<img src="Image00588.gif" alt> 的函数图，可以看出<img src="Image00588.gif" alt> 在<img src="Image00589.gif" alt> 处是连续的，但不可导。那么对于<img src="Image00583.gif" alt> ，它在 _d_ 维空间中任一<img src="Image00590.gif" alt> 的点都是不可导的。关于Lasso及相关问题的求解是前几年学术界研究的热点之一。这里不介绍如何求解<img src="Image00583.gif" alt> ，而直接介绍R中的<code>glmnet</code> 包以求解Lasso问题。</p>
<p><img src="Image00591.jpg" alt></p>
<p>图5-3 | _w j _ |的函数图</p>
<p>使用与岭回归同样的一组数据，通过选取不同的 _λ_ 值，可得不同的 _<strong>w</strong> _ 。图5-4显示了不同的 _λ_ 值对应的 _<strong>w</strong> _ 值。其中横坐标是<img src="Image00592.gif" alt> 值，纵坐标是<img src="Image00567.gif" alt> 的值，每条曲线代表了 _<strong>w</strong> _ 的一个分量。从图5-4中可以看出，当 _λ_ 增大时，越来越多的<img src="Image00567.gif" alt> 变成了0，这就是通常所说的稀疏解 （sparse solution）。</p>
<p><img src="Image00593.jpg" alt></p>
<p>图5-4 Lasso中 _w j _ 随着 _λ_ 值的变化情况</p>
<p>为什么使用<img src="Image00583.gif" alt> 范数<img src="Image00587.gif" alt> 可以得到稀疏解呢？我们使用一个二维的例子来说明为什么<img src="Image00583.gif" alt> 范数导致稀疏解。在这里需要了解一些关于优化算法的知识。根据拉格朗日定理，最小化损失函数<img src="Image00583.gif" alt> 等价于优化如下优化问题：</p>
<p><img src="Image00594.gif" alt></p>
<p>（5-21）</p>
<p>换言之，我们要在满足条件<img src="Image00595.gif" alt> 的前提下找出使得<img src="Image00586.gif" alt> 最小的解。我们试图给出比较直观的解释而省略复杂的数学证明。这里我们用一个简化的问题来说明原因。假设我们考虑二维的问题，即<img src="Image00596.gif" alt> ，且假设<img src="Image00597.gif" alt> 为单位矩阵，那么在式（5-21）中的优化问题简化为：</p>
<p><img src="Image00598.gif" alt></p>
<p>（5-22）</p>
<p>在图5-5中，我们画出了可行域<img src="Image00599.gif" alt><br>（图中黑色区域），那么最优解<img src="Image00531.gif" alt> 有如下几种可能。</p>
<ul>
<li>如果 _<strong>y</strong> _ 在可行域<img src="Image00599.gif" alt> 内，那么最优解就是 _<strong>y</strong> _ 本身。</li>
<li>如果 _<strong>y</strong> _ 在区域I（图中灰色区域）内，则最优解就是 _<strong>y</strong> _ 投影到可行域内的点。例如，在图5-6中， _<strong>y</strong> _ 1 （图中圆心）的投影就是<img src="Image00600.gif" alt> （图中圆和可行域的切点）。注意，<img src="Image00600.gif" alt> 不是一个稀疏解。</li>
<li>如果 _<strong>y</strong> _ 在区域II（白色区域）内，则最优解就是可行域的一个顶点。例如，在图5-7中， _<strong>y</strong> _ 2 （图中圆心）的投影就是<img src="Image00601.gif" alt> （图中圆和可行域的切点，也是可行域的一个顶点）。注意到<img src="Image00602.gif" alt> ，对应于一个稀疏解。</li>
</ul>
<p>从此例可以看出，当 _<strong>y</strong> _ 在可行域内或者区域I内时，所得的解不是稀疏解；当 _<strong>y</strong> _ 在区域II内时，对应的解是稀疏解。当维度 _d_ 增加时，区域II所对应的空间所占比例越来越高，从而导致得到稀疏解的可能性越来越大。</p>
<p><img src="Image00603.jpg" alt></p>
<p>图5-5 图中黑色区域是可行域，灰色区域是区域I，而其他的白色区域为区域II</p>
<p><img src="Image00604.jpg" alt></p>
<p>图5-6 _<strong>y</strong> _ 在区域I时的最优解。其中 _<strong>y</strong> _ 1 为圆心，<img src="Image00600.gif" alt> 是圆和可行域的切点</p>
<p><img src="Image00605.jpg" alt></p>
<p>图5-7 _<strong>y</strong> _ 在区域II时的最优解。其中 _<strong>y</strong> _ 2 为圆心，<img src="Image00601.gif" alt> 是圆和可行域的切点</p>
<h3 id="5-3-3-Elastic-Net"><a href="#5-3-3-Elastic-Net" class="headerlink" title="5.3.3 Elastic Net"></a>5.3.3 Elastic Net</h3><p>Elastic Net是岭回归和Lasso的结合。在Elastic Net中，我们同时考虑 _L_ 1 范数和<img src="Image00573.gif" alt> 范数。换言之，我们最小化如下损失函数：</p>
<p><img src="Image00606.gif" alt></p>
<p>（5-23）</p>
<p>这里有两个复杂参数<img src="Image00607.gif" alt> 和<img src="Image00608.gif" alt> 。写成矩阵的形式是：</p>
<p><img src="Image00609.gif" alt></p>
<p>（5-24）</p>
<p>与岭回归相比，Elastic Net能够得到稀疏的 _<strong>w</strong> _<br>；与Lasso相比，它能够更加有效地处理成组的高相关的变量，一般来说，能够取得更高的预测性能。但是代价是我们引入了更多的控制参数，在实际中根据数据确定参数值的工作量更大。</p>
<h2 id="5-4-回归算法的评价和选取"><a href="#5-4-回归算法的评价和选取" class="headerlink" title="5.4 回归算法的评价和选取"></a>5.4 回归算法的评价和选取</h2><p>本节首先简要介绍评价回归算法的常用标准，包括均方差 、均方根误差 和可决系数 。之后讨论不同模型的比较，尤其是模型的复杂度，并引入偏差-方差权衡。在下面的讨论中，假设对于测试集中数据的实际值为<img src="Image00610.gif" alt> ，模型的拟合值为<img src="Image00611.gif" alt> 。</p>
<h3 id="5-4-1-均方差和均方根误差"><a href="#5-4-1-均方差和均方根误差" class="headerlink" title="5.4.1 均方差和均方根误差"></a>5.4.1 均方差和均方根误差</h3><p>在模型评价中，通常会计算多种指标来衡量模型的好坏。对于回归算法，最常用的标准是均方差 （mean squared error，MSE）和均方根误差<br>（root mean squared error，RMSE），定义如下：</p>
<p><img src="Image00612.gif" alt></p>
<p>（5-25）</p>
<p><img src="Image00613.gif" alt></p>
<p>（5-26）</p>
<p>在一般实践中，MSE或者RMSE越小，所得的回归模型越好。</p>
<h3 id="5-4-2-可决系数"><a href="#5-4-2-可决系数" class="headerlink" title="5.4.2 可决系数"></a>5.4.2 可决系数</h3><p>可决系数 （R-Squared，又称为coefficient of determination），通常记为<img src="Image00614.gif" alt> 。首先我们定义<img src="Image00615.gif" alt> 为：</p>
<p><img src="Image00616.gif" alt></p>
<p>（5-27）</p>
<p>那么定义数据的总的离差平方和 <img src="Image00617.gif" alt> （total sum of squares）为：</p>
<p><img src="Image00618.gif" alt></p>
<p>（5-28）</p>
<p><img src="Image00619.gif" alt> 反映了数据<img src="Image00610.gif" alt> 波动的大小。</p>
<p>残差平方和 <img src="Image00620.gif" alt> （residual sum of squares）定义为：</p>
<p><img src="Image00621.gif" alt></p>
<p>（5-29）</p>
<p><img src="Image00620.gif" alt> 反映了<img src="Image00543.gif" alt> 与拟合值<img src="Image00622.gif" alt> 之间的差异。若<img src="Image00623.gif" alt> ，表明每个<img src="Image00543.gif" alt> 都可以由自变量的线性关系精确拟合；<img src="Image00620.gif" alt> 的值越大，表明<img src="Image00543.gif" alt> 与拟合值的偏差也越大。</p>
<p>回归平方和 <img src="Image00624.gif" alt> （regression sum of squares）定义为：</p>
<p><img src="Image00625.gif" alt></p>
<p>（5-30）</p>
<p>回归平方和反映了拟合值与<img src="Image00543.gif" alt> 的平均值之间的总偏差。</p>
<p>使用<img src="Image00620.gif" alt> 和<img src="Image00617.gif" alt> ，可决系数定义为：</p>
<p><img src="Image00626.gif" alt></p>
<p>（5-31）</p>
<p>根据可决系数的定义，我们有<img src="Image00627.gif" alt> 。一般来讲，<img src="Image00628.gif" alt> 介于0和1之间。如果 <img src="Image00629.gif" alt> 对于所有 _i_ 都成立，则<img src="Image00630.gif" alt> ；如果<img src="Image00631.gif" alt> 对于所有 _i_ 都成立，则<img src="Image00632.gif" alt> 。</p>
<p>从直观上讲，<img src="Image00628.gif" alt> 度量了因变量<img src="Image00543.gif" alt> 的变化中被预测值<img src="Image00622.gif" alt> 解释的部分。事实上，<img src="Image00628.gif" alt> 是关于相关度的度量：它度量了<img src="Image00633.gif" alt> 和真实值之间的相关度。一般来讲，<img src="Image00628.gif" alt> 的值越高，所得的回归模型越好。</p>
<h3 id="5-4-3-偏差-方差权衡"><a href="#5-4-3-偏差-方差权衡" class="headerlink" title="5.4.3 偏差-方差权衡"></a>5.4.3 偏差-方差权衡</h3><p>前面我们介绍了线性回归及其变体，包括岭回归、Lasso和Elastic Net。那么不同的算法之间更深层的区别到底是什么？在本节中，我们详细讨论不同的模型<br>（model）的区别，引入模型复杂度的概念并重点讨论偏差-方差权衡 （bias-variance tradeoff）。</p>
<p>在回归分析（这里并不局限于线性回归）中，我们假设残差<img src="Image00634.gif" alt> 的分布相互独立，且均值为0，方差<img src="Image00635.gif" alt> 恒定。这里<img src="Image00556.gif" alt> 的分布可以为正态分布，也可以是其他分布。那么，模型的均方差MSE的期望值可以分解为：</p>
<p><img src="Image00636.gif" alt></p>
<p>（5-32）</p>
<p>在式（5-32）中，<img src="Image00637.gif" alt> 是我们前面讨论的均方差的期望值，它反映了模型的优劣。在式（5-32）中，第一项<img src="Image00635.gif" alt> 通常认为是不可去除的错误项，是数据中<img src="Image00543.gif" alt> 的方差。这一项可以认为是数据本身的问题，无论什么样的回归模型皆无法克服。第二项是模型偏差的平方，它反映了该模型的函数形式能够以多精确的形式接近真正的数据。例如，如果数据本身是由<img src="Image00638.gif" alt> 产生的，用 _x_ 来构建线性模型<img src="Image00639.gif" alt> 显然会导致偏差较大。最后一项是所谓的模型的方差。对于一组数据，可以使用线性回归来拟合，也可以使用二次函数甚至高次函数来拟合。显然，二次函数的复杂度高于线性回归模型：它需要更多的参数，对于数据分布的假设也更多和更强，但模型的表达能力更强。例如，在这个例子中，二次函数的表达能力显然比线性函数强：线性函数是二次函数的一个特例。一般来讲，表达能力越强的模型，其方差也就越大。</p>
<p>图5-8中给出了一个实际例子来进一步阐述模型的偏差和方差。图5-8中画出了原始的数据集（由函数<img src="Image00640.gif" alt> 产生），以及高偏差的模型和高方差的模型。在图5-8中，黑色实线代表的是实际数据，红色长虚线代表模型1，蓝色短虚线代表模型2。模型1比较简单，就是直接将 _x_ 的取值区间分为3段，每段用一条水平直线（对应这段数据的平均值）来拟合数据。该模型方差较低，但是偏差较大。为什么我们说该模型方差较低呢？例如，又有一组也由<img src="Image00641.gif" alt> 产生的新的训练数据，我们也用这种多段直线的模型去拟合，模型本身的变化并不大。但是由图5-8也可以看出，该模型过于简单，对数据的拟合效果并不好，偏差较高。而在蓝色的模型中，我们用相邻前3点的平均来拟合，即<img src="Image00642.gif" alt> 。该模型对于这组数据拟合良好。但是，如果新产生另一组数据，模型本身就会发生较大变化。换言之，就是模型偏差小，但是方差大。</p>
<p><img src="Image00643.jpg" alt></p>
<p>图5-8 模型复杂度的实际例子</p>
<p>在我们讨论过的模型中，线性模型对应的( _Model Bias_ )2 =0，但是模型的方差较高。而岭回归、Lasso和Elastic Net对应的( _Model Bias_ )2 不为0，但是却将模型的方差缩小了，从而在整体上起到了降低式（5-32）中第二项和第三项之和的目的。注意，在岭回归、Lasso和Elastic Net这些模型中，损失函数中添加的正则化项使得所得系数有向0靠近的趋势，从而限制了模型的表达能力。实质上是降低了模型的复杂度，减少了模型的方差。这种通过调节模型偏差和方差的方法称为偏差-方差权衡 。</p>
<p>注意，并不是所有的数据都适合使用线性回归及其变体。在线性回归分析中，一个基本假设是因变量 _y_ 和自变量 _<strong>x</strong> _ 之间存在线性关系。以图5-9中的数据为例，图5-9（b）中的数据显然不适合使用回归分析，但是图5-9（a）中的数据是一个较好的回归分析的例子。</p>
<p><img src="Image00644.jpg" alt></p>
<p>图5-9 适用于及不适用于线性回归的数据</p>
<p>在本章中，我们主要讨论回归算法。事实上，在使用各类机器学习算法来分析数据时，一个普遍而且核心的问题就是：你需要多复杂的模型？或者换言之，如何选取合理的模型？一般而言，模型越复杂，方差越高，更容易在训练集上过拟合<br>（over-fitting）。反过来，简单的模型不易于过拟合，但是由于模型表达能力有限，会导致欠拟合 （under-fitting）的问题。在岭回归中，实际上牺牲了偏差来降低模型的方差。</p>
<p>图5-10显示了随着模型复杂度的增加，模型在训练集和测试集上的错误的变化。一般而言，我们在训练集上训练模型，然后计算所得模型在测试集上的性能。从图5-10中可以看出，当模型复杂度较低时，它在训练集和测试集上的错误都较大。此时模型的偏差较大，但是模型的方差较小。当模型的复杂度升高时，它在训练集上的错误会持续下降。但是，模型在测试数据上的错误一般会先下降；当模型在训练集上过拟合之后，我们可以看到模型在测试数据上的错误不降反升。在这一过程中，随着模型复杂度的升高，模型的偏差越来越小，但是模型的方差却越来越大。从图5-10中也可以看出，为了在测试集上取得最优的结果，模型的复杂度并不是越大越好。</p>
<p><img src="Image00645.jpg" alt></p>
<p>图5-10 模型方差和偏差的权衡</p>
<h2 id="5-5-案例分析"><a href="#5-5-案例分析" class="headerlink" title="5.5 案例分析"></a>5.5 案例分析</h2><p>在本节中，我们使用具体的数据集来讲解数据的读入和预处理，以及建立各种回归模型并计算相应RMSE和 _R_ 2 的全过程。具体来说，我们主要使用R中的<code>glmnet</code> 包 ⑤ 来建立前面讲解的多种线性回归模型。可以使用如下R命令来安装<code>glmnet</code> 包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&apos;glmnet&apos;, dependencies=T)</span><br></pre></td></tr></table></figure>
<p>关于包的安装和使用更多的细节可参见第2章。本节的所有代码都在文件Auto_MPG_regression.R中。感兴趣的读者可以直接下载并运行该程序。</p>
<h3 id="5-5-1-数据导入和探索"><a href="#5-5-1-数据导入和探索" class="headerlink" title="5.5.1 数据导入和探索"></a>5.5.1 数据导入和探索</h3><p>在本例中，我们使用Auto-MPG数据 ⑥ 。在该数据中，使用8个特征来预测汽车的MPG（miles per gallon），即每加仑 ⑦ 汽油可行驶英里数。表5-1总结了Auto-MPG数据集的各原始变量、含义及其类型。</p>
<p>表5-1 Auto-MPG数据集的各原始变量、含义及其类型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特 征 名</th>
<th>含 义</th>
<th>类 型  </th>
</tr>
</thead>
<tbody>
<tr>
<td>MPG</td>
<td>每加仑可行驶英里数</td>
<td>数值变量  </td>
</tr>
<tr>
<td>Cyl</td>
<td>发动机气缸数</td>
<td>数值变量  </td>
</tr>
<tr>
<td>Displacement</td>
<td>发动机排量</td>
<td>数值变量  </td>
</tr>
<tr>
<td>Horsepower</td>
<td>发动机马力</td>
<td>数值变量  </td>
</tr>
<tr>
<td>Weight</td>
<td>车辆重量</td>
<td>数值变量  </td>
</tr>
<tr>
<td>Acceleration</td>
<td>0～60英里/小时加速时间</td>
<td>数值变量  </td>
</tr>
<tr>
<td>Year</td>
<td>车辆年份</td>
<td>数值变量  </td>
</tr>
<tr>
<td>CountryCode</td>
<td>车辆原产地代码</td>
<td>分类变量  </td>
</tr>
<tr>
<td>Model</td>
<td>车辆型号</td>
<td>字符变量  </td>
</tr>
</tbody>
</table>
</div>
<p>由于数据是以csv文件提供的，因此可以使用R中的<code>read.csv</code> 函数来导入数据。具体代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f_in &lt;-&apos;Data/regression_autoMPG.csv&apos;</span><br><span class="line">D &lt;-read.csv(f_in)</span><br></pre></td></tr></table></figure>

</details>


<p>在进行数据分析时，很多情况下我们需要实际地查看数据。一种比较简单高效的方式是查看该数据集的前10行和最后10行。使用R中的<code>head</code> 函数可以查看数据集的前 _n_ 行。在下面的例子中，查看Auto-MPG数据集的前10行：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head(D, 10)</span><br></pre></td></tr></table></figure>

</details>


<p>显示结果如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">    　　　　MPG Cyl Displacement Horsepower Weight Acceleration Year CountryCode</span><br><span class="line">    　　 1　 18　 8　　　　　　307　　　　　130　 3504　　　　　 12.0　　70　　　　　 1</span><br><span class="line">    　　 2　 15　 8　　　　　　350　　　　　165　 3693　　　　　 11.5　　70　　　　　  1</span><br><span class="line">    　　 3　 18　 8　　　　　　318　　　　　150　 3436　　　　　 11.0　　70　　　　　  1</span><br><span class="line">    　　 4　 16　 8　　　　　　304　　　　　150　 3433　　　　　 12.0　　70　　　　　  1</span><br><span class="line">    　　 5　 17　 8　　　　　　302　　　　　140　 3449　　　　　 10.5　　70　　　　　  1</span><br><span class="line">    　　 6　 15　 8　　　　　　429　　　　　198　 4341　　　　　 10.0　　70　　　　　  1</span><br><span class="line">    　　 7　 14　 8　　　　　　454　　　　　220　 4354　　　　　　9.0　　70　　　　　   1</span><br><span class="line">    　　 8　 14　 8　　　　　　440　　　　　215　 4312　　　　　　8.5　　70　　　　　　  1</span><br><span class="line">    　　 9　 14　 8　　　　　　455　　　　　225　 4425　　　　　 10.0　　70　　　　　　  1</span><br><span class="line">    　　 10　15　 8　　　　　　390　　　　　190　 3850　　　　　　8.5　　70　　　　　　  1</span><br><span class="line">    　　　　　　　　　　　　　　　　Model</span><br><span class="line">    　　 1　chevrolet chevelle malibu</span><br><span class="line">    　　 2　　　　　 buick skylark 320</span><br><span class="line">    　　 3　　　　　plymouth satellite</span><br><span class="line">    　　 4　　　　　　　　amc rebel sst</span><br><span class="line">    　　 5　　　　　　　　　ford torino</span><br><span class="line">    　　 6　　　　　　ford galaxie 500</span><br><span class="line">    　　 7　　　　　　chevrolet impala</span><br><span class="line">    　　 8　　　　　 plymouth fury iii</span><br><span class="line">    　　 9　　　　　　pontiac catalina</span><br><span class="line">    　　 10　　　　amc ambassador dpl</span><br></pre></td></tr></table></figure>

</details>


<p>类似地，可以使用<code>tail</code> 函数查看该数据集的最后10行：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail(D, 10)</span><br></pre></td></tr></table></figure>

</details>


<p>还可以利用<code>class</code> 函数检查读入的数据框<code>D</code> 中每列的数据类型：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for(i in 1:ncol(D)) &#123;</span><br><span class="line">　msg &lt;-paste(&apos;col &apos;, i, &apos; and its type is &apos;, class(D[,i]))</span><br><span class="line">　print(msg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;col　1　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　2　and its type is　integer&quot;</span><br><span class="line">[1] &quot;col　3　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　4　and its type is　integer&quot;</span><br><span class="line">[1] &quot;col　5　and its type is　integer&quot;</span><br><span class="line">[1] &quot;col　6　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　7　and its type is　integer&quot;</span><br><span class="line">[1] &quot;col　8　and its type is　integer&quot;</span><br><span class="line">[1] &quot;col　9　and its type is　factor&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>此外，还可以使用<code>str</code> 函数来简洁地打印数据框<code>D</code> 的相关信息。在这个例子中，<code>str（D）</code> 的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; str(D)</span><br><span class="line">&apos;data.frame&apos;　 : 392 obs. of　9 variables:</span><br><span class="line"> $ MPG　　　　 : num　18 15 18 16 17 15 14 14 14 15 ...</span><br><span class="line"> $ Cyl　　　　 : int　8 8 8 8 8 8 8 8 8 8 ...</span><br><span class="line"> $ Displacement: num　307 350 318 304 302 429 454 440 455 390 ...</span><br><span class="line"> $ Horsepower　: int　130 165 150 150 140 198 220 215 225 190 ...</span><br><span class="line"> $ Weight　　　: int　3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...</span><br><span class="line"> $ Acceleration: num　12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...</span><br><span class="line"> $ Year　　　　: int　70 70 70 70 70 70 70 70 70 70 ...</span><br><span class="line"> $ CountryCode : int　1 1 1 1 1 1 1 1 1 1 ...</span><br><span class="line"> $ Model　　　 : Factor w/ 301 levels &quot;amc ambassador brougham&quot;,..: 49 36 230 </span><br><span class="line">14 160 141 54 222 240 2 ...</span><br></pre></td></tr></table></figure>

</details>


<h3 id="5-5-2-数据预处理"><a href="#5-5-2-数据预处理" class="headerlink" title="5.5.2 数据预处理"></a>5.5.2 数据预处理</h3><p>当使用<code>read.csv</code> 函数读入Auto-MPG数据集时，除了最后一列<code>Model</code> 因为含有字符被处理为因子 类型外，其他列都被转化为数值类型。变量<code>CountryCode</code> 由于其值属于集合{1,2,3}，因此也被R默认为数值类型。事实上，<code>CountryCode</code> 是分类变量，因为它的值只能是1、2、3之一，其中：</p>
<ul>
<li>1表示美国；</li>
<li>2表示欧洲；</li>
<li>3表示日本。</li>
</ul>
<p>这里的1～3之间并没有大小的关系，只是一个代码而已。注意到在使用线性回归时，我们要求所有的变量都是数值型的。因此，还需要进一步将<code>CountryCode</code> 变量转换为数值变量才能使用本章讨论的回归算法。后续章节将介绍可以直接处理分类变量的算法，如决策树、随机森林等。</p>
<p>将分类变量转换为数值变量的一个常用方法是将其转换为哑变量 。我们在第4章中已经讨论过哑变量。通常情况下，如果一个分类变量有 _k_ 个不同的值，则引入 _k_ -1个哑变量表示不同的取值。注意，如果引入 _k_ 个哑变量，则会引起变量的共线性，在线性回归中会导致所得的矩阵<img src="Image00562.gif" alt> 不可逆。在此例中，我们将<code>CountryCode</code> 转换为2个哑变量<code>CountryCode1</code> 和<code>CountryCode2</code> ，其中<code>CountryCode1</code> 为1表示原来的<code>CountryCode</code> 为1，<code>ContryCode2</code> 为1表示原来的<code>CountryCode</code> 为2。具体的转换关系可参见表5-2。</p>
<p>表5-2 <code>CountryCode</code> 变量转化为哑变量</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>CountryCode</th>
<th>CountryCode1</th>
<th>CountryCode2  </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0  </td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1  </td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0  </td>
</tr>
</tbody>
</table>
</div>
<p>在具体的实现中，使用如下R代码完成下面的转换。首先生成<code>CountryCode1</code> 和<code>CountryCode2</code> 变量，然后删除原来的<code>CountryCode</code> 变量。在我们提供的R代码中，utility_funcs.R文件中的函数<code>categorical2binary</code> 能够自动将分类变量转化为哑变量。同时在Auto-MPG数据中，因为<code>Model</code> 变量的用处不大，所以也直接删除该列。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">D$CountryCode1 &lt;-ifelse(D$CountryCode==1, 1, 0)</span><br><span class="line">D$CountryCode2 &lt;-ifelse(D$CountryCode==2, 1, 0)</span><br><span class="line">D$CountryCode &lt;-NULL</span><br><span class="line">D$Model &lt;-NULL</span><br></pre></td></tr></table></figure>

</details>


<h3 id="5-5-3-将数据集分成训练集和测试集"><a href="#5-5-3-将数据集分成训练集和测试集" class="headerlink" title="5.5.3 将数据集分成训练集和测试集"></a>5.5.3 将数据集分成训练集和测试集</h3><p>在机器学习中，通常将数据集分为训练集和测试集：通过训练集学习所要的模型；通过测试集可以测试所得的模型的性能。在下面的例子中，我们随机选取70%的数据作为训练集，并将剩余的30%作为测试集。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_ratio &lt;-0.7</span><br><span class="line">n_total &lt;-nrow(D)</span><br><span class="line">n_train &lt;-round(train_ratio * n_total)</span><br><span class="line">list_sample &lt;-sample(nrow(D))</span><br><span class="line">list_train &lt;-list_sample[1:n_train]</span><br><span class="line">list_test &lt;-list_sample[(n_train+1):n_total]</span><br><span class="line">D_train &lt;-D[list_train,]</span><br><span class="line">D_test &lt;-D[list_test,]</span><br><span class="line">y_train &lt;-D_train$MPG</span><br><span class="line">y_test &lt;-D_test$MPG</span><br></pre></td></tr></table></figure>

</details>


<p>有时候我们能够重复实验结果，在调用<code>sample</code> 函数之前可以使用<code>set.seed</code> 函数来设定随机数生成器的种子。注意，训练集和测试集的划分是随机的，读者在运行下面的程序时所得结果可能稍有不同。</p>
<h3 id="5-5-4-建立一个简单的线性回归模型"><a href="#5-5-4-建立一个简单的线性回归模型" class="headerlink" title="5.5.4 建立一个简单的线性回归模型"></a>5.5.4 建立一个简单的线性回归模型</h3><p>R中提供的<code>lm</code> 函数可以用来构造一个简单的线性回归模型。生成所得模型后，可以使用<code>predict</code> 函数直接得到测试数据上的预测。我们在文件regression_measures.R中提供的函数<code>R2_score</code> 和<code>RMSE</code> 可以分别计算所得预测在测试集上的 _R_ 2 和RMSE。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">M1 &lt;-lm(MPG˜., data = D_train)</span><br><span class="line">y_predict1 &lt;-predict(M1, newdata=D_test)</span><br><span class="line">source(&apos;regression_measures.R&apos;)</span><br><span class="line">R2_m1 &lt;-R2_score(y_test, y_predict1)</span><br><span class="line">rmse_m1 &lt;-RMSE(y_test, y_predict1)</span><br></pre></td></tr></table></figure>

</details>


<p>在使用<code>lm</code> 函数时，我们使用公式<code>MPG˜.</code> ，表示指定<code>MPG</code> 是我们要预测的目标变量，并且使用<code>D_train</code> 中的所有其他变量作为自变量。函数<code>lm</code> 返回已经训练好的模型<code>M1</code> ，这样就可以直接使用<code>predict</code> 函数来预测测试集<code>D_test</code> 上的<code>MPG</code> 。在使用<code>predict</code> 函数时，需要将<code>newdata</code> 参数设置为<code>D_test</code> 。</p>
<p>在R中，可以绘制出散点图来比较测试集上的预测值和真实值。图5-11显示了所得的散点图。在图5-11中还增加了一条直线<code>y_predict1=y_test</code> 来观察真实值和预测值之间的差距。下面是用来画图的R代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot(y_test, y_predict1, type=&apos;p&apos;)</span><br><span class="line">abline(c(0,1), col=&apos;red&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p><img src="Image00646.jpg" alt></p>
<p>图5-11 测试集上真实值和预测值的比较（其中 _x_ 轴对应真实值， _y_ 轴对应<code>lm</code> 模型的预测值。同时直线 _y_ = _x_ 也在图中标出）</p>
<h3 id="5-5-5-建立岭回归和Lasso模型"><a href="#5-5-5-建立岭回归和Lasso模型" class="headerlink" title="5.5.5 建立岭回归和Lasso模型"></a>5.5.5 建立岭回归和Lasso模型</h3><p>下面将介绍更加强大的<code>glmnet</code> 包 。该包涵盖了常用的适用于分类和回归的线性模型，包括：</p>
<ul>
<li>线性回归及变体，包括岭回归、Lasso和Elastic Net；</li>
<li>逻辑回归及其变体（将在第6章讨论）。</li>
</ul>
<p>下面给出使用<code>glmnet</code> 进行岭回归、Lasso和Elastic Net的具体例子。<code>glmnet</code> 包同时支持<img src="Image00583.gif" alt> 范数和<img src="Image00647.gif" alt> 范数的优化。换言之，对本章介绍的回归算法，我们都可以使用<code>glmnet</code> 包来完成计算。互联网上有关于<code>glmnet</code> 包的详细介绍 ⑧ ，本书将介绍<code>glmnet</code> 包的常用方法。</p>
<p>在使用<code>glmnet</code> 包进行回归分析时，主要使用如下两个函数：</p>
<ul>
<li><code>glmnet()</code> 函数 ，该函数名与包的名字相同，主要用于利用训练数据建立模型；</li>
<li><code>predict()</code> 函数 （实质上是<code>predict.glmnet</code> 函数），利用该函数和已经得到的模型，可以对新数据进行预测。</li>
</ul>
<p>在<code>glmnet</code> 函数中，<img src="Image00648.gif" alt> 范数和<img src="Image00647.gif" alt> 范数的权重由参数<code>alpha</code> 和<code>lambda</code> 来确定。其中，<img src="Image00648.gif" alt> 范数的权重为<code>lambda</code><br>×<code>alpha</code> ，而<img src="Image00647.gif" alt> 范数的权重为<code>lambda</code> ×(<code>1-alpha</code> )/<code>2</code> 。注意，在本章中，我们将<code>glmnet</code> 函数的<code>family</code> 参数设为<code>&#39;gaussian&#39;</code> ，表示要使用的是线性回归算法。事实上<code>glmnet</code> 也支持逻辑回归，只需要将<code>family</code> 参数设置为相应的值即可。我们在第6章会更加详细地讨论<code>glmnet</code> 函数的用法。</p>
<p>在下面的例子中计算岭回归，并将<img src="Image00647.gif" alt> 范数的权重设为0.25。注意，在岭回归中，需要将参数<code>alpha</code> 设为0。在使用<code>glmnet</code> 时，必须将自变量表示为矩阵的形式。由于<code>D_train</code> 是数据框类型，因此需要使用<code>as.matrix</code> 函数将其显式地转换为矩阵的形式。<code>glmnet</code> 的输出是一个<code>glmnet</code> 类，我们所要的解最终保存在<code>W2_1</code> 中。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">family &lt;-　&apos;gaussian&apos;</span><br><span class="line">alpha &lt;-0</span><br><span class="line">lambda &lt;-0.5</span><br><span class="line">M2_1 &lt;-glmnet(as.matrix(D_train[,2:ncol(D_train)]), </span><br><span class="line">　　　　　　　 y_train, family = family, </span><br><span class="line">　　　　　　　 lambda=lambda, alpha=alpha)</span><br><span class="line">W2_1 &lt;-M2_1$beta</span><br></pre></td></tr></table></figure>

</details>


<p>下面这个例子计算Lasso，并将 _L_ 1 范数的权重设为1。由于只需要设置 _L_ 1 范数的权重，因此将<code>alpha</code> 和<code>lambda</code> 都设为1。下面是对应的R代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">family &lt;-&apos;gaussian&apos;</span><br><span class="line">alpha &lt;-1</span><br><span class="line">lambda &lt;-1</span><br><span class="line">M1_1 &lt;-glmnet(as.matrix(D_train[,2:ncol(D_train)]), </span><br><span class="line">　　　　　　　 y_train, family = family, </span><br><span class="line">　　　　　　　 lambda=lambda, alpha=alpha)</span><br><span class="line">W1_1 &lt;-M1_1$beta</span><br></pre></td></tr></table></figure>

</details>


<p>所得的解<code>W1_1</code> 为稀疏解，表5-3是所得<code>W1_1</code> 的值。从表5-3中可以看出，<code>W1_1</code> 有4个分量的值为0。</p>
<p>表5-3 <code>W1_1</code> 中各分量的值</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>分 量</th>
<th>值  </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Cyl</code></td>
<td>0  </td>
</tr>
<tr>
<td><code>Displacement</code></td>
<td>0  </td>
</tr>
<tr>
<td><code>Horsepower</code></td>
<td>−0.00309348156260112  </td>
</tr>
<tr>
<td><code>Weight</code></td>
<td>−0.00543817447042681  </td>
</tr>
<tr>
<td><code>Accel e ration</code></td>
<td>0  </td>
</tr>
<tr>
<td><code>Year</code></td>
<td>0.544922253902601  </td>
</tr>
<tr>
<td><code>CountryCode1</code></td>
<td>−0.784951577206556  </td>
</tr>
<tr>
<td><code>CountryCode2</code></td>
<td>0  </td>
</tr>
</tbody>
</table>
</div>
<p>类似地，可以同时使用<img src="Image00583.gif" alt> 范数和<img src="Image00573.gif" alt> 范数，从而得到Elastic Net模型。在下面的例子中，<img src="Image00583.gif" alt> 范数的权重为1，<img src="Image00573.gif" alt> 范数的权重为2。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">family &lt;-&apos;gaussian&apos;</span><br><span class="line">alpha &lt;-0.2</span><br><span class="line">lambda &lt;-5</span><br><span class="line">M_elastic &lt;-glmnet(as.matrix(D_train[,2:ncol(D_train)]),</span><br><span class="line">　　　　　　　　　　y_train, family = family, </span><br><span class="line">　　　　　　　　　　lambda=lambda, alpha=alpha)</span><br><span class="line">W_elastic &lt;-M_elastic$beta</span><br></pre></td></tr></table></figure>

</details>


<p>在所有的这些例子中，都可以使用<code>predict</code> 函数来得到训练所得模型在测试集上的预测值。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_predict_elastic &lt;-predict(M_elastic, </span><br><span class="line">　　　　　　　　　　　　　　 newx=as.matrix(D_test[,2:ncol(D_test)]))</span><br></pre></td></tr></table></figure>

</details>


<p>与<code>lm</code> 函数稍稍不同，在这里的<code>predict</code> 函数中我们需要使用<code>newx</code> 参数来指定测试集的数据。注意，与<code>glmnet</code> 函数类似，这里输入自变量的时候也需要显式地转换为矩阵的形式。</p>
<h3 id="5-5-6-选取合适的模型"><a href="#5-5-6-选取合适的模型" class="headerlink" title="5.5.6 选取合适的模型"></a>5.5.6 选取合适的模型</h3><p>在实际使用回归算法时，大多数时候我们不知道 _L_ 1 范数和 _L_ 2 范数的具体权重。更多时候，我们需要测试一系列权值并从中选取最优的模型。<code>glmnet</code> 的好处是可以直接输入一系列的参数值，并自动计算出每个参数值对应的解。</p>
<p>在下面这个例子中，我们测试岭回归，并将<code>lambda</code> 的值从1以0.05的步长递减到0（共21个值）。<code>glmnet</code> 对于每个<code>lambda</code> 值都建立一个线性模型并将其存于<code>M_ridge_list</code> 中。最后我们使用<code>predict</code> 函数，使用所得的21个模型对测试集进行处理。所得的<code>y_predict_test_ridge</code> 是一个矩阵，每一列对应一个模型的预测值。通过同时测试多个参数，我们可以选取最优的<code>lambda</code> 值。一般来说，在机器学习中，我们通常使用交叉检验<br>（cross-validation）将训练集分为多个子集组成训练/测试集，并测试多个参数，最后选取性能最好的参数。在回归分析中，我们可以使用可决系数或者RMSE作为选取标准。我们将在第6章详细讨论交叉检验的原理和具体流程。在这里，我们直接比较各个<code>lambda</code> 值所得的模型在测试集上的RMSE，并保存在向量<code>RMSE_test_list_ridge</code> 中。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">lambda_list &lt;-seq(1, 0, by=-0.05)</span><br><span class="line">lambda_num &lt;-length(lambda_list)</span><br><span class="line">X_train &lt;-as.matrix(D_train[, 2:ncol(D_train)])</span><br><span class="line">X_test &lt;-as.matrix(D_test[, 2:ncol(D_test)])</span><br><span class="line">RMSE_test_list_ridge &lt;-rep(0, lambda_num)</span><br><span class="line">family &lt;-&apos;gaussian&apos;</span><br><span class="line">alpha &lt;-0</span><br><span class="line">M_ridge_list &lt;-glmnet(X_train, y_train, </span><br><span class="line">　　　　　　　　　　　family = family,</span><br><span class="line">　　　　　　　　　　　lambda=lambda_list, </span><br><span class="line">　　　　　　　　　　　alpha=alpha)</span><br><span class="line">y_predict_test_ridge &lt;-predict(M_ridge_list, X_test)</span><br><span class="line">for (i in 1:lambda_num) &#123;</span><br><span class="line">　　RMSE_test_list_ridge[i] &lt;-RMSE(y_test, </span><br><span class="line">　　　　　　　　　　　　　　　　　 y_predict_test_ridge[,i])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>


<p>这里我们将计算RMSE的函数<code>RMSE</code> 保存在文件regression_measures.R，因此，在前面我们使用<code>source(&#39;regression_measures.R&#39;)</code> 来载入该函数。</p>
<p>类似地，可以使用同样的<code>lambda</code> 值来训练Lasso模型。R代码如下。主要区别在于将<code>alpha</code> 的值设为1。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">RMSE_test_list_lasso &lt;-rep(0, lambda_num)</span><br><span class="line">family &lt;-&apos;gaussian&apos;</span><br><span class="line">alpha &lt;-1</span><br><span class="line">M_lasso_list &lt;-glmnet(X_train, y_train,</span><br><span class="line">　　　　　　　　　　　family = family,</span><br><span class="line">　　　　　　　　　　　lambda=lambda_list,</span><br><span class="line">　　　　　　　　　　　alpha=alpha)</span><br><span class="line">y_predict_test_lasso &lt;-predict(M_lasso_list, X_test)</span><br><span class="line">for (i in 1:lambda_num) &#123;</span><br><span class="line">　　RMSE_test_list_lasso[i] &lt;-RMSE(y_test,</span><br><span class="line">　　　　　　　　　　　　　　　　　　y_predict_test_lasso[,i])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>


<p>图5-12显示了岭回归和Lasso两种算法在测试集上RMSE的变化。注意，当<code>lambda</code> 的值为0时，就是线性回归。从图5-12中可以看出，当<code>lambda</code> 为0.35时，岭回归和Lasso同时取得最好的RMSE，但是Lasso的表现更好。从图5-12中还可以看出，岭回归的RMSE对<code>lambda</code> 不是很敏感，但是Lasso则比较敏感。在实际中，一定要根据自己的实际情况选择合适的模型。</p>
<p><img src="Image00649.jpg" alt></p>
<p>图5-12 岭回归和Lasso在测试集上的RMSE比较</p>
<h3 id="5-5-7-构造新的变量"><a href="#5-5-7-构造新的变量" class="headerlink" title="5.5.7 构造新的变量"></a>5.5.7 构造新的变量</h3><p>在使用线性回归或者相关算法时，我们假设目标和变量之间存在线性关系。如果实际数据中线性关系并不存在的话，我们并不能直接使用线性回归模型。相应地，我们可以根据现有数据构造更多的变量，然后使用线性回归模型。一个简单的例子是假设我们有一系列由<img src="Image00650.gif" alt> 生成的数据集。显然，用关于 _x_ 的线性模型来预测 _y_ 不能解决问题。但是，如果我们用关于<img src="Image00651.gif" alt> 的线性模型来预测 _y_ ，则可以取得很好的性能表现。</p>
<p>在实际中，应根据数据的具体特征和具体的问题来构造新的变量。此外，一定要关注变量间的相互关系。例如，因变量 _y_ 与变量<img src="Image00508.gif" alt> 、<img src="Image00509.gif" alt> 都不存在线性关系，但是与<img src="Image00652.gif" alt> 存在线性关系。在这种情况下，新变量<img src="Image00652.gif" alt> 对于提升线性回归性能的效果非常明显，但是需要读者对于数据有更加深入的了解和认识。</p>
<h2 id="5-6-小结"><a href="#5-6-小结" class="headerlink" title="5.6 小结"></a>5.6 小结</h2><p>本章介绍了几种基本的回归算法，同时也引入了机器学习中的许多重要概念，包括：</p>
<ul>
<li>损失函数；</li>
<li>模型的偏差-方差权衡；</li>
<li>模型的复杂度；</li>
<li>模型的评价和选取；</li>
<li>变量的选取。</li>
</ul>
<p>在后面的章节中，这些概念还会被反复提到。举个简单的例子，通过选用不同的损失函数，可推导得到一种常用的分类算法——逻辑回归。</p>
<p>对于本章所介绍的线性回归算法，在实际中使用最多的是岭回归。它不仅计算简单，而且能取得较好的效果。Lasso是近年来研究的热点之一，其最吸引人的地方是能够得到稀疏解，从而选取最有用的变量。由于本章介绍的算法都是线性算法，而且要求所有的变量都是数值型的，因此在实际问题中，很多时候需要先对数据进行预处理，如需要将分类变量转化为哑变量等。此外，为了使用线性模型取得更好的效果，还需要将变量进行转化以生成更多的新变量。为了克服线性模型表达能力有限的问题，我们在后继章节将会介绍非线性的回归算法。</p>
<hr>
<p>① 事实上，计算矩阵 _<strong>A</strong> _ 的逆矩阵的精确度是由 _<strong>A</strong> _ 的条件数（condition number）决定的。感兴趣的读者可以查阅参考文献[24]中相关章节内容。</p>
<p>② 在贝叶斯统计中，岭回归的基本假设是误差项 的先验分布也是正态分布，本书不做详细推导。</p>
<p>③ 数据可在UCI直接下载，网址为<a href="https://archive.ics.uci.edu/ml/datasets/Housing" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/Housing</a> 。</p>
<p>④ Lasso的全称是Least Absolute Shrinkage and Selection Operator，参见R. Tibshirani所著《Regression Shrinkage and Selection via the Lasso》（Journal of the Royal Statistical Society, Series B, vol. 58, no. 1, pp. 267-288, 1996）。</p>
<p>⑤ <a href="https://cran.r-project.org/web/packages/glmnet/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/glmnet/index.html</a></p>
<p>⑥ 该数据集可在<a href="https://archive.ics.uci.edu/ml/datasets/Auto+MPG" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/Auto+MPG</a> 下载。</p>
<p>⑦ 加仑是容量单位，1加仑=3.78升。</p>
<p>⑧ <a href="http://cran.r-project.org/web/packages/glmnet/glmnet.pdf" target="_blank" rel="noopener">http://cran.r-project.org/web/packages/glmnet/glmnet.pdf</a></p>
<h1 id="第6章-分类算法"><a href="#第6章-分类算法" class="headerlink" title="第6章 分类算法"></a>第6章 分类算法</h1><p>在分类 （classification）中，我们的任务是将对象 （object）归类到已经定义好的若干类中。如果只有两类，则称为两类分类 （binary classification）。如果多于两类，则称为多类分类 （multi-class classification）。与在回归问题中的讨论类似，每个对象都由一系列特征描述。</p>
<p>分类算法在实际中有大量的应用，同时它也是很多更复杂算法的基础。本章介绍分类问题的基本思想及常用的分类算法，特别是以下3种算法：</p>
<ul>
<li>决策树；</li>
<li>逻辑回归；</li>
<li>支持向量机。</li>
</ul>
<p>本章首先讨论决策树。注意，决策树不是线性分类器，但它原理简单，同时也是很多复杂分类算法的基础，因此首先予以讨论。此外，我们还会介绍损失函数 （loss function），并讨论不同的分类算法如何对应不同的损失函数，以及如何使用正则化项来控制模型的复杂度。与回归算法的讨论类似，我们也讨论分类算法的诸多评价标准。</p>
<p>为了解决实际中更复杂的分类问题，我们将会详细讨论如何解决不平衡分类问题。在本章中，我们同时会介绍R中对应的软件包，这样读者可以直接尝试使用各种分类算法。特别地，我们将介绍<code>caret</code> 包，这样读者能够简单地使用交叉检验来为机器学习中的大量常用算法选取最优参数。</p>
<h2 id="6-1-分类的基本思想"><a href="#6-1-分类的基本思想" class="headerlink" title="6.1 分类的基本思想"></a>6.1 分类的基本思想</h2><p>在分类问题中，每个样本数据一般表示成<img src="Image00653.gif" alt> 的形式，这里 <strong>_x_
</strong> 是特征，而 _y_ 是对应的类标 （class label）。在两类问题中，我们通常使用整数来表示不同的 _y_ 值。 _y_ 不同的取值包括[<img src="Image00654.gif" alt> ]或者<img src="Image00655.gif" alt> 。一般而言，<img src="Image00656.gif" alt> 表示正类，0或者-1表示负类。</p>
<p>表6-1给出了著名的<code>iris</code> 数据集中的15个样本，其中前4列是特征，包括<code>Sepal.Length</code> （萼片长度）、<code>Sepal.Width</code><br>（萼片宽度）、<code>Petal.Length</code> （花瓣长度）和<code>Petal.Width</code> （花瓣宽度）；最后一列<code>Species</code> 是类标，包括3种不同的类别：<code>setosa</code> 、<code>versicolor</code> 和<code>virginica</code> ，因此这是一个多类分类问题。在这个例子中，所有的特征都是数值变量。表6-2给出了另一个数据集<code>income</code> 。该数据集搜集了一些美国人的教育程度、家庭状况、工作时间和年收入等数据。其中的特征既有数值变量，也有分类变量；最后一列<code>income</code> 是类标，有两个不同的值≤50K和&gt;50K，因此这是一个两类分类问题。</p>
<p>表6-1 <code>iris</code> 数据集（其中前4列是特征，最后一列是类标）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>Sepal.Length</code></th>
<th><code>Sepal.Width</code></th>
<th><code>Petal.Length</code></th>
<th><code>Petal.Width</code></th>
<th><code>Species</code>  </th>
</tr>
</thead>
<tbody>
<tr>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa  </td>
</tr>
<tr>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa  </td>
</tr>
<tr>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
<td>setosa  </td>
</tr>
<tr>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
<td>setosa  </td>
</tr>
<tr>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa  </td>
</tr>
<tr>
<td>7.0</td>
<td>3.2</td>
<td>4.7</td>
<td>1.4</td>
<td>versicolor  </td>
</tr>
<tr>
<td>6.4</td>
<td>3.2</td>
<td>4.5</td>
<td>1.5</td>
<td>versicolor  </td>
</tr>
<tr>
<td>6.9</td>
<td>3.1</td>
<td>4.9</td>
<td>1.5</td>
<td>versicolor  </td>
</tr>
<tr>
<td>5.5</td>
<td>2.3</td>
<td>4.0</td>
<td>1.3</td>
<td>versicolor  </td>
</tr>
<tr>
<td>6.5</td>
<td>2.8</td>
<td>4.6</td>
<td>1.5</td>
<td>versicolor  </td>
</tr>
<tr>
<td>6.3</td>
<td>3.3</td>
<td>6.0</td>
<td>2.5</td>
<td>virginica  </td>
</tr>
<tr>
<td>5.8</td>
<td>2.7</td>
<td>5.1</td>
<td>1.9</td>
<td>virginica  </td>
</tr>
<tr>
<td>7.1</td>
<td>3.0</td>
<td>5.9</td>
<td>2.1</td>
<td>virginica  </td>
</tr>
<tr>
<td>6.3</td>
<td>2.9</td>
<td>5.6</td>
<td>1.8</td>
<td>virginica  </td>
</tr>
<tr>
<td>6.5</td>
<td>3.0</td>
<td>5.8</td>
<td>2.2</td>
<td>virginica  </td>
</tr>
</tbody>
</table>
</div>
<p>在分类中，我们可以将第 _i_ 个对象的特征用 _d_ 维向量<img src="Image00501.gif" alt> 表示。在分类问题中，我们的目标是要根据每个样本的特征 <strong>_x_ </strong> 构建一个函数<img src="Image00506.gif" alt> ，使得<img src="Image00506.gif" alt> 能够输出 <strong>_x_ </strong> 对应的类标。同回归问题的处理类似，在实践中通常也将数据分为训练集 （training set）和测试集 （test set）。利用训练集，使用学习算法得到相应的模型。然后，可以将模型应用到测试集上，估计所得模型的性能。</p>
<p>根据上面的描述，我们可以知道分类问题和回归问题是非常相似的。它们的区别在于，在分类问题中，类标必须是离散的，而在回归问题中，目标变量是连续的。</p>
<p>在解决分类问题时，不同的算法有不同的策略。一类方法称为“懒学习”。在懒学习中，我们在得到了训练集时什么也不干。等到测试集到来时，我们再利用训练集对测试集进行处理。懒学习的典型例子是 _k_ 近邻 （ _k_ -Nearest Neighbor， _k_ NN）算法。下面我们简单地讨论一下 _k_ NN算法。</p>
<p>在 _k_ NN中，测试集中每个样本的类别由训练集中与它最相似的 _k_ 个样本决定。在得到最相似的 _k_ 个样本后，我们使用投票的方法将 _k_ 个样本中出现频率最高的类标作为该样本的类标。通常在 _k_ NN中我们需要保存整个训练集并且要有效地为新样本从训练集中找到最相似的样本，一般来讲，算法的效率较低。如果训练集中某些样本的类标不准确或者存在噪声的话，都会影响最终的分类结果。</p>
<p>与懒学习相对的是“积极学习”。在积极学习中，我们首先挖掘训练数据，从中构建相应的模型。而在懒学习中，我们拖延学习这一过程直至测试集中新的样本到来。在积极学习中，当新的样本到来时，我们直接使用训练所得的模型得到相应的结果。因此，在积极学习中，主要的时间花在如何根据训练集训练模型。一旦得到模型，处理新的样本一般非常迅速。</p>
<p>本章介绍的分类方法都是积极学习的方法。</p>
<p>表6-2 <code>income</code> 数据集（其中最后一列是类标，表示该样本的年收入是否高于5万美元）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>age</code></th>
<th><code>workclass</code></th>
<th><code>fnlwgt</code></th>
<th><code>education</code></th>
<th><code>ed-num</code></th>
<th><code>marital-status</code></th>
<th><code>occupation</code></th>
<th><code>relationship</code></th>
<th><code>race</code></th>
<th><code>sex</code></th>
<th><code>capital-gain</code></th>
<th><code>capital-loss</code></th>
<th><code>hours-per-week</code></th>
<th><code>native-country</code></th>
<th><code>income</code>  </th>
</tr>
</thead>
<tbody>
<tr>
<td>39</td>
<td>State-gov</td>
<td>77516</td>
<td>Bachelors</td>
<td>13</td>
<td>Never-married</td>
<td>Adm-clerical</td>
<td>Not-in-family</td>
<td>White</td>
<td>Male</td>
<td>2174</td>
<td>0</td>
<td>40</td>
<td>United-States</td>
<td>≤50K  </td>
</tr>
<tr>
<td>50</td>
<td>Self-emp-not-inc</td>
<td>83311</td>
<td>Bachelors</td>
<td>13</td>
<td>Married-civ-spouse</td>
<td>Exec-managerial</td>
<td>Husband</td>
<td>White</td>
<td>Male</td>
<td>0</td>
<td>0</td>
<td>13</td>
<td>United-States</td>
<td>≤50K  </td>
</tr>
<tr>
<td>38</td>
<td>Private</td>
<td>215646</td>
<td>HS-grad</td>
<td>9</td>
<td>Divorced</td>
<td>Handlers-cleaners</td>
<td>Not-in-family</td>
<td>White</td>
<td>Male</td>
<td>0</td>
<td>0</td>
<td>40</td>
<td>United-States</td>
<td>≤50K  </td>
</tr>
<tr>
<td>53</td>
<td>Private</td>
<td>234721</td>
<td>11th</td>
<td>7</td>
<td>Married-civ-spouse</td>
<td>Handlers-cleaners</td>
<td>Husband</td>
<td>Black</td>
<td>Male</td>
<td>0</td>
<td>0</td>
<td>40</td>
<td>United-States</td>
<td>≤50K  </td>
</tr>
<tr>
<td>28</td>
<td>Private</td>
<td>338409</td>
<td>Bachelors</td>
<td>13</td>
<td>Married-civ-spouse</td>
<td>Prof-specialty</td>
<td>Wife</td>
<td>Black</td>
<td>Female</td>
<td>0</td>
<td>0</td>
<td>40</td>
<td>Cuba</td>
<td>≤50K  </td>
</tr>
<tr>
<td>37</td>
<td>Private</td>
<td>284582</td>
<td>Masters</td>
<td>14</td>
<td>Married-civ-spouse</td>
<td>Exec-managerial</td>
<td>Wife</td>
<td>White</td>
<td>Female</td>
<td>0</td>
<td>0</td>
<td>40</td>
<td>United-States</td>
<td>≤50K  </td>
</tr>
<tr>
<td>49</td>
<td>Private</td>
<td>160187</td>
<td>9th</td>
<td>5</td>
<td>Married-spouse-absent</td>
<td>Other-service</td>
<td>Not-in-family</td>
<td>Black</td>
<td>Female</td>
<td>0</td>
<td>0</td>
<td>16</td>
<td>Jamaica</td>
<td>≤50K  </td>
</tr>
<tr>
<td>52</td>
<td>Self-emp-not-inc</td>
<td>209642</td>
<td>HS-grad</td>
<td>9</td>
<td>Married-civ-spouse</td>
<td>Exec-managerial</td>
<td>Husband</td>
<td>White</td>
<td>Male</td>
<td>0</td>
<td>0</td>
<td>45</td>
<td>United-States</td>
<td>&gt;50K  </td>
</tr>
<tr>
<td>31</td>
<td>Private</td>
<td>45781</td>
<td>Masters</td>
<td>14</td>
<td>Never-married</td>
<td>Prof-specialty</td>
<td>Not-in-family</td>
<td>White</td>
<td>Female</td>
<td>14084</td>
<td>0</td>
<td>50</td>
<td>United-States</td>
<td>&gt;50K  </td>
</tr>
<tr>
<td>42</td>
<td>Private</td>
<td>159449</td>
<td>Bachelors</td>
<td>13</td>
<td>Married-civ-spouse</td>
<td>Exec-managerial</td>
<td>Husband</td>
<td>White</td>
<td>Male</td>
<td>5178</td>
<td>0</td>
<td>40</td>
<td>United-States</td>
<td>&gt;50K  </td>
</tr>
</tbody>
</table>
</div>
<h2 id="6-2-决策树"><a href="#6-2-决策树" class="headerlink" title="6.2 决策树"></a>6.2 决策树</h2><p>本节我们讨论决策树 （decision tree）。决策树原理简单，是处理分类问题的最基本的算法之一。在实际中，我们很少直接使用决策树来处理分类问题。但多种基于决策树的分类算法，如随机森林和提升树，因其简单易用和良好的性能而在工业界得到了广泛应用。因此，本节着重介绍决策树的基本原理，为以后介绍随机森林和提升树打好基础。</p>
<h3 id="6-2-1-基本原理"><a href="#6-2-1-基本原理" class="headerlink" title="6.2.1 基本原理"></a>6.2.1 基本原理</h3><p>顾名思义，决策树就是将规则以树的形式组织起来，从而对样本进行分类。在决策树中，有以下两种类型的结点。</p>
<ul>
<li>非叶结点：每个非叶结点对应于一个特征或者属性，且有两个或多个子结点。</li>
<li>叶结点：每个叶结点对应一个类别。</li>
</ul>
<p>在决策树中，每条边对应着一个判定条件。</p>
<p>图6-1给出了根据<code>iris</code> 数据集，使用R中的<code>rpart</code> 包构建的决策树。在该决策树中，根结点对应于特征<code>Petal.Length</code> ，根结点所连接的两条边分别对应判定条件<code>Petal.Length&lt;2.45</code> 和<code>Petal.Length&gt;=2.45</code> 。最左下角的叶结点所对应的类标是<code>setosa</code> 。</p>
<p><img src="Image00657.jpg" alt></p>
<p>图6-1 使用R中的<code>rpart</code> 软件包对<code>iris</code> 数据集生成的决策树</p>
<p>利用已有的决策树，可以很简单地将一个新的样本分类。基本原理是从根结点出发，不断地测试对应的特征和相关的条件，直到到达一个叶结点为止。这样，我们可以将叶结点所对应的类标作为该样本的类标。决策树的计算复杂度低，特别是对新数据进行判定时非常快。</p>
<p>下面我们简单讨论一下如何利用图6-1中的决策树将实际样本分类。考虑表6-3中的第一个测试样本。首先我们考虑根结点对应的特征<code>Petal.Length</code> 和条件<code>Petal.Length&lt;2.45</code> 。由于该样本对应的<code>Petal.Length</code> 值为1.4，因此我们要沿着根结点左侧的边到达相应结点。注意，该结点是叶结点且对应的类标为<code>setosa</code> ，按照该决策树，我们应该将该样本分类为<code>setosa</code> 。类似地，我们可以应用该决策树处理表6-3中的第二个样本。由于该样本对应的<code>Petal.Length</code> 值为6.0，因此，从根结点出发，我们应该沿着右侧的边到达相应结点。该结点不是叶结点，其对应的特征是<code>Petal.Width</code> 。由于该样本对应的<code>Petal.Width</code> 值为2.5，满足右侧的边所对应的条件（即<code>Petal.Width&gt;=1.75</code><br>），因此我们应该沿着右侧的边到达最右下角的叶结点。注意，该结点对应的类标是<code>virginica</code> ，因此，我们应该将该样本分类为<code>virginica</code> 。可以看出，简单的决策树模型成功地分类了表6-3中的两个样本。</p>
<p>表6-3 测试样本</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>Sepal.Length</code></th>
<th><code>Sepal.Width</code></th>
<th><code>Petal.Length</code></th>
<th><code>Petal.Width</code></th>
<th><code>Species</code>  </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>5.1</code></td>
<td><code>3.5</code></td>
<td><code>1.4</code></td>
<td><code>0.2</code></td>
<td><code>setosa</code>  </td>
</tr>
<tr>
<td><code>6.3</code></td>
<td><code>3.3</code></td>
<td><code>6.0</code></td>
<td><code>2.5</code></td>
<td><code>virginica</code>  </td>
</tr>
</tbody>
</table>
</div>
<h3 id="6-2-2-决策树学习"><a href="#6-2-2-决策树学习" class="headerlink" title="6.2.2 决策树学习"></a>6.2.2 决策树学习</h3><p>本节讨论如何从训练数据中学习得到决策树模型。</p>
<p>从理论上讲，对于一个给定的训练数据集，由于可能的决策树模型太多，因此在实际中我们无法遍历所有可能的决策树以确定最优的决策树。这里我们可做一个简单的估算。假设输入数据有 _d_ 个特征（或属性），则第一层的根结点有 _d_ 种不同的选择。在决定了根结点对应的特征后，假设后面我们不选择重复的属性。第二层中的每个非叶结点对应的属性又有<img src="Image00658.gif" alt> 种不同的选择。因此，所有可能的决策树的总数是 _d_ 的指数级别的。</p>
<p>实际中，为了有效地构建决策树模型 ，我们通常采用自顶向下的“贪心”算法。在这里我们给出贪心算法的基本流程。注意，在不同的决策树算法（如C4.5和CART）中，生成决策树的算法稍有不同，这里我们仅介绍一般性的流程。</p>
<p>在使用训练集训练决策树时，每个结点都对应着训练集的一个子集。首先，我们将整个训练集都赋给根结点；然后，考虑所有的属性及判定条件，使得训练集能够被划分为两个或多个部分，并为每部分构造一个新的结点；最后，递归地处理每个新结点，直到不能再划分为止。在每次划分后，我们希望每部分的训练集“纯度”尽可能高一些。最理想的情况是：如果问题是两类分类问题，划分后我们希望每部分都只包含一类的所有样本。具体的步骤如算法6-1所示。</p>
<p>算法6-1 决策树的生成</p>
<blockquote>
<p>（1）创建空的结点集合 _N_ 、边集合 _E_ 。 &gt; &gt; （2）创建根结点，将其加入集合 _N_ ，并将整个训练集赋给该结点。 &gt; &gt; （3）对集合 _N_ 中的每个未处理结点 _i_ ，设其所对应的训练集为<img src="Image00659.gif" alt> ： &gt; &gt;   <em> 如果<img src="Image00659.gif" alt> 中的样本都属于同一类，则将结点 _i_ &gt; 标识为叶结点，且将该结点的类标标为<img src="Image00659.gif" alt> 中样本所对应的类标。 &gt; &gt;   </em> 如果<img src="Image00659.gif" alt> 中的样本属于多个类，选择一个特征及对应的判定条件将<img src="Image00659.gif" alt> 分为两个或者多个子集；为每个子集构建相应的子结点，加入集合 _N_ ，并将<img src="Image00659.gif" alt> 中相应的数据赋给对应子结点；以判定条件构造结点 _i_ 到各子结点的边，并加入集合 _E_ 。 &gt; &gt;</p>
<blockquote>
<p>（4）重复执行步骤3，直到 _N_ 中的所有结点都得到处理。此时结点集合 _N_ 与边集合 _E_ 联合构成所求决策树。</p>
</blockquote>
</blockquote>
<p>算法6-1比较粗略，但基本上所有实际中的决策树算法 都可以认为是算法6-1的细化版。在细化算法6-1的过程中，从训练集生成决策树的两个关键问题如下：</p>
<p>（1）在构建新结点时如何选择特征及对应的判定条件；</p>
<p>（2）如何停止构建新结点？或者换言之，什么样的结点应该被认为是叶结点？理想情况下，所有样本的类别都一致时，我们可以停止构建新结点，但在实际情况中一般难以做到。</p>
<p>在下面的章节中，我们将详细讨论这两个问题。对于第一个问题，我们介绍CART中使用的基尼指数 的增益和C4.5中使用的信息增益，这里CART和C4.5是常用的两种商用决策树实现。对于第二个问题，通常采取限制叶结点总数或叶结点所对应的训练集中样本数目的办法。在下面关于剪枝的讨论中，我们将予以详细介绍。</p>
<h4 id="1．熵与基尼指数"><a href="#1．熵与基尼指数" class="headerlink" title="1．熵与基尼指数"></a>1．熵与基尼指数</h4><p>如前所述，在决策树的生成过程中，我们实际上希望在划分树的结点所对应的样本集合后，所得每个子集的“纯度”尽可能高，尽量属于一个类。为了度量数据的“不纯程度”或“多样性”，可引入信息论中熵的概念。对于一个给定的数据集，我们可以根据集合中样本的类别来计算该数据集对应的熵 。直观地讲，我们想要用熵来帮助我们区分一个数据集是否是：</p>
<p>（1）已经完全分类好的数据集；</p>
<p>（2）已经近似分类好的数据集；</p>
<p>（3）还需要进一步分类的数据集。</p>
<p>首先考虑两类分类问题。严格地讲，对于数据集 _S_ ，其对应的熵的定义如下：</p>
<p><img src="Image00660.gif" alt></p>
<p>（6-1）</p>
<p>这里<img src="Image00661.gif" alt> 是样本属于正类的概率，<img src="Image00662.gif" alt> 是样本属于负类的概率。特别地，当<img src="Image00663.gif" alt> 或<img src="Image00664.gif" alt> 时，定义熵为0。我们可以将上面熵的定义推广到多类问题中。假设有 _c_ 个类，使用<img src="Image00665.gif" alt> 表示样本属于第 _i_ 个类的概率，则对应的熵可定义为：</p>
<p><img src="Image00666.gif" alt></p>
<p>（6-2）</p>
<p>图6-2中的实线给出了两类问题中熵的图像。其中 _x_ 轴是<img src="Image00661.gif" alt> ， _y_ 轴是对应的熵。从图6-2中可以看出，当<img src="Image00661.gif" alt> 的值为0.5时，熵达到最大值1。当<img src="Image00661.gif" alt> 从0增加到0.5时，熵是单调递增的；当<img src="Image00661.gif" alt> 从0.5增加到1时，熵是单调递减的。</p>
<p><img src="Image00667.jpg" alt></p>
<p>图6-2 熵和基尼指数的变化曲线</p>
<p>根据熵的计算公式和对应图像可知：当数据集中所有的样本点都属于同一类时，所得的熵为0；在两类问题中，当样本集中每类各占一半时，熵取最大值<img src="Image00668.gif" alt> ；当某一类在样本集中占大多数时，熵的值介于两者之间。</p>
<p>根据熵的定义，在分类问题中，我们需要使熵尽可能小。换言之，就是使得数据的“纯度”尽可能高。在建立决策树时，我们不断地建立新规则，使得划分之后数据集的“纯度”更高，即熵更小。因此，从某种程度上讲，我们可以把熵视为一种损失函数。</p>
<p>另外一种广泛使用的度量是基尼指数 （Gini index）。在两类分类问题中，基尼指数定义为：</p>
<p><img src="Image00669.gif" alt></p>
<p>（6-3）</p>
<p>在多类问题中，对应的定义为：</p>
<p><img src="Image00670.gif" alt></p>
<p>（6-4）</p>
<p>图6-2中的虚线给出了基尼指数的图像。从图6-2中可以看出，基尼指数也是用于衡量数据“不纯程度”的度量。当数据来自同一类时，基尼指数为0；当数据来自两类且各占一半时，基尼指数达到最大值0.5。</p>
<p>下面我们通过例6-1和例6-2来说明熵和基尼指数的计算。</p>
<p><strong>例6-1</strong> 假设当前的训练集 _S_ 中有15个样本，其中5个是正类，另外10个是负类。该训练集对应的熵可计算如下：</p>
<p><img src="Image00671.gif" alt></p>
<p>基尼指数为：</p>
<p><img src="Image00672.gif" alt></p>
<p><strong>例6-2</strong> 假设当前的训练集 _S_ 中有15个样本，其中1个是正类，另外14个是负类。该训练集对应的熵可计算如下：</p>
<p><img src="Image00673.gif" alt></p>
<p>基尼指数为：</p>
<p><img src="Image00674.gif" alt></p>
<p>从上面例6-1和例6-2可以看出，例6-2所对应的数据“纯度”更高，所对应的熵和基尼指数都更小。</p>
<h4 id="2．信息增益"><a href="#2．信息增益" class="headerlink" title="2．信息增益"></a>2．信息增益</h4><p>根据熵的定义，如果一个数据集的熵较大，意味着该数据集对应的类别“纯度”较低，需要继续划分该数据集。在建立决策树的过程中，在每一步都需要选择分类能力最强的特征。简单地讲，分类能力较强的特征能够使得将训练集划分成两个或者多个子集之后，每个子集的“纯度”较高，即熵较小。我们将划分前后熵的差称为由此特征划分所导致的信息增益<br>（information gain）。</p>
<p>严格地讲，假设当前结点对应的训练集是 _S_ ，我们使用特征 _v_ 将 _S_ 划分为 _k_ 个不相交的子集<img src="Image00675.gif" alt></p>
<p><img src="Image00676.gif" alt></p>
<p>（6-5）</p>
<p>则使用特征 _v_ 划分 _S_ 的信息增益<img src="Image00677.gif" alt> 计算公式如下：</p>
<p><img src="Image00678.gif" alt></p>
<p>（6-6）</p>
<p>我们知道<img src="Image00679.gif" alt> ，因此<img src="Image00680.gif" alt> 。根据信息增益的定义，分类能力越强的特征所对应的信息增益越大。在理想情况下，如果在一个两类问题中，特征 _v_ 将 _S_ 划分为<img src="Image00681.gif" alt> 、<img src="Image00682.gif" alt> ，且每个子集中都只包含一类的样本，即特征 _v_ 能够完美地将 _S_ 分类，则<img src="Image00683.gif" alt> 达到最大值<img src="Image00684.gif" alt> 。</p>
<p>在上面的讨论中，我们可以将熵替换成基尼指数，所有的结论仍然成立。对于基尼指数，我们可以定义相应的增益如下：</p>
<p><img src="Image00685.gif" alt></p>
<p>（6-7）</p>
<p>注意，信息增益 特指基于熵的增益。</p>
<p>在构建决策树时，对于当前结点，我们基于所对应的训练样本集 _S_ 和所有可考虑的特征 _v_ ，计算所有可能的<img src="Image00677.gif" alt> ，从中选择使得<img src="Image00677.gif" alt> 最大的特征 _v_ ，进而构造相应的子结点和边。在本书中，为了讨论的方便，我们采用CART中的办法，在构造决策树时假设所得的决策树都是二叉树。读者可以简单扩展这里所讨论的方法从而构建多叉树。</p>
<p>在选择最优的特征时，我们需要考虑不同特征的类型。下面我们分别讨论分类变量和数值变量所对应的<img src="Image00677.gif" alt> 及对应的判定条件。</p>
<p>对于分类变量，使用其作为标准将决策树的当前结点划分为两个子集时，所选用的判定条件比较简单。通常的形式为变量的取值是否在所有取值的一个子集中。例如，若变量 _v_ 表示性别，且有3种取值{男，女，未知}，则我们选取的判定条件可为</p>
<p>_v_ ==男？ 或者 _v_ ==女？ 或者 _v_ ==未知？</p>
<p>对于每个条件，我们要计算相应的增益，并选取增益最大的作为判定条件。在这个例子中，因为变量 _v_ 只有3种不同的取值，所以我们只需要比较3种不同的条件即可。当变量的不同取值增加时，我们需要考虑的条件更多。在CART中，通常要考虑分类变量的所有可能的取值，并从中选取最优的划分。</p>
<p>对于数值变量，我们通常选取一个分割点，并划分数据。例如，对于数值变量 _v_ ，我们可以使用条件 _v_ &lt; _t_ 来将数据分为两部分，这里 _t_ 就是分割点。在实际中，对于数值变量 _v_ ，我们可以搜集训练集中该变量的所有取值，并按从小到大的顺序排列起来；然后将相邻取值的算术平均值作为分割点，并从中选出最优的分割点。具体来说，假设数值变量 _v_ 在训练集中的值按照从小到大的顺序排列为<img src="Image00686.gif" alt> ，则我们需要考虑如下不同的分割条件：</p>
<p><img src="Image00687.gif" alt></p>
<p>对于每个分割条件，我们可以计算对应的增益，并从中选出最优的划分。</p>
<p>下面我们通过例6-3来计算熵和相关特征的信息增益。</p>
<p><strong>例6-3</strong> 在本例中，我们要找出表6-4中对应最大信息增益的变量及判定条件。表6-4中数据有两个特征，即<code>color</code> 和<code>weight</code><br>；从第二行起每行对应一个样本，每列（不包含最后一列）对应一个特征；最后一列对应类标<code>class</code><br>（1表示正类，0表示负类）。下面我们计算每个变量及对应的判定条件，根据最大信息增益的原则找出最优划分。</p>
<p>表6-4 示例数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>color</code></th>
<th><code>weight</code></th>
<th><code>class</code>  </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Red</code></td>
<td><code>100</code></td>
<td><code>1</code>  </td>
</tr>
<tr>
<td><code>Red</code></td>
<td><code>50</code></td>
<td><code>1</code>  </td>
</tr>
<tr>
<td><code>Blue</code></td>
<td><code>120</code></td>
<td><code>0</code>  </td>
</tr>
<tr>
<td><code>Green</code></td>
<td><code>80</code></td>
<td><code>0</code>  </td>
</tr>
<tr>
<td><code>Red</code></td>
<td><code>150</code></td>
<td><code>0</code>  </td>
</tr>
<tr>
<td><code>Green</code></td>
<td><code>90</code></td>
<td><code>1</code>  </td>
</tr>
</tbody>
</table>
</div>
<p>变量<code>color</code> 有3个不同的值<code>Red</code> 、<code>Blue</code> 和<code>Green</code> ，对该变量我们需要考虑如下3个划分：</p>
<ul>
<li><code>color==Red</code> 和<code>color!=Red</code></li>
<li><code>color==Blue</code> 和<code>color!=Blue</code></li>
<li><code>color==Green</code> 和<code>color!=Green</code></li>
</ul>
<p>在划分之前的数据集里有3个正类样本、3个负类样本，因此，划分之前熵为<img src="Image00688.gif" alt> 。考虑划分<code>color==Red</code> 和<code>color!=Red</code> 。满足<code>color==Red</code> 的有3个样本，其中有2个正类、1个负类，对应的熵为<img src="Image00689.gif" alt> ；满足<code>color!=Red</code> 的有3个样本，其中有1个正类和2个负类，对应的熵为<img src="Image00690.gif" alt> 。因此，划分<code>color==Red</code> 和<code>color!=Red</code> 对应的信息增益为：</p>
<p><img src="Image00691.gif" alt></p>
<p>下面考虑划分<code>color==Blue</code> 和<code>color!=Blue</code> 。满足<code>color==Blue</code> 的有1个负类样本，对应的熵为0；满足<code>color!=Blue</code> 的有5个样本，其中3个正类、2个负类，对应的熵为<img src="Image00692.gif" alt> 。因此，划分<code>color==Blue</code> 和<code>color!=Blue</code> 对应的信息增益为：</p>
<p><img src="Image00693.gif" alt></p>
<p>类似地，划分<code>color==Green</code> 和<code>color!=Green</code> 对应的信息增益为：</p>
<p><img src="Image00694.gif" alt></p>
<p>因此，对于变量<code>color</code> ，最优的划分是<code>color==Blue</code> 和<code>color!=Blue</code> ，对应的信息增益是0.19。</p>
<p>接下来我们考虑第二个变量 <code>weight</code> 。将 <code>weight</code> 所有不同的取值排好序，我们得到50,80, 90, 100,120,150。因此，我们需要考虑如下划分：</p>
<ul>
<li><code>weight</code> &lt;<img src="Image00695.gif" alt></li>
<li><code>weight</code> &lt;<img src="Image00696.gif" alt></li>
<li><code>weight</code> &lt;<img src="Image00697.gif" alt></li>
<li><code>weight</code> &lt;<img src="Image00698.gif" alt></li>
<li><code>weight</code> &lt;<img src="Image00699.gif" alt></li>
</ul>
<p>下面只讨论前面两个判定条件所对应的信息增益计算，其他划分可以采用类似的方法计算。事实上，有快速算法可以计算所有划分所对应的信息增益，这里不详细讨论。</p>
<p>考虑判定条件<code>weight</code> &lt;65。满足该条件的有1个正类样本，所对应的熵为0；不满足该条件有2个正类、3个负类样本，所对应的熵为<img src="Image00700.gif" alt> 。因此，该判定条件所对应的信息增益为：</p>
<p><img src="Image00693.gif" alt></p>
<p>接下来考虑判定条件<code>weight</code> &lt;85。满足该条件的有1个正类样本和1个负类样本，所对应的熵为<img src="Image00701.gif" alt><br>；不满足该条件的有2个正类样本、2个负类样本，所对应的熵为<img src="Image00702.gif" alt> 。因此，该判定条件所对应的信息增益为：</p>
<p><img src="Image00703.gif" alt></p>
<p>表6-5列出了变量<code>weight</code> 的所有判定条件所对应的信息增益。从表6-5中我们可以看出，对于变量<code>weight</code> ，最优的划分是<code>weight</code> &lt;110，对应的信息增益是0.46。</p>
<p>表6-5 特征<code>weight</code> 不同的判定条件对应的信息增益</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>判 定 条 件</th>
<th>信 息 增 益  </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>weight&lt;65</code></td>
<td><code>0.19</code>  </td>
</tr>
<tr>
<td><code>weight&lt;85</code></td>
<td><code>0</code>  </td>
</tr>
<tr>
<td><code>weight&lt;95</code></td>
<td><code>0.08</code>  </td>
</tr>
<tr>
<td><code>weight&lt;110</code></td>
<td><code>0.46</code>  </td>
</tr>
<tr>
<td><code>weight&lt;135</code></td>
<td><code>0.19</code>  </td>
</tr>
</tbody>
</table>
</div>
<p>综合考虑两个变量可知，最优的划分是<code>weight</code> &lt;110。</p>
<h3 id="6-2-3-过拟合和剪枝"><a href="#6-2-3-过拟合和剪枝" class="headerlink" title="6.2.3 过拟合和剪枝"></a>6.2.3 过拟合和剪枝</h3><p>奥卡姆剃刀 （Occam’s Razor, 也写作Ockham’s Razor）是机器学习中广泛使用的一条规则，由14世纪英格兰逻辑学家William of Occam（约1287—1347年）提出。简单地讲，就是优先选择简单的模型 。</p>
<p>对于同一组数据，假设我们有两个分类性能相似的决策树模型，那么，在实际中我们应该优先选择更简单的模型。在第 5 章回归算法中，我们讨论了算法的复杂度。在决策树中，通常我们可以使用叶结点的数目或者决策树的深度来描述决策树模型的复杂度。决策树本身的特点决定了它比较容易导致过拟合。在这种情况下，通过剪枝<br>（pruning）来控制模型的复杂度是一种非常有效和必要的手段。</p>
<p>在决策树中，有两种剪枝方法来控制决策树的复杂度：</p>
<ul>
<li>事前剪枝 （pre-pruning）；</li>
<li>事后剪枝 （post-pruning）。</li>
</ul>
<p>事前剪枝就是在决策树的生成过程中设定一些指标条件，用来决定每个结点是否应该继续划分。具体来说，就是在决策树的生成过程中，如果某一结点对应的统计指标达到某一阈值，则停止划分；否则继续划分该结点。在实际中使用的指标条件包括：</p>
<ul>
<li>该结点对应的训练样本数目低于某一阈值；</li>
<li>继续划分虽然导致熵或者基尼指数降低，但降低量低于某一阈值。</li>
</ul>
<p>事前剪枝能够避免生成过于复杂的决策树，计算复杂度较低，但也有“近视”的缺点。在一些实际的决策树生成过程中，有可能对当前结点进一步划分不能提高分类效果，但是在下一层再划分却有显著的效果。</p>
<p>事后剪枝就是先让决策树充分生长，然后自底向上进行剪枝，以降低模型的复杂度。一般而言，我们可以将一棵子树用一个叶结点取代（叶结点的类标由对应训练集中样本数最多的类标决定）。与事前剪枝相比，事后剪枝可以避免近视的弱点，但计算复杂度更高。</p>
<p>下面我们介绍R中的<code>rpart</code> 软件包 ① 如何剪枝。首先，在使用<code>rpart</code> 函数构建决策树时，我们可以使用如下3个参数来控制决策树的生成，属于事前剪枝的范畴。</p>
<ul>
<li><code>minsplit</code> ：在生成决策树的过程中，该结点对应的训练集样本数必须超过<code>minsplit</code> 值才能考虑进一步划分。</li>
<li><code>minbucket</code> ：叶结点所对应的训练样本最小数目。</li>
<li><code>maxdepth</code> ：决策树的最大深度。</li>
</ul>
<p>在得到相应的决策树之后，我们可以进一步使用交叉检验（参见6.6.1节）来对所得的决策树进行事后剪枝。<code>rpart</code> 中有一个参数<code>cp</code> 可以用于控制模型的复杂度，设置不同的<code>cp</code> 值可以得到结点数目差异很大的决策树。这里<code>cp</code> 是Complexity Parameter的简写。我们可以使用多个<code>cp</code> 值构建相应的决策树，并利用交叉检验计算所得模型在检验集上的错误，从而选取最优的<code>cp</code> 值并利用其剪枝。</p>
<h3 id="6-2-4-实际使用"><a href="#6-2-4-实际使用" class="headerlink" title="6.2.4 实际使用"></a>6.2.4 实际使用</h3><p>在前面的章节中我们讨论的都是关于决策树的一些通用的技术。在具体的实现中，有两种较知名的决策树实现：</p>
<ul>
<li>CART（classification and regression tree）</li>
<li>C4.5</li>
</ul>
<p>这两种算法都实现了决策树，但在一些具体实现上有差别。</p>
<ul>
<li>在CART中，得到的决策树是二叉树，而在C4.5中，可以得到多叉树。</li>
<li>标准的CART算法使用基尼指数来确定最优的特征和对应的分割点，而C4.5使用熵和信息增益。</li>
<li>两种算法的剪枝方法不同。</li>
<li>对于缺失值的处理不同。</li>
</ul>
<p>R中的<code>rpart</code> 软件包主要基于CART算法实现，但在很多地方做了扩充。这里包名<code>rpart</code> 是recursive partitioning的简写。例如，在<code>rpart</code> 中我们可以选择使用基尼指数的增益或者信息增益来选择下一步的最优特征和对应的分割点。与CART相同，在<code>rpart</code> 中我们只能得到二叉树。</p>
<p>在<code>rpart</code> 包中，我们使用同名函数<code>rpart</code> 构建决策树。在构建决策树时，主要参数包括<code>cp</code> 、<code>minsplit</code> 、<code>minbucket</code> 和<code>maxdepth</code> ，其含义在前面的剪枝部分已经予以介绍。</p>
<p>与回归问题类似，对于当前的决策树，我们可以在训练集上定义并计算错误率。在决策树的生成过程中，虽然划分结点可以降低错误率，但同时也增加了模型的复杂度。在<code>rpart</code> 中，我们使用参数<code>cp</code> 来控制模型在训练集上的错误率和模型的结点数目之间的关系。在<code>rpart</code> 中，<code>cp</code> 值一般在0和1之间。当<code>cp</code> =1时，我们得到只有一个根结点的决策树（模型复杂度最低，但在训练集上的分类结果不理想）；当<code>cp</code> =0时，我们在训练时不控制结点的数目（模型复杂度很高，但在训练集上的分类结果很理想）。通常我们会尝试多个<code>cp</code> 值并通过交叉检验得到最优的<code>cp</code> 值。</p>
<p>使用<code>rpart</code> 函数时，我们先利用<code>rpart.control</code> 函数将上面所讨论的控制参数放到一起，再调用<code>rpart</code> 函数构建决策树。<code>rpart</code> 函数的用法如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpart(formula, data, weights, subset, na.action = na.rpart, method,</span><br><span class="line">　　　model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)</span><br></pre></td></tr></table></figure>

</details>


<p>其中第一个参数<code>formula</code> 是公式，表示在给定的数据中，哪一列对应类标 _y_ ，哪些列对应 <strong>_x_ </strong> ；第二个参数<code>data</code> 表示输入数据；参数<code>weights</code> 、<code>subset</code> 、<code>na.action</code> 、<code>model</code> 、<code>x</code> 、<code>y</code> 、<code>parms</code> 、<code>cost</code> 等都是可选的；参数<code>method</code> 表示生成决策树的类型，最常用的取值包括<code>&#39;class&#39;</code> 和<code>&#39;anova&#39;</code> ，分别表示构建分类树和回归树；参数<code>control</code> 的值可以使用上面讨论的<code>rpart.control</code> 传入。</p>
<p>在得到训练所得的决策树后，可以使用<code>printcp</code> 函数打印多个<code>cp</code> 值对应的交叉检验中训练集和检验集上的错误。根据交叉检验的结果，我们可以选取最优的<code>cp</code> 值。</p>
<p>在得到一个决策树后，可以使用<code>prune</code> 函数剪枝。在使用<code>prune</code> 时，第一个参数就是已有的<code>rpart</code> 生成的决策树对象，第二个参数是我们选择的<code>cp</code> 值。利用<code>printcp</code> 的输出，我们可以选出使得检验集分类最优的<code>cp</code> 值，并利用该<code>cp</code> 值对所得决策树剪枝。</p>
<p>在下面的讨论中，我们使用实际的数据和例子来说明如何使用<code>rpart</code> 构建决策树。这里我们着重讲解如何使用<code>rpart</code> 来生成不同的决策树、如何剪枝、如何使用决策树来处理新的测试数据等，完整的R代码在文件rpart_example_kyphosis.R中。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"># Step 0. Check required packages are installed or not. If not, install them.</span><br><span class="line">rpart.installed &lt;-&apos;rpart&apos; %in% rownames(installed.packages())</span><br><span class="line">if (rpart.installed) &#123;</span><br><span class="line">　print(&quot;the rpart package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the rpart package first...&quot;)</span><br><span class="line">　install.packages(&apos;rpart&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(&apos;rpart&apos;)</span><br><span class="line"># The partykit is also used to draw the decision tree</span><br><span class="line"># partykit provides some very good graphing and visualization of tree models</span><br><span class="line">partykit.installed &lt;-&apos;partykit&apos; %in% rownames(installed.packages())</span><br><span class="line">if (partykit.installed) &#123;</span><br><span class="line">　print(&quot;the partykit package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the partykit package first...&quot;)</span><br><span class="line">　install.packages(&apos;partykit&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(&apos;partykit&apos;)</span><br><span class="line"></span><br><span class="line">#========================</span><br><span class="line"># Step 1. Load the data</span><br><span class="line"># load the data</span><br><span class="line">data(kyphosis)</span><br><span class="line">D &lt;-kyphosis</span><br><span class="line">print(&apos;the basic information of D&apos;)</span><br><span class="line">str(D)</span><br><span class="line"></span><br><span class="line"># Step 2. Train decision tree models using rpart</span><br><span class="line"># Step 2.1 Train a rpart model using default settings</span><br><span class="line">M_rpart1 &lt;-rpart(Kyphosis~., data = D, method = &apos;class&apos;)</span><br><span class="line">print(&apos;show the trained model&apos;)</span><br><span class="line">print(M_rpart1)</span><br><span class="line">print(&apos;show the detailed summary of splits&apos;)</span><br><span class="line">summary(M_rpart1)</span><br><span class="line">print(&apos;plot the tree&apos;)</span><br><span class="line">plot(as.party(M_rpart1), main=&apos;decision tree using default setting&apos;)</span><br><span class="line"></span><br><span class="line"># Step 2.2 Check cp and the corresponding CV error</span><br><span class="line"># &apos;cp&apos; stands for Complexity　Parameter of the tree.</span><br><span class="line"># plot a complexity parameter table for an Rpart Fit</span><br><span class="line">plotcp(M_rpart1)</span><br><span class="line"># display CP table for fitted Rpart object</span><br><span class="line">C &lt;-printcp(M_rpart1)</span><br><span class="line"># Step 2.3. Prune the resulting decision using the optimal cp value and the </span><br><span class="line"># prune function</span><br><span class="line">cp_optimal &lt;-C[which.min(C[,&apos;xerror&apos;]), &apos;CP&apos;]</span><br><span class="line">M_rpart1.1 &lt;-prune(M_rpart1, cp = cp_optimal)</span><br><span class="line">print(M_rpart1.1)</span><br><span class="line">print(&apos;plot the pruned tree&apos;)</span><br><span class="line">plot(as.party(M_rpart1.1), main=&apos;pruned decision tree&apos;)</span><br><span class="line"></span><br><span class="line"># Step 2.4 Train a tree using more complicated parameter setting</span><br><span class="line">M_rpart2 &lt;-rpart(Kyphosis~., data = D, method = &apos;class&apos;,</span><br><span class="line">　　　　　　　 control = rpart.control(minsplit = 10, minbucket = 5, maxdepth = 4))</span><br><span class="line">print(&apos;show the summary of the trained model&apos;)</span><br><span class="line">print(M_rpart2)</span><br><span class="line"># plot the resulting tree</span><br><span class="line">plot(as.party(M_rpart2), main=&apos;decision tree using advanced parameter setting&apos;)</span><br><span class="line"></span><br><span class="line"># Step 3. Make prediction using the predict function, and compute accuracy</span><br><span class="line"># We use the first 10 samples from D as the test data set</span><br><span class="line">D_test &lt;-D[1:10,]</span><br><span class="line"># Output the probability first</span><br><span class="line">y_test_prob &lt;-predict(M_rpart1, D_test)</span><br><span class="line">print(y_test_prob)</span><br><span class="line"># Get the explicit class label next</span><br><span class="line">y_test_label &lt;-predict(M_rpart1, D_test, type=&apos;class&apos;)</span><br><span class="line">print(y_test_label)</span><br><span class="line"># Compute the corresponding accuracy</span><br><span class="line">accuracy_test &lt;-sum(D_test$Kyphosis==y_test_label) / length(y_test_label)</span><br><span class="line">print(paste0(&apos;accuracy on the test data set is &apos;, accuracy_test))</span><br></pre></td></tr></table></figure>

</details>


<p>这段程序首先检查<code>rpart</code> 和<code>partykit</code> 软件包有没有安装。如果没有安装的话，则要安装它们。这里我们使用<code>partykit</code> 软件包来可视化生成的决策树，因此也要预先安装<code>partykit</code> 包。事实上，<code>rpart</code> 软件包也提供了可视化决策树的函数，但在显示效果上不如<code>partykit</code> 包。我们稍后会详细介绍多种可视化决策树的方法。</p>
<p><code>rpart</code> 包中包含了<code>kyphosis</code> 数据集。我们可以直接使用<code>data(kyphosis)</code> 导入该数据集（得到一个名为<code>kyphosis</code> 的数据框），并将其赋给变量<code>D</code> 。在R中，我们可以使用<code>str</code> 函数来显示任何R对象的主要信息。下面是<code>str(D)</code> 的输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;the basic information of D&quot;</span><br><span class="line">&apos;data.frame&apos;: 81 obs. of　4 variables:</span><br><span class="line"> $ Kyphosis: Factor w/ 2 levels &quot;absent&quot;,&quot;present&quot;: 1 1 2 1 1 1 1 1 1 2 ...</span><br><span class="line"> $ Age　　 : int　71 158 128 2 1 1 61 37 113 59 ...</span><br><span class="line"> $ Number　: int　3 3 4 5 4 2 2 3 2 6 ...</span><br><span class="line"> $ Start　 : int　5 14 5 1 15 16 17 16 16 12 ...</span><br></pre></td></tr></table></figure>

</details>


<p>从以上输出可以看出，<code>D</code> 有4列，分别是<code>Kyphosis</code> 、<code>Age</code> 、<code>Number</code> 和<code>Start</code> 。这组数据集对应一个分类问题，其中的类标是<code>Kyphosis</code> 列，有两个不同的值<code>absent</code> 和<code>present</code> 来表示不同的类标。这里<code>Kyphosis</code> 是“驼背”的意思，我们要用其他的3个变量来预测它。</p>
<p>在上面的R代码中，我们构建了多个不同的决策树模型。在第一个模型<code>M</code> _<code>rpart1</code> 中，我们直接使用了默认的参数配置来构建决策树：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_rpart1 &lt;-rpart(Kyphosis~., data = D, method = &apos;class&apos;)</span><br></pre></td></tr></table></figure>
<p>这里第一个参数是一个公式对象，我们使用<code>Kyphosis~.</code> 表示需要预测<code>Kyphosis</code> ，并使用所有的其他变量来进行预测。此处将<code>method</code> 参数设置为<code>&#39;class&#39;</code> 表示要构建一个适用于分类问题的决策树。</p>
<p>使用<code>print</code> 函数可以直接打印生成的决策树<code>M</code> _<code>rpart1</code> 的主要信息。这里其对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">n= 81 </span><br><span class="line"></span><br><span class="line">node), split, n, loss, yval, (yprob)</span><br><span class="line">　　　* denotes terminal node</span><br><span class="line"></span><br><span class="line"> 1) root 81 17 absent (0.79012346 0.20987654)　</span><br><span class="line">　 2) Start&gt;=8.5 62　6 absent (0.90322581 0.09677419)　</span><br><span class="line">　　 4) Start&gt;=14.5 29　0 absent (1.00000000 0.00000000) *</span><br><span class="line">　　 5) Start&lt; 14.5 33　6 absent (0.81818182 0.18181818)　</span><br><span class="line">　　　10) Age&lt; 55 12　0 absent (1.00000000 0.00000000) *</span><br><span class="line">　　　11) Age&gt;=55 21　6 absent (0.71428571 0.28571429)　</span><br><span class="line">　　　　22) Age&gt;=111 14　2 absent (0.85714286 0.14285714) *</span><br><span class="line">　　　　23) Age&lt; 111 7　3 present (0.42857143 0.57142857) *</span><br><span class="line">　 3) Start&lt; 8.5 19　8 present (0.42105263 0.57894737) *</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，<code>print(M</code> _<code>rpart1)</code> 将生成的决策树以文本的形式打印出来，并打印了每个结点所对应的训练集的分布情况。我们还可以使用<code>summary</code> 函数打印更详细的信息，这里就不列出了。读者可以直接运行我们提供的R程序并查看相应的输出。</p>
<p>由前面的讨论可以看出使用<code>rpart</code> 构建决策树时参数<code>cp</code> 的重要性。我们可以使用<code>printcp</code> 打印出不同<code>cp</code> 取值下，所构建的决策树<code>M_rpart1</code> 在交叉检验中的训练集和检验集上的错误率等信息。下面给出了<code>printcp(M_rpart1)</code> 的输出，从中可以看到决策树中仅使用了两个变量。在最后打印出的表中，显示了不同的<code>cp</code> 值、对应分割结点数<code>nsplit</code><br>（即决策树中非叶结点的数目，决策树的总叶结点数是该值加1）以及所得决策树在训练集上的错误率（列<code>rel error</code><br>）、在检验集上的错误率（列<code>xerror</code> ）、在检验集上错误率的标准差（列<code>xstd</code> ）。注意，在<code>rel error</code> 、<code>xerror</code> 中我们都使用了相对的错误率。从打印结果可以知道，根结点对应的错误率是17/81=0.20988。同时我们也知道当<code>cp</code> =0.019608时，对应的<code>xerror</code> =0.82353是相对于根结点的错误率的比值，此时真正的错误率应该是0.82353×0.20988=0.1728425。由此也可以看出，当<code>cp</code> 值小于0.019608之后，或者决策树的总结点数大于或等于2之后，在检验集上的错误率都没有变化。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Classification tree:</span><br><span class="line">rpart(formula = Kyphosis ~ ., data = D, method = &quot;class&quot;)</span><br><span class="line"></span><br><span class="line">Variables actually used in tree construction:</span><br><span class="line">[1] Age　 Start</span><br><span class="line"></span><br><span class="line">Root node error: 17/81 = 0.20988</span><br><span class="line"></span><br><span class="line">n= 81 </span><br><span class="line"></span><br><span class="line">　　　　CP　nsplit　rel error　 xerror　　 xstd</span><br><span class="line">1 0.176471　　　 0　　1.00000　1.00000　0.21559</span><br><span class="line">2 0.019608　　　 1　　0.82353　0.88235　0.20565</span><br><span class="line">3 0.010000　　　 4　　0.76471　0.88235　0.20565</span><br></pre></td></tr></table></figure>

</details>


<p><code>plotcp</code> 函数经常与<code>printcp</code> 一起使用，它以图像的形式呈现了<code>cp</code> 值与<code>xerror</code> 的关系。图6-3给出了<code>plotcp</code> 函数的输出。</p>
<p><img src="Image00704.jpg" alt></p>
<p>图6-3 在不同<code>cp</code> 值检验集上错误率（<code>xerror</code> ）的变化（<code>size of tree</code> 是<code>cp</code> 值对应的决策树的结点总数）</p>
<p>这里我们选取最优<code>cp</code> 值的标准是使得<code>xerror</code> 最小，同时决策树的结点总数最小。我们在代码中将最优结果保存在<code>cp_optimal</code> 中，在所用数据集上<code>cp_optimal</code> =0.01960784。这样我们就可以直接使用<code>prune</code> 函数对决策树模型<code>M_rpart1</code> 进行剪枝，得到模型<code>M_rpart1.1</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_rpart1.1 &lt;-prune(M_rpart1, cp = cp_optimal)</span><br></pre></td></tr></table></figure>
<p>相比<code>M_rpart1</code> ，新的决策树模型<code>M_rpart1.1</code> 更加简单。在上面的R程序中，我们使用<code>plot(as.party(M_rpart1)</code> 和<code>plot(as.party(M_rpart1.1)</code> 来可视化剪枝前后的两个决策树模型，结果如图6-4所示。</p>
<p><img src="Image00705.jpg" alt></p>
<p>图6-4 剪枝前后的决策树比较</p>
<p>接下来我们演示了如何在训练决策树中设置更多的参数，包括<code>minsplit</code> 、<code>minbucket</code> 、<code>maxdepth</code> 等。</p>
<p>最后我们使用<code>predict</code> 函数来预测新的样本。在<code>predict</code> 函数中，我们要注意参数<code>type</code> 。当<code>type=&#39;prob&#39;</code> 时，<code>predict</code> 函数输出每个样本属于每个类的概率；当<code>type</code> 的值设定为<code>&#39;class&#39;</code> 时，<code>predict</code> 函数直接输出所得的类标。对于分类树来说，<code>type</code> 的默认值是<code>&#39;prob&#39;</code> 。因此，上面程序中两个<code>predict</code> 函数的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; print(y_test_prob)</span><br><span class="line">　　 absent　 present</span><br><span class="line">1　0.4210526 0.5789474</span><br><span class="line">2　0.8571429 0.1428571</span><br><span class="line">3　0.4210526 0.5789474</span><br><span class="line">4　0.4210526 0.5789474</span><br><span class="line">5　1.0000000 0.0000000</span><br><span class="line">6　1.0000000 0.0000000</span><br><span class="line">7　1.0000000 0.0000000</span><br><span class="line">8　1.0000000 0.0000000</span><br><span class="line">9　1.0000000 0.0000000</span><br><span class="line">10 0.4285714 0.5714286</span><br><span class="line">&gt; print(y_test_label)</span><br><span class="line">　　　1　　　 2　　　 3　　　 4　　　 5　　　 6　　　 7　　　 8　　　 9　　 10 </span><br><span class="line">present　absent present present　absent　absent　absent　absent　absent present</span><br><span class="line">Levels: absent present</span><br></pre></td></tr></table></figure>

</details>


<p>此外，我们还根据所得的预测结果计算了分类的准确率。</p>
<p>对于<code>rpart</code> 包生成的决策树对象，除了<code>rpart</code> 包中提供了较为简单的可视化方法外，还有多个包提供了可视化相应的方法来可视化决策树。下面我们分别对这些方法予以介绍。完全的R代码在文件rpart_example_tree_plot.R中。</p>
<p>基于<code>rpart</code> 包，我们可以使用<code>plot</code> 和<code>text</code> 函数来作图。<code>rpart</code> 包中提供了<code>plot.rpart</code> 函数和<code>text.rpart</code> 函数。其中<code>plot.rpart</code> 函数根据生成的决策树画出相应的图像；而<code>text.rpart</code> 函数则在<code>plot.rpart</code> 函数生成的图中加入相应的文字标注。这里虽然<code>rpart</code> 中提供的画图函数叫<code>plot.rpart</code> ，但我们可以直接使用<code>plot</code> 函数来为<code>rpart</code> 生成的决策树对象画图。当我们调用R中的<code>plot</code> 函数时，R会根据输入参数的类别自动调用对应的<code>plot.rpart</code> 函数。相应地，我们可以直接调用R中的<code>text</code> 函数来为生成的图添加文字注释。</p>
<p>对于前面使用<code>rpart</code> 生成的决策树对象<code>M_rpart1</code> ，我们可以使用如下的R代码来作图并标注：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  plot(M_rpart1)</span><br><span class="line">text(M_rpart1)</span><br></pre></td></tr></table></figure>
<p>生成的决策树图像如图6-5所示，可以看出所生成的图像不是特别理想。</p>
<p>在使用<code>plot.rpart</code> 和<code>text.rpart</code> 时，我们可以设置相应的参数，这样可以画出更加满意的图像。在下面的这段代码中，我们使用<code>main</code> 参数为所生成的图像添加标题，使用<code>cex</code> 参数调整所添加文字的比例，使用<code>uniform</code> 参数使得决策树各层结点之间的距离一致。所生成的决策树图像如图6-6所示。</p>
<p><img src="Image00706.jpg" alt></p>
<p>图6-5 使用<code>rpart</code> 中提供的函数生成的决策树图像</p>
<p><img src="Image00707.jpg" alt></p>
<p>图6-6 进一步调整参数所生成的决策树图像</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot(M_rpart1, uniform=TRUE, main=&quot;Classification Tree in rpart&quot;, cex = 0.5)</span><br><span class="line">text(M_rpart1, use.n=TRUE, all=TRUE, cex=0.8)</span><br></pre></td></tr></table></figure>

</details>


<p>此外，我们还可以使用<code>rpart.plot</code> 包中的同名函数来可视化<code>rpart</code> 生成的决策树。注意，这里也要首先安装<code>rpart.plot</code> 包。使用下面的R代码，我们可以生成如图6-7所示的决策树图像。</p>
<p><img src="Image00708.jpg" alt></p>
<p>图6-7 使用<code>rpart.plot</code> 包生成的决策树图像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpart.plot(M_rpart1, main=&apos;Decision Tree in rpart.plot&apos;)</span><br></pre></td></tr></table></figure>
<p>我们还可以使用<code>rattle</code> 软件包来可视化<code>rpart</code> 中生成的决策树。<code>rattle</code> 软件包依赖的软件包较多，需要较长时间下载和安装。安装好<code>rattle</code> 软件包之后，我们可以使用其中的<code>fancyRpartPlot</code> 函数来生成相应的图像：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fancyRpartPlot(M_rpart1, main=&apos;Decision Tree in rattle&apos;)</span><br></pre></td></tr></table></figure>
<p>所生成的图像如图6-8所示。</p>
<p><img src="Image00709.jpg" alt></p>
<p>图6-8 使用<code>rattle</code> 软件包生成的决策树图像</p>
<h3 id="6-2-5-讨论"><a href="#6-2-5-讨论" class="headerlink" title="6.2.5 讨论"></a>6.2.5 讨论</h3><p>决策树最大的优点是它生成的分类器可以用一些规则来表示，因此非常容易解释所得的分类器。此外，决策树能够处理数值特征、类别特征等多种特征，对数据中的缺失值也能很自然地进行处理。事实上，使用决策树对数据进行分类时，基本不需要对数据进行预处理（如标准化等）。同时，训练决策树和对测试集进行分类的计算复杂度都较低。</p>
<p>但是，决策树对于噪声比较敏感。虽然在实践中剪枝广泛采用，但决策树还是较容易出现过拟合的问题。由于决策树存在着这些问题，因此在实际中一般很少直接使用。然而，通过集成学习，我们可以综合大量的决策树模型从而避免决策树的不足。其中，基于决策树的随机森林和提升决策树的梯度提升算法在实践中受到了广泛的欢迎。我们将在第9章具体地讨论这些基于决策树的集成学习算法。</p>
<h2 id="6-3-逻辑回归"><a href="#6-3-逻辑回归" class="headerlink" title="6.3 逻辑回归"></a>6.3 逻辑回归</h2><p>本节我们讨论逻辑回归 （logistic regression）。虽然逻辑回归称为回归，但是它假定了目标 _y_ 属于集合{0,1}。因此，逻辑回归是适用于分类而不是回归的算法。在下面的讨论中，我们假设要解决的是两类分类问题，并将正类记为1，负类记为0。</p>
<p>在逻辑回归中，一个关键假设是样本 <strong>_x_ </strong> 属于正类的概率可以用下面的式子来表示：</p>
<p><img src="Image00710.gif" alt></p>
<p>（6-8）</p>
<p>这里sig()称为sigmoid函数 ，其定义如下：</p>
<p><img src="Image00711.gif" alt></p>
<p>（6-9）</p>
<p>由于逻辑回归直接返回一个概率值，因此它在很多实际应用中非常受欢迎。例如，在银行和金融数据处理中，逻辑回归已经是一些应用的事实标准算法。</p>
<p>比较逻辑回归和线性回归，可以发现区别在于sigmoid函数。这里sigmoid函数称为连接函数 （link function）。我们知道<img src="Image00712.gif" alt> 是线性模型，通过连接函数来转换<img src="Image00713.gif" alt> 的模型称为广义线性模型 （Generalized Linear Model, GLM）。下面我们首先介绍sigmoid函数的性质，然后讨论如何从训练集中学习出模型的参数 <strong>_w_ </strong> 。</p>
<h3 id="6-3-1-sigmoid函数的性质"><a href="#6-3-1-sigmoid函数的性质" class="headerlink" title="6.3.1 sigmoid函数的性质"></a>6.3.1 sigmoid函数的性质</h3><p>这里我们首先介绍sigmoid函数的性质。图6-9给出了sigmoid函数的图像。由图6-9可以看出，sigmoid函数的形状类似于S形。该函数的输入值可以从<img src="Image00714.gif" alt> 到<img src="Image00715.gif" alt> ，但是其输出的区间是(0,1)。这样对于任意值的<img src="Image00712.gif" alt> ，通过sigmoid函数，我们都能够得到0到1之间的概率。</p>
<p><img src="Image00716.jpg" alt></p>
<p>图6-9 sigmoid函数的图像</p>
<p>下面介绍sigmod函数的一些具体性质，包括：</p>
<ul>
<li><img src="Image00717.gif" alt></li>
<li><img src="Image00718.gif" alt> 关于 _t_ 的导数是<img src="Image00719.gif" alt></li>
<li><img src="Image00720.gif" alt> 关于 _<strong>w</strong> _ 的导数是<img src="Image00721.gif" alt></li>
<li>0&lt;sig( _t_ )&lt;1</li>
</ul>
<p>这里简单对上述性质予以证明。首先证明<img src="Image00717.gif" alt> 如下：</p>
<p><img src="Image00722.gif" alt></p>
<p>（6-10）</p>
<p>接下来推导sig( _t_ )关于 _t_ 的导数：</p>
<p><img src="Image00723.gif" alt></p>
<p>（6-11）</p>
<p>利用上述结论及微分中的链式法则，可得到</p>
<p><img src="Image00724.gif" alt></p>
<p>（6-12）</p>
<p>注意到exp (-_t_ )&gt;0，因此，根据sigmoid函数的定义，可以直接得到0&lt;sig( _t_ )&lt;1。</p>
<p>此外，根据sigmoid函数的定义，可以得到它的反函数——logit函数：</p>
<p><img src="Image00725.gif" alt></p>
<p>（6-13）</p>
<h3 id="6-3-2-通过极大似然估计来估计参数"><a href="#6-3-2-通过极大似然估计来估计参数" class="headerlink" title="6.3.2 通过极大似然估计来估计参数"></a>6.3.2 通过极大似然估计来估计参数</h3><p>在逻辑回归中唯一的参数是 <strong>_w_ </strong> 。本节讨论如何通过训练集使用极大似然估计 来估计该参数。</p>
<p>假设我们有训练集<img src="Image00726.gif" alt> ，目标是通过该训练集学习出逻辑回归的参数 <strong>_w_ </strong> 。在回归分析中，我们已经引入损失函数，其基本方法是定义训练集上的损失函数，并找出使得损失函数最小的参数值作为最优解；而在逻辑回归中，我们引入极大似然估计并找出最优解。根据对数似然函数，我们定义相应的损失函数。</p>
<p>首先考虑样本<img src="Image00727.gif" alt> 。我们使用<img src="Image00728.gif" alt> 来标记样本<img src="Image00534.gif" alt> 是正类的概率：</p>
<p><img src="Image00729.gif" alt></p>
<p>（6-14）</p>
<p>于是，给定<img src="Image00730.gif" alt> 时<img src="Image00543.gif" alt> 是0的概率为<img src="Image00731.gif" alt> 。综合考虑这两种情况，我们可以将<img src="Image00540.gif" alt> 的似然函数写为：</p>
<p><img src="Image00732.gif" alt></p>
<p>（6-15）</p>
<p>注意，由于<img src="Image00733.gif" alt> 只能是0或者1，因此该公式始终只有一项。</p>
<p>因此，对于整个训练集<img src="Image00726.gif" alt> 而言，似然函数可以写为：</p>
<p><img src="Image00734.gif" alt></p>
<p>（6-16）</p>
<p>这里<img src="Image00735.gif" alt> 。对于给定的训练集，我们可以最大化似然<img src="Image00736.gif" alt> ，从而得到最优的 <strong>_w_ </strong> 。在机器学习中，通常最小化损失函数。在逻辑回归中，引入一个新的损失函数，称为交叉熵<br>（cross-entropy）损失函数：</p>
<p><img src="Image00737.gif" alt></p>
<p>（6-17）</p>
<p>可以看出，交叉熵损失函数实际上就是似然函数先取对数再取相反数的结果。因此，最大化似然函数等价于最小化交叉熵损失函数。</p>
<p>在数值优化中，假设我们要最小化函数 _f_ ，通常会利用其导数提供的信息。例如，在每一步我们可以沿着负导数的方向搜索，这样就可以在优化的每一步降低函数 _f_ 的值。在最小化交叉熵损失函数时，我们可以计算交叉熵损失函数对于参数 <strong>_w_ </strong> 的导数 <strong>_g_ </strong> 和二阶导数 <strong>_H_ </strong> 。</p>
<p>首先，我们可以求<img src="Image00738.gif" alt> 关于 <strong>_w_ </strong> 的导数 <strong>_g_ </strong> ，如下：</p>
<p><img src="Image00739.gif" alt></p>
<p>（6-18）</p>
<p>利用前面的性质，我们有：</p>
<p><img src="Image00740.gif" alt></p>
<p>（6-19）</p>
<p>将其代入式（6-18），则有</p>
<p><img src="Image00741.gif" alt></p>
<p>（6-20）</p>
<p>注意，这里导数<img src="Image00742.gif" alt> 是一个向量。我们也可以将一阶导数 _<strong>g</strong> _ 写成矩阵的形式：</p>
<p><img src="Image00743.gif" alt></p>
<p>（6-21）</p>
<p>这里矩阵<img src="Image00744.gif" alt> 为数据矩阵，<img src="Image00745.gif" alt> 为对应的类别信息，<img src="Image00746.gif" alt> 为逻辑回归所构建的模型的预测值。</p>
<p>类似地，我们可以求得二阶导数<img src="Image00747.gif" alt> ：</p>
<p><img src="Image00748.gif" alt></p>
<p>（6-22）</p>
<p>这里矩阵<img src="Image00749.gif" alt> 是一个对角阵，其定义为：</p>
<p><img src="Image00750.gif" alt></p>
<p>（6-23）</p>
<p>注意，此处0&lt;<img src="Image00751.gif" alt> <1，因此对角元素满足![](image00752.gif)>0。因此，矩阵 <strong>_S_ </strong> 是正定矩阵。根据优化方面的知识，这里的交叉熵损失函数是严格凸函数。我们知道这也意味着它有唯一的全局最优解，并且局部最优解就是全局最优解。</1，因此对角元素满足![](image00752.gif)></p>
<p>下面的问题就是如何求出使得交叉熵损失函数最小的 <strong>_w_ </strong> 。从 <strong>_g_ </strong> 的计算公式中可以看出，它跟当前的“错误项”<img src="Image00753.gif" alt> 直接相关。与我们在线性回归中的处理办法类似，一种朴素的想法是令 <strong>_g_ </strong> =0来直接求解 <strong>_w_ </strong> 。在线性回归中，损失函数是关于所求参数<br><strong>_w_ </strong> 的二次函数，因此可以直接推导出解析解。但注意到<img src="Image00754.gif" alt> ，换言之，因为非线性的sigmoid函数的引入，所以 <strong>_w_ </strong> 是以非线性的形式出现在导数 <strong>_g_ </strong> 中的，使得我们无法得到关于 <strong>_w_
</strong> 的解析解。因此，在这种情况下，我们一般使用数值解法，采用逐步逼近的方法来求出 <strong>_w_ </strong> 的最优解。</p>
<h3 id="6-3-3-牛顿法"><a href="#6-3-3-牛顿法" class="headerlink" title="6.3.3 牛顿法"></a>6.3.3 牛顿法</h3><p>在求解逻辑回归时，我们一般使用牛顿法 来求得最优解。在该算法中，每一步我们都假设可以用一个二次函数来近似我们要最小化的目标函数，从而逐步接近最优解。</p>
<p>首先回忆一下泰勒展式。在泰勒展式中，对于函数 _f_ ( _w_ )，当 _w_ 在 _a_ 附近时可以使用如下展式来逼近 _f_ ( _w_ )：</p>
<p><img src="Image00755.gif" alt></p>
<p>（6-24）</p>
<p>在牛顿法中，我们使用逐步逼近的方法来求解参数 _w_ 。这里我们用下标 _k_ 标识在第 _k_ 步的诸变量值。例如，用 _w k _ 表示第 _k_ 步的 _w_ 值。在第 _k_ 步，我们有当前估计<img src="Image00756.gif" alt> ，就可以将 _f_ ( _w_ )用其在<img src="Image00756.gif" alt> 处的二阶泰勒展式来近似：</p>
<p><img src="Image00757.gif" alt></p>
<p>（6-25）</p>
<p>上面的公式仅仅适用于一维的情况。在这里我们要优化的参数<img src="Image00758.gif" alt> ，上面的二阶泰勒展式可以写为：</p>
<p><img src="Image00759.gif" alt></p>
<p>（6-26）</p>
<p>根据泰勒展式的原理，<img src="Image00760.gif" alt> 在 _<strong>w</strong> _ 接近<img src="Image00302.gif" alt> 的时候是比较准确的。这里我们的目标是从<img src="Image00302.gif" alt> 推导出计算<img src="Image00761.gif" alt> 的方法。</p>
<p>这里为了简化推导过程，我们将<img src="Image00762.gif" alt> 简写为：</p>
<p><img src="Image00763.gif" alt></p>
<p>（6-27）</p>
<p>这里 <strong>_c_ </strong> 、 _d_ 分别定义为<img src="Image00764.gif" alt> 和<img src="Image00765.gif" alt> 。</p>
<p>由于<img src="Image00762.gif" alt> 是关于 _<strong>w</strong> _ 的二次函数，通过求导并将其置为0，我们可以获得使得<img src="Image00762.gif" alt> 最小的最优解，</p>
<p><img src="Image00766.gif" alt></p>
<p>（6-28）</p>
<p>因此，在牛顿法中，我们的更新公式是：</p>
<p><img src="Image00767.gif" alt></p>
<p>该更新公式是牛顿法迭代求解的核心。注意，在该更新公式中，我们要计算二阶导数<img src="Image00768.gif" alt> 的逆矩阵。当数据的维数 _d_ 很大时，计算复杂度很大。在实际使用中，牛顿法一般收敛较快，只需要较少的迭代步数就可以达到较高的精度。</p>
<p>在前面的推导过程中，我们直接利用了一阶导数<img src="Image00769.gif" alt> 和二阶导数<img src="Image00770.gif" alt> 。下面我们进一步简化逻辑回归中牛顿法的更新公式：</p>
<p><img src="Image00771.gif" alt></p>
<p>（6-29）</p>
<p>这里<img src="Image00772.gif" alt> 定义为：</p>
<p><img src="Image00773.gif" alt></p>
<p>（6-30）</p>
<p>因此，每一步实质上是求解一个线性方程组：</p>
<p><img src="Image00774.gif" alt></p>
<p>（6-31）</p>
<p>回忆在最小二乘法中，我们需要求解正规方程 （normal equation）：</p>
<p><img src="Image00775.gif" alt></p>
<p>（6-32）</p>
<p>逻辑回归中的方程与正规方程非常相似。而且我们注意到矩阵<img src="Image00776.gif" alt> 是一个对角矩阵，可以认为<img src="Image00777.gif" alt> 是数据<br><strong>_X_ </strong> 中第 _i_ 维的权重。因此，求解逻辑回归的牛顿法也称为重新加权迭代最小二乘法 （iterative reweighted least squares）。</p>
<h3 id="6-3-4-正则化项的引入"><a href="#6-3-4-正则化项的引入" class="headerlink" title="6.3.4 正则化项的引入"></a>6.3.4 正则化项的引入</h3><p>与回归问题中的最小二乘法类似，也可以在逻辑回归中引入正则化项以控制模型的复杂度。根据前面讨论的交叉熵损失函数，首先可以引入<img src="Image00573.gif" alt> 范数：</p>
<p><img src="Image00778.gif" alt></p>
<p>（6-33）</p>
<p>这里<img src="Image00779.gif" alt> 是控制范数权重的系数。这里前半部使用交叉熵损失函数来度量模型 _f_ 在训练数据上的表现，后半部使用<img src="Image00573.gif" alt> 范数来控制模型的复杂度。引入<img src="Image00573.gif" alt> 范数后，我们只需相应地更新一阶导数和二阶导数的计算公式，仍然可以使用牛顿法求解。</p>
<p>与Lasso类似，我们也可以引入<img src="Image00583.gif" alt> 范数</p>
<p><img src="Image00780.gif" alt></p>
<p>（6-34）</p>
<p>这里<img src="Image00781.gif" alt> 是控制范数<img src="Image00587.gif" alt> 权重的系数。因为<img src="Image00587.gif" alt> 在 _<strong>w</strong> _ =0点处连续但不可导，所以在求解上更加复杂，本书不作介绍。</p>
<p>与Elastic Net类似，我们可以同时引入<img src="Image00583.gif" alt> 范数和<img src="Image00573.gif" alt> 范数：</p>
<p><img src="Image00782.gif" alt></p>
<p>（6-35）</p>
<h3 id="6-3-5-实际使用"><a href="#6-3-5-实际使用" class="headerlink" title="6.3.5 实际使用"></a>6.3.5 实际使用</h3><p>本节我们使用<code>glmnet</code> 包来讲解如何在R中应用逻辑回归，包括使用不同的<img src="Image00583.gif" alt> 范数和<img src="Image00573.gif" alt> 范数的权重。R中的<code>glm2</code> 包也实现了逻辑回归，但在本书中我们着重讨论<code>glmnet</code> 包。本节完全的R代码在文件logistic_regression_example.R中。</p>
<p>在回归算法中，我们已经讨论过使用<code>glmnet</code> 来求解Lasso了。在使用<code>glmnet</code> 包中的<code>glmnet</code> 函数求解逻辑回归时，主要有如下参数需要考虑：</p>
<ul>
<li>参数<code>family</code> ，在回归中<code>family</code> 应设置为<code>&#39;gaussian&#39;</code> ，在两类分类问题中应设置为<code>&#39;binomial&#39;</code> ；</li>
<li>参数<code>lambda</code> ；</li>
<li>参数<code>alpha</code> 。</li>
</ul>
<p>回顾回归问题的讨论，我们知道参数<code>lambda</code> 和<code>alpha</code> 共同决定了<img src="Image00648.gif" alt> 范数和<img src="Image00573.gif" alt> 范数的权重。具体来说， _L_ 1 范数的权重是<code>lambda</code> ×<code>alpha</code> ， _L_ 2 范数的权重是<code>lambda</code> ×(<code>1-alpha</code> )/2。此外还要注意，<code>glmnet</code> 的输入数据必须显式表示为矩阵的形式，而不能使用R中更常用的表示数据的数据框的形式。如果数据不是矩阵的形式，我们需要使用函数<code>as.matrix()</code> 将输入数据转换为矩阵的形式。由于输入数据必须是矩阵的形式，因此，如果数据中有分类变量，我们也必须按照5.5.2节中转换分类变量的方法对其进行转换。</p>
<p>在得到<code>glmnet</code> 函数训练好的模型后，可以调用<code>predict</code> （其实是<code>predict.glmnet</code><br>）函数来预测新的数据对应的类别或者属于某一类的概率。<code>predict.glmnet</code> 中的参数<code>newx</code> 表示新的数据集。该函数中最重要的参数是<code>type</code> 。对于该参数使用不同的值可以得到不同的预测结果，常用的选择项包括：</p>
<ul>
<li><code>&#39;link&#39;</code> ：在分类和回归问题中都返回<img src="Image00713.gif" alt> 。注意，在回归问题中，<img src="Image00713.gif" alt> 为拟合值。</li>
<li><code>&#39;response&#39;</code> ：在分类问题中给出<img src="Image00783.gif" alt> ，即为测试数据属于正类的概率；在回归问题中为拟合值<img src="Image00713.gif" alt> 。</li>
<li><code>&#39;class&#39;</code> ：在分类问题中给出最后的分类结果，即对每个测试样本预测的类别。</li>
</ul>
<p>在R中，类标列一般是因子类型，其中“正类”指的是训练数据集中类标列因子类别中的第二项。假设<code>y_train</code> 是训练数据集对应的类别且类型为因子，则“正类”对应于<code>levels(y_train)[2]</code> 。</p>
<p>下面具体讲解文件logistic_regression_example.R中的R代码。首先检查如下3个包是否已经安装：</p>
<ul>
<li><code>glmnet</code></li>
<li><code>mlbench</code></li>
<li><code>pROC</code></li>
</ul>
<p>这里使用包<code>mlbench</code> 中的<code>Sonar</code> 数据，使用<code>pROC</code> 包中的<code>auc</code> 函数计算AUC指标（AUC是衡量算法分类性能的指标，详见6.7节）。如果没有安装的话，那么首先安装这3个包及它们所依赖的包。</p>
<p>在正式使用<code>glmnet</code> 构建逻辑回归模型之前，使用如下代码来导入数据，检查导入的数据框中每列的数据类型，并打印数据框的前10行和最后10行。注意，数据框<code>D</code> 的最后一列（列名为<code>Class</code> ）保存了每行对应的样本的类别信息。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">data(Sonar)</span><br><span class="line">D &lt;-Sonar</span><br><span class="line"># show the type for each column</span><br><span class="line">str(D)</span><br><span class="line"># Check the first 10 rows and save them in LaTeX format</span><br><span class="line">D_h10 &lt;-head(D, 10)</span><br><span class="line">print(&apos;the first 10 rows are:&apos;)</span><br><span class="line">print(D_h10)</span><br><span class="line">D_t10 &lt;-tail(D, 10)</span><br><span class="line">print(&apos;the last 10 rows are:&apos;)</span><br><span class="line">print(D_t10)</span><br></pre></td></tr></table></figure>

</details>


<p>之后将数据划分为训练集和测试集。使用训练集训练逻辑回归模型，并利用所得的模型计算其在测试集上的分类结果，最后计算分类的准确率和AUC。注意，由于<code>glmnet</code> 的输入数据 <strong>_X_ </strong> 必须是矩阵的形式，因此我们使用<code>as.matrix()</code> 函数将输入数据从数据框的格式显式转换为矩阵的格式。这里还使用了<code>set.seed()</code> 函数设置随机数生成器的种子以使得每次运行可以得到相同的结果。这里<code>sample(n,k)</code> 函数从1～ _n_ 的整数中随机选取 _k_ 个不重复的整数。<code>X_train$Class &lt;-NULL</code> 表示删除<code>X_train</code> 中的<code>Class</code> 列。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">set.seed(13)</span><br><span class="line">train_ratio &lt;-0.8</span><br><span class="line">n_total &lt;-nrow(D)</span><br><span class="line">n_train &lt;-round(train_ratio * n_total)</span><br><span class="line">n_test &lt;-n_total -n_train</span><br><span class="line">list_train &lt;-sample(n_total, n_train)</span><br><span class="line">y_train &lt;-D[list_train, &apos;Class&apos;]</span><br><span class="line">y_test &lt;-D[-list_train, &apos;Class&apos;]</span><br><span class="line">X_train &lt;-D[list_train,]</span><br><span class="line">X_train$Class &lt;-NULL</span><br><span class="line">X_train &lt;-as.matrix(X_train)</span><br><span class="line">X_test &lt;-D[-list_train,]</span><br><span class="line">X_test$Class &lt;-NULL</span><br><span class="line">X_test &lt;-as.matrix(X_test)</span><br></pre></td></tr></table></figure>

</details>


<p>接下来使用训练数据<code>X_train</code> 和<code>y_train</code> ，由易到难地建立多个不同的逻辑回归模型。</p>
<p>在第1个逻辑回归模型中，我们将<code>lambda</code> 参数设为0，这样在训练模型时没有使用任何的正则化项。所得的模型记为<code>M1</code> ，之后使用<code>predict</code> 函数得到测试集的预测结果，其中<code>y_test_predict1</code> 是直接的分类结果，<code>y_test_prob1</code> 保存了测试集中每个样本属于<code>&#39;R&#39;</code> 类（该数据有两类，分别为<code>&#39;M&#39;</code> 和<code>&#39;R&#39;</code> ）的概率，而<code>y_test_raw1</code> 保存了<img src="Image00713.gif" alt> 的值。也就是说，<code>y_test_prob1=1/(1+exp(-y_test_raw1))</code> 。利用<code>y_test_predict1</code> ，可以计算该模型在测试集上的准确率；利用<code>y_test_prob1</code> ，可以计算AUC。这里简要介绍一下<code>auc</code> 函数的使用方法。该函数的第一个参数是测试集的真实类别，第二个参数是预测的概率值（其实使用<code>y_test_raw1</code> 也可以，只要能够将测试集中的样本属于正类的概率从高到低进行排序即可，不一定是概率值）。注意，<code>y_test_prob1</code> 是一个矩阵类型的对象，要将其显式转换为向量对象。这里我们使用<code>as.numeric(y_test_prob1)</code> 进行转换；由于<code>y_test_prob1</code> 是一个只含有一列的矩阵，因此使用<code>y_test_prob1[,1]</code> 也可以。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">family &lt;-&apos;binomial&apos;</span><br><span class="line">lambda &lt;-0</span><br><span class="line">alpha &lt;-0</span><br><span class="line"># train a logistic regression model</span><br><span class="line">M1 &lt;-glmnet(X_train, y_train, family = family, lambda = lambda, alpha = alpha)</span><br><span class="line"># make the prediction on the test data set and compute the performance</span><br><span class="line">y_test_predict1 &lt;-predict(M1, X_test, type=&apos;class&apos;)</span><br><span class="line">y_test_prob1 &lt;-predict(M1, X_test, type=&apos;response&apos;)</span><br><span class="line">y_test_raw1 &lt;-predict(M1, X_test, type=&apos;link&apos;)</span><br><span class="line"></span><br><span class="line"># Compute accuracy and AUC</span><br><span class="line">y_test_numeric &lt;-ifelse(y_test==levels(y_test)[2], 1, 0)</span><br><span class="line">accuracy1 &lt;-sum(y_test==y_test_predict1)/n_test</span><br><span class="line">auc1 &lt;-auc(y_test_numeric, as.numeric(y_test_prob1))</span><br><span class="line">msg &lt;-paste(&apos;accuracy = &apos;, accuracy1)</span><br><span class="line">print(msg)</span><br><span class="line">msg &lt;-paste(&apos;auc = &apos;, auc1)</span><br><span class="line">print(msg)</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;accuracy =　0.761904761904762&quot;</span><br><span class="line">[1] &quot;auc =　0.776887871853547&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>在第2个逻辑回归模型中，我们将<code>alpha</code> 设为1，这样就仅使用 _L_ 1 范数作为正则化项；同时我们将<code>lambda</code> 的值设为0.05。在得到模型<code>M2</code> 后，可以利用<code>M2$beta</code> 查看所得模型的 <strong>_w_ </strong> 的具体数值。对应的R代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">family &lt;-&apos;binomial&apos;</span><br><span class="line">lambda &lt;-0.05</span><br><span class="line">alpha &lt;-1</span><br><span class="line">M2 &lt;-glmnet(X_train, y_train, family = family, lambda = lambda, alpha = alpha)</span><br><span class="line"># make the prediction on the test data set and compute the performance</span><br><span class="line">y_test_predict2 &lt;-predict(M2, X_test, type=&apos;class&apos;)</span><br><span class="line">y_test_prob2 &lt;-predict(M2, X_test, type=&apos;response&apos;)</span><br><span class="line">accuracy2 &lt;-sum(y_test==y_test_predict2)/n_test</span><br><span class="line">auc2 &lt;-auc(y_test, as.numeric(y_test_prob2))</span><br><span class="line">msg &lt;-paste(&apos;accuracy = &apos;, accuracy2)</span><br><span class="line">print(msg)</span><br><span class="line">msg &lt;-paste(&apos;auc = &apos;, auc2)</span><br><span class="line">print(msg)</span><br><span class="line">print(&apos;the coefficients of w are&apos;)</span><br><span class="line">print(M2$beta)</span><br></pre></td></tr></table></figure>

</details>


<p><code>M2$beta</code> 对应的输出为一个稀疏矩阵，可以看出很多分量为0。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;the coefficients of w are&quot;</span><br><span class="line">60 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span><br><span class="line">　　　　　　　s0</span><br><span class="line">V1　 -1.17769904</span><br><span class="line">V2　　.　　　　 </span><br><span class="line">V3　　.　　　　 </span><br><span class="line">V4　 -1.41760932</span><br><span class="line">V5　　.　　　　 </span><br><span class="line">V6　　.　　　　 </span><br><span class="line">V7　　.　　　　 </span><br><span class="line">V8　　.　　　　 </span><br><span class="line">V9　　.　　　　 </span><br><span class="line">V10　 .　　　　 </span><br><span class="line">V11　-2.76170230</span><br><span class="line">V12　-1.15366714</span><br><span class="line">V13　 .　　　　 </span><br><span class="line">V14　 .　　　　 </span><br><span class="line">V15　 .　　　　 </span><br><span class="line">V16　 0.07396110</span><br><span class="line">V17　 .　　　　 </span><br><span class="line">V18　 .　　　　 </span><br><span class="line">V19　 .　　　　 </span><br><span class="line">V20　-0.06880197</span><br><span class="line">V21　-0.40401291</span><br><span class="line">V22　 .　　　　 </span><br><span class="line">V23　-0.73455379</span><br><span class="line">V24　 .　　　　 </span><br><span class="line">V25　 .　　　　 </span><br><span class="line">V26　 .　　　　 </span><br><span class="line">V27　 .　　　　 </span><br><span class="line">V28　-0.11135632</span><br><span class="line">V29　 .　　　　 </span><br><span class="line">V30　 .　　　　 </span><br><span class="line">V31　 .　　　　 </span><br><span class="line">V32　 .　　　　 </span><br><span class="line">V33　 .　　　　 </span><br><span class="line">V34　 .　　　　 </span><br><span class="line">V35　 .　　　　 </span><br><span class="line">V36　 1.43526484</span><br><span class="line">V37　 .　　　　 </span><br><span class="line">V38　 .　　　　 </span><br><span class="line">V39　 .　　　　 </span><br><span class="line">V40　 .　　　　 </span><br><span class="line">V41　 .　　　　 </span><br><span class="line">V42　 .　　　　 </span><br><span class="line">V43　 .　　　　 </span><br><span class="line">V44　 .　　　　 </span><br><span class="line">V45　-2.56348735</span><br><span class="line">V46　 .　　　　 </span><br><span class="line">V47　 .　　　　 </span><br><span class="line">V48　 .　　　　 </span><br><span class="line">V49　-7.58532417</span><br><span class="line">V50　 .　　　　 </span><br><span class="line">V51 -10.02455351</span><br><span class="line">V52 -23.31103698</span><br><span class="line">V53　 .　　　　 </span><br><span class="line">V54　 .　　　　 </span><br><span class="line">V55　 .　　　　 </span><br><span class="line">V56　 .　　　　 </span><br><span class="line">V57　 .　　　　 </span><br><span class="line">V58　 .　　　　 </span><br><span class="line">V59　 .　　　　 </span><br><span class="line">V60　 .</span><br></pre></td></tr></table></figure>

</details>


<p>在第3个逻辑回归模型中，我们只考虑 _L_ 2 范数作为正则化项，其代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">family &lt;-&apos;binomial&apos;</span><br><span class="line">lambda &lt;-0.05</span><br><span class="line">alpha &lt;-0</span><br><span class="line">M3 &lt;-glmnet(X_train, y_train, family = family, lambda = lambda, alpha = alpha)</span><br></pre></td></tr></table></figure>

</details>


<p>在第4个逻辑回归模型中，我们同时考虑了 _L_ 1 范数和 _L_ 2 范数，其代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">family &lt;-&apos;binomial&apos;</span><br><span class="line">lambda &lt;-0.05</span><br><span class="line">alpha &lt;-0.3</span><br><span class="line">M4 &lt;-glmnet(X_train, y_train, family = family, lambda = lambda, alpha = alpha)</span><br></pre></td></tr></table></figure>

</details>


<p>其中 _L_ 1 范数对应的权重是<code>lambda</code> ×<code>alpha</code> =0.015， _L_ 2 范数的权重是<code>lambda</code> ×(<code>1-alpha</code> )/2= 0.0175。</p>
<p>在实际中，由于<code>lambda</code> 和<code>alpha</code> 是逻辑回归中最重要的两个参数，通常需要实验多个不同的值，得到多个不同的模型并从中选出最优的模型。在使用<code>glmnet</code> 时，可以输入多个<code>lambda</code> 值（但<code>alpha</code> 的值只允许一个），让<code>glmnet</code> 同时训练多个不同的模型。由于<code>glmnet</code> 中算法实现的缘故，同时输入多个<code>lambda</code> 的计算复杂度显著低于多次调用<code>glmnet</code> 处理单个<code>lambda</code> 值的计算复杂度。在具体使用<code>glmnet</code> 时，我们通常将<code>lambda_list</code> 中的<code>lambda</code> 值从大到小排列好（事实上，即使不排好，<code>glmnet</code> 也会在内部按照从大到小的顺序排好，但我们建议还是先排好，这样可以减少使用中的错误）。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">lambda_list &lt;-seq(0.05, 0, by=-0.005)</span><br><span class="line">alpha &lt;-0</span><br><span class="line">lambda_num &lt;-length(lambda_list)</span><br><span class="line">accuracy_train_list &lt;-rep(0, lambda_num)</span><br><span class="line">accuracy_test_list &lt;-rep(0, lambda_num)</span><br><span class="line">family &lt;-&apos;binomial&apos;</span><br><span class="line">M5_list &lt;-glmnet(X_train, y_train, family = family, lambda=lambda_list, alpha=alpha)</span><br><span class="line">y_test_predict_matrix &lt;-predict(M5_list, X_test, type=&apos;class&apos;)</span><br><span class="line">y_train_predict_matrix &lt;-predict(M5_list, X_train, type=&apos;class&apos;)</span><br><span class="line"># We compute accuracy for these models on training and test data sets</span><br><span class="line">for (i in 1:lambda_num) &#123;</span><br><span class="line">　accuracy_train_list[i] &lt;-sum(y_train==y_train_predict_matrix[,i])/n_train</span><br><span class="line">　accuracy_test_list[i] &lt;-sum(y_test==y_test_predict_matrix[,i])/n_test</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>


<p>这里<code>lambda_list</code> 包含11个不同的<code>lambda</code> 值；与此对应，<code>M5_list</code> 包含了11个不同的模型，且<code>M5_list$lambda</code> 记录了每个模型对应的<code>lambda</code> 值（严格按从大到小的顺序排列）。因此，如果输入的<code>lambda_list</code> 未排好的话，就要从<code>M5_list$lambda</code> 而不是<code>lambda_list</code> 中找到每个模型对应的<code>lambda</code> 。使用<code>predict</code> 函数计算测试集中数据的预测值时，我们调用了所有的11个模型。在这个例子中，<code>y_test_predict_matrix</code> 和<code>y_train_predict_matrix</code> 都是包含11列的矩阵，每列对应一个模型。</p>
<p>由于<code>glmnet</code> 内部能够同时处理多个不同的<code>lambda</code> 值，因此，使用下面的代码，<code>glmnet</code> 可以自动测试多个不同的<code>lambda</code> 值，并返回逻辑回归模型中参数 <strong>_w_ </strong> 随着<code>lambda</code> 的值变化的情况。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">M6 &lt;-glmnet(X_train, y_train, family = family, alpha=1)</span><br><span class="line">plot(M6)</span><br><span class="line">print(M6)</span><br><span class="line">coef6 &lt;-coef(M6, s=0.01)</span><br><span class="line">y_test_predict6 &lt;-predict(M6, X_test, s=c(0.005, 0.01), type=&apos;class&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>其中<code>plot(M6)</code> 输出解 <strong>_w_ </strong> 随着<code>lambda</code> 的变化情况，如图6-10所示。图6-10中每条曲线对应一个变量（这里有60个变量）， <strong>_x_ </strong> 轴是|| <strong>_w_ </strong> ||1 ， <strong>_y_
</strong> 轴对应 <strong>_w_ </strong> 中每个分量的大小。可以看出，当|| <strong>_w_ </strong> ||1 变小（此时<code>lambda</code> 值变大）时， <strong>_w_ </strong> 中越来越多的分量变为0。使用<code>print(M6)</code> 可以得到不同的<code>lambda</code> 值及 <strong>_w_ </strong> 中非零分量的数目。此外，使用<code>glmnet</code> 包中的<code>coef</code> 函数，可以得到<code>lambda</code> 取不同值时 <strong>_w_ </strong> 的具体值（注意，在<code>coef</code> 函数中，使用参数<code>s</code> 表示<code>lambda</code> 的值）。在上面的程序中，<code>coef6</code> 保存了当<code>lambda</code> 为0.01时的 <strong>_w_ </strong> 的具体值。对于<code>M6</code> ，在使用<code>predict</code> 函数时，可以指定参数<code>s</code> 为单个值或者多个值，<code>predict</code> 函数可以分别计算当<code>s</code> 取不同值时对应模型的预测值。在上面的代码中，<code>s</code> 为0.005和0.01，所得的<code>y_test_predict6</code> 是一个包含两列的矩阵，每列对应一个<code>s</code> 值。</p>
<p><code>glmnet</code> 包也支持交叉检验以选出最优的模型。在6.6节会详细讨论交叉检验，这里简单介绍一下<code>glmnet</code> 包中的<code>cv.glmnet</code> 函数。在<code>cv.glmnet</code> 函数中，使用<code>nfolds</code> 参数指定交叉检验的重数，利用<code>type.measure</code> 参数指定要优化的性能指标。在分类问题中，<code>type.measure</code> 的常用值包括：</p>
<ul>
<li><code>&#39;class&#39;</code> ：表示要优化分类的准确率；</li>
<li><code>&#39;auc&#39;</code> ：表示要优化分类的AUC。</li>
</ul>
<p><img src="Image00784.jpg" alt></p>
<p>图6-10 只使用 _L_ 1 范数时 <strong>_w_ </strong> 与 _L_ 1 范数的关系</p>
<p>在下面的代码中，使用5重交叉检验以选出最优的模型。使用<code>plot(M_cv)</code> ，可以得到检验集中错误率随<code>lambda</code> 的变化情况，如图6-11所示。其中第一条竖直直线对应使得检验值中准确率最高（即错误率最低）的<code>lambda</code> 值。也可以直接使用<code>M_cv$lambda.min</code> 得到最优的<code>lambda</code> 值。类似地，可以使用<code>coef</code> 函数得到<code>lambda.min</code> 对应的<br><strong>_w_ </strong> 值。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">M_cv = cv.glmnet(X_train, y_train, family = family, type.measure = &apos;class&apos;, nfolds=5)</span><br><span class="line">plot(M_cv)</span><br><span class="line"># the optimal lambda</span><br><span class="line">M_cv$lambda.min</span><br><span class="line"># get the coefficient corresponding the optimal lambda</span><br><span class="line">coef7 &lt;-coef(M_cv, s = &quot;lambda.min&quot;)</span><br><span class="line"># make prediction using the optimal lambda value</span><br><span class="line">y_test_predict7 &lt;-predict(M_cv, X_test, s=&quot;lambda.min&quot;, type=&apos;class&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>在前面的4个逻辑回归模型中，指定单个的<code>lambda</code> 和<code>alpha</code> 值并得到对应的模型。在实际使用中，通常考虑一系列的<code>lambda</code> 值，为每个<code>lambda</code> 值训练一个逻辑回归模型，并从中选取最优的模型。因此，后面讲解的<code>glmnet</code> 使用方法在实际中使用更多一些。</p>
<p><img src="Image00785.jpg" alt></p>
<p>图6-11 使用<code>cv.glmnet</code> 时检验集上错误率随<code>lambda</code> 的变化情况</p>
<h2 id="6-4-支持向量机"><a href="#6-4-支持向量机" class="headerlink" title="6.4 支持向量机"></a>6.4 支持向量机</h2><p>支持向量机（support vector machine, SVM）发端于统计学习理论，具有非常坚实的统计学习理论支撑。支持向量机最初于1992年提出，随后由于其在手写数字识别上的成功应用，迅速在很多领域取得了广泛的应用。特别是SVM与核方法<br>（kernel method）紧密联系起来，成为机器学习前些年较热门的研究方向之一。本节首先介绍SVM的基本思想，即最大化分类间隔。从最大化分类间隔出发，我们讨论SVM在训练数据线性可分及线性不可分情况下的具体形式，并介绍Hinge损失函数。然后，简要介绍SVM中所得优化问题的对偶形式以求解SVM；在此基础上，引入核<br>（kernel）的概念并简要介绍常用的核函数。最后，介绍R中的<code>e1071</code> 包，讲解如何使用该包来对实际数据进行分类。</p>
<h3 id="6-4-1-基本思想：最大化分类间隔"><a href="#6-4-1-基本思想：最大化分类间隔" class="headerlink" title="6.4.1 基本思想：最大化分类间隔"></a>6.4.1 基本思想：最大化分类间隔</h3><p>在前一节中我们已经讨论了线性分类器。线性分类器的核心思想是在特征空间中找出一个超平面来将正类样本和负类样本分开。在逻辑回归中，分类的超平面由使得<img src="Image00786.gif" alt> 的点组成，换言之，就是由使得<img src="Image00787.gif" alt> 的点组成。支持向量机也属于线性分类器。在支持向量机中，我们假设线性分类器表示为：</p>
<p><img src="Image00788.gif" alt></p>
<p>（6-36）</p>
<p>在前面的讨论中，我们都假设 _<strong>x</strong> _ 中插入了一个分量1，这样就不用显式地考虑 _b_ 了。在本节中，为了和大多数文献中的写法一致，我们直接考虑 _b_ 。同时，我们假设在本节的讨论中，类标<img src="Image00789.gif" alt> 或者-1。</p>
<p>什么样的线性分类器才是最好的线性分类器？我们从最简单的二维数据开始讨论线性分类器。在图6-12中，我们给出了一个二维空间的分类问题的例子。我们用圆点表示正类样本，用方块表示负类样本。二维空间中的线性分类器对应于一条直线<img src="Image00790.gif" alt> 。从图6-12中可以看出，我们可以构造出无数个线性分类器（图6-12中给出了3个不同的线性分类器）将这组数据分开。那么问题是：什么样的线性分类器是最优的呢？</p>
<p><img src="Image00791.jpg" alt></p>
<p>图6-12 不同的线性分类器都可以将这组数据分开</p>
<p>对于某个给定的线性分类器，可以定义它所对应的分类间隔 （classification margin）。这里分类间隔的定义为从<img src="Image00790.gif" alt> 出发，离<img src="Image00506.gif" alt> 最近的正类样本到<img src="Image00506.gif" alt> 的距离加上离<img src="Image00506.gif" alt> 最近的负类样本到<img src="Image00506.gif" alt> 的距离。图6-13中显示了两个线性分类器及它们对应的分类间隔。</p>
<p><img src="Image00792.jpg" alt></p>
<p>图6-13 两个不同的线性分类器 _f_ 1 （a）和 _f_ 2 （b）及它们对应的分类间隔</p>
<p>从直观上讲，使得分类间隔最大的线性分类器是一个最保险的线性分类器。当我们使用训练集确定线性分类器<img src="Image00506.gif" alt> 的位置时，事实上是使用训练集中有限的数据来推断整体的样本分布，从而构建最优的分类器。如果训练集中的数据出了一些差错，那么最大间隔的分类器犯错的机会最小。在统计学习中，可以严格地证明最大分类间隔的分类泛化<br>（generalization）能力最强。图6-13（a）中的线性分类器对应最大的分类间隔，是我们需要的最优线性分类器。</p>
<p>同时，我们也要注意到，线性分类器<img src="Image00506.gif" alt> 事实上只需要由离分类器最近的正类和负类样本就可以决定了，这些点称为支持向量 （support vector）。事实上，如果将训练集中除支持向量之外的样本去除，仍然能得到一样的分类器。</p>
<p>以上都是对最大化分类间隔的直观解释。在统计学习理论领域，有很多基于VC维 （VC dimension）的研究从学习理论的角度来解释为何最大化分类间隔是最优的。感兴趣的读者可参考相应文献[26]。</p>
<h3 id="6-4-2-最大分类间隔的数学表示"><a href="#6-4-2-最大分类间隔的数学表示" class="headerlink" title="6.4.2 最大分类间隔的数学表示"></a>6.4.2 最大分类间隔的数学表示</h3><p>接下来的问题是如何将最大化分类间隔表示成为一个优化问题并最终求解。这里首要的问题是将分类间隔用<img src="Image00506.gif" alt> 的参数 <strong>_w_ </strong> 、 _b_ 及训练集表示出来。</p>
<p>我们知道，<img src="Image00793.gif" alt> 和<img src="Image00794.gif" alt> 实质上是一条直线，这里<img src="Image00795.gif" alt> 是一个实数。换言之，一条直线有无数种表示方法，我们需要限制 <strong>_w_ </strong> 和 _b_ 的大小来确定唯一的解。在支持向量机中，一般规定分类间隔由直线<img src="Image00796.gif" alt> 和<img src="Image00797.gif" alt> 确定，其中直线<img src="Image00796.gif" alt> 经过最靠近分类边界的正类样本，直线<img src="Image00797.gif" alt> 经过最靠近分类边界的负类样本，而分类边界由直线<img src="Image00793.gif" alt> 确定，如图6-14所示。</p>
<p><img src="Image00798.jpg" alt></p>
<p>图6-14 分类中的3条直线：<img src="Image00799.gif" alt></p>
<p>这样，对于正类样本（即<img src="Image00800.gif" alt> ），我们要求：</p>
<p><img src="Image00801.gif" alt></p>
<p>（6-37）</p>
<p>对于负类样本（即<img src="Image00802.gif" alt> ），我们要求：</p>
<p><img src="Image00803.gif" alt></p>
<p>（6-38）</p>
<p>综合起来可以将约束条件写成：</p>
<p><img src="Image00804.gif" alt></p>
<p>（6-39）</p>
<p>在支持向量机中，我们的目标是最大化分类间隔，因此，要推导出给定训练集和分类器<img src="Image00805.gif" alt> 的情况下分类间隔的数学表示。利用解析几何的知识，我们知道给定一个点 <strong>_x_ </strong> ，它到直线<img src="Image00805.gif" alt> 的距离为：</p>
<p><img src="Image00806.gif" alt></p>
<p>（6-40）</p>
<p>这里<img src="Image00807.gif" alt> 。</p>
<p>根据之前的假设，分类间隔就是直线<img src="Image00808.gif" alt> 和直线<img src="Image00809.gif" alt> 之间的距离。在直线<img src="Image00809.gif" alt> 上任取一点<img src="Image00810.gif" alt> ，则该点满足：</p>
<p><img src="Image00811.gif" alt></p>
<p>（6-41）</p>
<p>这样我们可以直接利用前述的点到直线的距离公式，得到<img src="Image00810.gif" alt> 到<img src="Image00812.gif" alt> 的距离为：</p>
<p><img src="Image00813.gif" alt></p>
<p>（6-42）</p>
<p>根据分类间隔的计算公式，我们需要最大化<img src="Image00814.gif" alt> ，等价于最小化<img src="Image00815.gif" alt> ，即等价于最小化<img src="Image00816.gif" alt> 。因此，在训练集线性可分的情况下，要求解的优化问题是：</p>
<p><img src="Image00817.gif" alt></p>
<p>（6-43）</p>
<p>在这个优化问题中，要求解最优的 _<strong>w</strong> _ 和 _b_ 。注意，目标函数是关于 <strong>_w_ </strong> 的二次函数，而所有的约束条件都是关于 _<strong>w</strong> _ 和 _b_ 的线性函数。这样的优化问题称为二次规划（quadratic programming，QP）。由于二次规划仍然是凸优化问题，因此可以求得全局最优解。我们在下面讨论该优化问题的对偶问题，并引入核 （kernel）的概念。</p>
<h3 id="6-4-3-如何处理线性不可分的数据"><a href="#6-4-3-如何处理线性不可分的数据" class="headerlink" title="6.4.3 如何处理线性不可分的数据"></a>6.4.3 如何处理线性不可分的数据</h3><p>在实际应用中，很多时候数据是线性不可分的。换言之，不存在一个线性分类器<img src="Image00805.gif" alt> 能把所有的正类样本和负类样本分开，如图6-15所示。在这种情况下，理想的线性分类器应该既能够最大化分类间隔，又能减少错误分类的样本数目。</p>
<p><img src="Image00818.jpg" alt></p>
<p>图6-15 线性不可分的二维数据</p>
<p>既然不能将训练集中的所有样本都正确分类，那么一个很自然的想法是尽量多地正确分类训练集中的样本。换言之，就是最大化分类准确率。但在实际的分类算法中，很少有算法去直接最大化准确率（也即最小化错误分类的样本数），原因包括：（1）不好用简单的数学公式表示分类准确率；（2）就算表示出来，一般也不易求解所得的优化问题。</p>
<p>因此，很多分类算法都不直接处理错误分类的样本数，而用一些在优化问题中容易处理的“近似项”来逼近它，从而求得最优解。具体来说，在支持向量机中，当样本不能使用线性分类器直接分类时，对于训练集中的每个样本引入松弛变量<img src="Image00556.gif" alt> ，并且用松弛变量<img src="Image00556.gif" alt> 的和来近似错误分类的样本数，从而求解相应的优化问题。</p>
<p>严格地讲，对于每个样本，引入松弛变量<img src="Image00556.gif" alt> ，并要求下面的约束条件成立：</p>
<p><img src="Image00819.gif" alt></p>
<p>（6-44）</p>
<p>这里假设<img src="Image00789.gif" alt> 来简化我们的讨论，对于<img src="Image00802.gif" alt> 的讨论，与之类似。由上式，约束条件变为：</p>
<p><img src="Image00820.gif" alt></p>
<p>（6-45）</p>
<p>对于样本<img src="Image00821.gif" alt> ，考虑如下几种情况下满足上述不等式的最小的<img src="Image00556.gif" alt> 值：</p>
<ul>
<li>如果<img src="Image00821.gif" alt> 被正确分类且在分类间隔外，则<img src="Image00822.gif" alt> 。</li>
<li>如果<img src="Image00821.gif" alt> 被正确分类且在分类间隔中，则0&lt;<img src="Image00823.gif" alt> &lt;1。</li>
<li>如果<img src="Image00821.gif" alt> 没有被正确分类，则<img src="Image00824.gif" alt> 。</li>
</ul>
<p>具体而言，如果点<img src="Image00821.gif" alt> 被正确分类且在分类间隔外，则有<img src="Image00825.gif" alt> ，此时可取<img src="Image00822.gif" alt> 。如果<img src="Image00730.gif" alt> 被正确分类且在分类间隔中，当<img src="Image00821.gif" alt> 是正类时，我们有0&lt;<img src="Image00826.gif" alt> &lt;1，此时可取0&lt;<img src="Image00556.gif" alt> &lt;1。如果正类样本<img src="Image00821.gif" alt> 被错误地分成负类，则<img src="Image00827.gif" alt> ，此时可取<img src="Image00824.gif" alt> 。综合考虑这3种情况，<img src="Image00828.gif" alt> 是错误分类的样本数的上界，即<img src="Image00828.gif" alt> 始终大于或等于被错分的样本数。图6-16给出了3个点对应的<img src="Image00556.gif" alt> 。从图6-16中我们可以清楚地理解<img src="Image00556.gif" alt> 的几何意义。</p>
<p>严格地讲，在线性不可分的情况下我们最小化如下函数：</p>
<p><img src="Image00829.gif" alt></p>
<p>（6-46）</p>
<p><img src="Image00830.jpg" alt></p>
<p>图6-16 数据不是线性可分时所引入的惩罚项 _ε i _</p>
<p>这里的目标函数由两部分组成：第一部分对应于最大化分类间隔，第二部分对应于训练集中被错误分类的样本数的上界。注意，我们给第一部分使用了系数1/2是为了便于下面的推导。第一部分和第二部分的权重由参数 _c_ 来控制。一般在实际使用中，我们使用6.6节介绍的交叉检验来确定 _c_ 的最优值。</p>
<h3 id="6-4-4-Hinge损失函数"><a href="#6-4-4-Hinge损失函数" class="headerlink" title="6.4.4 Hinge损失函数"></a>6.4.4 Hinge损失函数</h3><p>在逻辑回归中我们已经讨论了交叉熵损失函数，本节讨论支持向量机中使用的Hinge损失函数 。</p>
<p>在分类问题中，最直接的损失函数就是所谓的“0-1”损失函数，定义如下：</p>
<p><img src="Image00831.jpg" alt></p>
<p>（6-47）</p>
<p>0-1损失函数的定义直接明了：如果对于样本 <strong>_x_ </strong> 分类正确，则损失函数为0；否则为1。那么，最小化0-1损失函数就是直接最小化被错分的样本数目。但0-1损失函数在实际中很少使用，因为它既不好表示，也不好求导，一般用在算法的性能评价中。实际上，在机器学习的很多算法中，我们都使用一个近似函数来逼近0-1损失函数作为要优化的对象。</p>
<p>在支持向量机中，我们使用<img src="Image00556.gif" alt> 的和作为0-1损失函数的上界，将所得的损失函数称为Hinge损失函数。我们再回到<img src="Image00828.gif" alt> 的分析。对于样本<img src="Image00730.gif" alt> ，假设<img src="Image00800.gif" alt> ，下面具体考虑<img src="Image00556.gif" alt> 与<img src="Image00832.gif" alt> 的关系。</p>
<ul>
<li>如果<img src="Image00821.gif" alt> 被正确分类且在分类间隔外，则<img src="Image00833.gif" alt> 。满足条件<img src="Image00834.gif" alt> 的<img src="Image00556.gif" alt> 的最小值为<img src="Image00822.gif" alt> 。</li>
<li>如果<img src="Image00730.gif" alt> 被正确分类且在分类间隔中，则0&lt;<img src="Image00832.gif" alt> &lt;1。满足条件<img src="Image00834.gif" alt> 的<img src="Image00556.gif" alt> 的最小值为<img src="Image00835.gif" alt> 。</li>
<li>如果<img src="Image00730.gif" alt> 没有被正确分类，则<img src="Image00836.gif" alt> 。满足条件<img src="Image00837.gif" alt> 的<img src="Image00556.gif" alt> 的最小值也是<img src="Image00838.gif" alt> 。</li>
</ul>
<p>综合以上3种情况，在<img src="Image00800.gif" alt> 时，满足条件<img src="Image00834.gif" alt> 的<img src="Image00556.gif" alt> 的最小值为：</p>
<p><img src="Image00839.gif" alt></p>
<p>（6-48）</p>
<p>当<img src="Image00802.gif" alt> 时，我们有类似的分析，满足条件<img src="Image00840.gif" alt> 的<img src="Image00556.gif" alt> 的最小值为：</p>
<p><img src="Image00841.gif" alt></p>
<p>（6-49）</p>
<p>将以上两种情况综合起来，满足条件<img src="Image00842.gif" alt> 的<img src="Image00556.gif" alt> 的最小值为：</p>
<p><img src="Image00843.gif" alt></p>
<p>（6-50）</p>
<p>根据上面的分析，我们引入Hinge损失函数，其严格定义如下：</p>
<p><img src="Image00844.gif" alt></p>
<p>（6-51）</p>
<p>图6-17给出了Hinge函数和0-1函数的图像，其中横坐标是 _yf_ （这里我们仍然假定 _y_ 的取值为1或者-1），纵坐标是对应的函数值。0-1函数无论错误多大，只要错了（当<img src="Image00845.gif" alt> 时），就给出惩罚1；只要分类正确（当 _yf_ &gt;0时），则不给予惩罚。根据Hinge函数的图像，我们可以看到 _f_ 如果分类错误（当<img src="Image00845.gif" alt> 时），错误越大，Hinge函数给予的惩罚越大；而且就算是 _f_ 分类正确，如果 _yf_ 的值小于1，Hinge函数仍然给予惩罚。在前面的讨论中，我们指出<img src="Image00846.gif" alt> 是被错分的样本数，即0-1损失函数的上界。从图6-17中也可以明确地看出，Hinge函数的值一直在0-1损失函数之上。</p>
<p><img src="Image00847.jpg" alt></p>
<p>图6-17 Hinge损失函数</p>
<p>损失函数在机器学习中非常重要，它定义了模型 _f_ 在训练集上对于训练数据的拟合好坏。除了损失函数，在机器学习中，我们一般在待优化的目标函数中添加正则化项来控制模型的复杂度，避免所得到的模型在训练集上过拟合。在支持向量机中，我们知道所对应的损失函数是Hinge函数，而正则化项实际对应于分类器的分类间隔。从直观的角度来看，分类间隔对应着模型的复杂度。分类间隔越大，模型的复杂度越低。</p>
<h3 id="6-4-5-对偶问题"><a href="#6-4-5-对偶问题" class="headerlink" title="6.4.5 对偶问题"></a>6.4.5 对偶问题</h3><p>接下来我们讨论支持向量机中优化问题的对偶问题并引出核学习的基本概念。阅读这部分内容需要一些优化方面的知识，如果读者对于支持向量机的具体求解不感兴趣，那么可以直接跳至6.4.7节了解如何使用R中的软件包来使用支持向量机进行分类。</p>
<p>由于线性可分所对应的优化问题是线性不可分的特例，我们直接讨论线性不可分的情况。在线性不可分的情况下，需要求解如下优化问题：</p>
<p><img src="Image00848.gif" alt></p>
<p>（6-52）</p>
<p>这是一个二次规划问题。求解的难点在于对约束条件的处理。在优化理论中，利用拉格朗日函数可以处理约束条件。下面我们推导出该优化问题的对偶问题 （dual problem）。首先引入拉格朗日函数 _L_ ：</p>
<p><img src="Image00849.gif" alt></p>
<p>（6-53）</p>
<p>这里引入的新变量<img src="Image00850.gif" alt> 、<img src="Image00851.gif" alt> （<img src="Image00517.gif" alt> ）称为拉格朗日乘子 （Lagrange multiplier）。对于优化问题中的每个约束条件，我们都引入一个拉格朗日乘子。注意，如果限定所有的拉格朗日乘子都为非负的话，就可以保证 _L_ 始终小于或等于原始问题的最优解。在对偶问题中，我们要消去原始变量 <strong>_w_ </strong> 、 _b_ 、 _ε i _ 。因此，首先求解 _L_ 关于原始变量<br><strong>_w_ </strong> 、 _b_ 、 _ε i _ 的导数并将其置为0，从而在 _L_ 中消去原始变量，只保留新引进的变量。然后，最大化所得的 _L_ ，其解等价于原始优化问题的最优解。这里我们省略深奥的对偶理论，有兴趣的读者可以阅读参考文献[27]的第5章。</p>
<p>将 _L_ 对原始变量 <strong>_w_ </strong> 、 _b_ 、 _ε i _ 求导并令结果为0，得到</p>
<p><img src="Image00852.gif" alt></p>
<p>（6-54）</p>
<p><img src="Image00853.gif" alt></p>
<p>（6-55）</p>
<p><img src="Image00854.gif" alt></p>
<p>（6-56）</p>
<p>因此，有</p>
<p><img src="Image00855.gif" alt></p>
<p>（6-57）</p>
<p><img src="Image00856.gif" alt></p>
<p>（6-58）</p>
<p><img src="Image00857.gif" alt></p>
<p>（6-59）</p>
<p>将其代入回 _L_ 中，可得</p>
<p><img src="Image00858.gif" alt></p>
<p><img src="Image00859.gif" alt></p>
<p>（6-60）</p>
<p>在Karush-Kuhn-Tucker（KKT）条件中，我们要求<img src="Image00860.gif" alt> 。根据<img src="Image00861.gif" alt> 可知<img src="Image00862.gif" alt> 。综上所述，在对偶问题中，我们要求解的优化问题是：</p>
<p><img src="Image00863.gif" alt></p>
<p><img src="Image00864.gif" alt></p>
<p>（6-61）</p>
<p><img src="Image00865.gif" alt></p>
<p>这个对偶问题依然是一个二次规划问题。很多求解二次规划的算法都可以用来求解该问题。近年来，很多新的算法也提出用来快速求解该问题。本书省略具体的算法，感兴趣的读者可以参阅libsvm<br>② 等求解方法。</p>
<p>下面简单讲解一下KKT条件。KKT条件是原始优化问题和对偶问题联系的纽带。我们这里只给出结论，对于KKT条件感兴趣的读者可以参考文献[27]。在KKT条件中，对于每条不等式约束条件<img src="Image00866.gif" alt> ，我们要求拉格朗日乘子和对应的约束条件 _g_ 的乘积为0，且对应的拉格朗日乘子非负。这里直接列出SVM中优化问题的KKT条件：</p>
<p><img src="Image00867.gif" alt></p>
<p><img src="Image00868.gif" alt></p>
<p><img src="Image00869.gif" alt></p>
<p>（6-62）</p>
<p><img src="Image00870.gif" alt></p>
<p><img src="Image00871.gif" alt></p>
<p><img src="Image00872.gif" alt></p>
<p>从原始问题的解我们已知<img src="Image00873.gif" alt> 。如果点<img src="Image00730.gif" alt> 被正确分类且在分类间隔之外，即</p>
<p><img src="Image00874.gif" alt> &gt;1</p>
<p>（6-63）</p>
<p>则<img src="Image00875.gif" alt> &gt;0。由条件<img src="Image00876.gif" alt> 可知<img src="Image00877.gif" alt> ；换言之，计算<br><strong>_w_ </strong> 时可省略该点。而那些位于分类间隔之间的点及错分的点，其对应的<img src="Image00878.gif" alt> &gt;0；换言之， <strong>_w_ </strong> 只由那些使得<img src="Image00878.gif" alt> &gt;0的点决定，我们把这样的点<img src="Image00821.gif" alt> 称为支持向量。</p>
<h3 id="6-4-6-非线性支持向量机和核技巧"><a href="#6-4-6-非线性支持向量机和核技巧" class="headerlink" title="6.4.6 非线性支持向量机和核技巧"></a>6.4.6 非线性支持向量机和核技巧</h3><p>在前面的对偶问题中，我们注意到，给定训练集<img src="Image00879.gif" alt> ，在学习 <strong>_w_ </strong> 和 _b_ 的时候，对于<img src="Image00880.gif" alt> ，我们只用到了<img src="Image00881.gif" alt> 。利用所得的分类器<img src="Image00805.gif" alt> 去预测一个新样本 <strong>_x_ </strong> 时，我们有：</p>
<p><img src="Image00882.gif" alt></p>
<p>（6-64）</p>
<p>可以看出，在预测的时候我们同样只需要计算<img src="Image00883.gif" alt> 的值即可。因此，在应用支持向量机时，除了类标信息<img src="Image00543.gif" alt> 外，数据本身的信息只需要用到样本间的内积<img src="Image00881.gif" alt> 。</p>
<p>另一方面，在前面所讨论的支持向量机中，我们所求的是特征空间中的一个超平面。在实际中，数据的分布可能很复杂，这时仅仅使用一个超平面无法将正负类的样本分开。在核学习中，我们将数据从原来的低维空间映射到高维甚至无限维空间中，使得正负类的样本能够在新的空间中容易分类。将数据映射到更高维空间的做法，与机器学习中处理数据的常用方法相悖。特别是高维数据容易带有冗余信息，引起维数灾难<br>（curse of dimensionality）。但在一些实际问题中，我们的确需要将数据映射到更高维空间中。下面用一个简单的分类问题来说明映射到高维空间的必要性。</p>
<p>图6-18（a）给出了一组无法使用线性分类器分类的一维数据。图6-18（a）中圆点对应一类，方块对应另一类。可以看出，在一维空间中无论如何构建线性分类器，都无法完美地将两类分开。为了将两类样本点分开，使用如下映射将数据从一维空间映射到二维空间中：</p>
<p><img src="Image00884.gif" alt></p>
<p>（6-65）</p>
<p>图6-18（b）显示了映射到二维空间后数据的分布。可以看出，在二维空间中，可以轻易地使用一条直线将两类样本分开。</p>
<p><img src="Image00885.jpg" alt></p>
<p>图6-18 映射到高维空间的必要性示例</p>
<p>在这个例子中，我们显式地定义了映射<img src="Image00886.gif" alt> 。而核学习的另一个优点是并不需要直接知道映射<img src="Image00886.gif" alt> ，而只需要知道<img src="Image00887.gif" alt> ，即<img src="Image00888.gif" alt> 映射到高维空间后的内积即可。下面利用支持向量机中的对偶问题来说明这一点。使用映射<img src="Image00889.gif" alt> 后，支持向量机求解的对偶问题是：</p>
<p><img src="Image00890.gif" alt></p>
<p><img src="Image00891.gif" alt></p>
<p>（6-66）</p>
<p><img src="Image00865.gif" alt></p>
<p>因此，在求解支持向量机的过程中，我们真正要利用到的信息是：</p>
<p><img src="Image00892.gif" alt></p>
<p>（6-67）</p>
<p>这里 _K_ 称为核函数 （kernel function）。这样对偶问题可以写为：</p>
<p><img src="Image00893.gif" alt></p>
<p>（6-68）</p>
<p>当求得高维空间中的支持向量机模型后，可以利用所得的分类器<img src="Image00894.gif" alt> 去预测一个新样本 _φ_ ( <strong>_x_ </strong> )：</p>
<p><img src="Image00895.gif" alt></p>
<p>（6-69）</p>
<p>因此，在整个使用支持向量机进行分类的过程中，只需要知道 _K_ 。直观地讲，<img src="Image00896.gif" alt> 反映了<img src="Image00730.gif" alt> 和<img src="Image00897.gif" alt> 之间的相似度。在计算中，对于 _K_ 对应的映射 _φ_ 其实并不需要知道。这种通过直接计算核函数而省略先计算映射 _φ_ 再计算内积的办法，称为核技巧 （kernel trick）。这里给出一个具体的例子来说明通过使用核技巧，可以快速地计算<img src="Image00896.gif" alt> 的值。</p>
<p><strong>例6-4</strong> 假设将三维向量<img src="Image00898.gif" alt> 和<img src="Image00899.gif" alt> 通过映射 _φ_ 映射到十维空间。其中映射 _φ_ 的定义如下：</p>
<p><img src="Image00900.gif" alt></p>
<p>（6-70）</p>
<p>这样，映射后计算<img src="Image00901.gif" alt> 和<img src="Image00902.gif" alt> 的内积可得：</p>
<p><img src="Image00903.gif" alt></p>
<p>（6-71）</p>
<p>因此，在这个例子中，无须计算映射<img src="Image00901.gif" alt> 和<img src="Image00902.gif" alt> ，只需要计算<img src="Image00904.gif" alt> 就可以得到<img src="Image00901.gif" alt> 和<img src="Image00902.gif" alt> 的内积。</p>
<p>在实际使用支持向量机时，常用的核包括以下几个。</p>
<ul>
<li>多项式核 （polynomial kernel）：</li>
</ul>
<p><img src="Image00905.gif" alt></p>
<p>（6-72）</p>
<p>这里 _d_ 是正整数，<img src="Image00906.gif" alt> 为控制参数。</p>
<ul>
<li>高斯核 （radial basis kernel）：</li>
</ul>
<p><img src="Image00907.gif" alt></p>
<p>（6-73）</p>
<p>这里<img src="Image00066.gif" alt> &gt;0是控制参数。</p>
<p>线性核 （linear kernel）：当<img src="Image00908.gif" alt> 时，把所得的核称为线性核。</p>
<p><img src="Image00909.gif" alt></p>
<p>（6-74）</p>
<p>在实际中，根据数据的具体特征，用户可选择适当的核函数。通过使用核技巧，能够将非线性引入到分类问题中，同时计算复杂度也不高。但核的引入也可能导致过拟合的问题，同时也有不直观的问题。</p>
<h3 id="6-4-7-实际使用"><a href="#6-4-7-实际使用" class="headerlink" title="6.4.7 实际使用"></a>6.4.7 实际使用</h3><p>这里利用R中的<code>e1071</code> ③ 软件包来使用支持向量机对数据进行分类。在R中，有多种软件包实现了支持向量机，例如，<code>LiblineaR</code> 包提供了<code>LIBLINEAR</code> ④ 的C/C++实现的R接口，但<code>LiblineaR</code> 只提供了线性核的实现。<code>e1071</code> 包提供了著名的<code>L</code> IBSVM的R接口，并提供了包括线性核、多项式核、高斯核等多种核的支持。<code>e1071</code> 软件包能够同时实现分类和回归，本节主要介绍如何使用<code>e1071</code> 软件包进行分类。</p>
<p>在<code>e1071</code> 包中，使用函数<code>svm</code> 训练支持向量机。在函数<code>svm</code> 中，有如下的主要参数需要设置。</p>
<ul>
<li>核的选定。在<code>e1071</code> 中，提供了线性核、高斯核、多项式核和sigmoid核。具体而言，参数<code>kernel</code> 可设为<code>&#39;linear&#39;</code> （线性核）、<code>&#39;radial&#39;</code> （高斯核）、<code>&#39;polynomial&#39;</code> （多项式核）和<code>&#39;sigmoid&#39;</code> （sigmoid核）。</li>
<li>对应核的参数。在高斯核中，对应的参数是<code>gamma</code> （这里<code>gamma</code> 等于前面高斯核公式中的<img src="Image00910.gif" alt> ）。在多项式核中，通过对应的参数<code>degree</code> 设置多项式的次数，通过<code>coef0</code> 设定 _R_ 。</li>
<li><code>cost</code> 值，即支持向量机目标函数中正则化项的系数 _c_ 。</li>
<li><code>scaling</code> 用来表示对各个特征是否进行正则化处理。默认设置为对所有变量都进行正则化处理。</li>
<li><code>type</code> 值用来指定是分类还是回归。在本节中全部使用默认设置”<code>C-classification</code> “。</li>
</ul>
<p>这里只介绍其中最重要的参数。关于<code>svm</code> 的更多参数设置及<code>e1071</code> 包中其他函数的使用，可参见其手册 ⑤ 。</p>
<p>在样例程序SVM_example1.R中，我们给出了6个例子。如设置一个线性核，并将<code>cost</code> 的值设为1：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svm.linear.model1 &lt;-svm(class ~., data = D_train, kernel = &apos;linear&apos;, cost = 1)</span><br></pre></td></tr></table></figure>
<p>也可以改变<code>cost</code> 的值为100：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svm.linear.model2 &lt;-svm(class ~., data = D_train, kernel = &apos;linear&apos;, cost = 100)</span><br></pre></td></tr></table></figure>
<p>同样，可以设置一个高斯核，并将<code>gamma</code> 参数设为0.01：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svm.radial.model1 &lt;-svm(class ~., data = D_train, kernel = &apos;radial&apos;, cost = 1, gamma = 1e-2)</span><br></pre></td></tr></table></figure>
<p>可通过如下代码改变<code>gamma</code> 的值为1：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svm.radial.model2 &lt;-svm(class ~., data = D_train, kernel = &apos;radial&apos;, cost = 1, gamma = 1)</span><br></pre></td></tr></table></figure>
<p>最后可设置多项式核，并将<code>degree</code> 设为2或3：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">svm.poly.model1 &lt;-svm(class ~., data = D_train, kernel = &apos;polynomial&apos;, degree = 2, cost = 1)</span><br><span class="line">svm.poly.model2 &lt;-svm(class ~., data = D_train, kernel = &apos;polynomial&apos;, degree = 3, cost = 1)</span><br></pre></td></tr></table></figure>

</details>


<p>在我们提供的示例代码中，将整个数据集合随机分为训练集和测试集，并通过设置不同的参数来学习得到不同的支持向量机分类器。这些分类器在测试集上的准确率也在运行时打印出来。</p>
<p>在分类中，用得最多且一般来说效果最好的核是高斯核。而且使用高斯核需要调节的参数也较少：<code>cost</code> 值和<code>gamma</code> 值。对于这两个参数值的设定，我们建议首先通过交叉检验来确定<code>cost</code> 的值。例如，我们可以将<code>cost</code> 的值设为1～1000之间（当然可以更大，因为参数的设置完全依赖于数据），然后选出最优的<code>cost</code> 值。确定好<code>cost</code> 的值之后，我们再试验几个不同的<code>gamma</code> 值来找到最优的<code>gamma</code> 值。当然，想要得到最优的参数设置组合，我们推荐格搜索 （grid search）。在<code>e1071</code> 包中，可以使用包中提供的<code>tune.svm</code> 函数完成格搜索。在下面的例子中，我们使用<code>tune.svm</code> 寻找高斯核的最优参数：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tune.obj &lt;-tune.svm(class~., data = D_train, </span><br><span class="line">　　　　　　　　　　 gamma = c(1e-4, 1e-3, 1e-2), </span><br><span class="line">　　　　　　　　　　 cost = c(1, 10))</span><br><span class="line">print(&apos;summary of the cross-validation&apos;)</span><br><span class="line">print(summary(tune.obj))</span><br><span class="line"># Extract the optimal parameter value</span><br><span class="line">gamma_best &lt;-tune.obj$best.parameters$gamma</span><br><span class="line">cost_best &lt;-tune.obj$best.parameters$cost</span><br></pre></td></tr></table></figure>

</details>


<p>但要注意，求解高斯核对应的优化问题的计算复杂度较大，因此，在实际使用支持向量机时，要根据数据的规模和可以允许的计算时间选择合理的参数。</p>
<h2 id="6-5-损失函数和不同的分类算法"><a href="#6-5-损失函数和不同的分类算法" class="headerlink" title="6.5 损失函数和不同的分类算法"></a>6.5 损失函数和不同的分类算法</h2><p>在前面的讨论中我们已经接触过不同的损失函数，如交叉熵损失函数和Hinge损失函数。损失函数在机器学习中非常重要，它是区别各个分类算法的核心因素。各个分类算法的差异中最突出的就是所优化的损失函数不同。</p>
<p>在机器学习的很多算法中，最小化的目标函数 _F_ 通常为如下形式：</p>
<p><img src="Image00911.gif" alt></p>
<p>（6-75）</p>
<p>其中 _L_ 是损失函数； _Reg_ 是正则化项<br>（regularization），用来控制所得模型的复杂度以避免所得的模型在训练集上过拟合；<img src="Image00912.gif" alt> 用于控制损失函数和正则化项的权重。</p>
<p>在前面关于回归和分类算法的讨论中，很多算法都可以归入这一框架：</p>
<ul>
<li>在Lasso中，损失函数是平方和损失函数，而正则化项是<img src="Image00583.gif" alt> 范数；</li>
<li>在岭回归中，损失函数也是平方和函数，但正则化项是<img src="Image00573.gif" alt> 范数；</li>
<li>在逻辑回归中，损失函数是交叉熵损失函数，正则化项是<img src="Image00573.gif" alt> 范数（也可以加入<img src="Image00583.gif" alt> 范数）；</li>
<li>在支持向量机中，损失函数是Hinge损失函数，正则化项是<img src="Image00573.gif" alt> 范数。</li>
</ul>
<p>在本节中，我们专门介绍分类算法中常用的各种损失函数。同时，也结合回归算法讨论一下不同的正则化项。在下面的讨论中，我们考虑数据集<img src="Image00879.gif" alt> ，这里<img src="Image00913.gif" alt> 是特征，<img src="Image00914.gif" alt> 是对应的类标。注意，这里为了讨论方便，假设<img src="Image00543.gif" alt> 的取值是-1或者1。假设考虑的模型表示为 _f_ ，对于<img src="Image00730.gif" alt> 的预测值记为<img src="Image00915.gif" alt> 。在损失函数中，因为我们通常直接考虑 _y_ 与<img src="Image00504.gif" alt> 的关系，所以通常也将 _L_ 记为<img src="Image00916.gif" alt> 。</p>
<h3 id="6-5-1-损失函数"><a href="#6-5-1-损失函数" class="headerlink" title="6.5.1 损失函数"></a>6.5.1 损失函数</h3><p>在分类问题中，定义在数据集<img src="Image00879.gif" alt> 上的损失函数可以表示为：</p>
<p><img src="Image00917.gif" alt></p>
<p>（6-76）</p>
<p>换言之，可以将损失函数分解为在每个样本点上的损失。下面我们逐一介绍对于样本点<img src="Image00727.gif" alt> 的常用损失函数，并将 _L_ 表示为关于<img src="Image00918.gif" alt> 的函数，包括：</p>
<ul>
<li>0-1损失函数；</li>
<li>Hinge损失函数；</li>
<li>交叉熵损失函数；</li>
<li>指数损失函数；</li>
<li>平方和损失函数。</li>
</ul>
<p>为什么在分类问题中要将损失函数 _L_ 表示为关于<img src="Image00918.gif" alt> 的函数呢？我们知道，在回归问题中我们感兴趣的是残差<img src="Image00919.gif" alt> ，因此经常将损失函数表示为关于残差的函数。在分类问题中，如果类标 _y_ 表示为1或者-1的话，那么<img src="Image00918.gif" alt> 就相当于回归问题中的<img src="Image00919.gif" alt> 。进一步讲，如果<img src="Image00918.gif" alt> &gt;0，那么表示 <strong>_x_ </strong> 被<img src="Image00504.gif" alt> 成功分类；如果<img src="Image00920.gif" alt> ，那么表示 <strong>_x_ </strong> 被<img src="Image00504.gif" alt> 错误分类。在损失函数的设计中，应该给错分的样本更大的惩罚值。从直观上讲，在分类问题中，损失函数应该对负值的<img src="Image00918.gif" alt> 给予更多的惩罚。</p>
<p>下面逐一介绍各个损失函数。</p>
<p>首先，在分类问题中，我们经常用来评价分类算法性能的准确率对应着0-1损失函数，记为<img src="Image00921.gif" alt> 。严格地讲，对于样本<img src="Image00727.gif" alt> 和<img src="Image00541.gif" alt> ，0-1损失函数<img src="Image00922.gif" alt> 定义为：</p>
<p><img src="Image00923.jpg" alt></p>
<p>（6-77）</p>
<p>这里sign是符号函数，其定义为：</p>
<p><img src="Image00924.jpg" alt></p>
<p>（6-78）</p>
<p>注意，<img src="Image00543.gif" alt> 的取值是-1或者1，因此可以将<img src="Image00922.gif" alt> 记为关于<img src="Image00925.gif" alt> 的函数：</p>
<p><img src="Image00926.jpg" alt></p>
<p>（6-79）</p>
<p>在支持向量机中，使用了Hinge函数，其定义为：</p>
<p><img src="Image00927.gif" alt></p>
<p>（6-80）</p>
<p>这里记<img src="Image00928.gif" alt> 。</p>
<p>在逻辑回归中，使用的交叉熵损失函数也可以写为关于<img src="Image00929.gif" alt> 的函数。这里我们假设<img src="Image00930.gif" alt> ，但要注意我们在关于交叉熵的讨论中假设类标的取值是0或者1。我们使用<img src="Image00931.gif" alt> 表示取值为0或者1的类标，则有：</p>
<p><img src="Image00932.gif" alt></p>
<p>（6-81）</p>
<p>根据原来的讨论，使用类标<img src="Image00931.gif" alt> ，交叉熵损失函数为：</p>
<p><img src="Image00933.gif" alt></p>
<p>（6-82）</p>
<p>这里<img src="Image00934.gif" alt> 。因此，当<img src="Image00800.gif" alt> 时，<img src="Image00935.gif" alt> ，<img src="Image00936.gif" alt> ；当<img src="Image00802.gif" alt> 时，<img src="Image00937.gif" alt> ，<img src="Image00938.gif" alt> 。综合考虑两种情况，可将<img src="Image00939.gif" alt> 表示为：</p>
<p><img src="Image00940.gif" alt></p>
<p>（6-83）</p>
<p>在线性回归中，我们使用的损失函数是平方和损失函数<img src="Image00941.gif" alt> 。利用<img src="Image00543.gif" alt> 的取值是-1或者1（即<img src="Image00942.gif" alt><br>），也可以将其写为关于<img src="Image00925.gif" alt> 的函数：</p>
<p><img src="Image00943.gif" alt></p>
<p>（6-84）</p>
<p>在AdaBoost中，我们使用的是指数损失函数 （exponential loss function），其定义为：</p>
<p><img src="Image00944.gif" alt></p>
<p>（6-85）</p>
<p>我们将在稍后的章节中详细解释AdaBoost及为何在AdaBoost中引入指数损失函数。</p>
<p>在上面的讨论中，我们将损失函数都写成了关于<img src="Image00918.gif" alt> 的函数。在图6-19中给出了这些损失函数的图像。图6-19中横坐标对应于<img src="Image00918.gif" alt> ，纵坐标则是相应的损失函数值。表6-6中给出了每个损失函数的具体定义。</p>
<p><img src="Image00945.jpg" alt></p>
<p>图6-19 分类中常用的损失函数</p>
<p>表6-6 分类中常用损失函数的具体定义</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>损 失 函 数</th>
<th>定 义  </th>
</tr>
</thead>
<tbody>
<tr>
<td>0-1损失函数</td>
<td><img src="Image00946.jpg" alt>  </td>
</tr>
<tr>
<td>Hinge损失函数</td>
<td><img src="Image00947.gif" alt>  </td>
</tr>
<tr>
<td>交叉熵损失函数</td>
<td><img src="Image00948.gif" alt>  </td>
</tr>
<tr>
<td>指数损失函数</td>
<td><img src="Image00949.gif" alt>  </td>
</tr>
<tr>
<td>平方和损失函数</td>
<td><img src="Image00950.gif" alt>  </td>
</tr>
</tbody>
</table>
</div>
<p>下面分析和比较一下各个损失函数的优缺点。</p>
<p>0-1损失函数对应于准确率，是分类结果理想的衡量标准，但它最大的缺点是不连续且不可导。而在机器学习中，可导是一个非常重要的性质，原因在于在机器学习中，我们基本上都使用数值优化算法来求得最优解。如果目标函数本身不可导，就会导致所得的优化问题很难甚至不能求解。此外，当样本线性可分时，0-1损失函数不能得到唯一解。从关于支持向量机的讨论可以发现，对于线性可分的训练集，我们可以构造无数个线性分类器对应于0-1损失函数为0。因此，对于分类问题，我们基本上不直接使用0-1损失函数。</p>
<p>其他的几种损失函数都可以看做0-1损失函数的凸函数近似，称为0-1损失函数的凸替代 （convex surrogate）函数。</p>
<p>平方和损失函数对于回归问题来说是一个很好的损失函数：它的导数简单易求，而且也符合回归问题的性质。但在分类问题中，平方和损失函数使用并不多。从图6-19可以看出，平方和损失函数和0-1损失函数在<img src="Image00918.gif" alt> &gt;0时差别较大。注意，平方和损失函数不是一个关于<img src="Image00918.gif" alt> 的单调减函数。当<img src="Image00918.gif" alt> 为正且较大时，平方和损失函数仍然给予了很大的惩罚项<img src="Image00951.gif" alt> ，而且该惩罚项是以平方的速度随着<img src="Image00918.gif" alt> 增长。我们知道，<img src="Image00918.gif" alt> 为较大的正值表示我们已经正确分类了 <strong>_x_
</strong> 且<img src="Image00504.gif" alt> 离分类边界较远，这表明我们正确分类的确信程度很高。在这种情况下，平方和损失函数给予较大的惩罚项在分类问题中是不合理的。</p>
<p>其他的几个损失函数，包括Hinge、交叉熵和指数损失函数都是关于<img src="Image00918.gif" alt> 的单调减函数，对于<img src="Image00918.gif" alt> &gt;0的情况惩罚很小或者为0。当<img src="Image00918.gif" alt> 较小时，可以看出指数损失函数给予的惩罚较大。从图6-19中还可以看出，当<img src="Image00918.gif" alt> 越来越小时，交叉熵损失函数和Hinge损失函数给予的惩罚项是线性增长的，而指数损失函数对应的惩罚项是指数增长的。因此，如果训练集中有较多样本被错分且离分类边界距离较远（即<img src="Image00925.gif" alt> &lt;0且<img src="Image00952.gif" alt> 较大），这些点对指数损失函数的影响要远大于对交叉熵和Hinge损失函数的影响。如果训练集中有噪声（如有些样本的类标<img src="Image00540.gif" alt> 是错的），则指数损失函数会受到较大影响，而交叉熵和Hinge损失函数则稳健一些。</p>
<h3 id="6-5-2-正则化项"><a href="#6-5-2-正则化项" class="headerlink" title="6.5.2 正则化项"></a>6.5.2 正则化项</h3><p>在机器学习的很多算法中，我们使用正则化项 来控制模型的复杂度。在回归分析中，我们已经分析了<img src="Image00583.gif" alt> 范数和<img src="Image00573.gif" alt> 范数。例如，Lasso和岭回归都使用平方和损失函数，但Lasso中的正则化项是<img src="Image00583.gif" alt> 范数，而岭回归中的正则化项是<img src="Image00573.gif" alt> 范数。在支持向量机中，我们知道所对应的损失函数是Hinge函数，而正则化项实际对应于分类间隔。通过最大化分类间隔，我们可以控制模型的复杂度。事实上，从支持向量机中关于优化问题的推导我们可以看出，最大化分类间隔等价于最小化<img src="Image00816.gif" alt> 。换言之，支持向量机中的正则化项还是<img src="Image00647.gif" alt> 范数。并且在支持向量机的例子中，我们清楚地看到最小化<img src="Image00573.gif" alt> 范数和控制模型复杂度的直接联系。</p>
<p>事实上，对于向量<img src="Image00953.gif" alt> ，在第3章我们已经定义了广义的<img src="Image00954.gif" alt> 范数：</p>
<p><img src="Image00955.gif" alt></p>
<p>（6-86）</p>
<p>当<img src="Image00956.gif" alt> 时，<img src="Image00957.gif" alt> 是凸函数，因此是良定义的范数。当0≤ _p_ &lt;1时，上面<img src="Image00957.gif" alt> 的定义不是良定义的范数，但很多时候大家还是直接称之为<img src="Image00958.gif" alt> 范数。特别指出，当<img src="Image00959.gif" alt> 时，<img src="Image00960.gif" alt> 定义为：</p>
<p><img src="Image00961.gif" alt></p>
<p>（6-87）</p>
<p>即<img src="Image00962.gif" alt> 定义 _<strong>w</strong> _ 中非零元素的数目。如果使用<img src="Image00962.gif" alt> 作为正则化项，我们能够直接最小化 <strong>_w_ </strong> 中非零元素的数目，所得的解称为稀疏解，能够在做回归或者分类的时候进行特征选取。</p>
<p>在实际问题求解中，因为凸优化问题易于求解，所以很多时候我们都是尽量将原始问题转化为凸优化问题求解。在求解稀疏解时，虽然<img src="Image00963.gif" alt> 范数直接对应稀疏解，但是很难求解，一般我们利用与<img src="Image00963.gif" alt> 范数最接近的<img src="Image00583.gif" alt> 范数去逼近它，再求解对应的凸优化问题。</p>
<p>图6-20给出了 _p_ =0.3,0.5,1,2,3,10时各个范数等高线的形状。</p>
<p><img src="Image00964.jpg" alt></p>
<p>图6-20 不同的正则化项对应范数的等高线</p>
<h2 id="6-6-交叉检验和caret包"><a href="#6-6-交叉检验和caret包" class="headerlink" title="6.6 交叉检验和caret包"></a>6.6 交叉检验和caret包</h2><p>本节首先介绍模型选择 （model selection）和交叉检验 （cross-validation），然后重点介绍R中的<code>caret</code> 包 ⑥ 。<code>caret</code> 包 实现了多种机器学习算法的参数估计，包括使用交叉检验来进行模型选择，是实际应用中非常受欢迎的软件包。</p>
<h3 id="6-6-1-模型选择和交叉检验"><a href="#6-6-1-模型选择和交叉检验" class="headerlink" title="6.6.1 模型选择和交叉检验"></a>6.6.1 模型选择和交叉检验</h3><p>在使用上面讨论的机器学习算法构建模型时，对于同样的算法，使用不同的参数会得到不同的模型。在模型选择中，我们要估计不同模型的性能并从中选出最优的模型。这里的最优指的是模型对新数据的泛化性能最优。在给定训练集的情况下，模型在训练集上的错误并不是该模型泛化性能的良好估计。从回归算法的讨论可知，当我们增加模型的复杂度时，模型在训练集上的错误会逐步降低。当模型的复杂度足够高时，在训练集上的错误能够达到（或接近）0。但这并不代表模型在新的测试集上的错误也是如此，而是表示模型在训练集上过拟合了。</p>
<p>为了更加准确地估计模型的泛化性能，在训练和选择模型时，可以将训练数据分为两部分（见图6-21）：</p>
<ul>
<li>训练集 （training set）；</li>
<li>检验集 （validation set）。</li>
</ul>
<p><img src="Image00965.jpg" alt></p>
<p>图6-21 将数据划分为训练集和检验集</p>
<p>在训练集上，根据选定的模型参数训练出相应的模型。由于检验集上的数据相对训练集来说都是新的，因此可以计算训练所得的模型在检验集上的错误，从而估计所得模型的泛化性能。在实际中，可以使用2/3的数据来训练样本，而使用剩余的1/3作为检验集来估计模型的性能。</p>
<p>在数据不是很充足时，为了最大程度地利用已有的数据，交叉检验是一个更好的选择。在数据较为有限的情况下，检验集通常都较小，使用检验集来估计模型的性能可能会存在偏差。而利用交叉检验，整个训练集都可以作为检验集，同时也能使用整个训练集来训练模型，因此对于模型性能的估计会更加准确。</p>
<p>具体来说，在 _k_ 重交叉检验 （ _k_ -fold cross-validation）中，将训练集 _S_ 划分为互不重叠的 _k_ 个子集<img src="Image00675.gif" alt> ，如图6-22所示。一般而言，每个<img src="Image00966.gif" alt> 中包括相同数目的样本。</p>
<p><img src="Image00967.jpg" alt></p>
<p>图6-22 _k_ 重交叉检验中数据的划分</p>
<p>在 _k_ 重交叉检验中，顺次从<img src="Image00675.gif" alt> 选出一个作为检验集，而将剩余的数据作为训练集训练模型，并计算该模型在检验集上的预测结果。该过程重复 _k_ 次，使得每个<img src="Image00966.gif" alt> 都被选为检验集一次。这样，我们就将整个训练集用作了检验集，从而得到模型的性能估计。根据交叉检验选定最优的模型参数后，我们使用整个训练集重新训练一个模型作为最终的输出。在实际中，根据数据的大小，可以将 _k_ 选为5、10等值。算法6-2给出了使用交叉检验进行模型选择的具体过程。</p>
<p>算法6-2 使用交叉检验进行模型选择</p>
<blockquote>
<p>确定一组要比较的模型参数<img src="Image00968.gif" alt> &gt; &gt; 将训练集 _S_ 划分为 _k_ 个含有相同样本数的子集：<img src="Image00969.gif" alt> &gt; &gt; for _i_ =1: _m_ &gt; &gt; for _j_ = 1: _k_ &gt; &gt; 使用<img src="Image00970.gif" alt> 作为训练集，用参数<img src="Image00971.gif" alt> &gt; 来训练模型<img src="Image00972.gif" alt> &gt; &gt; 使用<img src="Image00973.gif" alt> 作为检验集，得到模型<img src="Image00972.gif" alt> &gt; 在该检验集上的输出 &gt; &gt; 将输出综合起来，得到使用模型参数<img src="Image00971.gif" alt> 得到的所有输出 &gt; &gt; 比较每个参数对应的输出，选出最优的参数<img src="Image00974.gif" alt> &gt; &gt; 使用整个训练集 _S_ 和<img src="Image00975.gif" alt> 来训练一个新的模型 _f_ 作为输出</p>
</blockquote>
<p>交叉检验的缺点是计算复杂度较高。在 _k_ 重交叉检验中，我们要训练模型 _k_ 次，使得训练时间增加很多。特别是如果模型的参数比较多，会导致候选模型比较多（如在Elastic Net中要同时选择 _L_ 2 范数和 _L_ 1 范数的权重），这样使用交叉检验进行模型选择的时间会进一步提高。</p>
<h3 id="6-6-2-在R中实现交叉检验以及caret包"><a href="#6-6-2-在R中实现交叉检验以及caret包" class="headerlink" title="6.6.2 在R中实现交叉检验以及caret包"></a>6.6.2 在R中实现交叉检验以及caret包</h3><p>本节介绍如何在R中利用交叉检验进行模型选择，并重点介绍<code>caret</code> 软件包。</p>
<p>在介绍<code>caret</code> 包之前，以<code>glmnet</code> 为例，说明如何在R中进行交叉检验。虽然<code>glmnet</code> 包中的<code>cv.glmnet</code> 函数可以实现交叉检验，但是这里我们的重点在于说明交叉检验的一般流程。此外，在这个例子中，我们只考虑<img src="Image00647.gif" alt> 范数的权重<code>lambda</code> 。完全的R程序见文件CV_example.R。下面我们讲解其中的关键部分。</p>
<p>第一步，我们准备好数据 <strong>_X_ </strong> 和 <strong>_y_ </strong> ，这里 <strong>_X_ </strong> 是矩阵， <strong>_y_ </strong> 是一个向量。注意，使用<code>glmnet</code> 时输入数据不能是数据框，必须显式地将数据框转换为矩阵。第二步，将数据随机分成 _k_ 个子集，且保证每个子集的大小一致（如果样本总数不是 _k_ 的倍数，则要求每个子集的大小尽量接近）。在下面的代码中，使用向量<code>v_fold</code> 包含每个样本所对应子集的编号，使用<code>sample（n）</code> 产生一个1～ _n_ 的随机排列。这里使用<code>set.seed</code> 函数设定随机数生成器的种子，以便我们重复实验结果。第三步，对于每个<code>lambda</code> 参数，使用交叉检验来训练模型，并计算在检验集上的预测值，最后将结果保存在矩阵<code>Y_valid</code> 中。第四步，计算每个<code>lambda</code> 值对应的准确率，选择最优的<code>lambda</code> 值并利用整个训练集重新训练一个模型。在这个例子中，我们考虑了9个<code>lambda</code> 的值，包括0、0.001、0.005、0.01、0.02、0.03、0.05、0.1、1。在我们的实验结果中，<code>lambda_optimal</code> =0.01，最后的模型保存在<code>M_optimal</code> 中。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">library(glmnet)</span><br><span class="line"></span><br><span class="line"># Step 1. Load the data and set parameters</span><br><span class="line">f_in &lt;-&apos;Data/pima-indians-diabetes.csv&apos;</span><br><span class="line">D &lt;-read.csv(f_in)</span><br><span class="line">D$class &lt;-as.factor(D$class)</span><br><span class="line">y &lt;-D$class</span><br><span class="line">X &lt;-D</span><br><span class="line">X$class &lt;-NULL</span><br><span class="line">X &lt;-as.matrix(X)</span><br><span class="line">k &lt;-5</span><br><span class="line"></span><br><span class="line"># Step 2. Split the data into k folds</span><br><span class="line">n &lt;-nrow(D)</span><br><span class="line">set.seed(0)</span><br><span class="line">v_random &lt;-sample(n)</span><br><span class="line">v_fold &lt;-rep(1, n)</span><br><span class="line">sample_size_per_fold &lt;-floor(n/k)</span><br><span class="line">for (i in 1:k) &#123;</span><br><span class="line">　range_min = (i-1) * sample_size_per_fold + 1</span><br><span class="line">　range_max = i * sample_size_per_fold</span><br><span class="line">　v_fold[(v_random&gt;=range_min) &amp; (v_random&lt;=range_max)] &lt;-i</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Step 3. Perform cross-validation for several logistic regression models</span><br><span class="line">lambda_list &lt;-c(0, 0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 1)</span><br><span class="line">param_num &lt;-length(lambda_list)</span><br><span class="line"></span><br><span class="line"># We use a matrix to score the prediction on the validation data set</span><br><span class="line">Y_valid &lt;-matrix(0, n, param_num)</span><br><span class="line">for (i in 1:param_num) &#123;</span><br><span class="line">　lambda &lt;-lambda_list[i]</span><br><span class="line">　y_valid &lt;-rep(0, n)</span><br><span class="line">　for (j in 1:k) &#123;</span><br><span class="line">　　X_train &lt;-X[v_fold!=j, ]</span><br><span class="line">　　y_train &lt;-y[v_fold!=j]</span><br><span class="line">　　X_valid &lt;-X[v_fold==j,]</span><br><span class="line">　　# Train the model</span><br><span class="line">　　Mj &lt;-glmnet(X_train, y_train, family = &apos;binomial&apos;, lambda = lambda)</span><br><span class="line">　　# Make prediction on the validation data set</span><br><span class="line">　　y_valid_j &lt;-predict(Mj,　X_valid, type=&apos;class&apos;)</span><br><span class="line">　　y_valid[v_fold==j] &lt;-y_valid_j</span><br><span class="line">　&#125;</span><br><span class="line">　Y_valid[, i] &lt;-y_valid</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Step 4. Pick the optimal lambda and rebuild the model using the whole training </span><br><span class="line"># data set</span><br><span class="line">accuracy_list &lt;-rep(0, param_num)</span><br><span class="line">for (i in 1:param_num) &#123;</span><br><span class="line">　accuracy_list[i] &lt;-sum(Y_valid[,i]==y) / n</span><br><span class="line">&#125;</span><br><span class="line">lambda_optimal &lt;-lambda_list[which.max(accuracy_list)]</span><br><span class="line">msg = paste0(&apos;lambda_optimal = &apos;, lambda_optimal)</span><br><span class="line">print(msg)</span><br><span class="line">M_optimal &lt;-glmnet(X, y, family = &apos;binomial&apos;, lambda = lambda_optimal)</span><br></pre></td></tr></table></figure>

</details>


<p>下面具体介绍<code>caret</code> 包，并使用具体的例子讨论如何利用<code>caret</code> 包训练逻辑回归和决策树。</p>
<p>R是开源的，虽然R中的资源和各种软件非常丰富，但是也比较杂乱。例如，我们前面介绍的<code>rpart</code> 包和<code>glmnet</code> 的接口就不一样。在<code>rpart</code> 包中，我们可以直接使用保存为数据框的输入数据，但在<code>glmnet</code> 中，输入数据只能是矩阵而不能是数据框。R中的<code>caret</code><br>（classification and regression training）包为R中超过200个包提供了统一的接口，包括前面介绍的<code>glmnet</code> 、<code>rpart</code> 、<code>e1071</code> 等，也包括第9章将要介绍的<code>randomForest</code> 和<code>gbm</code> 等包。<code>caret</code> 目前支持的包的完全列表可以在<code>caret</code> 的主页查到 ⑦ 。</p>
<p>使用<code>caret</code> 包，我们可以很简单地建立、评价和比较多个模型。<code>caret</code> 提供了包括交叉检验在内的多种模型选择的方法。使用<code>caret</code> 包，交叉检验的过程将会变得很简单。此外，<code>caret</code> 还提供了数据预处理的一些功能。</p>
<p>具体来说，在<code>caret</code> 包中，常用的函数包括<code>train</code> 和<code>predict</code> ，此外还有一个重要的辅助函数<code>trainControl</code> 。</p>
<p>利用<code>train</code> 函数，可以直接指定训练数据、挑选模型的方法（如交叉检验）、挑选模型的指标（如回归中的RMSE和 _R_ 2 ，分类问题中的准确率和ROC等）、调用的包（如<code>glmnet</code> 或者<code>rpart</code> ）、交叉检验中要测试的参数等。这些参数有一些可以在<code>train</code> 函数中直接设置，有些需要在<code>trainControl</code> 中设置。<code>train</code> 函数的逻辑流程与算法6-2一致。得到最终选定的参数后，<code>caret</code> 使用所有的数据重新训练一个模型并返回。之后我们可以直接使用<code>predict</code> 函数（实质是<code>caret</code> 包中的<code>predict.train</code><br>）函数处理新数据。</p>
<p><code>train</code> 函数的主要参数如下。</p>
<ul>
<li><code>method</code> ：一个字符串，用于指定要调用的模型。使用<code>names(getModelInfo())</code> 可以得到当前<code>caret</code> 包中所支持的所有模型的列表。</li>
<li><code>metric</code> ：也是字符串，用来指定在模型选择中要优化的指标。在分类中可以是<code>&#39;Accuracy&#39;</code> 或<code>&#39;ROC&#39;</code> ，在回归中可以是<code>&#39;RMSE&#39;</code> 或<code>&#39;Rsquared&#39;</code> 。</li>
<li><code>trControl</code> ：一个列表，表示进行模型选择时的控制参数。通常，我们先使用<code>trainControl</code> 得到一个列表，再赋给<code>trControl</code> 。</li>
<li><code>tuneGrid</code> ：一个数据框，表示在模型选择中我们要考虑的参数可能的取值。该数据框的列名必须与对应模型中的控制参数同名。如果不提供，则<code>caret</code> 自动选择要遍历的参数值。</li>
<li><code>preProcess</code> ：一个字符串，表示对于数据的预处理操作，如<code>&#39;center&#39;</code> 、<code>&#39;scale&#39;</code> 等。在默认状态下，不进行任何预处理。</li>
</ul>
<p>在指定输入数据时，同R中的其他很多包类似，<code>train</code> 支持两种模式：<code>train(x, y)</code> 和<code>train(formula, data)</code> 。</p>
<p>在<code>train(x, y)</code> 中，<code>x</code> 表示输入数据（每行对应一个样本，每列对应一个特征），<code>y</code> 表示要预测的量。在<code>train(formula, data)</code> 中，<code>data</code> 表示所有的数据（包括所有特征和要预测的量），而在<code>formula</code> 中我们指定哪一列是要预测的量，以及要用哪些列来预测。</p>
<p>在使用<code>trainControl</code> 函数时，通常要使用的主要参数有以下几个。</p>
<ul>
<li><code>method</code> ：一个字符串，表示采用何种取样方法进行模型选择。最常用的是<code>&#39;repeatedcv&#39;</code> ，表示进行多次交叉检验。</li>
<li><code>number</code> ：一个整数，表示进行交叉检验时的重数。</li>
<li><code>repeats</code> ：一个整数，表示进行<code>repeatedcv</code> 时的重复次数。</li>
<li><code>classProbs</code> ：<code>TRUE</code> 或者<code>FALSE</code> ，表示在分类模型中是否计算样本属于每个类别的概率。例如，在计算ROC时，我们就需要明确计算概率。</li>
</ul>
<p><code>predict</code> 函数的使用则较简单。假设<code>M</code> 是前面<code>train</code> 函数的返回结果，<code>D_test</code> 是新的测试数据，则我们可以使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_test &lt;-predict(M, newdata=D_test)</span><br></pre></td></tr></table></figure>
<p>来得到预测结果。</p>
<p>在<code>caret</code> 中，主要使用重取样<br>（resampling）的方法来优化模型参数从而进行模型选择。重取样的思想是：从整个训练集中采用某种取样方法得到一个新的数据集，在该数据集上训练模型，并将原始训练集剩余的样本作为检验集来计算模型的性能；重复该过程多次，从而找出最优的模型参数。在交叉检验中，我们把整个训练集分成若干等份，然后在每次重取样时去掉一个子集作为检验集，剩余的数据作为训练集。在<code>caret</code> 中，还支持除交叉检验以外的其他重取样方法，如每次可以进行bootstrap取样（将在第9章讨论）。注意，在重取样中，一旦取样数据确定，可以并行地训练多个模型以加快计算速度。在R中，可以使用<code>foreach</code> 包及相关的包来实现。例如，对于多核计算机，可以用<code>foreach</code> 和<code>doMC</code> 来利用多个核，这两个包的具体使用读者可参阅相关文档 ⑧ 。</p>
<p>下面使用<code>mlbench</code> 中的<code>Sonar</code> 数据集来介绍使用<code>caret</code> 包进行交叉检验的具体例子。完全的R代码在文件caret_example.R中。在这个例子中，首先安装相应的软件包，包括<code>glmnet</code> 、<code>rpart</code> 、<code>caret</code> 和<code>mlbench</code> 。接下来第一步，使用<code>data（Sonar）</code> 载入数据，并使用<code>str（Sonar）</code> 查看数据每列的类型。注意，最后一列<code>Class</code> 是因子类型，表示样本的类别。</p>
<p>第二步，使用<code>caret</code> 包和<code>glmnet</code> 包，按照由简单到复杂的顺序依次建立5个逻辑回归模型。注意，<code>Sonar</code> 是一个数据框。在前面直接使用<code>glmnet</code> 时，需要将输入数据显式地转换为矩阵，而通过<code>caret</code> 调用<code>glmnet</code> 时，不需要显式转换。</p>
<p>在构建第一个逻辑回归模型<code>M_glmnet1</code> 时，首先使用<code>trainControl</code> 函数构建一个列表对象：指定<code>method</code> 参数为<code>&#39;repeatedcv&#39;</code> ，表示使用多次交叉检验；将<code>number</code> 设为5，表示进行5重交叉检验；将<code>repeats</code> 设为3，表示交叉检验要运行3次。在使用<code>train</code> 函数时，指定输入数据为<code>Sonar</code> ，且用公式<code>Class～.</code> 表示利用所有其他列来预测<code>Class</code> 列，用<code>method=&#39;glmnet&#39;</code> 表示<code>caret</code> 将调用<code>glmnet</code> 包构建一个逻辑回归模型。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">k = 5</span><br><span class="line">cv_repeat_num &lt;-3</span><br><span class="line">fitControl1 &lt;-trainControl(method = &apos;repeatedcv&apos;,</span><br><span class="line">　　　　　　　　　　　　　　number = k,</span><br><span class="line">　　　　　　　　　　　　　　repeats = cv_repeat_num)</span><br><span class="line">M_glmnet1 &lt;-train(Class~., </span><br><span class="line">　　　　　　　　　 data = Sonar, </span><br><span class="line">　　　　　　　　　 method = &apos;glmnet&apos;, </span><br><span class="line">　　　　　　　　　 trControl = fitControl1)</span><br></pre></td></tr></table></figure>

</details>


<p>在得到<code>train</code> 函数返回的<code>M_glmnet1</code> 对象后，可以检查使用多次交叉检验之后的模型选择结果。</p>
<ul>
<li><code>M_glmnet1$bestTune</code> 返回选择的最优模型对应的参数。在<code>glmnet</code> 中为对应的<code>alpha</code> 值和<code>lambda</code> 值。</li>
<li><code>M_glmnet1$method</code> 返回模型对应的包，这里是<code>glmnet</code> 。</li>
<li><code>M_glmnet1$metric</code> 返回在交叉检验中优化的性能指标，这里是分类中的准确率。</li>
<li><code>M_glmnet1$modelType</code> 返回模型的类型，这里是分类。</li>
<li><code>M_glmnet1$results</code> 返回在交叉检验中不同的参数对应的性能指标（这里包括准确率等）。</li>
</ul>
<p>上面介绍了<code>M_glmnet1</code> 的主要成员，读者可以使用<code>attributes(M_glmnet1)</code> 查看<code>M_glmnet1</code> 中的所有成员。</p>
<p>在这个例子中，对应的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt; # Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">&gt; M_glmnet1$bestTune</span><br><span class="line">　alpha　　 lambda</span><br><span class="line">3　 0.1 0.04318733</span><br><span class="line">&gt; # Check the model name, here it is glmnet</span><br><span class="line">&gt; M_glmnet1$method</span><br><span class="line">[1] &quot;glmnet&quot;</span><br><span class="line">&gt; # Check the metric to be optimized, here it is Accuracy</span><br><span class="line">&gt; M_glmnet1$metric</span><br><span class="line">[1] &quot;Accuracy&quot;</span><br><span class="line">&gt; # Check the model type, here it is classification</span><br><span class="line">&gt; M_glmnet1$modelType</span><br><span class="line">[1] &quot;Classification&quot;</span><br><span class="line">&gt; # Check the performance (accuracy) for different parameter values</span><br><span class="line">&gt; M_glmnet1$results</span><br><span class="line">　alpha　　　 lambda　Accuracy　　 Kappa AccuracySD　 KappaSD</span><br><span class="line">1　0.10 0.0004318733 0.7581107 0.5115405 0.07082978 0.1444585</span><br><span class="line">2　0.10 0.0043187332 0.7595819 0.5157644 0.07147509 0.1438111</span><br><span class="line">3　0.10 0.0431873324 0.7627952 0.5227198 0.06249494 0.1252137</span><br><span class="line">4　0.55 0.0004318733 0.7596980 0.5142755 0.07231546 0.1470977</span><br><span class="line">5　0.55 0.0043187332 0.7562911 0.5085341 0.07837867 0.1580675</span><br><span class="line">6　0.55 0.0431873324 0.7434766 0.4834793 0.08075096 0.1611272</span><br><span class="line">7　1.00 0.0004318733 0.7613628 0.5169023 0.07624527 0.1553086</span><br><span class="line">8　1.00 0.0043187332 0.7548587 0.5062906 0.07481600 0.1506429</span><br><span class="line">9　1.00 0.0431873324 0.7467286 0.4909440 0.06744425 0.1336721</span><br></pre></td></tr></table></figure>

</details>


<p>在第二个逻辑回归模型中，指定了在交叉检验中要比较的不同参数值。我们在<code>lambda_list</code> 和<code>alpha_list</code> 中指定所有可能的<code>lambda</code> 值和<code>alpha</code> 值；然后使用<code>expand.grid</code> 函数生成一个数据框<code>myParamGrid</code> ，其中包含<code>lambda</code> 和<code>alpha</code> 的所有组合。注意，该数据框的列名必须要与对应模型中的控制参数同名。在使用<code>train</code> 函数时，指定<code>tuneGrid</code> 参数为<code>myParamGrid</code> 即可。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">lambda_list &lt;-c(0, 0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 1)</span><br><span class="line">alpha_list &lt;-c(0, 0.1)</span><br><span class="line">myParamGrid &lt;-expand.grid(lambda=lambda_list, alpha=alpha_list)</span><br><span class="line">fitControl2 &lt;-trainControl(method = &apos;repeatedcv&apos;,</span><br><span class="line">　　　　　　　　　　　　　　number = k,</span><br><span class="line">　　　　　　　　　　　　　　repeats = cv_repeat_num)</span><br><span class="line">M_glmnet2 &lt;-train(Class~., </span><br><span class="line">　　　　　　　　　 data = Sonar, </span><br><span class="line">　　　　　　　　　 method = &apos;glmnet&apos;, </span><br><span class="line">　　　　　　　　　 trControl = fitControl2, </span><br><span class="line">　　　　　　　　　 tuneGrid = myParamGrid)</span><br></pre></td></tr></table></figure>

</details>


<p>在这个例子中，使用<code>expand.grid</code> 生成的<code>myParamGrid</code> 数据框如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt; myParamGrid</span><br><span class="line">　 lambda alpha</span><br><span class="line">1　 0.000　 0.0</span><br><span class="line">2　 0.001　 0.0</span><br><span class="line">3　 0.005　 0.0</span><br><span class="line">4　 0.010　 0.0</span><br><span class="line">5　 0.020　 0.0</span><br><span class="line">6　 0.030　 0.0</span><br><span class="line">7　 0.050　 0.0</span><br><span class="line">8　 0.100　 0.0</span><br><span class="line">9　 1.000　 0.0</span><br><span class="line">10　0.000　 0.1</span><br><span class="line">11　0.001　 0.1</span><br><span class="line">12　0.005　 0.1</span><br><span class="line">13　0.010　 0.1</span><br><span class="line">14　0.020　 0.1</span><br><span class="line">15　0.030　 0.1</span><br><span class="line">16　0.050　 0.1</span><br><span class="line">17　0.100　 0.1</span><br><span class="line">18　1.000　 0.1</span><br></pre></td></tr></table></figure>

</details>


<p>与第一个模型类似，我们检查所得最优模型，其对应的输出如下。可以看出，在给定<code>alpha</code> 和<code>lambda</code> 值的情况下，根据<code>M_glmnet2$bestTune</code> ，最优组合是<code>alpha=0</code> 、<code>lambda=0.1</code> 。同时，<code>M_glmnet2$results</code> 给出了不同的<code>alpha</code> 和<code>lambda</code> 组合对应的模型性能。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt; # Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">&gt; M_glmnet2$bestTune</span><br><span class="line">　alpha lambda</span><br><span class="line">8　　 0　　0.1</span><br><span class="line">&gt; # Check the performance (accuracy) for different parameter values</span><br><span class="line">&gt; M_glmnet2$results</span><br><span class="line">　 alpha lambda　Accuracy　　 Kappa AccuracySD　 KappaSD</span><br><span class="line">1　　0.0　0.000 0.7773123 0.5517966 0.06008416 0.1215905</span><br><span class="line">2　　0.0　0.001 0.7773123 0.5517966 0.06008416 0.1215905</span><br><span class="line">3　　0.0　0.005 0.7773123 0.5517966 0.06008416 0.1215905</span><br><span class="line">4　　0.0　0.010 0.7773123 0.5517966 0.06008416 0.1215905</span><br><span class="line">5　　0.0　0.020 0.7773123 0.5517966 0.06008416 0.1215905</span><br><span class="line">6　　0.0　0.030 0.7821516 0.5614362 0.05702015 0.1160416</span><br><span class="line">7　　0.0　0.050 0.7804869 0.5587645 0.05216941 0.1051730</span><br><span class="line">8　　0.0　0.100 0.7821885 0.5619241 0.05439456 0.1104805</span><br><span class="line">9　　0.0　1.000 0.7615851 0.5185915 0.06908800 0.1407203</span><br><span class="line">10　 0.1　0.000 0.7532318 0.5028267 0.06820656 0.1382080</span><br><span class="line">11　 0.1　0.001 0.7660112 0.5292147 0.06393240 0.1282588</span><br><span class="line">12　 0.1　0.005 0.7691489 0.5357482 0.06823736 0.1379260</span><br><span class="line">13　 0.1　0.010 0.7789788 0.5547546 0.05820813 0.1179553</span><br><span class="line">14　 0.1　0.020 0.7821147 0.5619627 0.05340633 0.1072235</span><br><span class="line">15　 0.1　0.030 0.7756494 0.5488414 0.05623293 0.1132750</span><br><span class="line">16　 0.1　0.050 0.7724343 0.5426609 0.05394738 0.1082715</span><br><span class="line">17　 0.1　0.100 0.7630248 0.5243107 0.05814217 0.1154458</span><br><span class="line">18　 0.1　1.000 0.7355743 0.4583782 0.04822279 0.1012266</span><br></pre></td></tr></table></figure>

</details>


<p>在前两个逻辑回归模型的交叉检验中，我们都最大化了准确率。在第三个逻辑回归模型中，我们要最大化AUC（Area Under ROC Curve）。AUC也是分类中常用的一个性能指标，6.7节会详细介绍。在计算AUC时，我们需要知道样本属于每个类的概率。因此，在<code>trainControl</code> 函数中，可以将<code>classProbs</code> 参数设为真，同时将计算预测结果好坏的函数置为<code>caret</code> 包中的<code>twoClassSummary</code> 函数。在调用<code>train</code> 函数时，还要显式地将<code>metric</code> 指定为<code>&#39;ROC&#39;</code> 。之后检查所得的最优参数、交叉检验中调用的包和要优化的指标，以及各个参数值对应的算法性能。对应的R代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">fitControl3 &lt;-trainControl(method = &apos;repeatedcv&apos;,</span><br><span class="line">　　　　　　　　　　　　　　number = k,</span><br><span class="line">　　　　　　　　　　　　　　repeats = cv_repeat_num,</span><br><span class="line">　　　　　　　　　　　　　　classProbs = T,</span><br><span class="line">　　　　　　　　　　　　　　summaryFunction = twoClassSummary)</span><br><span class="line">M_glmnet3 &lt;-train(Class~., </span><br><span class="line">　　　　　　　　　 data = Sonar, </span><br><span class="line">　　　　　　　　　 method = &apos;glmnet&apos;, </span><br><span class="line">　　　　　　　　　 trControl = fitControl3, </span><br><span class="line">　　　　　　　　　 metric=&apos;ROC&apos;)</span><br><span class="line"># Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">M_glmnet3$bestTune</span><br><span class="line"># Check the model name</span><br><span class="line">M_glmnet3$method</span><br><span class="line"># Check the metric to be optimized</span><br><span class="line">M_glmnet3$metric</span><br><span class="line"># Check the performance (accuracy) for different parameter values</span><br><span class="line">M_glmnet3$results</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下。可以看出，<code>M_glmnet3$metric</code> 是<code>&#39;ROC&#39;</code> 。此外，<code>M_glmnet3$results</code> 的输出也与前面两个例子不同：没有计算<code>Accuracy</code> ，但输出了<code>ROC</code> 。我们将在6.7节详细介绍<code>sensitivity(Sens)</code> 、<code>specificity(Spec)</code> 等性能指标。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; # Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">&gt; M_glmnet3$bestTune</span><br><span class="line">　alpha　　 lambda</span><br><span class="line">3　 0.1 0.04318733</span><br><span class="line">&gt; # Check the model name</span><br><span class="line">&gt; M_glmnet3$method</span><br><span class="line">[1] &quot;glmnet&quot;</span><br><span class="line">&gt; # Check the metric to be optimized</span><br><span class="line">&gt; M_glmnet3$metric</span><br><span class="line">[1] &quot;ROC&quot;</span><br><span class="line">&gt; # Check the performance (accuracy) for different parameter values</span><br><span class="line">&gt; M_glmnet3$results</span><br><span class="line">　alpha　　　lambda　　　ROC　　     Sens　　 Spec　　     ROCSD　　SensSD　　  SpecSD</span><br><span class="line">1　0.10 0.0004318733 0.8250607 0.7595520 0.7194737 0.05394260 0.09463985 0.10113034</span><br><span class="line">2　0.10 0.0043187332 0.8454164 0.7805007 0.7298246 0.04466097 0.09994404 0.09204975</span><br><span class="line">3　0.10 0.0431873324 0.8689009 0.8069829 0.7405263 0.04560518 0.10710990 0.10737911</span><br><span class="line">4　0.55 0.0004318733 0.8234772 0.7624506 0.7194737 0.05639064 0.11546886 0.10867497</span><br><span class="line">5　0.55 0.0043187332 0.8483690 0.7864295 0.7403509 0.04386016 0.09338402 0.09333055</span><br><span class="line">6　0.55 0.0431873324 0.8472582 0.7801054 0.7300000 0.04503293 0.08608097 0.10888869</span><br><span class="line">7　1.00 0.0004318733 0.8199213 0.7594203 0.7091228 0.05695943 0.11890709 0.11820309</span><br><span class="line">8　1.00 0.0043187332 0.8488787 0.7952569 0.7266667 0.04297800 0.09286074 0.09309281</span><br><span class="line">9　1.00 0.0431873324 0.8404629 0.7864295 0.7292982 0.04698343 0.08865779 0.08191700</span><br></pre></td></tr></table></figure>

</details>


<p>在第四个逻辑回归模型中，在交叉检验中最大化AUC，但同时使用我们自己确定的<code>alpha</code> 和<code>lambda</code> 值。与第三个模型相比，在<code>train</code> 函数中将<code>tuneGrid</code> 参数指定为<code>myParamGrid</code> 即可。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">fitControl4 &lt;-trainControl(method = &apos;repeatedcv&apos;,</span><br><span class="line">　　　　　　　　　　　　　　number = k,</span><br><span class="line">　　　　　　　　　　　　　　repeats = cv_repeat_num,</span><br><span class="line">　　　　　　　　　　　　　　classProbs = T,</span><br><span class="line">　　　　　　　　　　　　　　summaryFunction = twoClassSummary)</span><br><span class="line">M_glmnet4 &lt;-train(Class~., </span><br><span class="line">　　　　　　　　　 data = Sonar, </span><br><span class="line">　　　　　　　　　 method = &apos;glmnet&apos;, </span><br><span class="line">　　　　　　　　　 trControl = fitControl4,</span><br><span class="line">　　　　　　　　　 tuneGrid = myParamGrid,</span><br><span class="line">　　　　　　　　　 metric=&apos;ROC&apos;)</span><br><span class="line"># Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">M_glmnet4$bestTune</span><br><span class="line"># Check the model name</span><br><span class="line">M_glmnet4$method</span><br><span class="line"># Check the metric to be optimized</span><br><span class="line">M_glmnet4$metric </span><br><span class="line"># Check the performance (accuracy) for different parameter values</span><br><span class="line">M_glmnet4$results</span><br></pre></td></tr></table></figure>

</details>


<p>对应的输出如下。从<code>M_glmnet4$results</code> 可以看出，最优的<code>alpha</code> 值和<code>lambda</code> 值对应最高的ROC值。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt; # Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">&gt; M_glmnet4$bestTune</span><br><span class="line">　alpha lambda</span><br><span class="line">8　　 0　　0.1</span><br><span class="line">&gt; # Check the model name</span><br><span class="line">&gt; M_glmnet4$method</span><br><span class="line">[1] &quot;glmnet&quot;</span><br><span class="line">&gt; # Check the metric to be optimized</span><br><span class="line">&gt; M_glmnet4$metric</span><br><span class="line">[1] &quot;ROC&quot;</span><br><span class="line">&gt; # Check the performance (accuracy) for different parameter values</span><br><span class="line">&gt; M_glmnet4$results</span><br><span class="line">　 alpha lambda　　　 ROC　　　Sens　　　Spec　　　ROCSD　　 SensSD　　 SpecSD</span><br><span class="line">1　　0.0　0.000 0.8771347 0.8052701 0.7564912 0.05540251 0.09926736 0.10691776</span><br><span class="line">2　　0.0　0.001 0.8771347 0.8052701 0.7564912 0.05540251 0.09926736 0.10691776</span><br><span class="line">3　　0.0　0.005 0.8771347 0.8052701 0.7564912 0.05540251 0.09926736 0.10691776</span><br><span class="line">4　　0.0　0.010 0.8771347 0.8052701 0.7564912 0.05540251 0.09926736 0.10691776</span><br><span class="line">5　　0.0　0.020 0.8771347 0.8052701 0.7564912 0.05540251 0.09926736 0.10691776</span><br><span class="line">6　　0.0　0.030 0.8779592 0.8051383 0.7600000 0.05733968 0.09330731 0.10139607</span><br><span class="line">7　　0.0　0.050 0.8774225 0.7960474 0.7635088 0.05785979 0.09384216 0.10695939</span><br><span class="line">8　　0.0　0.100 0.8781121 0.8077734 0.7738596 0.05964284 0.08636892 0.12334968</span><br><span class="line">9　　0.0　1.000 0.8590198 0.8227931 0.7187719 0.05797275 0.05535949 0.11722819</span><br><span class="line">10　 0.1　0.000 0.8399761 0.7898551 0.6910526 0.04417983 0.08314388 0.08165652</span><br><span class="line">11　 0.1　0.001 0.8514229 0.7989460 0.6945614 0.04598263 0.08842463 0.07543874</span><br><span class="line">12　 0.1　0.005 0.8655700 0.8077734 0.7356140 0.05232582 0.09452718 0.09316717</span><br><span class="line">13　 0.1　0.010 0.8707312 0.7992095 0.7461404 0.05523448 0.09542990 0.09928946</span><br><span class="line">14　 0.1　0.020 0.8753308 0.8023715 0.7598246 0.05758868 0.09610051 0.10410294</span><br><span class="line">15　 0.1　0.030 0.8768705 0.7963109 0.7633333 0.05776288 0.09636278 0.09829606</span><br><span class="line">16　 0.1　0.050 0.8744848 0.8021080 0.7668421 0.06007231 0.09227325 0.10160368</span><br><span class="line">17　 0.1　0.100 0.8680854 0.7869565 0.7701754 0.05787892 0.07471969 0.10861973</span><br><span class="line">18　 0.1　1.000 0.8212329 0.8977602 0.5819298 0.05607401 0.04997118 0.10050263</span><br></pre></td></tr></table></figure>

</details>


<p>在第五个逻辑回归模型中，首先，使用<code>caret</code> 包中的<code>createDataPartition</code> 函数产生训练集对应的样本在数据<code>Sonar</code> 中的行号（保存在<code>trainingIndex</code> 中）。<code>createDataPartition</code> 函数主要用于分层抽样 （stratified sampling）：在分类问题中按照正类和负类样本的比例取样，使得抽样取得的样本集中正负类样本的比例保持不变。<code>createDataPartition</code> 函数中的参数<code>y</code> 表示总体中的类标列，<code>p</code> 表示取样比例，<code>list</code> 表示返回的结果是否是列表。然后，利用<code>trainingIndex</code> 得到训练集<code>D_train</code> 和测试集<code>D_test</code> 。在利用交叉检验得到模型<code>M_glmnet5</code> 之后，调用<code>predict</code> 函数得到测试集上的预测类别。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trainingIndex &lt;-createDataPartition(y=Sonar$Class, p=0.6, list=F)</span><br><span class="line">D_train &lt;-Sonar[trainingIndex,]</span><br><span class="line">D_test &lt;-Sonar[-trainingIndex,]</span><br><span class="line">fitControl5 &lt;-trainControl(method = &apos;repeatedcv&apos;,</span><br><span class="line">　　　　　　　　　　　　　　number = k,</span><br><span class="line">　　　　　　　　　　　　　　repeats = cv_repeat_num)</span><br><span class="line">M_glmnet5 &lt;-train(Class~., </span><br><span class="line">　　　　　　　　　 data = D_train, </span><br><span class="line">　　　　　　　　　 method = &apos;glmnet&apos;, </span><br><span class="line">　　　　　　　　　 trControl=fitControl5)</span><br><span class="line">y_test &lt;-predict(M_glmnet5, newdata = D_test)</span><br></pre></td></tr></table></figure>

</details>


<p>第三步，使用<code>caret</code> 训练一个<code>rpart</code> 模型。在<code>rpart</code> 中，<code>caret</code> 只支持<code>cp</code> 。在下面的R代码中，我们得到了两个决策树模型<code>M_rpart1</code> 和<code>M_rpart2</code> 。在第一个模型中，主要使用<code>train</code> 中的默认参数，在第二个模型中使用自定义的参数<code>cp</code> 的值。与前面的<code>glmnet</code> 模型相比，最主要的区别在于<code>train</code> 函数中的<code>method</code> 参数要设置为<code>&#39;rpart&#39;</code> ，同时生成数据框<code>myParamGrid_rpart</code> 时要保证对应的列名为<code>cp</code> 。</p>
<p>具体代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Train a rpart model using default parameters provided by caret and cross-validation</span><br><span class="line">fitControl_rpart1 &lt;-trainControl(method = &apos;repeatedcv&apos;,</span><br><span class="line">　　　　　　　　　　　　　　　　　number = k,</span><br><span class="line">　　　　　　　　　　　　　　　　　repeats = cv_repeat_num)</span><br><span class="line">M_rpart1 &lt;-train(Class~., </span><br><span class="line">　　　　　　　　　data = Sonar, </span><br><span class="line">　　　　　　　　　method = &apos;rpart&apos;, </span><br><span class="line">　　　　　　　　　trControl = fitControl_rpart1)</span><br><span class="line"># Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">M_rpart1$bestTune</span><br><span class="line"># Check the model name</span><br><span class="line">M_rpart1$method</span><br><span class="line"># Check the metric to be optimized</span><br><span class="line">M_rpart1$metric</span><br><span class="line"># Check the performance (accuracy) for different parameter values</span><br><span class="line">M_rpart1$results</span><br><span class="line"></span><br><span class="line"># Train a rpart model using provided parameter values and cross-validation</span><br><span class="line">fitControl_rpart2 &lt;-trainControl(method = &apos;repeatedcv&apos;,</span><br><span class="line">　　　　　　　　　　　　　　　　　number = k,</span><br><span class="line">　　　　　　　　　　　　　　　　　repeats = cv_repeat_num)</span><br><span class="line">cp_list &lt;-c(0, 0.01, 0.1, 1)</span><br><span class="line">myParamGrid_rpart &lt;-expand.grid(cp=cp_list)</span><br><span class="line">M_rpart2 &lt;-train(Class~., </span><br><span class="line">　　　　　　　　　data = Sonar, </span><br><span class="line">　　　　　　　　　method = &apos;rpart&apos;, </span><br><span class="line">　　　　　　　　　trControl = fitControl_rpart2,</span><br><span class="line">　　　　　　　　　tuneGrid = myParamGrid_rpart)</span><br><span class="line"># Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">M_rpart2$bestTune</span><br><span class="line"># Check the model name</span><br><span class="line">M_rpart2$method</span><br><span class="line"># Check the metric to be optimized</span><br><span class="line">M_rpart2$metric </span><br><span class="line"># Check the performance (accuracy) for different parameter values</span><br><span class="line">M_rpart2$results</span><br></pre></td></tr></table></figure>

</details>


<p>下面给出<code>M_rpart2</code> 对应的输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; # Check the optimal model parameter of the selected model via cross-validation</span><br><span class="line">&gt; M_rpart2$bestTune</span><br><span class="line">　 cp</span><br><span class="line">3 0.1</span><br><span class="line">&gt; # Check the model name</span><br><span class="line">&gt; M_rpart2$method</span><br><span class="line">[1] &quot;rpart&quot;</span><br><span class="line">&gt; # Check the metric to be optimized</span><br><span class="line">&gt; M_rpart2$metric</span><br><span class="line">[1] &quot;Accuracy&quot;</span><br><span class="line">&gt; # Check the performance (accuracy) for different parameter values</span><br><span class="line">&gt; M_rpart2$results</span><br><span class="line">　　cp　Accuracy　　 Kappa　AccuracySD　 KappaSD</span><br><span class="line">1 0.00 0.6910704 0.3757473 0.077546696 0.1567621</span><br><span class="line">2 0.01 0.6910704 0.3757473 0.077546696 0.1567621</span><br><span class="line">3 0.10 0.7114766 0.4143188 0.069656667 0.1376974</span><br><span class="line">4 1.00 0.5336845 0.0000000 0.008177767 0.0000000</span><br></pre></td></tr></table></figure>

</details>


<p>利用<code>caret</code> 包，可以采用统一的接口简单地调用R中的200多个包，并使用不同的重取样方法来确定最优模型的参数。但在选择模型参数时，<code>caret</code> 已经事先选定了可以调节的模型参数。例如，在<code>glmnet</code> 中就是参数<code>alpha</code> 和<code>lambda</code> ，在<code>rpart</code> 中只有一个参数<code>cp</code> 。如果要调节更多的参数，有两种选择：</p>
<ul>
<li>继续使用<code>caret</code> 包，但需要做更多的定制化工作；</li>
<li>不使用<code>caret</code> 包，自己直接使用对应的包，并实现交叉检验的对应代码。</li>
</ul>
<p>在一般情况下，如果模型所对应的包不是特别生僻，那么通常推荐第二种方法。</p>
<p>对于第二种方法，我们提供了在<code>caret</code> 包中对<code>gbm</code> 进行定制的示例。关于<code>gbm</code> 的详细用法请参照9.5.3节。这里我们使用<code>gbm</code> 作为定制<code>caret</code> 包的一个例子。我们提供了3个程序：caret_custom_example.R、gbm_CV_func.R和classification_measures.R。其中classification_ measures.R实现了多种不同的分类算法评价指标，包括多类分类算法的评价指标。gbm_CV_func.R包详细讲解了如何在<code>caret</code> 包中对<code>gbm</code> 进行定制，包括如何在<code>caret</code> 包中优化自己实现的算法评价指标。caret_custom_example.R文件讲解了如何调用gbm_CV_func.R中的函数在实际数据上进行交叉检验。感兴趣的读者可以阅读并运行相关程序。</p>
<h2 id="6-7-分类算法的评价和比较"><a href="#6-7-分类算法的评价和比较" class="headerlink" title="6.7 分类算法的评价和比较"></a>6.7 分类算法的评价和比较</h2><p>在上一节中，我们在交叉检验中讨论了模型选择，其中的一个关键点就是模型的性能评价。本节介绍用于计算分类算法性能的各种评价标准，包括准确率、混淆矩阵、精确率、召回率、F1度量、ROC曲线和AUC等。同时，我们也会介绍R中用来计算这些指标的常用的包和函数。</p>
<p>在分类算法中，我们的预测主要有两种形式：离散型的输出和连续型的输出。离散型的输出就是直接输出分类结果，即每个样本的预测类别。与回归问题类似，分类模型也可以产生连续值的输出。通常连续型输出表现为概率的形式，如每个样本属于每一类的概率。对于实际中的大部分例子，最终的预测类别是必需的。例如，在欺诈检测中，我们希望分类算法能够明确回答每个交易是否是欺诈。</p>
<p>直观地讲，明确的类别输出更易使用。然而，连续型的输出（如概率）能够给我们更多的关于分类模型的信息。还是以欺诈检测为例，假设我们规定模型输出的概率值大于0.5则判定为欺诈，否则不为欺诈。考虑两个样本，第一个对应的概率是0.51，第二个对应的概率是0.99。根据我们的假设，两个样本都要被判定为欺诈。但是，显然我们对于第二个样本判定为欺诈的信心更高。在一些应用中，分类模型的输出可作为新的特征来训练下一步的模型。这时，连续型的输出能够提供更多有用的信息。在欺诈检测中，利用分类模型输出的概率值，并考虑实际调查该欺诈交易的成本和潜在损失，我们可以决定是否进行下一步的调查。</p>
<p>在下面介绍的算法评价标准中，除了ROC曲线和AUC需要分类算法的连续型输出，其他指标都只需要直接的分类结果。</p>
<h3 id="6-7-1-准确率"><a href="#6-7-1-准确率" class="headerlink" title="6.7.1 准确率"></a>6.7.1 准确率</h3><p>准确率 （accuracy）是分类问题中最常用的度量。使用准确率时，我们要求算法返回明确的类别。准确率的定义为：</p>
<p><img src="Image00976.jpg" alt></p>
<p>根据定义，准确率的取值在0和1之间，越大说明分类结果越好。</p>
<p>但在很多实际应用中，只用准确率一项来衡量分类结果会有失偏颇。如果在分类问题中有一类的样本占大多数，而分类的重点在样本数较少的那一类，那么简单计算准确率并不能较好地反映算法的性能。在前面讨论的欺诈检测的例子中，假设我们有一个包含100个样本的数据集，其中97个是正常交易，而剩下的3个是欺诈交易。如果算法A将所有的样本都判定为正常交易，而算法B将8个交易判定为欺诈交易（包括3个真正的欺诈交易）。根据准确率的定义，算法A的准确率为0.97，算法B的准确率为0.95。虽然算法A的准确率更高，但是它本质上并没有给我们提供关于欺诈的有效信息。因此，虽然算法B的准确率较低，但是我们认为在这个应用场景中算法B提供了更多关于欺诈的有效信息，此处算法B比算法A更适用。</p>
<h3 id="6-7-2-混淆矩阵"><a href="#6-7-2-混淆矩阵" class="headerlink" title="6.7.2 混淆矩阵"></a>6.7.2 混淆矩阵</h3><p>在上面的欺诈检测的例子中，其中错误的分类有两种：</p>
<ul>
<li>正常的交易被判定为欺诈；</li>
<li>欺诈的交易被判定为正常。</li>
</ul>
<p>显然，我们对第二种错误更感兴趣，因为它所对应的代价更高，更应该避免。在这类错误代价不同的应用中，简单使用准确率并不能很好地衡量算法的好坏。在这种情况下，我们可以考虑使用混淆矩阵来比较算法A和算法B。</p>
<p>混淆矩阵 （confusion matrix），又称为可能性表格或错误矩阵。混淆矩阵全面考虑了各种错分的情况，并将分类的结果以“类别 _a_ 被判定为类别 _b_ 的样本数为多少”的形式展示。其中，每一行代表一个实际的类别，每一列代表一个预测的类别。对于最常见的两类分类，混淆矩阵是一个2×2的矩阵。如果是3类分类问题，则混淆矩阵是一个3×3的矩阵。</p>
<p>下面用一个具体的例子来说明如何计算混淆矩阵。在前面的欺诈检测例子中，我们有两个算法。算法A将所有的样本都判定为正常的交易，那么它对应的混淆矩阵见表6-7。</p>
<p>表6-7 算法A对应的混淆矩阵</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实际的类别</th>
<th>预测的类别  </th>
</tr>
</thead>
<tbody>
<tr>
<td>欺诈</td>
<td>正常  </td>
</tr>
<tr>
<td>欺诈</td>
<td>0</td>
<td>3  </td>
</tr>
<tr>
<td>正常</td>
<td>0</td>
<td>97  </td>
</tr>
</tbody>
</table>
</div>
<p>算法B将8个交易判定为欺诈交易（包括3个真正的欺诈交易），对应的混淆矩阵见表6-8。其中所有的3个欺诈交易的样本都被正确判定为欺诈；而正常的97个样本中，有5个也被判定为欺诈，剩余的92个正常样本被判定为正常。</p>
<p>表6-8 算法B对应的混淆矩阵</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实际的类别</th>
<th>预测的类别  </th>
</tr>
</thead>
<tbody>
<tr>
<td>欺诈</td>
<td>正常  </td>
</tr>
<tr>
<td>欺诈</td>
<td>3</td>
<td>0  </td>
</tr>
<tr>
<td>正常</td>
<td>5</td>
<td>92  </td>
</tr>
</tbody>
</table>
</div>
<p>从表6-7和表6-8的对比可以看出，算法A把欺诈判定为正常的有3例，而算法B为0。对于这个欺诈检测例子，由于错分所导致的代价不一样，因此相对准确率来说，混淆矩阵是一个更好的评价工具。在实际中，我们可以根据混淆表的每一项对应的权重来选择更优的算法。</p>
<p>在两类分类问题中，2×2的混淆矩阵的各个元素有专门的名字，见表6-9。其中True Positive（TP）表示实际是正类，同时也被判定为正类；False Positive（FP）表示实际是负类，但是被判定为正类；False Negative（FN）表示实际是正类，但是被判定为负类；True Negative（TN）表示实际是负类，同时也被判定为负类。</p>
<p>表6-9 TP、FP、TN、FN的定义</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实际的类别</th>
<th>预测的类别  </th>
</tr>
</thead>
<tbody>
<tr>
<td>正类</td>
<td>负类  </td>
</tr>
<tr>
<td>正类</td>
<td>True Positive</td>
<td>False Negative（Type II Error）  </td>
</tr>
<tr>
<td>负类</td>
<td>False Positive（Type I Error）</td>
<td>True Negative  </td>
</tr>
</tbody>
</table>
</div>
<p>在表6-9中，我们把False Positive的情况称为第一类错误 （Type I Error），把False Negative的情况称为第二类错误<br>（Type II Error）。对于True Positive和True Negative，由于我们将其正确分类了，因此就不属于错误之列了。</p>
<p>对于两类分类问题，基于混淆矩阵，还可以定义True Positive Rate（TPR），或者称为灵敏度<br>（sensitivity）。其定义为正类样本中被正确分类的比例：</p>
<p><img src="Image00977.gif" alt></p>
<p>（6-88）</p>
<p>类似地，可以定义True Negative Rate（TNR），或者称为特异度 （specificity），其定义为负类样本中被正确分类的比例：</p>
<p><img src="Image00978.gif" alt></p>
<p>（6-89）</p>
<p>还可以定义False Positive Rate（FPR）为负类样本中被错误分类的比例：</p>
<p><img src="Image00979.gif" alt></p>
<p>（6-90）</p>
<p>定义False Negative Rate（FNR）为正类样本中被错误分类的比例：</p>
<p><img src="Image00980.gif" alt></p>
<p>（6-91）</p>
<p>利用混淆矩阵，可以简单地计算准确率（accuracy）：</p>
<p><img src="Image00981.gif" alt></p>
<p>（6-92）</p>
<h3 id="6-7-3-精确率、召回率和F1度量"><a href="#6-7-3-精确率、召回率和F1度量" class="headerlink" title="6.7.3 精确率、召回率和F1度量"></a>6.7.3 精确率、召回率和F1度量</h3><p>在很多实际应用中，我们主要关注正类样本的分类情况。例如，在前述的欺诈检测例子中，我们主要关注欺诈的交易，可以将欺诈的交易定义为正类，而正常的交易定义为负类。对于这样的问题，常用的评价标准有精确率<br>（precision）、召回率 （recall）和F1度量 （F1-Measure），其具体定义如下：</p>
<p><img src="Image00982.gif" alt></p>
<p>（6-93）</p>
<p><img src="Image00983.gif" alt></p>
<p>（6-94）</p>
<p><img src="Image00984.gif" alt></p>
<p>（6-95）</p>
<p>其中F1度量是精确率和召回率的调和平均数。事实上，召回率就是我们前面所讨论的TPR，亦等于1−FNR。</p>
<p>精确率度量了在算法判定为正类的所有样本中，真正的正类样本所占的比例。如果精确率为1，说明所有判定为正类的样本都是正类的。精确率越高，说明判定为正类的样本中事实上为负类的样本越少。而召回率度量了所有的正类样本被正确分类的比例。召回率越高，说明正类样本被错误分类的比例越小。</p>
<p>但在实际例子中，很多算法并不能很好地同时优化精确率和召回率。例如，如果一个算法把所有的样本都判定为正类，根据精确率和召回率的定义，我们知道召回率为1，但精确率却很低；另外一个极端的例子是，如果算法只把那些非常肯定为正类的样本判定为正类，虽然可以得到很高的精确率，但是召回率却很低。</p>
<p>因此，在实际应用中，需要同时考虑精确率和召回率。F1度量较好地解决了这个问题。根据F1度量的定义，它是精确率和召回率的调和平均数。而调和平均数更接近于精确率和召回率中更小的那个。因此，如果F1度量较大，则意味着精确率和召回率都较好，算法的性能较好。</p>
<p>下面我们用一个具体的例子说明如何计算前面讨论过的这些分类算法的评价指标。</p>
<p><strong>例6-5</strong> 我们仍然以欺诈检测中的两个算法为例计算它们的各种指标。考虑算法B的分类结果，其混淆矩阵见表6-8。以上讨论过的分类指标计算如下：</p>
<p><img src="Image00985.gif" alt></p>
<p><img src="Image00986.gif" alt></p>
<p><img src="Image00987.gif" alt></p>
<p><img src="Image00988.gif" alt></p>
<p><img src="Image00989.gif" alt></p>
<p><img src="Image00990.gif" alt></p>
<p><img src="Image00991.gif" alt></p>
<p><img src="Image00992.gif" alt></p>
<p><img src="Image00993.gif" alt></p>
<h3 id="6-7-4-ROC曲线和AUC"><a href="#6-7-4-ROC曲线和AUC" class="headerlink" title="6.7.4 ROC曲线和AUC"></a>6.7.4 ROC曲线和AUC</h3><p>前面所讨论的评价标准仅适用于分类器的输出是离散的情况（即输出明确的类别）。当分类器的输出为连续值（如概率值）时，我们需要合适的评价标准。这里我们首先引入接收者操作特征曲线<br>（receiver operating characteristic curve，简称ROC曲线）。</p>
<p>ROC曲线最早发端于信号处理。在ROC曲线中，横坐标是False Positive Rate（FPR），纵坐标为True Positive Rate（TPR）。使用ROC曲线的前提是分类算法能够输出连续值。在画ROC曲线时，我们将分类阈值从大变到小，从将所有的样本都判定为负类到将所有的样本都判定为正类。这样我们可以得到不同的分类结果，对应不同的FPR和TPR的值，从而得到ROC曲线。下面我们通过考虑ROC曲线的几个关键点来说明ROC曲线的具体画法。图6-23就是一个简单的ROC曲线。</p>
<p>在ROC曲线中，我们要着重考虑以下4个点，通过它们可以进一步理解ROC曲线。其中第一个点是(0,0)，即FPR=TPR=0，也即FP=TP=0。此时分类阈值极大，分类器将所有的样本都判定为负样本了。第二个点是(0,1)，即FPR=0，TPR=1，也即FP=FN=0。此时分类器将所有的样本都正确分类了。第三个点是(1,0)，即FPR=1，TPR=0，也即TP=TN=0。此时分类器将所有的样本都错误分类了。第四个点是(1,1)，即FPR=1，TPR=1，也即TN=FN=0。此时分类阈值极小，分类器将所有的样本都判定为正类了。</p>
<p>因此，ROC曲线越接近左上角（点(0,1)），分类器的性能越好。虽然ROC曲线给出了丰富的信息，但是在很多情况下，我们需要一个类似于准确率的单个值来衡量分类模型的好坏。实际中人们经常使用ROC曲线下的面积来衡量算法的好坏，称为AUC（area under ROC curve）。可以看出0≤AUC≤1。如果分类器 _f_ 的输出对于任意的负类样本<img src="Image00994.gif" alt> 和正类样本<img src="Image00995.gif" alt> 满足<img src="Image00996.gif" alt> &lt;<img src="Image00997.gif" alt> ，则该分类器的ROC曲线经过点(0,1)，对应的AUC值为1。如果一个分类器随机猜测样本对应的类别，其对应的AUC为0.5。一般来说，一个“正常”分类器的AUC介于0.5～1之间。如果一个分类器的AUC低于0.5，则意味着它还不如随机猜测。</p>
<p>除了ROC曲线下的面积，AUC还有直观的解释。可以严格地证明AUC等于任意选择的一对负类样本<img src="Image00996.gif" alt> 和正类样本<img src="Image00997.gif" alt> 满足<img src="Image00996.gif" alt> &lt;<img src="Image00997.gif" alt> 的概率，即</p>
<p><img src="Image00998.gif" alt></p>
<p>（6-96）</p>
<p>因此，AUC值越大，意味着分类器的性能越好。利用这个公式，可以很容易地理解为什么一个随机猜测的分类器对应的AUC为0.5。在实际中，有些算法会直接利用式（6-96）来优化AUC值。</p>
<p>ROC曲线的另外一个良好的性质是：即便在类不平衡的时候，它都是一个良好的算法评价指标。而在这种情况下，常用的准确率等评价指标容易受到样本数较多的类的影响，不能很好地反映算法实际的好坏。</p>
<p>下面通过一个例子来详细介绍如何画ROC曲线和计算对应的AUC。</p>
<p><strong>例6-6</strong> 假设我们有表6-10所示的8个样本，其中第二列给出了我们的预测<img src="Image00999.gif" alt> ，已经按<img src="Image00999.gif" alt> 的取值进行了排序；第三列给出了实际类别，其中1是正类，-1是负类。下面我们画出ROC曲线并计算对应的AUC。</p>
<p>表6-10 诸样本对应的预测 _P_ ( _y_ =1| _x_ )和真实类别</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>样 本 编 号</th>
<th>_P_ ( _y_ =1</th>
<th>_x_ )</th>
<th>实 际 类 别  </th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.1</td>
<td>−1  </td>
</tr>
<tr>
<td>2</td>
<td>0.3</td>
<td>1  </td>
</tr>
<tr>
<td>3</td>
<td>0.4</td>
<td>−1  </td>
</tr>
<tr>
<td>4</td>
<td>0.5</td>
<td>1  </td>
</tr>
<tr>
<td>5</td>
<td>0.55</td>
<td>1  </td>
</tr>
<tr>
<td>6</td>
<td>0.7</td>
<td>−1  </td>
</tr>
<tr>
<td>7</td>
<td>0.8</td>
<td>1  </td>
</tr>
<tr>
<td>8</td>
<td>0.85</td>
<td>1  </td>
</tr>
</tbody>
</table>
</div>
<p>为了画出ROC曲线，我们需要改变分类阈值得到不同的FPR和TPR的值。开始时我们假定所有的样本都被判定为负类，这样可得FPR=TPR=0，对应的TP、TN、FP、FN、FPR和TPR等具体计算结果见表6-11的“正类数目为0”行。接下来我们改变分类阈值，使得8个样本中7个被判定为负类，而只有编号为8的样本被判定为正类。相应地，我们可以再次计算FPR和TPR等值，参见表6-11的“正类数目为1”行。随着分类阈值的不断增加，最终所有的样本都被判定为正类，此时有FPR=TPR=1。</p>
<p>表6-11 利用排序更有效地计算FPR和TPR</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>正类数目</th>
<th>TP</th>
<th>TN</th>
<th>FP</th>
<th>FN</th>
<th>FPR</th>
<th>TPR  </th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>5</td>
<td>0</td>
<td>0  </td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>3</td>
<td>0</td>
<td>4</td>
<td>0</td>
<td>0.2  </td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>3</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>0.4  </td>
</tr>
<tr>
<td>3</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>0.3333</td>
<td>0.4  </td>
</tr>
<tr>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>0.3333</td>
<td>0.6  </td>
</tr>
<tr>
<td>5</td>
<td>4</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>0.3333</td>
<td>0.8  </td>
</tr>
<tr>
<td>6</td>
<td>4</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>0.6667</td>
<td>0.8  </td>
</tr>
<tr>
<td>7</td>
<td>5</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>0.6667</td>
<td>1  </td>
</tr>
<tr>
<td>8</td>
<td>5</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>1</td>
<td>1  </td>
</tr>
</tbody>
</table>
</div>
<p>利用表6-11中的FPR和TPR的值，可以得到如图6-23所示的ROC曲线，同时也可以得到AUC=0.7333。</p>
<p><img src="Image01000.jpg" alt></p>
<p>图6-23 示例数据所对应的ROC曲线</p>
<h3 id="6-7-5-R中评价标准的计算"><a href="#6-7-5-R中评价标准的计算" class="headerlink" title="6.7.5 R中评价标准的计算"></a>6.7.5 R中评价标准的计算</h3><p>本节主要介绍如何使用R中的包和相关函数来计算上面介绍的分类算法常用评价标准。具体的R代码参见文件classification_performance_metric.R。</p>
<p>这里我们仍使用表6-10中的数据。该数据集包括8个样本，在代码中用<code>label_list</code> 表示其真实的类别，<code>prob_list</code> 表示分类算法输出的概率。我们规定概率大于或等于0.5的样本判定为正类，并将结果保存在<code>pred_list</code> 中。注意，这里我们将<code>label_list</code> 和<code>pred_list</code> 显式地转换为因子类型。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">prob_list = c(0.1, 0.3, 0.4, 0.5, 0.55, 0.7, 0.8, 0.85)</span><br><span class="line">pred_list = ifelse(prob_list&gt;=0.5, 1, -1)</span><br><span class="line">label_list = c(-1, 1, -1, 1, 1, -1, 1, 1)</span><br><span class="line"># In R, we use factor to store the label list</span><br><span class="line">pred_list &lt;-as.factor(pred_list)</span><br><span class="line">label_list &lt;-as.factor(label_list)</span><br></pre></td></tr></table></figure>

</details>


<p>首先计算准确率并打印出来：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Part 1. Compute accuracy</span><br><span class="line">accuracy = sum(pred_list==label_list) / length(label_list)</span><br><span class="line">msg = paste0(&apos;accuracy = &apos;, accuracy)</span><br><span class="line">print(msg)</span><br></pre></td></tr></table></figure>

</details>


<p>其输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;accuracy = 0.75&quot;</span><br></pre></td></tr></table></figure>
<p>然后调用<code>caret</code> 包中提供的3个函数来计算混淆矩阵、灵敏度和特异度：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(&apos;caret&apos;)</span><br><span class="line">confM1 &lt;-confusionMatrix(data=pred_list, reference=label_list, positive=&apos;1&apos;)</span><br><span class="line">print(&apos;the confusion matrix and other metrics are&apos;)</span><br><span class="line">print(confM1)</span><br><span class="line"></span><br><span class="line">sens &lt;-sensitivity(data=pred_list, reference=label_list, positive=&apos;1&apos;)</span><br><span class="line">spec &lt;-specificity(data=pred_list, reference=label_list, negative=&apos;-1&apos;)</span><br><span class="line">msg &lt;-paste0(&apos;sensitivity = &apos;, sens, &apos; and specificity = &apos;, spec)</span><br><span class="line">print(msg)</span><br></pre></td></tr></table></figure>

</details>


<p>上述代码使用<code>caret</code> 包中的<code>confusionMatrix</code> 函数计算混淆矩阵，使用<code>sensitivity</code> 函数计算灵敏度，使用<code>specificity</code> 函数计算特异度。在使用这3个函数时，<code>data</code> 参数对应我们预测的类别，而<code>reference</code> 对应真正的类别；我们可以使用参数<code>positive</code> 或者<code>negative</code> 来分别指定对应的正类或者负类。下面是对应的输出：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;the confusion matrix and other metrics are&quot;</span><br><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">　　　　　Reference</span><br><span class="line">Prediction -1 1</span><br><span class="line">　　　　-1　2 1</span><br><span class="line">　　　　1　 1 4</span><br><span class="line"></span><br><span class="line">　　　　　　　 Accuracy : 0.75　　　　　　</span><br><span class="line">　　　　　　　　 95% CI : (0.3491, 0.9681)</span><br><span class="line">　　No Information Rate : 0.625　　　　　 </span><br><span class="line">　　P-Value [Acc &gt; NIR] : 0.3697　　　　　</span><br><span class="line"></span><br><span class="line">　　　　　　　　　Kappa : 0.4667　　　　　</span><br><span class="line"> Mcnemar&apos;s Test P-Value : 1.0000　　　　　</span><br><span class="line"></span><br><span class="line">　　　　　　Sensitivity : 0.8000　　　　　</span><br><span class="line">　　　　　　Specificity : 0.6667　　　　　</span><br><span class="line">　　　　 Pos Pred Value : 0.8000　　　　　</span><br><span class="line">　　　　 Neg Pred Value : 0.6667　　　　　</span><br><span class="line">　　　　　　 Prevalence : 0.6250　　　　　</span><br><span class="line">　　　　 Detection Rate : 0.5000　　　　　</span><br><span class="line">　 Detection Prevalence : 0.6250　　　　　</span><br><span class="line">　　　Balanced Accuracy : 0.7333　　　　　</span><br><span class="line"></span><br><span class="line">　　　 &apos;Positive&apos; Class : 1　　　　　　　 </span><br><span class="line"></span><br><span class="line">[1] &quot;sensitivity = 0.8 and specificity = 0.666666666666667&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，调用<code>confusionMatrix</code> 函数时，它除了计算混淆矩阵外，还计算了准确率、灵敏度和特异度等很多相关的评价指标。而<code>sensitivity</code> 函数和<code>specificity</code> 函数仅仅计算了相关的灵敏度和特异度。</p>
<p>接下来我们使用<code>pROC</code> 包中的<code>roc</code> 和<code>auc</code> 函数分别画出ROC曲线和计算对应的AUC值。在前面的讨论中，我们介绍了理论上画ROC曲线和计算AUC的方法。在实际中，我们推荐读者直接使用<code>pROC</code> 包中的对应函数。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(&apos;pROC&apos;)</span><br><span class="line"># create an roc object using the roc function in pROC</span><br><span class="line">rocCurve &lt;-roc(response=label_list,</span><br><span class="line">　　　　　　　　predictor=prob_list)</span><br><span class="line"># compute the AUC using the auc function</span><br><span class="line">auc_value = auc(rocCurve)</span><br><span class="line">print(auc_value)</span><br><span class="line"># plot the ROC curve </span><br><span class="line">plot(rocCurve, legacy.axes=T)</span><br></pre></td></tr></table></figure>

</details>


<p>在使用<code>pROC</code> 包中的<code>roc</code> 函数时，首先使用<code>roc</code> 函数创建一个<code>roc</code> 对象。在使用<code>roc</code> 函数时，<code>response</code> 参数表示真正的类别，而<code>predictor</code> 参数表示我们的预测值。得到<code>roc</code> 对象后，可使用<code>auc</code> 函数计算ROC曲线对应的AUC值，也可使用<code>plot</code> 函数画出对应的ROC曲线。<code>print（auc_value）</code> 的输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Area under the curve: 0.7333</span><br></pre></td></tr></table></figure>
<p>使用<code>plot</code> 函数生成的ROC曲线如图6-23所示。</p>
<p>表6-12总结了在R中计算这些常用评价标准的包和函数。</p>
<p>表6-12 R中常用的计算分类算法标准的包和函数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评 价 标 准</th>
<th>R包</th>
<th>函 数  </th>
</tr>
</thead>
<tbody>
<tr>
<td>混淆矩阵</td>
<td><code>caret</code></td>
<td><code>confusionMatrix</code>  </td>
</tr>
<tr>
<td><code>AUC</code></td>
<td><code>pROC</code></td>
<td><code>roc 和 auc</code>  </td>
</tr>
<tr>
<td><code>ROC</code></td>
<td><code>pROC</code></td>
<td><code>roc</code>  </td>
</tr>
<tr>
<td>灵敏度</td>
<td><code>caret</code></td>
<td><code>sensitivity</code>  </td>
</tr>
<tr>
<td>特异度</td>
<td><code>caret</code></td>
<td><code>specificity</code>  </td>
</tr>
</tbody>
</table>
</div>
<h2 id="6-8-不平衡分类问题"><a href="#6-8-不平衡分类问题" class="headerlink" title="6.8 不平衡分类问题"></a>6.8 不平衡分类问题</h2><p>本节讨论不平衡分类 （imbalanced classification）问题。以两类分类问题为例，在不平衡分类问题中，训练集中的样本主要属于某一类，而另一类的样本很少，使得分类器难以学到关于较少样本对应的类的特征。在实际中，如果我们对样本更少的那类更感兴趣，简单使用前面讨论的分类算法很难得到“好”的分类器。我们前面讨论的欺诈检测就是这样一个例子。在欺诈检测中，大部分交易都是正常的，真正欺诈的交易太少，而我们恰恰对欺诈的交易最感兴趣。</p>
<p>不失一般性，在不平衡问题中，我们假设正类的样本很少而负类的样本很多。因此，正类样本的一些“模式”淹没在负类样本的“模式”中了，导致在绝大多数情况下分类器都很难直接对正类样本做出较好的处理。本节详细讨论处理不平衡分类的有效方法，主要包括优化不同的算法评价标准、改变样本权值、通过取样改变训练集分布和代价敏感学习<br>（cost-sensitive learning）。</p>
<h3 id="6-8-1-使用不同的算法评价标准"><a href="#6-8-1-使用不同的算法评价标准" class="headerlink" title="6.8.1 使用不同的算法评价标准"></a>6.8.1 使用不同的算法评价标准</h3><p>在前面的欺诈检测例子中，由于正类样本的数目太少，同时用户对于正类样本的正确分类更为关注，因此准确率在该应用中并不是一个良好的算法评价标准。</p>
<p>一种较简单的解决方案是选择最符合实际的算法评价标准来选择最优的参数值。例如，在欺诈检测的例子中，我们可以最大化AUC，这也是我们提出不同算法评价标准的初衷之一。</p>
<h3 id="6-8-2-样本权值"><a href="#6-8-2-样本权值" class="headerlink" title="6.8.2 样本权值"></a>6.8.2 样本权值</h3><p>很多分类算法都允许给每个训练样本赋予不同的权值。这样，对于样本数目较少的类别，我们可以给其对应的样本赋予更大的权值，从而使得算法能够发现可以对它们进行准确分类的“模式”。赋予样本不同权值的方法在集成学习的boosting中应用很广。但该方法的缺陷是需要分类算法支持为每个训练样本设置不同的权值。如果分类算法本身不支持样本权重，这种方法就不好直接应用。</p>
<h3 id="6-8-3-取样方法"><a href="#6-8-3-取样方法" class="headerlink" title="6.8.3 取样方法"></a>6.8.3 取样方法</h3><p>取样是一种广泛用来解决不平衡分类问题的方法。取样方法的优点是只需要通过取样改变训练集中正负类样本的分布，使得样本的分布从不平衡到平衡，而分类算法不需要做任何改变就可以直接应用了。事实上，取样方法可以认为是6.8.2节样本权值方法中将权重设为整数的特殊情况。举个简单的例子，假设我们在新的训练集中有两个样本<br><strong>_x_ </strong> i ，等价于赋予样本 <strong>_x_ </strong> i 权值2。</p>
<p>在不平衡分类问题中，主要有两种取样方法：下取样 （down-sampling）和上取样 （up-sampling，也称为oversampling）。其中，下取样就是在样本多的类中取样，使得该类的样本数降低；上取样则相反，通过模拟或者直接复制正类样本点，使得正类样本点的样本数增加。此外，还有下取样和上取样同时使用的混合型取样方法。</p>
<p>下面我们仍以欺诈检测为例来介绍下取样和上取样的具体方法。在欺诈检测的例子中，假设原始的训练集中有50个正类样本（欺诈样本）和5000个负类样本（正常样本）。</p>
<p>在一个简单的上取样方法中，我们可以把每个正类样本复制99次从而得到5000个正类样本，然后和原来的5000个负类样本一起组成新的训练集。在这个新的训练集中，没有类不平衡的问题。该方法简单，但是如果正类样本中有噪声的话，则噪声也被放大了若干倍，从而影响分类器的最终性能。</p>
<p>在一个简单的下取样方法中，我们随机地从5000个负类样本中取出50个负类样本，然后和原来的50个正类样本一起组成新的训练集。但是由于我们损失了很多负类样本的信息，利用这个新训练集学习得到的分类器有可能遗失了负类样本的一些重要信息。</p>
<p>在实际中，有很多改进措施可以提高下取样的性能。例如，可以在取样时偏向那些离分类界限较近的负类样本而忽视那些离分类界限较远的负类样本。更理想的做法是引入集成学习的一些做法。例如，可以使用前面讨论的简单下取样方法 _k_ 次，并且在每个训练集上训练一个分类器。这样就得到了 _k_ 个分类器，最后的分类器是所有 _k_ 个分类器的平均。这个方法考虑了更多的负类样本的信息，通常可以取得较好的分类性能，缺点是计算复杂度偏大。更复杂的方法包括：在每次取样时使用bootstrap取样（可重复取样，将在第9章介绍），并保证每类所取样的样本数相等。和前一种方法类似，对于每个取样得到的训练集，我们建立一个分类器，而最终的分类器是所得的多个分类器的平均。这样既解决了不平衡分类的问题，同时又在建立多个分类器时引入了多样性，从而使得最终的分类器性能较好。实质上，该方法类似于集成学习的思想，而随机森林（见第9章）的一种具体实现就和我们这里描述的下取样方法非常相似。</p>
<p>总之，取样方法的优点是取样后所有的分类算法都可直接应用；缺点是在取样过程中，会遗失有效信息（下取样）或者添加一些噪声（上取样）。</p>
<h3 id="6-8-4-代价敏感学习"><a href="#6-8-4-代价敏感学习" class="headerlink" title="6.8.4 代价敏感学习"></a>6.8.4 代价敏感学习</h3><p>我们知道，在机器学习中，很多分类算法都是通过最小化损失函数得到相应的模型。代价敏感学习 （cost-sensitive learning）的核心思想是在分类算法所考虑的损失函数中，为正类（样本数较少）的样本错分所导致的错误赋予较大的权值，从而能够在学习模型的过程中对正类样本给予更多的关注。例如，在欺诈检测的例子中，将正类（欺诈）样本错误分类成负类（正常交易）的代价远高于将负类样本错误分类为正类样本的代价。通过直接改变损失函数，我们能够得到更好的处理不平衡分类的分类器。与前面介绍的方法相比，代价敏感学习要求对分类算法本身进行修改。</p>
<p>在两类分类问题中，我们考虑给不同的分类情况赋予不同的权重，见表6-13。其中，将正类分为正类的权重为<img src="Image01001.gif" alt> ，将正类分为负类的权重为<img src="Image01002.gif" alt> ，将负类分为正类的权重为<img src="Image01003.gif" alt> ，将负类分为负类的权重为<img src="Image01004.gif" alt> 。</p>
<p>表6-13 不同分类给予的不同权重</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实际的类别</th>
<th>预测的类别  </th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive Class（正类）</td>
<td>Negative Class（负类）  </td>
</tr>
<tr>
<td>Positive Class（正类）</td>
<td>_c_ 11</td>
<td>_c_ 10  </td>
</tr>
<tr>
<td>Negative Class（负类）</td>
<td>_c_ 01</td>
<td>_c_ 00  </td>
</tr>
</tbody>
</table>
</div>
<p>在损失函数中，我们通常只考虑错误分类的情况，包括：（1）将正类错分为负类；（2）将负类错分为正类。换言之，<img src="Image01005.gif" alt> 。考虑不同的权重<img src="Image01002.gif" alt> 和<img src="Image01006.gif" alt> 后，损失函数可以表示为</p>
<p><img src="Image01007.gif" alt></p>
<p>（6-97）</p>
<p>这里我们使用<img src="Image01008.gif" alt> 、<img src="Image01009.gif" alt> 分别表示在分类中被错分为负类和正类的样本集合。如果使用最简单的0-1损失函数，我们有</p>
<p><img src="Image01010.gif" alt></p>
<p>（6-98）</p>
<p>下面以支持向量机和决策树作为两个实际的例子来进一步说明如何进行代价敏感学习。</p>
<p>对于支持向量机而言，在损失函数中我们可以为每一类赋予不同的权重。在标准的支持向量机中，对于两类分类问题我们考虑如下的损失函数（包括正则化项）：</p>
<p><img src="Image01011.gif" alt></p>
<p>（6-99）</p>
<p>这里<img src="Image01012.gif" alt> ，表示样本<img src="Image00730.gif" alt> 被错分时的损失。在标准的支持向量机中，我们对所有的样本都一视同仁。在上面的公式中，我们给所有样本的错分都赋予了权重 _c_ 。在不平衡分类中，我们可以赋予不同的权重，从而得到新的损失函数（包括正则化项）：</p>
<p><img src="Image01013.gif" alt></p>
<p>（6-100）</p>
<p>这里我们把正类和负类样本的错分权重分别设为 _c_ 1 和 _c_ 2 ，被错分为负类和正类的样本集合分别为 _C_ 1 和 _C_ 2 。例如，在欺诈检测的例子中，可以令<img src="Image01014.gif" alt> 来着重强调正类的正确分类。</p>
<p>在构建决策树的过程中，通过为不同的类引入不同的代价，我们可以改变构建决策树的过程，例如，在每一步如何选择特征进行分割、如何决定叶结点的类别等。下面我们简单介绍一下如何在引入不同代价时决定叶结点的类别。</p>
<p>在决策树中，我们通常直接按照少数服从多数的原则来决定叶结点对应的类别。换言之，训练集中属于该结点的样本中最主要的类别就是整个叶结点对应的类别。但是在代价敏感学习中，由于我们给不同的错误引入了不同的权重，所以需要引入新的规则。在两类问题中，假设我们仍旧考虑最简单的0-1损失函数，并假设某叶结点对应的正类样本数目为<img src="Image01015.gif" alt> ，负类样本数目为<img src="Image01016.gif" alt> 。根据前面的讨论，损失函数为</p>
<p><img src="Image01010.gif" alt></p>
<p>（6-101）</p>
<p>当把所有结点都判定为正类时，我们有</p>
<p><img src="Image01017.gif" alt></p>
<p>（6-102）</p>
<p>对应的损失函数<img src="Image01018.gif" alt> 为：</p>
<p><img src="Image01019.gif" alt></p>
<p>（6-103）</p>
<p>当把所有结点都判定为负类时，我们有：</p>
<p><img src="Image01020.gif" alt></p>
<p>（6-104）</p>
<p>对应的损失函数<img src="Image01021.gif" alt> 为：</p>
<p><img src="Image01022.gif" alt></p>
<p>（6-105）</p>
<p>当<img src="Image01023.gif" alt> &lt;<img src="Image01021.gif" alt> 时，我们把该叶结点判定为正类，因为此时对应的损失函数<img src="Image01023.gif" alt> 更小，对应于如下条件：</p>
<p><img src="Image01024.gif" alt></p>
<p>（6-106）</p>
<p>换言之，当<img src="Image01025.gif" alt> &lt;<img src="Image01026.gif" alt> 时，该叶结点应判定为正类；否则判定为负类。当<img src="Image01027.gif" alt> 时，该规则为</p>
<p><img src="Image01028.gif" alt> &lt;1</p>
<p>（6-107）</p>
<p>这就是我们前面讨论的少数服从多数原则。在欺诈检测中，我们令<img src="Image01029.gif" alt> 以突出欺诈的样本，则规则为</p>
<p><img src="Image01028.gif" alt> &lt;100</p>
<p>（6-108）</p>
<p>在新的规则下，就算是负类（正常样本）更多，但是只要负类与正类样本数的比值低于100，我们仍然将其判定为正类（欺诈）。</p>
<hr>
<p>① <a href="https://cran.r-project.org/web/packages/rpart/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/rpart/index.html</a></p>
<p>② <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</a></p>
<p>③ <a href="https://cran.r-project.org/web/packages/e1071/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/e1071/index.html</a></p>
<p>④ <a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></p>
<p>⑤ <a href="http://cran.r-project.org/web/packages/e1071/e1071.pdf" target="_blank" rel="noopener">http://cran.r-project.org/web/packages/e1071/e1071.pdf</a></p>
<p>⑥ <a href="https://cran.r-project.org/web/packages/caret/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/caret/index.html</a></p>
<p>⑦ <a href="http://topepo.github.io/caret/modelList.html" target="_blank" rel="noopener">http://topepo.github.io/caret/modelList.html</a></p>
<p>⑧ <a href="https://cran.r-project.org/web/packages/doMC/vignettes/gettingstartedMC.pdf" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/doMC/vignettes/gettingstartedMC.pdf</a></p>
<h1 id="第7章-推荐算法"><a href="#第7章-推荐算法" class="headerlink" title="第7章 推荐算法"></a>第7章 推荐算法</h1><p>在信息时代，大多数人的生活中都会接触超量的信息。为了帮助用户从海量的信息中提取有用的信息，推荐系统 （recommendation system）应运而生。目前，推荐系统已经广泛应用于各个领域，最典型的例子是电子商务领域。例如，在淘宝网或者Amazon.com上，根据每个用户过去的购买记录，网站可以推测该用户会对哪些产品感兴趣并进行推荐以帮助用户购买商品。在视频推荐领域，Netflix公司在2006年举办了关于电影推荐的Netflix Prize ① 比赛并取得了极大的成功，有力地推动了推荐算法的研究。目前微软 ② 和谷歌 ③ 等公司都推出了各自的通用推荐系统引擎。</p>
<p>在本章，我们将介绍推荐算法的原理，并讨论不同类型的推荐算法，包括基于内容的推荐算法和基于协同过滤 （collaborative filtering）的推荐算法。由于协同过滤推荐算法在工业界取得的巨大成功，因此我们主要介绍该类算法，包括基于矩阵分解和基于邻域的推荐算法。本章介绍的算法都是较为实用的基本推荐算法。在实际使用推荐算法时，用户应该结合数据本身的特性选择合适的算法。</p>
<p>在介绍具体的推荐算法之前，首先讨论推荐系统基础，包括推荐系统中的一些基本概念、推荐算法的评价标准等。此外，为了方便读者使用常用的推荐算法，还将介绍如何使用R中的<code>recommenderlab</code> 软件包。</p>
<h2 id="7-1-推荐系统基础"><a href="#7-1-推荐系统基础" class="headerlink" title="7.1 推荐系统基础"></a>7.1 推荐系统基础</h2><p>推荐系统的主要任务是根据用户、商品的历史信息，尤其是用户和商品的交互信息，为每个用户推荐他最喜欢的商品。</p>
<p>在推荐算法中，有两类基本对象：</p>
<p>（1）用户（user）；</p>
<p>（2）商品（item）。</p>
<p>事实上，商品是我们在推荐系统中所有待推荐对象的统称，在英文文献关于推荐系统的讨论中一般称为item。根据实际问题，商品的具体定义会有相应的变化。例如，在淘宝网或者Amazon.com上，每件商品（item）就是一件待售的商品。在视频网站（如优酷）上，商品就是一段视频。</p>
<p>在推荐算法中，一个核心问题是：根据用户和商品的历史信息，如何估计用户 _u_ 对某件商品 _i_ 的喜好程度。根据估计的喜好程度，系统可以向用户推荐更加新颖和有趣的商品。因此，在推荐系统中的一个核心概念是用户对于商品的喜好程度，这也是我们建模的核心。</p>
<p>在实际的推荐问题中，我们可以利用的数据主要是用户以往的购买历史，以及用户以往对于商品的评价数据。例如，用户 _u_ 在Amazon.com购物后对所购商品很满意，给了一个五星的评价。除了直接的评价数据，很多时候我们还可以利用用户的间接评价数据。例如，用户浏览某在线商店网站时，虽然没有购买某件商品，但是浏览过，显示用户对该商品感兴趣。</p>
<p>除了用户-商品的交互信息外，其他可以利用的数据包括：</p>
<p>（1）商品的信息，包括商品的价格、类型；</p>
<p>（2）用户的信息，如用户的性别、年龄、居住地点等。</p>
<p>下面我们讨论推荐算法的一些基础知识。根据算法的实现原理，推荐算法可以分为两大类：</p>
<p>（1）基于内容的推荐算法 ；</p>
<p>（2）基于协同过滤的推荐算法 。</p>
<p>基于内容的推荐算法的核心思想是利用商品的相关信息为每件商品构建一个特征向量来表示对应的商品。例如，淘宝网上每件商品的价格、用途、生产厂家等。在此基础上，根据用户过去购买和评价相关商品的历史数据，可以为每个用户构建相应的特征向量以代表该用户。这样就能通过计算用户和商品之间的相似度来预测用户对于商品的喜好程度。</p>
<p>与基于内容的推荐算法相比，基于协同过滤的推荐算法的核心思想是利用用户和商品之间的相互关系来构建推荐模型和进行推荐。所谓协同过滤，就是找出类似的用户或者商品（这一阶段可以称为“协同”），再利用类似用户的喜好或者类似商品所受到的评价情况，来预测用户对商品的喜好程度并挑出最喜欢的商品（这一阶段可以称为“过滤”）。在目前的工业界实践中，基于协同过滤的推荐算法取得了极大的成功和广泛的应用。</p>
<p>在协同过滤中，我们用<img src="Image01030.gif" alt> 表示用户 _u_ 和商品 _i_ 的关系。例如，在网络购物网站中，可以利用用户-商品的购买信息来分析两者之间的关系。这里的用户-商品的关系可以表示为：</p>
<ul>
<li><img src="Image01031.gif" alt> ，如果用户 _u_ 购买过商品 _i_ ；</li>
<li><img src="Image01032.gif" alt> ，如果用户 _u_ 没有购买过商品 _i_ 。</li>
</ul>
<p>用户-商品信息的其他例子包括用户对于商品的评价信息（如1星～5星）、用户对于商品的浏览信息等。根据实际问题的不同，用户和商品的关系<img src="Image01033.gif" alt> 的形式通常可以分为4类。</p>
<p>（1）标量型 （scalar）或者数值型 （numeric）。例如，用户对于电影的评价可以使用评分1～5来评价，分数越高，评价越好。</p>
<p>（2）排序型 （ordinal）。一个典型例子是问卷调查中的答案，如强烈同意/同意/中立/不同意/强烈不同意等。</p>
<p>（3）布尔型 （boolean）：用户对于商品的评价只有两个值，如喜欢/不喜欢。</p>
<p>（4）一元值型 （unary）：用户购买了某件商品，或者用户浏览了某段视频。</p>
<p>我们可以将所有的<img src="Image01033.gif" alt> 放在一个矩阵中，其中每一行对应一个用户，每一列对应一件商品。一般来说，协同过滤算法主要利用该矩阵进行建模和推荐。当然，在实际中，如果还有其他可用的信息，也可以在算法中使用。在下面的讨论中，我们将<img src="Image01033.gif" alt> 一律称为用户 _u_ 对于商品 _i_ 的评价（rating）。事实上，<img src="Image01033.gif" alt> 描述的是用户 _u_ 和商品 _i_ 的相互关系，对于不同的应用有不同的含义。但是，我们讨论的算法可以简单地移植到相应的情况中。</p>
<p>协同过滤算法可以进一步分为以下两类算法：</p>
<p>（1）基于矩阵分解的推荐算法 （recommendation based on matrix factorization）；</p>
<p>（2）基于邻域的推荐算法 （neighborhood-based recommendation）。</p>
<p>基于矩阵分解的算法将用户和商品都用一个低维空间的向量来表示，然后利用该低维表示来进行推荐。从2006年开始进行的Netflix Prize比赛引起了全世界顶级推荐算法高手的广泛兴趣，最后获胜的算法就是基于矩阵分解的协同过滤算法。</p>
<p>基于邻域的推荐算法主要考虑商品之间的关系，或者用户之间的关系。基于邻域的推荐算法的基本思想：当我们要向用户 _u_ 推荐商品 _i_ 时，找出与用户 _u_ 相似的用户，或者与商品 _i_ 相似的商品，从相似的用户或者商品的历史数据中推断用户 _u_ 对商品 _i_ 的喜好程度。基于邻域的推荐算法可以进一步分为基于商品的邻域推荐算法和基于用户的邻域推荐算法。</p>
<p>在本章中，我们着重介绍基于协同过滤的推荐算法，即基于矩阵分解的推荐算法和基于邻域的推荐算法。</p>
<p>在介绍具体的推荐算法之前，我们讨论评价信息中商品的长尾分布 （long-tail distribution）。在很多实际的推荐问题中，在所得的评价信息<img src="Image01033.gif" alt> 中，一小部分商品出现的频率很高，而大部分商品的出现频率很低。换言之，在我们收集到的<img src="Image01033.gif" alt> 里面，大部分是关于很少一部分商品的；而对于绝大多数商品，只有很少的评价信息。图7-1给出了一个实际推荐问题评价数据中商品出现的次数的分布。在图7-1中， _x_ 轴对应的是商品按照在评价信息中的流行程度从高到低排序后的编号， _y_ 轴是商品在评价信息中出现的次数。从图7-1中可以明显看出，这里商品出现次数的分布是一个长尾分布。</p>
<p>长尾分布的存在对于推荐问题来说是一个挑战。在推荐算法中，推荐那些流行的商品是一个比较安全的选择：用户对于这些推荐的流行商品不会觉得是完全无关的商品。但是在实际中，那些很流行的产品对于商家来说基本上都是走量的商品，利润通常都比较低；反而是那些评价很少的商品利润更高一些。因此，在实际部署推荐系统时，商家对于推荐那些不流行的商品的热情更高。一个典型的例子就是Amazon.com从销售那些（总体而言）不那么流行的书籍中获利颇丰。另一方面，对于用户来说，推荐系统推荐的都是那些非常流行的商品，也会有些枯燥。如果推荐系统能从那些不是那么流行的商品中推荐对用户来说有意思的商品，对于用户也是具有吸引力的。但是我们也要注意到，在推荐那些不那么流行的商品的过程中，推荐系统更容易犯错，因为我们对于流行商品有着充足的信息，那些不那么流行的商品所拥有的信息则少很多（从图7-1可以直观地看出）。</p>
<p><img src="Image01034.jpg" alt></p>
<p>图7-1 评价数据中商品出现次数的长尾分布示例</p>
<h3 id="7-1-1-常用符号"><a href="#7-1-1-常用符号" class="headerlink" title="7.1.1 常用符号"></a>7.1.1 常用符号</h3><p>本章中，用户集记为 _U_ ，商品集记为 _I_ 。用户的总数记为 _m_ ，商品的总数记为 _n_ ，即<img src="Image01035.gif" alt> 。特别地，我们使用 _u_ 、 _v_ 来作为用户的下标，而使用 _i、j_ 、 _l_ 作为商品的下标。</p>
<p>我们假设每个用户对于一件商品只有一个评价值，并把用户<img src="Image01036.gif" alt> 对于商品<img src="Image01037.gif" alt> 的评价记为<img src="Image01033.gif" alt> 。所有用户对商品的评价值的集合记为 _S_ 。例如，在数值型评分中，很多时候用户用1星～5星来评价，则<img src="Image01038.gif" alt> ；在布尔型评分中 _S_ 的取值可以为 _S_ =｛喜欢，不喜欢｝。我们把<img src="Image01033.gif" alt> 相应的预测值记为<img src="Image01039.gif" alt> 。如果我们将推荐模型记为 _f_ 的话，可以将<img src="Image01039.gif" alt> 记为<img src="Image01040.gif" alt> 。所有用户-商品对已知的集合记为 _K_ ={( _u_ , _i_ )| _r ui _ 已知}。</p>
<p>我们将所有的<img src="Image01033.gif" alt> 放在一个矩阵中，其中每一行对应一个用户，每一列对应一件商品。我们称这个矩阵为评价矩阵（rating matrix），并记为 <strong>_R_ </strong> 。</p>
<p>对于用户 _u_ ，我们把他/她所评价过的商品集记为 _I_ ( _u_ )。对于商品 _i_ ，我们把所有评价过它的用户集记为 _U_ ( _i_ )$。我们把同时被用户 _u_ 、 _v_ 评价过的商品集记为<img src="Image01041.gif" alt> 。类似地，我们把同时评价过商品 _i_ 、 _j_ 的用户集记为<img src="Image01042.gif" alt> 。</p>
<h3 id="7-1-2-推荐算法的评价标准"><a href="#7-1-2-推荐算法的评价标准" class="headerlink" title="7.1.2 推荐算法的评价标准"></a>7.1.2 推荐算法的评价标准</h3><p>在计算推荐算法的评价标准时，假设我们有一个单独的测试集记为 _T_ 。</p>
<p>在评价推荐算法时，我们可以使用与回归问题类似的指标。通常，我们可以使用平均绝对误差 （mean absolute error, MAE）和均方根误差<br>（root mean squared error, RMSE）。对于模型 _f_ ，我们记<img src="Image01043.gif" alt> 是模型 _f_ 对于用户 _u_ 对商品 _i_ 的评价的预测，则其对应的平均绝对误差和均方根误差定义为：</p>
<p><img src="Image01044.gif" alt></p>
<p>（7-1）</p>
<p><img src="Image01045.gif" alt></p>
<p>（7-2）</p>
<p>在推荐系统中，通常最重视最前面的推荐项。因此，更常用的一种考察推荐算法性能的方法是考察算法返回的前 _k_ 个推荐项。一般情况下，可以计算精确率<br>（precision）和召回率 （recall）。特别是当<img src="Image01030.gif" alt> 为0或者1的情况下，这些评价标准特别有效。例如，在有些应用中，<img src="Image01031.gif" alt> 表示用户购买了商品，为0则表示没有购买。在下面的讨论中，使用这个购买的例子来说明精确率和召回率的计算和含义。假设对于用户 _u_ ，我们推荐了 _k_ 件商品，并按照（模型预测的）用户 _u_ 对于商品的喜好程度<img src="Image01039.gif" alt> 从高到低排列，把排好的序列记为<img src="Image01046.gif" alt> 。在测试集 _T_ 中，把用户 _u_ 购买的商品记为<img src="Image01047.gif" alt> ，则可以定义精确率和召回率：</p>
<p><img src="Image01048.gif" alt></p>
<p>（7-3）</p>
<p><img src="Image01049.gif" alt></p>
<p>（7-4）</p>
<p>其中<img src="Image01050.gif" alt> 是精确率，而<img src="Image01051.gif" alt> 是召回率。这里我们考虑了所有的用户的精确率和召回率，并计算平均值作为最后的结果。</p>
<p>一般来讲，精确率较高，表示返回的结果 _L_ 中很多都是用户真正购买的商品。而召回率则反映了我们能够在多大程度上预测用户所购买的所有商品。</p>
<p>在推荐系统中，用户一般对最前面的结果最感兴趣。越是排在前面的，重要性越高。而在精确率和召回率中，我们把所有在返回序列的商品都一视同仁（当然，可以取不同的 _k_ 值来计算精确率和召回率）。在返回的序列 _L_ 中，可以进一步给每个返回商品根据其在 _L_ 中的位置赋予不同的权重，位置越高，权重越大。一种采用这种策略的评价标准称为平均反击中率 （average reciprocal hit-rank, ARHR），其定义如下：</p>
<p><img src="Image01052.gif" alt></p>
<p>（7-5）</p>
<p>这里<img src="Image01053.gif" alt> 是商品 _i_ 在序列<img src="Image01046.gif" alt> 中的排序编号。如果在最前边，<img src="Image01054.gif" alt><br>；如果排在第二，则<img src="Image01055.gif" alt><br>；如果不在序列<img src="Image01046.gif" alt> 中，则<img src="Image01056.gif" alt> 。</p>
<p>此外，还可以使用排序算法中的一些指标来度量推荐算法的好坏，如DCG和NDCG等。读者可以参阅排序算法中的对应章节。</p>
<h2 id="7-2-基于内容的推荐算法"><a href="#7-2-基于内容的推荐算法" class="headerlink" title="7.2 基于内容的推荐算法"></a>7.2 基于内容的推荐算法</h2><p>本章的重点在于协同过滤，包括基于矩阵的算法和基于邻域的算法。因此，在本节我们简单介绍基于内容的推荐算法 （content-based recommendation），包括其基本原理及优缺点。</p>
<p>在基于内容的推荐中，首先为每件商品 _i_ 构建一个向量 <strong>_x_ </strong> i 来表示该商品的特征。例如，在新闻推荐中，为每篇新闻我们可以使用TF-IDF（Term Frequency-Inverse Document Frequency，词频-逆文档频率<br>）来表示。另一个例子是在网络购物网站中，对每件商品可以使用商品的名称、类别、价格等来构建向量 <strong>_x_ </strong> i 。</p>
<p>在构建完每件商品的特征后，可以根据商品的特征和用户-商品的交互关系为每个用户构建相应的特征。对于用户 _u_ ，考虑所有在集合<img src="Image01057.gif" alt> 中的商品。一种简单的方式是将用户 _u_ 的特征表示为<img src="Image01057.gif" alt> 中所有商品 _i_ 的加权和，且权重为<img src="Image01033.gif" alt> ：</p>
<p><img src="Image01058.gif" alt></p>
<p>（7-6）</p>
<p>在得到商品和用户的特征后，就可以计算它们之间的相似度。例如，可以使用余弦相似度来计算 <strong>_x_ </strong> i 和<img src="Image01059.gif" alt> 的相似度：</p>
<p><img src="Image01060.gif" alt></p>
<p>（7-7）</p>
<p>这样，对于用户 _u_ ，推荐相似度最高的商品即可。</p>
<p>基于内容的推荐算法依赖于我们对于商品的了解。如果我们对商品没有足够的了解，就无法为每件商品构建准确的特征以进行合适的推荐。事实上，很多时候商品已有的信息并不足以构建有效的特征，因此无法直接使用基于内容的推荐算法。</p>
<p>完全基于商品的特征可能在实际中会导致较差的推荐结果。例如，两本同样关于C++语言的书，使用的词汇基本相同，使得这两本书的特征很相似。但是一本很畅销，另一本的销售却很差。如果完全使用基于内容的推荐算法，系统就会把两本书同时向相应的用户推荐。此外，基于内容的推荐算法永远只能给用户推荐他/她已经熟悉的商品，而很多与他/她已经熟悉的商品不同的商品则基本不会推荐。而在实际中，人们往往愿意尝试一些新的商品或者非常流行的商品。从商家的角度讲，也希望给用户推荐一些新的有趣的商品以提高销售业绩。</p>
<p>在实际部署推荐系统时，一般来说，如果没有足够多的用户-商品交互数据，那么基于内容的推荐算法是一个较好的选择，它能够较好地解决冷启动 （cold start）的问题。但是，一旦我们有了足够多的历史数据，协同过滤一般能够取得更高的性能。当然，如果可以的话，同时部署不同类型的推荐算法再综合一般能够得到更为准确的推荐结果。</p>
<h2 id="7-3-基于矩阵分解的算法"><a href="#7-3-基于矩阵分解的算法" class="headerlink" title="7.3 基于矩阵分解的算法"></a>7.3 基于矩阵分解的算法</h2><p>由于在Netflix Prize中的巨大成功，基于矩阵分解的推荐算法在实际中取得了广泛的应用。在本节中，我们由易到难，详细讨论常用的多种矩阵分解算法，包括：</p>
<ul>
<li>无矩阵分解的基准方法；</li>
<li>基于奇异值分解（SVD）的推荐算法；</li>
<li>基于SVD推荐算法的变体<ul>
<li>AFM模型；</li>
<li>翻转的AFM模型；</li>
<li>ASVD模型（或者SVD++模型）；</li>
<li>翻转的ASVD模型；</li>
<li>引入时间信息的模型。</li>
</ul>
</li>
</ul>
<p>对于每种矩阵分解算法，首先介绍其基本原理，然后介绍其对应的求解算法。对于矩阵分解算法，通常采用随机梯度下降 （stochastic gradient descent）算法来求解模型对应的参数。</p>
<p>基于矩阵分解的推荐算法的基本假设：每个用户和每件商品都可以使用低维向量来表示。与基于内容的算法不同，在基于矩阵分解的算法中，这些低维向量是从用户-商品评价矩阵中“学习”出来的，而在基于内容的推荐算法中，用户和商品的特征向量一般都是从商品的特征中提取出来的。而在这个学习过程中，我们希望这些低维表示能够帮助我们建模用户和商品之间已知的相互关系，进而能够准确地预测未知的<img src="Image01030.gif" alt> 的值。在本章接下来的讨论中，我们假设<img src="Image01030.gif" alt> 越高，用户 _u_ 对商品 _i_ 的兴趣越大。</p>
<h3 id="7-3-1-无矩阵分解的基准方法"><a href="#7-3-1-无矩阵分解的基准方法" class="headerlink" title="7.3.1 无矩阵分解的基准方法"></a>7.3.1 无矩阵分解的基准方法</h3><p>在推荐问题中，我们要考虑用户和商品之间的关系。在讨论复杂的矩阵分解算法之前，首先介绍基准算法 （baseline algorithm）。</p>
<p>在很多实际应用中，虽然用户-商品之间的相关关系很重要，但是很多时候用户对于某件商品的评价首先是受到用户自身或者商品本身的性质决定的。在实际中，有的用户倾向于给更高的评价，而有的用户则一般给出较低的评价。举个简单的例子，用户 _u_ 在某在线商店购物时一般都是给出3星或者以上的评价（可选的评价是1星～5星），而用户 _v_ 则很少给出高于3星的评价，一般都是2星左右。类似地，如果一件商品非常畅销，则其收到的评价一般较高；如果一件商品存在一些问题，则很有可能所收到的评价一般都较低。</p>
<p>在基准算法中，我们暂时不考虑用户和商品之间的相互关系。我们只考虑每个用户总体的偏好和每件商品的总体评价。具体来说，我们可以用如下公式来表示：</p>
<p><img src="Image01061.gif" alt></p>
<p>（7-8）</p>
<p>这里<img src="Image01062.gif" alt> 表示基准算法对于评价<img src="Image01033.gif" alt> 的预测，<img src="Image01063.gif" alt> 和<img src="Image01064.gif" alt> 分别是用户 _u_ 和商品 _i_ 各自对应的偏差， _μ_ 是所有评价的平均值，即</p>
<p><img src="Image01065.gif" alt></p>
<p>（7-9）</p>
<p>下面介绍如何估计<img src="Image01066.gif" alt> 和<img src="Image01064.gif" alt> 。与机器学习中的很多算法类似，可以在推荐问题中引入损失函数，通过最小化损失函数，得到参数的最佳估计值。这里采用平方和（sum-of-squares）损失函数，求解如下问题：</p>
<p><img src="Image01067.gif" alt></p>
<p>（7-10）</p>
<p>与前面讨论过的算法类似，我们需要考虑模型的过拟合问题。类似地，我们也加入正则化项 （regularization），则需要求解如下问题：</p>
<p><img src="Image01068.gif" alt></p>
<p>（7-11）</p>
<p>这里<img src="Image01069.gif" alt> 是控制正则化项权重的参数。</p>
<p>对于该优化问题，通常我们使用随机梯度下降 算法来求解。在下面介绍基于SVD的模型时，将详细介绍如何使用随机梯度下降算法来求解更复杂的模型。</p>
<h3 id="7-3-2-基于奇异值分解的推荐算法"><a href="#7-3-2-基于奇异值分解的推荐算法" class="headerlink" title="7.3.2 基于奇异值分解的推荐算法"></a>7.3.2 基于奇异值分解的推荐算法</h3><p>在数学基础中，我们详细讨论了奇异值分解 （singular value decomposition，SVD）。这里我们讨论基于SVD的推荐算法。基于SVD的推荐算法的一个核心思想：用户和商品的特征都可以用一个低维的向量表示，而用户与商品之间的相似度可以用它们对应的向量的内积来表示。</p>
<p>为什么可以假设将用户和商品都用低维向量来表示呢？我们在考虑评价矩阵 <strong>_R_ </strong> 时，发现 <strong>_R_ </strong> 并不是杂乱无章的。这里我们以Netflix竞赛中的场景为例讨论。在Netflix竞赛中，需要考虑用户和电影之间的关系。对于每一个用户而言，他可能只对某一类电影或者某些演员出演的电影比较感兴趣。换句话说，矩阵<br><strong>_R_ </strong> 本身虽然是 _m_ × _n_ 维的，但是其实存在着一些内在特性使得我们可以利用。</p>
<p>我们从一个简单的例子开始讨论。在这个例子中，假设有用户4人：A、B、C和D；假设有6部电影，记为M1～M6。其中前面3部电影是喜剧片，后面3部电影是动作片。在这4个用户中，A和B对喜剧片更感兴趣，C和D对动作片更感兴趣。每个用户对电影进行打分，分数为1～5，越高越好。我们得到的评价矩阵<br><strong>_R_ </strong> 为：</p>
<p><img src="Image01070.gif" alt></p>
<p>可以计算关于 <strong>_R_ </strong> 的奇异值分解<img src="Image01071.gif" alt> ，其中：</p>
<p><img src="Image01072.gif" alt></p>
<p><img src="Image01073.gif" alt></p>
<p><img src="Image01074.gif" alt></p>
<p>我们取 <strong>_U_ </strong> 、 <strong>_V_ </strong> 的前两列和<img src="Image00116.gif" alt> 中的前两个奇异值来逼近 <strong>_R_ </strong> ：</p>
<p><img src="Image01075.gif" alt> <img src="Image01076.gif" alt> <img src="Image01077.gif" alt> <img src="Image01078.gif" alt></p>
<p>可以看出<img src="Image01079.gif" alt> 和 <strong>_R_ </strong> 是比较接近的。但是<img src="Image01079.gif" alt> 的秩是2。换言之，我们只使用了矩阵 <strong>_R_ </strong> 的SVD的一部分信息，但是仍然保留了矩阵 <strong>_R_ </strong> 中最主要的信息。</p>
<p>从上面的例子可以看出，我们可以使用低秩矩阵来近似表达原始矩阵 <strong>_R_ </strong> 。如果将每个评价<img src="Image01033.gif" alt> 都看成用户 _u_ 对应的向量和商品 _i_ 对应的向量的内积的话，就可以使用低维向量来表示用户和商品。</p>
<p>下面使用严格的数学语言来描述。我们将低维空间的维度记为 _d_ ，将用户 _u_ 的低维表示记为<img src="Image01080.gif" alt> ，将商品 _i_ 的低维表示记为<img src="Image01081.gif" alt> 。那么我们可以用它们的内积<img src="Image01082.gif" alt> 来表示用户 _u_ 对商品 _i_ 的喜好程度。同时，也要考虑基准模型中的<img src="Image01063.gif" alt> 和<img src="Image01064.gif" alt> 。将这些因素综合考虑在一起，可以将预测的评价<img src="Image01083.gif" alt> 写成：</p>
<p><img src="Image01084.gif" alt></p>
<p>（7-12）</p>
<p>式（7-12）是基于SVD的推荐算法的基本假设。下面讨论的很多矩阵分解的推荐算法都可以视为该算法的扩展。</p>
<p>与基准模型类似，我们也可通过最小化相应的损失函数来得到最优的参数<img src="Image01066.gif" alt> 、<img src="Image01064.gif" alt> 、<img src="Image01085.gif" alt> 、<img src="Image01086.gif" alt> 。如果我们采用平方和损失函数的话，其对应的损失函数<img src="Image01087.gif" alt> 为：</p>
<p><img src="Image01088.gif" alt></p>
<p>（7-13）</p>
<p>我们要最小化损失函数<img src="Image01089.gif" alt> 以求得最优参数<img src="Image01063.gif" alt> 、<img src="Image01064.gif" alt> 、<img src="Image01085.gif" alt> 、<img src="Image01086.gif" alt> 。这里我们已经考虑了关于<img src="Image01085.gif" alt> 、<img src="Image01086.gif" alt> 的正则化项。控制参数<img src="Image01090.gif" alt> 、<img src="Image01091.gif" alt> 一般可以通过交叉检验得到，以求得最优的参数<img src="Image01063.gif" alt> 、<img src="Image01064.gif" alt> 、<img src="Image01085.gif" alt> 、<img src="Image01086.gif" alt> 。在实际应用中，用户根据数据的具体特征还可以为每个参数选取不同的控制参数。例如，可以将<img src="Image01092.gif" alt> 的权重对应的控制参数设为<img src="Image01093.gif" alt> ，将<img src="Image01094.gif" alt> 权重的控制参数设为<img src="Image01095.gif" alt> ，并分别调节<img src="Image01093.gif" alt> 和<img src="Image01095.gif" alt> 的值。这样，损失函数中<img src="Image01085.gif" alt> 和<img src="Image01086.gif" alt> 对应的正则化项就变为<img src="Image01096.gif" alt> 。</p>
<p>下面我们介绍如何求解该优化问题。因为该优化问题不是机器学习中常见的凸优化 （convex optimization）问题，所以理论上不能同时求得<img src="Image01085.gif" alt> 和<img src="Image01086.gif" alt> 的最优解。一般来讲，我们有两种算法来求解：（1）交替最小二乘法 ；（2）随机梯度下降算法 。</p>
<p>交替最小二乘法是交替算法 （alternating algorithm）的一种。交替算法的基本思想：在一个优化问题中有多个参数需要求解时，如果同时求解所有参数的难度太高，那么可以先固定一部分参数（该参数集记为<img src="Image01097.gif" alt><br>）的值，而去优化剩余的参数（记为<img src="Image01098.gif" alt><br>）；然后固定刚刚求得的参数<img src="Image01098.gif" alt> 的值，来优化<img src="Image01097.gif" alt><br>）。反复使用该流程直到所有的参数都收敛到最优解。注意，使用交替算法的时候我们一般求得的都是局部最优解（而不是全局最优解）。</p>
<p>具体来说，在优化<img src="Image01087.gif" alt> 时，我们不能同时求解<img src="Image01085.gif" alt> 和<img src="Image01086.gif" alt> 。但是，如果我们固定<img src="Image01085.gif" alt> ，那么求解<img src="Image01086.gif" alt> 可以直接使用最小二乘法。固定<img src="Image01086.gif" alt> ，求解<img src="Image01085.gif" alt> 也可使用同样的算法。</p>
<p>在随机梯度下降算法中，逐次遍历评价矩阵 <strong>_R_ </strong> 中每个已知元素<img src="Image01033.gif" alt> ，并更新模型中的所有对应参数，包括<img src="Image01063.gif" alt> 、<img src="Image01064.gif" alt> 、<img src="Image01085.gif" alt> 、<img src="Image01086.gif" alt> 。在随机梯度下降算法中，每一步我们求解目标函数对于每个参数的梯度（或者导数），然后沿着负梯度的方向按照一定步长更新对应的参数。</p>
<p>这里以参数<img src="Image01085.gif" alt> 为例来说明随机梯度算法如何工作。注意，损失函数<img src="Image01087.gif" alt> 考虑了集合 _K_ 中的所有评价值。当顺次处理评价项<img src="Image01033.gif" alt> 时，损失函数<img src="Image01087.gif" alt> 与<img src="Image01033.gif" alt> 相关的项<img src="Image01099.gif" alt> 为：</p>
<p><img src="Image01100.gif" alt></p>
<p>（7-14）</p>
<p>可以计算<img src="Image01101.gif" alt> 关于<img src="Image01085.gif" alt> 的梯度如下：</p>
<p><img src="Image01102.gif" alt></p>
<p>（7-15）</p>
<p>把<img src="Image01103.gif" alt> 记为<img src="Image01104.gif" alt> ，则<img src="Image01105.gif" alt> 可记为：</p>
<p><img src="Image01106.gif" alt></p>
<p>（7-16）</p>
<p>在随机梯度下降算法中，可以利用如下公式来更新参数<img src="Image01085.gif" alt> ：</p>
<p><img src="Image01107.gif" alt></p>
<p>（7-17）</p>
<p>这里<img src="Image01108.gif" alt> &gt;0称为步长或者学习率 （learning rate）。</p>
<p>对于其他参数，可以使用类似的技巧来更新它们。这里列出所有的更新公式而省略具体的推导过程。</p>
<p><img src="Image01109.gif" alt></p>
<p>（7-18）</p>
<p><img src="Image01110.gif" alt></p>
<p>（7-19）</p>
<p><img src="Image01111.gif" alt></p>
<p>（7-20）</p>
<p>注意，这里为所有的参数都使用了同样的步长 _γ_ 。在实际中，用户可以根据实际的数据对不同的参数使用不同的步长。此外，对于不同的参数的正则化项，还可使用不同的系数。</p>
<p>算法7-1给出了使用随机梯度下降算法训练一个完整的SVD推荐模型的具体步骤。</p>
<p>算法7-1 适用于SVD模型的随机梯度下降算法</p>
<blockquote>
<p>输入：评价矩阵 <strong>_R_ </strong> 。 &gt; &gt; 控制参数：学习率 _γ_ ，低维维度 _d_ ，正则化参数<img src="Image00290.gif" alt> &gt; 、<img src="Image00294.gif" alt> 。 &gt; &gt; 初始化用户对应的参数<img src="Image01112.gif" alt> &gt; 和<img src="Image01113.gif" alt> &gt; ，商品对应的向量<img src="Image01114.gif" alt> &gt; 和<img src="Image01115.gif" alt> 。 &gt; &gt; 计算评价的平均值<img src="Image01116.gif" alt> 。 &gt; &gt; while 停止标准没有满足 &gt; &gt; 对于所有的<img src="Image01117.gif" alt> &gt; &gt; 计算<img src="Image01118.gif" alt> &gt; &gt; 计算错误<img src="Image01119.gif" alt> &gt; &gt; 更新相关参数： &gt; &gt; <img src="Image01120.gif" alt> &gt; &gt; <img src="Image01121.gif" alt> &gt; &gt; <img src="Image01122.gif" alt> &gt; &gt; <img src="Image01123.gif" alt></p>
</blockquote>
<p>在算法7-1中，可以使用不同的停止标准。例如，在实际中，可以单独划分一个检验集 （validation set）。当所学习到的模型在检验集上的性能达到要求（如在检验集上的RMSE小于规定值）时，可以认为停止标准达到了。</p>
<p><strong>讨论</strong> !</p>
<p>在第3章中我们讨论了奇异值分解（SVD）。在本节所讨论的推荐算法中，虽然这里的算法也称为基于SVD的模型，但是它们之间有一些不同：</p>
<ul>
<li>在标准的SVD中，整个矩阵都是已知的；而在推荐问题中，通常只知道矩阵的一部分，并且经常是稀疏的，这样矩阵计算中已有的求解SVD的算法不能直接适用于推荐问题。</li>
<li>在推荐问题中，由于评价矩阵 <strong>_R_ </strong> 太稀疏，因此必须非常注意过拟合问题。在前面的讨论中，引入了正则化项，在实际使用中也需要读者根据数据的具体情况选择合适的参数值。</li>
<li>推荐算法中的SVD模型更加接近于用一个低秩的矩阵来逼近矩阵 <strong>_R_ </strong> ：</li>
</ul>
<p><img src="Image01124.gif" alt></p>
<p>（7-21）</p>
<p>这里</p>
<p><img src="Image01125.gif" alt></p>
<p>（7-22）</p>
<p>根据式（7-21），我们把 <strong>_R_ </strong> 分解为两个低秩矩阵的积，所以有些文献（如参考文献[28]）中也把这种矩阵分解称为UV分解。为了避免过拟合，通常还进一步对矩阵 <strong>_P_ </strong> 和<br><strong>_Q_ </strong> 做出了一些限制。在上面讨论的算法中，为<img src="Image01085.gif" alt> 和<br><strong>_q_ </strong> i 都引入了正则化项。</p>
<h3 id="7-3-3-基于SVD推荐算法的变体"><a href="#7-3-3-基于SVD推荐算法的变体" class="headerlink" title="7.3.3 基于SVD推荐算法的变体"></a>7.3.3 基于SVD推荐算法的变体</h3><p>前面讨论的基于奇异值分解的基本算法原理简单，也易于实现。但是在实际中，如果数据较为复杂，并且数据和该算法的基本假设存在较大差异的话，则很难取得很好的效果。在本节我们进一步介绍基于SVD模型的变体，包括AFM模型 、翻转的AFM模型、ASVD模型 、翻转的ASVD模型 ，以及引入时间信息的矩阵分解模型 。</p>
<h4 id="1．AFM模型"><a href="#1．AFM模型" class="headerlink" title="1．AFM模型"></a>1．AFM模型</h4><p>AFM模型一般称为不对称因子模型 （asymmetric factor model, AFM）。在该模型中，只有与商品有关的参数。具体而言，对于每个评价<img src="Image01030.gif" alt> ，我们假设可以用下面的公式来表示：</p>
<p><img src="Image01126.gif" alt></p>
<p>（7-23）</p>
<p>其关键性的假设：对于每件商品，还是使用<img src="Image01127.gif" alt> 来给出它对应的低维表示；对于每个用户，我们假设对应的低维向量可以由其在训练集中评价过的商品来表示。这里对于用户 _u_ ，将其在训练集中评价过的商品的集合记为<img src="Image01057.gif" alt> ，并用<img src="Image01057.gif" alt> 中的商品来表示用户 _u_ 。用户 _u_ 的低维表示为：</p>
<p><img src="Image01128.gif" alt></p>
<p>（7-24）</p>
<p>对于相关的商品<img src="Image01129.gif" alt> ，用向量<img src="Image01130.gif" alt> 表示该商品对于用户 _u_ 对应向量的贡献。此外，还是用<img src="Image01131.gif" alt> 来对用户 _u_ 进行标准化处理，这里<img src="Image01132.gif" alt> 是用户 _u_ 评价过的商品的总数。注意，这里对于每件商品 _i_ ，构建了两个不同的向量。</p>
<ul>
<li><img src="Image01081.gif" alt> ：用来表示商品 _i_ 在低维空间的表示；</li>
<li><img src="Image01133.gif" alt> ：用来表示商品 _i_ 对于相关用户对应的低维向量的贡献。</li>
</ul>
<p>在实际中，我们要从训练数据中为每件商品学出<img src="Image01086.gif" alt> 和<img src="Image01130.gif" alt> 。</p>
<h4 id="2．翻转的AFM模型"><a href="#2．翻转的AFM模型" class="headerlink" title="2．翻转的AFM模型"></a>2．翻转的AFM模型</h4><p>在AFM模型 中，我们将用户在低维空间的表示也用相关商品的低维表示来表示。类似地，也可以将AFM“翻转”过来：我们只有用户的低维表示，商品的低维表示由对应用户的低维表示决定。</p>
<p>具体来说，对于每个用户 _u_ ，构建两个向量。</p>
<ul>
<li><img src="Image01080.gif" alt> ：用来表示用户 _u_ 的低维表示；</li>
<li><img src="Image01134.gif" alt> ：用来表示用户 _u_ 对于相关商品对应的低维向量的贡献。</li>
</ul>
<p>这样，对于商品 _i_ ，可以使用<img src="Image01135.gif" alt> 来表示：</p>
<p><img src="Image01136.gif" alt></p>
<p>（7-25）</p>
<p>这里 _U_ ( _i_ )是训练集中评价过商品 _i_ 的用户集合，<img src="Image01137.gif" alt> 是评价过商品 _i_ 的用户总数。</p>
<p>这样每个评价<img src="Image01033.gif" alt> 的预测值可以用如下的公式来表示：</p>
<p><img src="Image01138.gif" alt></p>
<p>（7-26）</p>
<p>AFM模型和翻转的AFM模型 都可以使用随机梯度下降算法来求解。下面我们介绍更加通用的ASVD模型，并介绍对应的随机梯度下降算法。由于AFM模型和翻转的AFM模型是ASVD模型的特例，因此可以将所介绍的随机梯度下降算法简化，这样就可以直接求解了。</p>
<h4 id="3．ASVD模型"><a href="#3．ASVD模型" class="headerlink" title="3．ASVD模型"></a>3．ASVD模型</h4><p>在ASVD模型 中，综合考虑了SVD模型和AFM模型。它也称为SVD++模型。一般而言，在很多场合下，ASVD模型（或者SVD++模型）的性能要优于前面讨论的基本SVD模型。</p>
<p>具体而言，在ASVD模型中，用户 _u_ 的低维表示为：</p>
<p><img src="Image01139.gif" alt></p>
<p>（7-27）</p>
<p>与前面的SVD模型相比，对于用户 _u_ ，添加了来自于他/她所评价的商品的信息。这样，我们希望能够更加准确地描述用户与商品之间的关系。</p>
<p>这样，可以将<img src="Image01030.gif" alt> 的预测值<img src="Image01140.gif" alt> 写成：</p>
<p><img src="Image01141.gif" alt></p>
<p>（7-28）</p>
<p>下面介绍适用于ASVD模型的随机梯度下降算法。与SVD模型类似，我们假设使用平方和损失函数，并考虑了各参数的正则化项：</p>
<p><img src="Image01142.gif" alt></p>
<p><img src="Image01143.gif" alt></p>
<p><img src="Image01144.gif" alt></p>
<p>（7-29）</p>
<p>这里<img src="Image00781.gif" alt> 、<img src="Image01145.gif" alt> 、<img src="Image01146.gif" alt> ，它们是控制正则化项权重的参数。</p>
<p>在随机梯度下降算法中，我们顺次遍历评价矩阵 <strong>_R_ </strong> 中每个已知元素<img src="Image01033.gif" alt> ，并更新模型中的所有对应参数，包括<img src="Image01063.gif" alt> 、<img src="Image01064.gif" alt> 、<img src="Image01085.gif" alt> 、<img src="Image01086.gif" alt> 、<img src="Image01130.gif" alt> 。当我们顺次处理评价项<img src="Image01033.gif" alt> 时，其对应的损失函数<img src="Image01147.gif" alt> 为：</p>
<p><img src="Image01148.gif" alt></p>
<p>（7-30）</p>
<p>这里逐个计算<img src="Image01147.gif" alt> 对于各个参数的偏导数。如前面的讨论，记<img src="Image01149.gif" alt> ，则有：</p>
<p><img src="Image01150.gif" alt></p>
<p>（7-31）</p>
<p><img src="Image01151.gif" alt></p>
<p>（7-32）</p>
<p><img src="Image01152.gif" alt></p>
<p>（7-33）</p>
<p><img src="Image01153.gif" alt></p>
<p>（7-34）</p>
<p><img src="Image01154.gif" alt></p>
<p>（7-35）</p>
<p>在随机梯度下降算法中，我们都是用负导数来更新对应的参数。因此，在ASVD模型中，使用如下的公式来更新诸参数：</p>
<p><img src="Image01155.gif" alt></p>
<p>（7-36）</p>
<p><img src="Image01156.gif" alt></p>
<p>（7-37）</p>
<p><img src="Image01157.gif" alt></p>
<p>（7-38）</p>
<p><img src="Image01158.gif" alt></p>
<p>（7-39）</p>
<p><img src="Image01159.gif" alt></p>
<p>（7-40）</p>
<p>ASVD模型还能进一步扩展。在很多实际应用中，有所谓的隐式反馈 （implicit feedback）。如果我们在淘宝上购物，除了用户购买的商品外，用户点击的商品也反映了用户的兴趣。在很多实际应用中，考虑隐式反馈的信息很容易获得，而直接的信息（如购买）却很少。下面我们讨论一种考虑隐式反馈的ASVD模型。记<img src="Image01160.gif" alt> 为用户 _u_ 所提供的隐式反馈所对应的商品集，在ASVD模型的基础上，可以将用户 _u_ 在低维空间的表示写成如下形式：</p>
<p><img src="Image01161.gif" alt></p>
<p>（7-41）</p>
<p>在式（7-41）中，第二项<img src="Image01162.gif" alt> 对应于直接反馈的商品的贡献，第三项<img src="Image01163.gif" alt> 对应于隐式反馈的商品的贡献。而对商品 _i_ 而言，需要考虑以下3项。</p>
<ul>
<li><img src="Image01081.gif" alt> ：用来表示商品 _i_ 在低维空间的表示。</li>
<li><img src="Image01133.gif" alt> ：用来表示商品 _i_ 对于直接相关用户对应的低维向量的贡献。</li>
<li><img src="Image01164.gif" alt> ：用来表示商品 _i_ 对于隐式反馈相关用户对应的低维向量的贡献。</li>
</ul>
<p>这样，可以将<img src="Image01033.gif" alt> 的预测值<img src="Image01039.gif" alt> 写成：</p>
<p><img src="Image01165.gif" alt></p>
<p>（7-42）</p>
<p>类似地，利用随机梯度下降算法，也可以求解该模型。</p>
<blockquote>
<p><strong>讨论</strong> &gt; &gt; 与简单的SVD模型相比，ASVD模型引入的参数更多。为了避免过拟合，我们更倾向于选择较小的 _d_ 值，即低维空间的维度较低，从而控制模型的复杂度。</p>
</blockquote>
<h4 id="4．翻转的ASVD模型"><a href="#4．翻转的ASVD模型" class="headerlink" title="4．翻转的ASVD模型"></a>4．翻转的ASVD模型</h4><p>与前面讨论类似，我们也有翻转的ASVD模型 。在翻转的ASVD模型中，对于每个用户，使用<img src="Image01085.gif" alt> 来表示他/她的低维表示，而商品 _i_ 的低维表示为：</p>
<p><img src="Image01166.gif" alt></p>
<p>（7-43）</p>
<p>这里为每个用户 _v_ 定义了<img src="Image01167.gif" alt> ，表示他/她对相关商品的贡献。这样，可以将<img src="Image01033.gif" alt> 的预测值<img src="Image01039.gif" alt> 写成：</p>
<p><img src="Image01168.gif" alt></p>
<p>（7-44）</p>
<p>此外，也可以引入隐式反馈，将<img src="Image01033.gif" alt> 的预测值<img src="Image01039.gif" alt> 写成：</p>
<p><img src="Image01169.gif" alt></p>
<p>（7-45）</p>
<p>这里将在隐式反馈中将商品 _i_ 相关的用户集合记为<img src="Image01170.gif" alt> 。</p>
<p>类似地，对于翻转的ASVD模型，也可以导出相应的导数，并利用随机梯度下降算法来求解。其基本过程和公式与上一节中的讨论非常类似，这里就不再重复了。</p>
<h4 id="5．引入时间信息的模型"><a href="#5．引入时间信息的模型" class="headerlink" title="5．引入时间信息的模型"></a>5．引入时间信息的模型</h4><p>在前面的讨论中，我们介绍了如何利用直接反馈和隐式反馈。这里我们介绍如何进一步引入时间信息。在很多应用中，如果评价数据与时间信息存在某些周期性的关系，那么引入时间特征可以显著地提高推荐算法的性能。</p>
<p>在本节中，我们从简单的例子（无矩阵分解的基准算法）开始，讨论如何引入时间信息，然后介绍如何将时间信息引入到矩阵分解的算法中。在这里我们强调，时间信息的引入依赖于问题的具体定义和实际数据，读者需要灵活应用时间信息以得到最优性能。</p>
<p>首先我们讨论如何在无矩阵分解的基准算法中加入时间信息。在实际中，推荐系统的实际表现和时间关系很大。例如，在前面的讨论中，每件商品都有对应的<img src="Image01064.gif" alt> 。在电影推荐中，很多电影对应的<img src="Image01064.gif" alt> 随着电影的档期发生了明显的变化。在购物推荐中，用户的喜好在不同的季节有着明显的变化。因此，在很多实际应用中，<img src="Image01064.gif" alt> 和<img src="Image01063.gif" alt> 都是关于时间的函数。那么，在无矩阵分解的基准方法中，如果我们引入时间信息，则可将<img src="Image01171.gif" alt> 表示为</p>
<p><img src="Image01172.gif" alt></p>
<p>（7-46）</p>
<p>这里<img src="Image01173.gif" alt> 是评价<img src="Image01033.gif" alt> 产生的时间。</p>
<p>如果我们将<img src="Image01064.gif" alt> 和<img src="Image01063.gif" alt> 都看成是关于时间<img src="Image01174.gif" alt> 的连续函数，那么需要足够的数据才能准确地估计对应的函数。此外，直接将<img src="Image01064.gif" alt> 和<img src="Image01063.gif" alt> 都当做是关于时间<img src="Image01174.gif" alt> 的连续函数会导致推荐模型的复杂度较高，在实际中容易过拟合。</p>
<p>在实际中，一种有效的方法是将时间轴划分为若干段，并将<img src="Image01064.gif" alt> 和<img src="Image01063.gif" alt> 在每段的值视为一恒定值。举一个简单的例子，在上网购物时，有的用户习惯于上午或下午购物，而有的用户更偏向于下班之后的晚上购物。这样，我们可以把时间轴分为3段：上午（记为时间段1）、下午（记为时间段2）及晚上（记为时间段3），从而分别考虑<img src="Image01064.gif" alt> 和<img src="Image01063.gif" alt> 在不同时间段的值。这样就可将<img src="Image01171.gif" alt> 表示为：</p>
<p><img src="Image01175.gif" alt></p>
<p>（7-47）</p>
<p>这里</p>
<p><img src="Image01176.gif" alt></p>
<p>（7-48）</p>
<p><img src="Image01177.gif" alt></p>
<p>（7-49）</p>
<p>这里<img src="Image01178.gif" alt> 、<img src="Image01179.gif" alt> 、<img src="Image01180.gif" alt> 分别代表上午、下午和晚上对<img src="Image01063.gif" alt> 的调节值。根据当时的时间<img src="Image01174.gif" alt> ，我们从<img src="Image01178.gif" alt> 、<img src="Image01179.gif" alt> 、<img src="Image01180.gif" alt> 中选出对应的项来调节<img src="Image01063.gif" alt> 的值。类似地，<img src="Image01181.gif" alt> 、<img src="Image01182.gif" alt> 、<img src="Image01183.gif" alt> 分别代表上午、下午和晚上对<img src="Image01064.gif" alt> 的调节值。</p>
<p>这种做法的好处：（1）需要估计的参数值一般不多，模型的复杂度也不高；（2）能够得到足够多的数据来估计参数值；（3）在实际中，一般能够取得比较理想的性能。但缺点是需要根据数据和问题手动地确定分段。</p>
<p>接下来我们考虑如何在矩阵分解模型中进一步加入时间信息。与前面的基准方法类似，对于商品 _i_ ，除了其低维表示<img src="Image01086.gif" alt> 外，还要考虑其随着时间变化而变化的项。简而言之，可以将商品 _i_ 的新的低维表示记为：</p>
<p><img src="Image01184.gif" alt></p>
<p>其中<img src="Image01185.gif" alt> 是关于时间的函数。</p>
<p>对于用户 _u_ ，也可以使用类似的项来调节用户的低维表示：</p>
<p><img src="Image01186.gif" alt></p>
<p>其中<img src="Image01187.gif" alt> 是关于时间的函数。</p>
<p>在实际应用中，与前面讨论的基准方法类似，我们可以：（1）将<img src="Image01188.gif" alt> 和<img src="Image01189.gif" alt> 考虑成关于时间的连续函数；（2）将时间轴分为若干段，但是每段都对应一个恒定值。这里举一个简单的例子来说明如何使用第二种方法。在购物网站中，一般而言，用户在周末和非周末的购买行为和习惯会有所不同，因此我们将时间分为周末和非周末，并分别用<img src="Image01190.gif" alt> 和<img src="Image01191.gif" alt> 来表示周末和非周末的调节量。于是，用户 _u_ 的低维表示为：</p>
<p><img src="Image01192.gif" alt></p>
<p>与前面的讨论类似，可以引入对应的损失函数（包括正则化项）并求解。</p>
<h2 id="7-4-基于邻域的推荐算法"><a href="#7-4-基于邻域的推荐算法" class="headerlink" title="7.4 基于邻域的推荐算法"></a>7.4 基于邻域的推荐算法</h2><p>在基于矩阵分解的推荐算法兴起之前，最常用的推荐算法是基于邻域的推荐算法 （neighborhood-based recommendation）。基于邻域的推荐算法的基本思想：当我们要向用户 _u_ 推荐商品 _i_ 时，找出与用户 _u_ 相似的用户，或者与商品 _i_ 相似的商品，从相似的用户或者商品的历史数据中推断用户 _u_ 对商品 _i_ 的喜好程度。基于邻域的推荐算法也称为基于记忆的推荐算法<br>（memory-based recommendation）或者基于启发式的推荐算法 （heuristic-based recommendation）。</p>
<p>在基于邻域的推荐算法中，主要利用用户-用户之间的相似度或者商品-商品之间的相似度来预测未知的评价值<img src="Image01030.gif" alt> 。具体而言，可以从寻找相似的用户出发，也可以从寻找相似的商品开始处理。因此，算法分为如下两类。</p>
<ul>
<li>基于用户的邻域推荐算法：如果用户 _u_ 和用户 _v_ 对很多商品的喜好一致，那么可以使用用户 _u_ 对于某一商品 _i_ 的已知评价来推断用户 _v_ 对该商品的评价。</li>
<li>基于商品的邻域推荐算法：如果商品 _i_ 和商品 _j_ 在很多用户的评价中都比较一致，那么可以使用用户 _u_ 对商品 _i_ 的已知评价来推断用户 _u_ 对商品 _j_ 的评价。</li>
</ul>
<p>与基于矩阵分解的推荐算法相比，基于邻域的推荐算法在原理上十分简单直观，并且易于实现。基于邻域的推荐算法的主要控制参数包括邻域的大小和相似度度量的选择。因此，需要学习的参数较少，模型的复杂度也比较低。此外，当有了新的数据需要更新模型时，只需要更新与新数据相关的用户和商品即可，不需要重新训练整个模型。在实际中，基于邻域的推荐算法更容易解释和理解。而基于矩阵分解的推荐算法依赖于用户和商品的低维表示，模型不容易解释。下面我们具体讨论基于用户的邻域推荐算法和基于商品的邻域推荐算法。</p>
<h3 id="7-4-1-基于用户的邻域推荐算法"><a href="#7-4-1-基于用户的邻域推荐算法" class="headerlink" title="7.4.1 基于用户的邻域推荐算法"></a>7.4.1 基于用户的邻域推荐算法</h3><p>在推荐系统中，我们面临的首要问题是：对于用户 _u_ 和商品 _i_ ，如何估计用户 _u_ 对商品 _i_ 的喜好程度<img src="Image01033.gif" alt> ？在基于用户的邻域推荐算法 （user-based neighborhood recommendation）中，我们的基本思想是从已经评价过商品 _i_ 的用户集<img src="Image01193.gif" alt> 中，找出与用户 _u_ 相似的用户，并利用这些用户对商品 _i_ 的评价来推断用户 _u_ 对商品 _i_ 的喜好程度。简而言之，就是从商品 _i_ 相关的用户群中找出与用户 _u_ 相似的用户，从而进一步推断<img src="Image01033.gif" alt> 。</p>
<p>严格地讲，就是在评价过商品 _i_ 的用户集<img src="Image01193.gif" alt> 中，寻找与用户 _u_ 相似的 _k_ 个用户组成的集合（记为最近邻集<img src="Image01194.gif" alt><br>），并利用<img src="Image01194.gif" alt> 来估计<img src="Image01033.gif" alt> ：</p>
<p><img src="Image01195.gif" alt></p>
<p>（7-50）</p>
<p>式（7-50）简洁明了，在实际中是较实用的公式。其缺陷是没有仔细考虑<img src="Image01196.gif" alt> 中的每个用户和用户 _u_ 之间的相似程度。如果考虑每个用户<img src="Image01197.gif" alt> 与用户 _u_ 的相似度（记为<img src="Image01198.gif" alt><br>），则可以使用如下公式来估计<img src="Image01033.gif" alt> ：</p>
<p><img src="Image01199.gif" alt></p>
<p>（7-51）</p>
<p>这里在分母中使用了<img src="Image01200.gif" alt> ，因为在很多情况下<img src="Image01198.gif" alt> 可能为负值。为了保证<img src="Image01039.gif" alt> 落在正常的范围之内，我们使用了<img src="Image01201.gif" alt> 。</p>
<h4 id="评价的标准化"><a href="#评价的标准化" class="headerlink" title="评价的标准化"></a>评价的标准化</h4><p>上面的式（7-50）和式（7-51）直接根据邻域<img src="Image01194.gif" alt> 中的用户评价来计算加权平均值。在前面的讨论中，我们知道每个用户对应不同的<img src="Image01063.gif" alt> 项：有的用户更加愿意给出较高的评价，有些用户则倾向于给出较低的评价。因此，我们希望在综合多个用户的评价时，能够消除用户的个人偏见。一个常用的办法是将所有的评价先标准化<br>（normalization），然后综合。这里我们主要介绍两种常用的标准化方法：基于平均值的标准化 和Z分值标准化 。</p>
<p>在基于平均值的标准化 中，我们将评分<img src="Image01033.gif" alt> 与对应的平均值相比较，将所得的差值作为新的评分。这里的平均值可以是用户 _u_ 评分的平均值，也可以是商品 _i_ 所有的评分的平均值。下面我们具体讨论在基于用户的邻域方法中如何使用基于平均值的标准化。</p>
<p>在基于用户的邻域方法中，我们将<img src="Image01033.gif" alt> 与用户 _u_ 的评分的平均值<img src="Image01202.gif" alt> 比较，得到标准化之后的评分：</p>
<p><img src="Image01203.gif" alt></p>
<p>（7-52）</p>
<p>这里<img src="Image01204.gif" alt> 是标准化之后的评分，<img src="Image01202.gif" alt> 为用户 _u_ 当前评分的平均值：</p>
<p><img src="Image01205.gif" alt></p>
<p>（7-53）</p>
<p>使用前面的公式计算综合多个用户的评价之后，还需要将所得的评价分数转化到原始的评价空间中。这里我们需要重新加上<img src="Image01206.gif" alt> 。因此，<img src="Image01033.gif" alt> 的预测值可以表示为：</p>
<p><img src="Image01207.gif" alt></p>
<p>（7-54）</p>
<p>可以看出，这里只是简单地使用每个用户的评价的平均值来对用户的原始评价进行调整。</p>
<p>在很多实际应用中，我们除了考虑用户评分的平均值外，还要考虑用户评分时的偏差。假设我们有两个用户：A和B。用户A和用户B的评分均值都是3分，但是用户A基本上对所有的商品的评价都是3分（分数为1分～5分，越高越好），而用户B的评分却从1分到5分都有。如果对于同一件商品，用户A和用户B都给出了5分，则可以认为用户A对该商品的满意程度高于用户B。因此，在对评分进行标准化处理时，我们除了要考虑评分的均值外，还需要考虑评分的“散布程度”，就是评分的方差 。注意，在基于用户的邻域算法和基于商品的邻域算法中，Z分值标准化 的具体实施稍有不同。</p>
<p>在基于用户的邻域算法中，使用如下公式来修正<img src="Image01030.gif" alt> ：</p>
<p><img src="Image01208.gif" alt></p>
<p>（7-55）</p>
<p>这里<img src="Image01209.gif" alt> 是用户 _u_ 评分的标准差。采用Z分值标准化之后，预测<img src="Image01033.gif" alt> 的公式则为：</p>
<p><img src="Image01210.gif" alt></p>
<p>（7-56）</p>
<p>如果一个用户的评价太少，则会导致其对应的方差<img src="Image01211.gif" alt> ，导致无法使用Z 分值标准化。一般来讲，如果用户或者商品的评价足够多的话，那么使用Z分值标准化能够得到更好的结果。特别是如果评分<img src="Image01033.gif" alt> 的取值范围较大的话，使用方差进行处理是非常合理的。但是使用Z分值标准化时有可能导致预测值<img src="Image01039.gif" alt> 的取值超过原始定义的空间。例如，原来<img src="Image01212.gif" alt> ，使用Z分值之后<img src="Image01039.gif" alt> 有可能是6。</p>
<p>在实际应用中，以上讨论的这两种方法在很多时候性能都比较相近，因此要根据数据的具体情况采用相应的标准化方法。</p>
<p>我们在第4章中讨论了数据的标准化处理。这里的讨论与前面的讨论在原理上是一致的，但是这里的讨论更适用于推荐问题。</p>
<p>下面我们通过一个简单的例子来说明如何使用基于用户的邻域推荐算法来进行推荐。</p>
<p>例7-1 在这个简化的例子中，我们有4个用户，即U1～U4；有4部电影，即M1～M4。用户对电影的评分为1～5，见表7-1。这里我们要根据已有的评分数据，估计用户U2对电影M2的评分。</p>
<p>表7-1 使用基于用户的邻域推荐算法来估计</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>电影评分</th>
<th>M1</th>
<th>M2</th>
<th>M3</th>
<th>M4  </th>
</tr>
</thead>
<tbody>
<tr>
<td>U1</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>2  </td>
</tr>
<tr>
<td>U2</td>
<td>1</td>
<td>?</td>
<td>4</td>
<td>3  </td>
</tr>
<tr>
<td>U3</td>
<td>2</td>
<td>5</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>U4</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>2  </td>
</tr>
</tbody>
</table>
</div>
<p>对于用户U2，假设与其最相似的两个用户是U3和U4，并假设其相似度为<img src="Image01213.gif" alt> 、<img src="Image01214.gif" alt> 。当 _k_ =2时，我们利用式（7-51）估计U2对电影M2的评分为：</p>
<p><img src="Image01215.gif" alt></p>
<p>在算法实践中，其中一个关键参数是邻域的大小，即参数 _k_ 的值。当 _k_ 值太小时，我们只使用了非常有限的信息，导致预测的精度会较低；当 _k_ 值增大时，我们考虑了更多的用户信息，精度会进一步提高。但是当 _k_ 值较大时，用户 _u_ 的邻域信息在某种程度上被“稀释”了，或者换句话说，有更多的噪声或者无关用户“混入”了邻域，使得推荐的精度下降。由于具体 _k_ 值的选取高度依赖于具体的数据，因此在实际中我们一般采用交叉检验来确定最优的 _k_ 值。</p>
<p>在算法的具体实现中，我们需要为每个用户保存与其最相似的 _k_ 个用户组成的集合。在实际中，我们通常采用过滤 的方法来保存最相似的用户-用户或者商品-商品相似度。主要有如下两种策略。</p>
<p>（1）保存最相似的<img src="Image01216.gif" alt> 个用户（或者商品）及对应的相似度。这里的<img src="Image01216.gif" alt> ≥ _k_ 。我们一般要选择较大的<img src="Image01216.gif" alt> 值，这样在稍后选择 _k_ 值时会有更多的余地。</p>
<p>（2）如果用户与用户之间的相似度大于一个事先选定的阈值，则保持；否则直接丢弃。</p>
<h3 id="7-4-2-基于商品的邻域推荐算法"><a href="#7-4-2-基于商品的邻域推荐算法" class="headerlink" title="7.4.2 基于商品的邻域推荐算法"></a>7.4.2 基于商品的邻域推荐算法</h3><p>在基于商品的邻域推荐算法 中，我们实际上翻转了用户和商品之间的关系。在估计用户 _u_ 对商品 _i_ 的喜好程度<img src="Image01033.gif" alt> 时，其基本思想是首先考虑用户 _u_ 评价过的所有商品，从这些商品中选出与商品 _i_ 最相似的 _k_ 件商品，并将该商品集记为<img src="Image01217.gif" alt> 。然后，考虑用户 _u_ 对<img src="Image01217.gif" alt> 中每件商品 _j_ 的评价<img src="Image01218.gif" alt> ，将对<img src="Image01033.gif" alt> 的估计写为<img src="Image01218.gif" alt> 的线性组合：</p>
<p><img src="Image01219.gif" alt></p>
<p>（7-57）</p>
<p>类似地，也可以考虑导入权重<img src="Image01220.gif" alt> （商品 _i_ 和商品 _j_ 之间的相似度记为<img src="Image01221.gif" alt> ），则<img src="Image01033.gif" alt> 的估计可以表示为：</p>
<p><img src="Image01222.gif" alt></p>
<p>（7-58）</p>
<p>与基于用户的邻域推荐算法类似，也同样可以对商品的评价引入基于平均值的标准化和Z分值标准化，并对公式中的<img src="Image01223.gif" alt> 进行调整。</p>
<p>将<img src="Image01224.gif" alt> 记为已知的关于商品 _i_ 的评价的平均值：</p>
<p><img src="Image01225.gif" alt></p>
<p>（7-59）</p>
<p>使用基于商品平均值的标准化之后，评分<img src="Image01030.gif" alt> 可以表示为：</p>
<p><img src="Image01226.gif" alt></p>
<p>（7-60）</p>
<p>因此，在基于商品的邻域推荐算法中，<img src="Image01030.gif" alt> 的预测值可以表示为：</p>
<p><img src="Image01227.gif" alt></p>
<p>（7-61）</p>
<p>在基于商品的邻域推荐算法中，使用Z分值对<img src="Image01030.gif" alt> 进行标准化之后为：</p>
<p><img src="Image01228.gif" alt></p>
<p>（7-62）</p>
<p>这里<img src="Image00347.gif" alt> 是商品 _i_ 所收到的评分的标准差。在采用Z分值之后，预测<img src="Image01033.gif" alt> 的公式为：</p>
<p><img src="Image01229.gif" alt></p>
<p>（7-63）</p>
<p><strong>例7-2</strong> 类似地，我们也可以用基于商品的邻域推荐算法来估计表7-1中用户U2对电影M2的评分。对于电影M2，假设与其最相似的两部电影M1和M3，并假设其相似度为<img src="Image01230.gif" alt> 、<img src="Image01231.gif" alt> 。当 _k_ =2时，利用式（7-58）估计用户U2对电影M2的评分为：</p>
<p><img src="Image01232.gif" alt></p>
<h3 id="7-4-3-混合算法"><a href="#7-4-3-混合算法" class="headerlink" title="7.4.3 混合算法"></a>7.4.3 混合算法</h3><p>我们也可以将基于矩阵分解的模型和基于邻域的推荐算法结合起来。一种比较简单的方法：先使用矩阵分解算法得到用户和商品的低维表示<img src="Image01233.gif" alt> 和<img src="Image01086.gif" alt> ，然后利用<img src="Image01085.gif" alt> 和<img src="Image01086.gif" alt> 计算相似度和邻域。</p>
<h3 id="7-4-4-相似度的计算"><a href="#7-4-4-相似度的计算" class="headerlink" title="7.4.4 相似度的计算"></a>7.4.4 相似度的计算</h3><p>在构建基于邻域的推荐算法时，一个核心问题是如何计算相似度<br>（similarity）。这里的相似度既包括用户与用户之间的相似度，也包括商品与商品之间的相似度。相似度的计算不但决定了邻域如何确定，也决定了邻域中每个样本的权重。因此，相似度的选择是基于邻域的推荐算法的核心部分。在本节，我们介绍几种常用的相似度度量，包括余弦相似度 、Jaccard相似度 、Pearson相关系数 和Spearman秩相关系数 。在下面的讨论中，我们以用户-用户相似度为例进行讨论。商品-商品相似度可以通过将适用于用户-用户相似度的公式进行简单的修改得到。</p>
<h4 id="1．余弦相似度"><a href="#1．余弦相似度" class="headerlink" title="1．余弦相似度"></a>1．余弦相似度</h4><p>余弦相似度 是常用的关于两个向量之间相似度的度量。其具体的定义如下：</p>
<p><img src="Image01234.gif" alt></p>
<p>（7-64）</p>
<p>这里向量<img src="Image01235.gif" alt> 。实质上<img src="Image01236.gif" alt> 是向量<img src="Image01237.gif" alt> 、<img src="Image01238.gif" alt> 在向量空间的夹角的余弦值。</p>
<p>在推荐问题中，可以利用以往的历史评价记录来为用户和商品构造向量。首先考虑如何计算用户的相似度。计算商品的相似度可以使用类似的方法得到。对于用户 _u_ ，可以构建一个维度为 _n_ （商品总数目）的向量 <strong>_x_ </strong> u 如下：</p>
<p><img src="Image01239.jpg" alt></p>
<p>（7-65）</p>
<p>这样，我们就可以使用余弦相似度来计算用户 _u_ 和用户 _v_ 之间的相似度<img src="Image01240.gif" alt> ：</p>
<p><img src="Image01241.gif" alt></p>
<p>（7-66）</p>
<p>在上面的公式中，我们在计算余弦相似度时考虑了所有的商品。另一种计算余弦相似度的方法是在分母中只考虑用户 _u_ 和用户 _v_ 共同评价过的商品，对应的余弦相似度的定义为：</p>
<p><img src="Image01242.gif" alt></p>
<p>（7-67）</p>
<h4 id="2．Jaccard相似度"><a href="#2．Jaccard相似度" class="headerlink" title="2．Jaccard相似度"></a>2．Jaccard相似度</h4><p>假设我们有两个集合 _S_ 和 _T_ ，则它们的Jaccard相似度 定义为：</p>
<p><img src="Image01243.gif" alt></p>
<p>（7-68）</p>
<p>换言之，就是它们的交集的大小和它们并集大小的比例。</p>
<p>在使用Jaccard相似度来计算用户 _u_ 和用户 _v_ 之间的相似度时，可以直接计算用户 _u_ 评价过的商品集<img src="Image01244.gif" alt> 和用户 _v_ 评价过的商品集<img src="Image01245.gif" alt> 之间的Jaccard相似度作为两者之间的相似度：</p>
<p><img src="Image01246.gif" alt></p>
<p>（7-69）</p>
<p>类似地，商品 _i_ 和商品 _j_ 之间的Jaccard相似度为：</p>
<p><img src="Image01247.gif" alt></p>
<p>（7-70）</p>
<p>使用Jaccard相似度时，因为我们没有考虑不同的评分，所以损失了一些信息。但是，如果在推荐系统中<img src="Image01030.gif" alt> 是0或者1的形式，如我们只有用户购买商品与否的信息，那么Jaccard相似度是一个较好的选择。</p>
<h4 id="3．Pearson相关系数"><a href="#3．Pearson相关系数" class="headerlink" title="3．Pearson相关系数"></a>3．Pearson相关系数</h4><p>Pearson相关系数 （Pearson correlation coefficient）是一种在推荐算法中广泛使用的相似度度量。在计算<img src="Image01240.gif" alt> 时，我们没有考虑每个用户的评价的平均值和方差的影响。因此，我们可以使用Pearson相关系数来度量两个用户之间的相似度：</p>
<p><img src="Image01248.gif" alt></p>
<p>（7-71）</p>
<p>当我们考虑用户 _u_ 和用户 _v_ 之间的Pearson相关系数时，只考虑<img src="Image01249.gif" alt> 中的商品，即他们都评价过的商品，而不是所有商品。因此，在分母中，我们也只考虑<img src="Image01041.gif" alt> 中的商品对应的评分值所对应的方差。注意，根据该公式，所得的Pearson相关系数并不等同于将原始的评价值利用平均值进行标准化再计算余弦相似度。</p>
<p>注意，在这里计算<img src="Image01202.gif" alt> 和<img src="Image01250.gif" alt> 时有两种选择。我们考虑了用户 _u_ 和用户 _v_ 各自所有已知的评价情况，并使用前面讨论过的公式</p>
<p><img src="Image01251.gif" alt></p>
<p>（7-72）</p>
<p>来计算。这种方法的好处是对于每个用户来说，只需要计算一次均值即可。</p>
<p>严格地讲，这里我们是根据用户 _u_ 和用户 _v_ 都评价过的商品中的评价信息来计算 _u_ 和 _v_ 之间的相似度。因此，在一些文献中也提出严格按照Pearson相关系数的计算方法，只用<img src="Image01249.gif" alt> 中的商品来计算<img src="Image01202.gif" alt> 和<img src="Image01250.gif" alt> ：</p>
<p><img src="Image01252.gif" alt></p>
<p>（7-73）</p>
<p>在实际的比较中，没有明确的证据证明其中一种方法优于另一种方法。若采用后一种计算方法，在计算每对用户 _u_ 和 _v_ 时，都要重新计算<img src="Image01206.gif" alt> 和<img src="Image01250.gif" alt> 。因此，在实际中，出于降低计算复杂度的考虑，通常采用前一种方法，使用用户 _u_ 的所有评价信息来计算其评价的平均值<img src="Image01202.gif" alt> 。</p>
<p>类似地，也可以使用Pearson相关系数来计算商品之间的相似度：</p>
<p><img src="Image01253.gif" alt></p>
<p>（7-74）</p>
<p>在式（7-74）中，我们使用<img src="Image01254.gif" alt> 作为均值来修正<img src="Image01033.gif" alt> 。在实际中，人们发现使用用户的均值<img src="Image01202.gif" alt> 来修正<img src="Image01033.gif" alt> 远比使用<img src="Image01224.gif" alt> 来修正<img src="Image01033.gif" alt> 对于推荐更有效。主要原因在于不同的用户评分习惯差异较大，使得使用用户的均值<img src="Image01202.gif" alt> 来修正<img src="Image01033.gif" alt> 更有效。因此，在实际中，人们又提出了修正的余弦相似度 （adjusted cosine similarity）。与前面<img src="Image01255.gif" alt> 的区别在于将<img src="Image01224.gif" alt> 替换为<img src="Image01202.gif" alt> ：</p>
<p><img src="Image01256.gif" alt></p>
<p>（7-75）</p>
<p>在实际中计算商品间相似度时，使用<img src="Image01257.gif" alt> 通常比<img src="Image01258.gif" alt> 更有效。</p>
<h4 id="4．Spearman秩相关系数"><a href="#4．Spearman秩相关系数" class="headerlink" title="4．Spearman秩相关系数"></a>4．Spearman秩相关系数</h4><p>在计算Pearson相关系数时，我们直接使用了评价值<img src="Image01033.gif" alt> 并利用平均值进行修正。在Spearman秩相关系数 中，我们先将评价值<img src="Image01033.gif" alt> 转化为它对应的排序。举一个简单的例子，如果对用户 _u_ ，如果他/她给3件商品的评分分别为4、3、5，那么评分对应的排序为2、1、3（从小到大的排列）。我们首先将用户 _u_ 所有的评价<img src="Image01033.gif" alt> 转化为对应的排序，并记为<img src="Image01259.gif" alt> ，并将所有<img src="Image01259.gif" alt> 的平均值记为<img src="Image01260.gif" alt> 。之后，我们可以使用下面的式（7-76）来计算Spearman秩相关系数（Spearman rank correlation，SRC）：</p>
<p><img src="Image01261.gif" alt></p>
<p>（7-76）</p>
<p>注意，在计算中我们也只考虑了<img src="Image01041.gif" alt> 中的商品。在使用Spearman秩相关系数时，我们不需要考虑评分值的标准化问题，因为我们已经预先将其转化为排序了。但是，当评价值只取有限的几个值时（例如，只能取1或者2），在将<img src="Image01033.gif" alt> 转化为<img src="Image01259.gif" alt> 的过程中，很多评价值都会取相同的排序值<img src="Image01259.gif" alt> ，此时计算Spearman秩相关系数的意义不大。</p>
<h4 id="5．相似度的显著性"><a href="#5．相似度的显著性" class="headerlink" title="5．相似度的显著性"></a>5．相似度的显著性</h4><p>在计算相似度时，假设我们有5个商品，并假设用户如果购买过该商品，则<img src="Image01031.gif" alt> ，否则<img src="Image01032.gif" alt> 。考虑如下4个用户：</p>
<p><img src="Image01262.gif" alt></p>
<p>和</p>
<p><img src="Image01263.gif" alt></p>
<p>假设都采用余弦相似度，则有：</p>
<p><img src="Image01264.gif" alt></p>
<p>虽然这两对用户的相似度都为1.0，但是直觉告诉我们<img src="Image00351.gif" alt> 与<img src="Image00352.gif" alt> 应该更相似，因为我们有更多的证据证明他们购买了更多相同的商品。</p>
<p>因此，在计算相似度的时候，我们要考虑相似度的显著性<br>（significance），就是相似度计算中证据的强弱。其基本原理：如果相似度的计算只是基于较少的评价值，则我们要降低相似度的绝对值。在前面所讨论的多种计算用户 _u_ 和用户 _v_ 之间相似度的方法中，基本上都依赖于用户共同评价过的商品的数目，即<img src="Image01265.gif" alt> 。一种简单的方法是根据<img src="Image01265.gif" alt> 的大小对相似度进行修正：如果<img src="Image01265.gif" alt> 大于或等于某一阈值 _γ_ ，对相似度不修正；如果<img src="Image01266.gif" alt> 小于阈值 _γ_ ，要对算出的相似度进行“惩罚”，通常是降低其相似度的值。这种方法称为显著性权值 （significance weighting）。在计算用户之间的相似度时，通常采用的修正公式为：</p>
<p><img src="Image01267.gif" alt></p>
<p>（7-77）</p>
<p>这里 _γ_ &gt;0，<img src="Image01268.gif" alt> 是原始的相似度。这里，如果相似度<img src="Image01268.gif" alt> 的计算所基于的共同评价个数<img src="Image01265.gif" alt> 小于 _γ_ ，则要乘以系数<img src="Image01269.gif" alt> 。</p>
<p>类似地，计算商品间相似度时，可以用下面的式（7-78）来修正：</p>
<p><img src="Image01270.gif" alt></p>
<p>（7-78）</p>
<p>在实际中，一般要使用交叉检验来得到最优的 _γ_ 值。</p>
<h4 id="6．相似度计算的实例"><a href="#6．相似度计算的实例" class="headerlink" title="6．相似度计算的实例"></a>6．相似度计算的实例</h4><p>这里我们用一个实例来说明如何计算上面讨论的那些相似度。</p>
<p><strong>例7-3</strong> 在这个简化的例子中，假设有6件商品，用户U1和用户U2对这6件商品的评分见表7-2。</p>
<p>表7-2 用于相似度计算的实例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>-</th>
<th>商品I1</th>
<th>商品I2</th>
<th>商品I3</th>
<th>商品I4</th>
<th>商品I5</th>
<th>商品I6  </th>
</tr>
</thead>
<tbody>
<tr>
<td>用户U1</td>
<td></td>
<td>2</td>
<td>5</td>
<td>4</td>
<td></td>
<td>3  </td>
</tr>
<tr>
<td>用户U2</td>
<td>5</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>在这个例子中，用户U1对商品2、3、4和6进行了评价；用户U2对商品1、2、3和4进行了评价。</p>
<p>（1）余弦相似度。构建用户U1和用户U2对应的向量如下：</p>
<p><img src="Image01271.gif" alt></p>
<p>则他们之间的余弦相似度为：</p>
<p><img src="Image01272.gif" alt></p>
<p>（2）Jaccard相似度。根据表7-2，有：</p>
<p><img src="Image01273.gif" alt></p>
<p>因此，有：</p>
<p><img src="Image01274.gif" alt></p>
<p>直接使用Jaccard相似度的计算公式，有：</p>
<p><img src="Image01275.gif" alt></p>
<p>（3）Pearson相关系数。首先计算用户U1和U2评分的平均值：</p>
<p><img src="Image01276.gif" alt></p>
<p>在计算Pearson相关系数时，我们只考虑用户U1和用户U2都评价过的商品。因此，这里只需要考虑商品I2、I3和I4。带入前面的计算公式中，有：</p>
<p><img src="Image01277.gif" alt></p>
<p>（4）Spearman秩相关系数。首先我们将原始的评分转化为排序。注意，用户U2的评分排好之后为3、4、4、5，对应的排序位置为1、2.5、2.5、4。由于有两个相同的4分，因此我们赋予2.5作为新的评分。转化后的新评分见表7-3，则对于用户U1和用户U2，其对应的新的评分的平均值分别为<img src="Image01278.gif" alt> 和<img src="Image01279.gif" alt> 。最后的Spearman秩相关系数为：</p>
<p><img src="Image01280.gif" alt></p>
<p>表7-3 转化为排序的新评分</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>-</th>
<th>商品I1</th>
<th>商品I2</th>
<th>商品I3</th>
<th>商品I4</th>
<th>商品I5</th>
<th>商品I6  </th>
</tr>
</thead>
<tbody>
<tr>
<td>用户U1</td>
<td></td>
<td>1</td>
<td>4</td>
<td>3</td>
<td></td>
<td>2  </td>
</tr>
<tr>
<td>用户U2</td>
<td>4</td>
<td>2.5</td>
<td>2.5</td>
<td>1</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>从这个简单的例子可以看出，采用不同的相似度计算方法，就会取得不同的用户-用户相似度值，从而影响后面的用户-商品评分预测。因此，在实际中，我们需要根据数据的具体特征，选取最合适的计算方法。</p>
<h2 id="7-5-R中recommenderlab的实际使用"><a href="#7-5-R中recommenderlab的实际使用" class="headerlink" title="7.5 R中recommenderlab的实际使用"></a>7.5 R中recommenderlab的实际使用</h2><p>目前在R中关于推荐算法的软件包还较少，在本节我们介绍其中比较流行的<code>recommenderlab</code> 软件包 ④ 。该软件包实现了我们前面讨论的几类基本算法：</p>
<ul>
<li>基于奇异值分解的推荐算法；</li>
<li>基于用户的邻域推荐算法；</li>
<li>基于商品的邻域推荐算法。</li>
</ul>
<p>此外，该软件包还实现了若干简单的推荐算法，如为每个用户推荐最流行的商品等。使用该软件包，用户可以直接调用相关的算法，为每个用户推荐他/她最喜欢的商品或者直接预测特定用户-商品的评价值。</p>
<p>该软件包的缺点是实现的算法有限，同时不能处理大数据。例如，我们在前面介绍的很多复杂的矩阵分解算法都没有提供。我们在这里介绍<code>recommenderlab</code> 包的主要目的是为了让用户在R中直接使用前面介绍的一些基本推荐算法，获得使用推荐算法的第一手经验。</p>
<p>下面我们首先讨论如何在<code>recommenderlab</code> 包中保存和操作评价矩阵；之后介绍如何构建和使用推荐模型；在此基础上，我们讨论该包中的<code>evaluationScheme()</code> 函数和<code>calcPredictionAccuracy()</code> 函数，介绍如何划分评价矩阵和如何比较多个不同的推荐模型。更完全的关于该软件包的介绍可以参见网络上的相关资料 ⑤ 。</p>
<p>在<code>recommenderlab</code> 包中，评价矩阵是用一个抽象的<code>ratingMatrix</code> 类来表示的。该抽象类定义了很多矩阵方面操作的方法。在实际中，我们使用它的两个具体的例子来保存评价矩阵：</p>
<ul>
<li><code>binaryRatingMatrix；</code></li>
<li><code>realRatingMatrix。</code></li>
</ul>
<p>在<code>binaryRatingMatrix</code> 中，我们只能保存布尔型的评价值，而<code>realRatingMatrix</code> 则保存实数型的评价值。在实现上，它们都利用了R中相应的稀疏矩阵来保存和操作评价矩阵。</p>
<p>在使用<code>recommenderlab</code> 包中的<code>binaryRatingMatrix</code> 和<code>realRatingMatrix</code> 时，我们可以使用如下的方法来探索评价矩阵。</p>
<ul>
<li><code>dim()</code> ：返回评价矩阵的行数和列数。</li>
<li><code>nrow()和ncol()</code> ：分别返回评价矩阵的行数和列数（即用户数目和商品数目）。</li>
<li><code>nratings()</code> ：返回评价矩阵中所有已知评价值的总数。</li>
<li><code>dimnames()</code> ：返回评价矩阵中每行对应的用户名列表和每列对应的商品名列表。</li>
<li><code>rowCounts()</code> 和<code>colCounts()</code> ：分别计算每行和每列中已知的评价值的数目。</li>
<li><code>rowMeans()</code> 和<code>colMeans()</code> ：分别计算每行和每列中已知评价值的平均值。</li>
<li><code>rowSums()</code> 和<code>colSums()</code> ：分别计算每行和每列中已知评价值的和。</li>
</ul>
<p>在后面的示例代码中，我们将利用具体的例子讲解如何使用这些函数来探索评价矩阵。</p>
<p>对于一个评价矩阵，可以使用<code>as()</code> 函数将<code>binaryRatingMatrix</code> 和<code>realRatingMatrix</code> 转化为列表、矩阵和数据框。假设<code>RM</code> 是一个<code>recommenderlab</code> 中的评价矩阵，则可以使用如下代码来将其进行类型转换：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RM_list &lt;-as(RM, &apos;list&apos;)</span><br><span class="line">RM_matrix &lt;-as(RM, &apos;matrix&apos;)</span><br><span class="line">RM_df &lt;-as(RM, &apos;data.frame&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>对于评价矩阵，可以利用<code>recommenderlab</code> 中的<code>normalize()</code> 来进行标准化处理。在使用<code>normalize()</code> 函数时，我们主要有两个重要参数，即<code>method</code> 和<code>row</code> ，下面是两种不同的用法：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">`normalize(RM, method=&apos;center&apos;, row=T)`</span><br><span class="line">`normalize(RM, method=&apos;Z-score&apos;, row=F)`</span><br></pre></td></tr></table></figure>

</details>


<p>这里<code>RM</code> 是一个评价矩阵，参数<code>method</code> 表示不同的预处理方法，其中<code>&#39;center&#39;</code> 表示对每个用户或者商品的评价值进行预处理使得其平均值是0，而<code>&#39;Z-score&#39;</code> 表示标准化处理后每个用户或者商品的评价值的平均值是0且标准差是1；参数<code>row</code> 的取值为<code>T</code> 或者<code>F</code> ，分别表示对每个用户（即每行）或者每个商品（即每列）分别做预处理。</p>
<p>我们使用<code>recommenderlab</code> 的核心任务是构建一个推荐模型，以及利用推荐模型来预测推荐结果。利用提供的<code>Recommender()</code> 函数，可以训练不同的推荐模型，其用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Recommender(RM, method, parameter = NULL)</span><br></pre></td></tr></table></figure>
<p>这里的<code>RM</code> 是一个用来训练模型的评价矩阵，可以使用参数<code>method</code> 和<code>parameter</code> 来进一步指定所要构建的模型。这里<code>method</code> 用来指定模型的类型，常用的模型类型有以下几个。</p>
<ul>
<li><code>RANDOM</code> ：随机推荐模型。</li>
<li><code>POPULAR</code> ：利用训练数据得到最流行的 _k_ 个商品，并将它们推荐给所有用户。</li>
<li><code>IBCF</code> ：基于商品的协同过滤 （item-based collaborative filtering），即基于商品的邻域推荐算法。</li>
<li><code>UBCF</code> ：基于用户的协同过滤 （user-based collaborative filtering），即基于用户的邻域推荐算法。</li>
<li><code>SVD</code> 和<code>SVDF</code> ：基于矩阵分解的推荐模型。</li>
</ul>
<p>这里参数<code>parameter</code> 用来指定每个模型的具体控制参数。</p>
<p>在使用<code>Recommender()</code> 函数构建模型时，我们需要知道当前所有可以使用的参数选择。我们可以使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recommenderRegistry$get_entries()</span><br></pre></td></tr></table></figure>
<p>得到<code>recommenderlab</code> 包当前提供的所有可以使用的参数设置。进一步，我们可以使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recommenderRegistry$get_entries(dataType = &quot;realRatingMatrix&quot;)</span><br></pre></td></tr></table></figure>
<p>得到所有适用于<code>realRatingMatrix</code> 的设置；可以使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recommenderRegistry$get_entries(dataType = &quot;binaryRatingMatrix&quot;)</span><br></pre></td></tr></table></figure>
<p>得到所有适用于<code>binaryRatingMatrix</code> 的设置。这里我们首先列出对于<code>realRatingMatrix</code> 适用的所有参数设置：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$IBCF_realRatingMatrix</span><br><span class="line">Recommender method: IBCF</span><br><span class="line">Description: Recommender based on item-based collaborative filtering (real data).</span><br><span class="line">Parameters:</span><br><span class="line">　 k method normalize normalize_sim_matrix alpha na_as_zero</span><br><span class="line">1 30 Cosine　　center　　　　　　　　FALSE　 0.5　　　FALSE</span><br><span class="line"></span><br><span class="line">$POPULAR_realRatingMatrix</span><br><span class="line">Recommender method: POPULAR</span><br><span class="line">Description: Recommender based on item popularity (real data).</span><br><span class="line">Parameters:</span><br><span class="line">　normalize aggregationRatings aggregationPopularity</span><br><span class="line">1　　center　　　　 &lt;function&gt;　　　　　　&lt;function&gt;</span><br><span class="line"></span><br><span class="line">$RANDOM_realRatingMatrix</span><br><span class="line">Recommender method: RANDOM</span><br><span class="line">Description: Produce random recommendations (real ratings).</span><br><span class="line">Parameters: None</span><br><span class="line"></span><br><span class="line">$RERECOMMEND_realRatingMatrix</span><br><span class="line">Recommender method: RERECOMMEND</span><br><span class="line">Description: Re-recommends highly rated items (real ratings).</span><br><span class="line">Parameters:</span><br><span class="line">　randomize minRating</span><br><span class="line">1　　　　 1　　　　NA</span><br><span class="line"></span><br><span class="line">$SVD_realRatingMatrix</span><br><span class="line">Recommender method: SVD</span><br><span class="line">Description: Recommender based on SVD approximation with column-mean imputation (real data).</span><br><span class="line">Parameters:</span><br><span class="line">　 k maxiter normalize</span><br><span class="line">1 10　　 100　　center</span><br><span class="line"></span><br><span class="line">$SVDF_realRatingMatrix</span><br><span class="line">Recommender method: SVDF</span><br><span class="line">Description: Recommender based on Funk SVD with gradient descend (real data).</span><br><span class="line">Parameters:</span><br><span class="line">　 k gamma lambda min_epochs max_epochs min_improvement normalize</span><br><span class="line">1 10 0.015　0.001　　　　 50　　　　200　　　　　 1e-06　　center</span><br><span class="line">　verbose</span><br><span class="line">1　 FALSE</span><br><span class="line"></span><br><span class="line">$UBCF_realRatingMatrix</span><br><span class="line">Recommender method: UBCF</span><br><span class="line">Description: Recommender based on user-based collaborative filtering (real data).</span><br><span class="line">Parameters:</span><br><span class="line">　method nn sample normalize</span><br><span class="line">1 cosine 25　FALSE　　center</span><br></pre></td></tr></table></figure>

</details>


<p>这里我们简单解释一下上面列出的参数设置情况。以基于用户的邻域算法为例，其参数设置对应于上面的<code>$UBCF_realRatingMatrix</code> 。在使用时，有4个参数可以设置：</p>
<ul>
<li><code>method</code></li>
<li><code>nn</code></li>
<li><code>sample</code></li>
<li><code>normalize</code></li>
</ul>
<p>这里参数<code>method</code> 是计算相似度的方法，对于<code>realRatingMatrix</code> ，在<code>recommenderlab</code> 包中提供了<code>&#39;pearson&#39;</code> 、<code>&#39;cosine&#39;</code> 和<code>&#39;jaccard&#39;</code> 。参数<code>nn</code> 表示邻域大小。参数<code>sample</code> 值是<code>T</code> 或者<code>F</code> ，表示是否对训练集进行取样来训练推荐模型。参数<code>normalize</code> 表示对评价值进行预处理的方法，常用的有<code>&#39;center&#39;</code> 和<code>&#39;Z-score&#39;</code> ，其含义与我们前面讨论<code>normalize()</code> 函数时相同。</p>
<p>对于<code>binaryRatingMatrix</code> ，所有适用的参数设置包括：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$AR_binaryRatingMatrix</span><br><span class="line">Recommender method: AR</span><br><span class="line">Description: Recommender based on association rules.</span><br><span class="line">Parameters:</span><br><span class="line">　support confidence maxlen　　measure verbose decreasing</span><br><span class="line">1　　 0.1　　　　0.3　　　2 confidence　 FALSE　　　 TRUE</span><br><span class="line"></span><br><span class="line">$IBCF_binaryRatingMatrix</span><br><span class="line">Recommender method: IBCF</span><br><span class="line">Description: Recommender based on item-based collaborative filtering (binary   </span><br><span class="line">rating data).</span><br><span class="line">Parameters:</span><br><span class="line">　 k　method normalize_sim_matrix alpha</span><br><span class="line">1 30 Jaccard　　　　　　　　FALSE　 0.5</span><br><span class="line"></span><br><span class="line">$POPULAR_binaryRatingMatrix</span><br><span class="line">Recommender method: POPULAR</span><br><span class="line">Description: Recommender based on item popularity (binary data).</span><br><span class="line">Parameters: None</span><br><span class="line"></span><br><span class="line">$RANDOM_binaryRatingMatrix</span><br><span class="line">Recommender method: RANDOM</span><br><span class="line">Description: Produce random recommendations (binary ratings).</span><br><span class="line">Parameters: None</span><br><span class="line"></span><br><span class="line">$UBCF_binaryRatingMatrix</span><br><span class="line">Recommender method: UBCF</span><br><span class="line">Description: Recommender based on user-based collaborative filtering (binary data).</span><br><span class="line">Parameters:</span><br><span class="line">　 method nn weighted sample</span><br><span class="line">1 jaccard 25　　 TRUE　FALSE</span><br></pre></td></tr></table></figure>

</details>


<p>同样地，我们仍以基于用户的邻域算法为例说明对于<code>binaryRatingMatrix</code> 的参数设置。其参数设置对应于上面的<code>$UBCF_binaryRatingMatrix</code> 。在使用时，有4个参数可以设置：</p>
<ul>
<li><code>method</code></li>
<li><code>nn</code></li>
<li><code>weighted</code></li>
<li><code>sample</code></li>
</ul>
<p>这里参数<code>method</code> 是计算相似度的方法；参数<code>nn</code> 表示邻域大小；参数<code>sample</code> 值是<code>T</code> 或者<code>F</code> ，表示是否对训练集进行取样来训练推荐模型。与前面对于<code>realRatingMatrix</code> 的参数设置不同的是，这里的参数<code>weighted</code> 表示在计算预测值时是否考虑其邻域中不同用户的权值。</p>
<p>在构建推荐模型后，可以使用<code>predict()</code> 函数为用户进行推荐。使用<code>predict()</code> 有两种模式：</p>
<ul>
<li>为每个用户推荐topN个最喜欢的商品；</li>
<li>为指定的用户-商品对预测评价值。</li>
</ul>
<p>该函数的用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(M, newdata, n=10, type=&quot;topNList&quot;,...).</span><br></pre></td></tr></table></figure>
<p>这里<code>M</code> 是前面使用<code>Recommender()</code> 函数返回的对象（即已经构建的推荐模型），<code>newdata</code> 则指定了新的用户及其历史评价数据。这里<code>newdata</code> 也可以是训练<code>M</code> 时所用的评价矩阵所对应的用户的一部分，此时我们可直接指定<code>newdata</code> 为训练用的评价矩阵中用户对应的索引。参数<code>type</code> 可以设置为<code>&quot;topNList&quot;</code> 、<code>&quot;ratings&quot;</code> 、<code>&quot;ratingMatrix&quot;</code> 之一，用来指定不同的预测任务。其中<code>&quot;topNList&quot;</code> 表示返回<code>newdata</code> 中每个用户最喜好的前 _n_ （对应参数<code>n</code><br>）个商品；当<code>type</code> 为<code>&quot;ratings&quot;</code> 或 <code>&quot;ratingMatrix&quot;</code> 时，参数<code>n</code> 被忽略，<code>predict()</code> 函数预测用户-商品的评价值，并返回一个<code>realRatingMatrix</code> 对象，其中每行对应一个用户。这里<code>&quot;ratings&quot;</code> 和<code>&quot;ratingMatrix&quot;</code> 的区别是使用<code>ratingMatrix</code> 时我们将返回的<code>realRatingMatrix</code> 中所有<code>newdata</code> 中的已知用户-商品评价对的评价值直接设为<code>newdata</code> 中的真实评价值。</p>
<p>与前面的分类或者回归问题类似，需要对所得模型在测试集上的效果进行评价。在<code>recommenderlab</code> 包中，我们利用所提供的<code>evaluationScheme()</code> 和<code>calcPredictionAccuracy()</code> 函数来比较多个推荐模型的性能。利用<code>evaluationScheme()</code> 函数，可将数据分为3部分：</p>
<ul>
<li>训练数据；</li>
<li>测试数据中的已知数据；</li>
<li>测试数据中的未知数据。</li>
</ul>
<p>在划分数据时，我们将用户分为两部分：训练集中的用户和测试集中的用户。对于测试集中的用户对应的评价值，进一步分为两部分：已知数据和未知数据。其中训练数据是用来训练推荐模型的。而为测试集中的用户推荐商品时，我们利用已经训练好的模型和测试数据中的已知数据来进行推荐，并将推荐结果与未知部分进行比较从而知道所得模型的好坏。在下面的表7-4～表7-7中，我们使用一个具体的例子来说明如何使用<code>evaluaitonScheme()</code> 来划分评价矩阵。在这个例子中，我们有4个用户和4件商品，将用户User1和User2划入训练数据，将User3和User4划入测试数据。表7-4是原始的评价矩阵，表7-5是划分所得的训练数据，表7-6是测试数据中的已知部分，表7-7是测试数据的未知部分。在这个例子中，在测试数据的未知部分中，每个用户只有一件商品。</p>
<p>表7-4 原始的评价矩阵</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Rating matrix</th>
<th>Item1</th>
<th>Item2</th>
<th>Item3</th>
<th>Item4  </th>
</tr>
</thead>
<tbody>
<tr>
<td>User1</td>
<td>4</td>
<td>5</td>
<td></td>
<td>1  </td>
</tr>
<tr>
<td>User2</td>
<td>3</td>
<td>4</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>User3</td>
<td>1</td>
<td></td>
<td>3</td>
<td>5  </td>
</tr>
<tr>
<td>User4</td>
<td>4</td>
<td>5</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>表7-5 训练数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Rating matrix</th>
<th>Item1</th>
<th>Item2</th>
<th>Item3</th>
<th>Item4  </th>
</tr>
</thead>
<tbody>
<tr>
<td>User1</td>
<td>4</td>
<td>5</td>
<td></td>
<td>1  </td>
</tr>
<tr>
<td>User2</td>
<td>3</td>
<td>4</td>
<td>1</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>表7-6 测试数据中的已知部分</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Rating matrix</th>
<th>Item1</th>
<th>Item2</th>
<th>Item3</th>
<th>Item4  </th>
</tr>
</thead>
<tbody>
<tr>
<td> User3</td>
<td>1</td>
<td></td>
<td></td>
<td>5  </td>
</tr>
<tr>
<td> User4</td>
<td></td>
<td>5</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>   表7-7 测试数据中的未知部分</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Rating matrix</th>
<th>Item1</th>
<th>Item2</th>
<th>Item3</th>
<th>Item4  </th>
</tr>
</thead>
<tbody>
<tr>
<td>User3</td>
<td></td>
<td></td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>User4</td>
<td>4</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>在具体使用<code>evaluationScheme()</code> 函数时，其用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evaluationScheme(data, method=&quot;split&quot;, train=0.9, k=10, given=3)</span><br></pre></td></tr></table></figure>
<p>首先我们用<code>data</code> 来指定评价矩阵，然后用参数<code>method</code> 来指定不同的划分方法，<code>recommenderlab</code> 中提供的选择包括以下几个。</p>
<ul>
<li><code>&#39;split&#39;</code> ：简单的划分。</li>
<li><code>&#39;bootstrap&#39;</code> ：bootstrap取样。</li>
<li><code>&#39;cross-validation&#39;</code> ：交叉检验。</li>
</ul>
<p>参数<code>train</code> 表示其中训练集的比例，参数<code>k</code> 表示交叉检验中的重数，参数<code>given</code> 表示在测试集中已知数据部分对于每个用户包含多少个评价值。注意，这里<code>given</code> 可以取负值，例如，<code>given=-10</code> 表示对于每个用户，我们对测试集中的用户选择10个评价值作为测试数据中的未知数据。在表7-4至表7-7给出的例子中我们将<code>given</code> 设为<code>-1</code> 。</p>
<p>在得到<code>evaluationScheme()</code> 函数生成的结果后，可以使用<code>getData()</code> 函数得到所划分的3组数据。下面我们使用<code>recommenderlab</code> 包自带的<code>MovieLense</code> 数据来说明：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ML_e &lt;-evaluationScheme(MovieLense,</span><br><span class="line">　　　　　　　　　　　　 method=&apos;split&apos;,</span><br><span class="line">　　　　　　　　　　　　 train=0.9,</span><br><span class="line">　　　　　　　　　　　　 given = -10,</span><br><span class="line">　　　　　　　　　　　　 goodRating=5)</span><br><span class="line">ML_train &lt;-getData(ML_e, &apos;train&apos;)</span><br><span class="line">ML_test_known &lt;-getData(ML_e, &apos;known&apos;)</span><br><span class="line">ML_test_unknown &lt;-getData(ML_e, &apos;unknown&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>在这个例子中，首先使用<code>evaluationScheme()</code> 函数生成一个划分<code>ML_e</code> ，然后使用<code>getData()</code> 函数和不同的参数（<code>&#39;train&#39;</code> 、<code>&#39;known&#39;</code> 、<code>&#39;unknown&#39;</code><br>）得到所划分的3个数据子集。注意，这里我们还设置了参数<code>goodRating</code> 。在<code>MovieLense</code> 数据集中，评价值的取值为1～5。这里，<code>goodRating=5（</code> 表示评价值为5）或者以上的部分我们认为是正类，其他则视为负类，这些设置在使用<code>calcPredictionAccuracy()</code> 函数评估模型性能时会用到。</p>
<p>对于推荐模型的评价，可以使用<code>recommenderlab</code> 包中提供的<code>calcPredictionAccuracy()</code> 函数来计算。对于评价值的预测（即调用<code>predict()</code> 函数时将<code>type</code> 设为<code>&#39;ratings&#39;</code> 或者<code>&#39;ratingMatrix&#39;</code><br>），使用<code>calcPredictionAccuracy()</code> 函数可以得到：</p>
<ul>
<li>平均误差（mean average error，MAE）；</li>
<li>均方差（mean squared error，MSE）；</li>
<li>均方根误差（root mean squared error，RMSE）。</li>
</ul>
<p>当预测结果是每个用户对应的topN件商品时，使用<code>calcPredictionAccuracy()</code> 函数可以得到TP、FP、FN、TN、TPR、FPR，以及召回率和精确率。这里我们需要有“正类”和“负类”的概念，这也是对<code>realRatingMatrix</code> 类型的评价矩阵使用<code>evalutionScheme()</code> 函数时需要指定<code>goodRating</code> 的原因。在使用<code>calcPredictionAccuracy()</code> 函数时，第一个输入参数对应模型的预测结果，第二个输入参数对应真实数据。</p>
<p>下面我们用具体的示例代码来说明使用<code>recommenderlab</code> 包来探索数据、建立模型，以及评价、比较模型的全过程。完全的R代码在文件<code>recommenderlab_example.R</code> 中。</p>
<p>在代码中，首先检查<code>recommenderlab</code> 包是否已经安装；如果没有，则使用<code>install.packages()</code> 函数安装该软件包。</p>
<p>接下来载入<code>recommenderlab</code> 包自带的3个数据集：<code>MovieLense</code> 、<code>Jester5k</code> 和<code>MSWeb</code> 。每个数据集对应一个同名的评价矩阵对象。其中<code>MovieLense</code> 和<code>Jester5k</code> 是<code>realRatingMatrix</code> ，而<code>MSWeb</code> 是<code>binaryRatingMatrix</code> 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  data(MovieLense)</span><br><span class="line">data(Jester5k)</span><br><span class="line">data(MSWeb)</span><br></pre></td></tr></table></figure>
<p>然后，探索数据的评价矩阵，代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Visualizing a sample of this</span><br><span class="line">image(sample(MovieLense, 500), main = &quot;Raw ratings&quot;)</span><br><span class="line">image(MSWeb[1:500,], main=&apos;raw ratings&apos;)</span><br><span class="line"></span><br><span class="line">summary(getRatings(MovieLense))</span><br><span class="line"># check the size of MovieLense directly</span><br><span class="line">n_user_num_movie &lt;-dim(MovieLense)[1]</span><br><span class="line">n_item_num_movie &lt;-dim(MovieLense)[2]</span><br><span class="line"># Other functions we can use to explore rating matrix dimensions</span><br><span class="line">user_list_movie &lt;-dimnames(MovieLense)[1]</span><br><span class="line">item_list_movie &lt;-dimnames(MovieLense)[2]</span><br><span class="line"># Get all ratings as a numeric vector</span><br><span class="line">Movie_v &lt;-getRatings(MovieLense)</span><br><span class="line">hist(Movie_v, main=&apos;Histogram of ratings&apos;, xlab=&apos;Rating&apos;)</span><br><span class="line"># Get all ratings as a sparse matrix</span><br><span class="line">Movie_Msp &lt;-getRatingMatrix(MovieLense)</span><br><span class="line"># use the list of functions provided by the package</span><br><span class="line"># we can print out the class information</span><br><span class="line">str(Movie_v)</span><br><span class="line">str(Movie_Msp)</span><br><span class="line"># Next we compute some statistics</span><br><span class="line">Movie_n_ratings &lt;-nratings(MovieLense)</span><br><span class="line">hist(rowCounts(MovieLense), breaks=50, xlab=&quot;Num of users&quot;, ylab=&quot;Num of movies rated&quot;)</span><br><span class="line"># Explore the mean rating for each movie</span><br><span class="line">hist(colMeans(MovieLense), breaks=50, xlab=&quot;Rating&quot;, ylab=&quot;Num of movies&quot;)</span><br></pre></td></tr></table></figure>

</details>


<p>在这一步首先使用<code>image()</code> 函数简单地可视化<code>MovieLense</code> 和<code>MSWeb</code> 两个评价矩阵。这里，使用<code>sample()</code> 函数从<code>MovieLense</code> 中随机选择500个用户对应的评价数据进行可视化。可视化结果分别如图7-2和图7-3所示。从图7-2和图7-3中可以看出，<code>MSWeb</code> 比<code>MovieLense</code> 要稀疏很多。</p>
<p><img src="Image01281.jpg" alt></p>
<p>图7-2 <code>MovieLense</code> 中的评价矩阵</p>
<p>我们使用<code>dim()</code> 函数检查<code>MovieLense</code> 数据集中用户数目和商品数目，使用<code>dimnames()</code> 函数得到了所有的用户名和商品名。这里我们着重使用了如下几个函数来从<code>MovieLense</code> 中得到评价值。</p>
<ul>
<li><code>getRatings()</code> 函数：将评价矩阵中所有已知的评价值以向量的形式返回。</li>
<li><code>getRatingMatrix()</code> 函数：将评价矩阵中所有已知的评价值以矩阵的形式返回，其行数和列数与输入的评价矩阵相同。</li>
<li><code>nratings()</code> 函数：返回评价矩阵中所有已知评价值的数目，即<code>nratings(R)</code> 等价于<code>length(getRatings(R))</code> ，这里<code>R</code> 是输入的评价矩阵。</li>
</ul>
<p>上面一段程序的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(getRatings(MovieLense))</span><br><span class="line">　 Min. 1st Qu.　Median　　Mean 3rd Qu.　　Max. </span><br><span class="line">　 1.00　　3.00　　4.00　　3.53　　4.00　　5.00 </span><br><span class="line">&gt; str(Movie_v)</span><br><span class="line"> num [1:99392] 5 4 4 4 4 3 1 5 4 5 ...</span><br><span class="line">&gt; str(Movie_Msp)</span><br><span class="line">Formal class &apos;dgCMatrix&apos; [package &quot;Matrix&quot;] with 6 slots</span><br><span class="line">　..@ i　　　 : int [1:99392] 0 1 4 5 9 12 14 15 16 17 ...</span><br><span class="line">　..@ p　　　 : int [1:1665] 0 452 583 673 882 968 994 1386 1605 1904 ...</span><br><span class="line">　..@ Dim　　 : int [1:2] 943 1664</span><br><span class="line">　..@ Dimnames:List of 2</span><br><span class="line">　.. ..$ : chr [1:943] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...</span><br><span class="line">　.. ..$ : chr [1:1664] &quot;Toy Story (1995)&quot; &quot;GoldenEye (1995)&quot; &quot;Four Rooms (1995)&quot; &quot;Get Shorty (1995)&quot; ...</span><br><span class="line">　..@ x　　　 : num [1:99392] 5 4 4 4 4 3 1 5 4 5 ...</span><br><span class="line">　..@ factors : list()</span><br></pre></td></tr></table></figure>

</details>


<p><img src="Image01282.jpg" alt></p>
<p>图7-3 <code>MSWeb</code> 数据集中的评价矩阵</p>
<p>在上面的那段程序中，我们还使用<code>hist()</code> 函数来探索评价矩阵的分布。首先使用<code>hist(Movie_v, main=&#39;Histogram of ratings&#39;, xlab=&#39;Rating&#39;)</code> 来得到所有评价值的分布，如图7-4所示。从图7-4中可以看出，评价值为1～5，其中得分为4的评价值最多。此外，我们还探索了每个用户评价过的电影数的分布和每部电影的平均评分值的分布，分别如图7-5和图7-6所示。</p>
<p><img src="Image01283.jpg" alt></p>
<p>图7-4 <code>MovieLense</code> 数据集中评价值的分布</p>
<p><img src="Image01284.jpg" alt></p>
<p>图7-5 <code>MovieLense</code> 数据集中每个用户评价过的电影数的分布</p>
<p><img src="Image01285.jpg" alt></p>
<p>图7-6 <code>MovieLense</code> 数据集中每部电影的平均评分值的分布</p>
<p>为了方便操作评价矩阵，我们也包含了一段代码，将<code>MovieLense</code> 评价矩阵的前3行分别转化为列表、矩阵和数据库。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ML_3u_list &lt;-as(MovieLense[1:3,], &apos;list&apos;)</span><br><span class="line">str(ML_3u_list)</span><br><span class="line">ML_3u_matrix &lt;-as(MovieLense[1:3,], &apos;matrix&apos;)</span><br><span class="line">str(ML_3u_matrix)</span><br><span class="line">ML_3u_df &lt;-as(MovieLense[1:3,], &apos;data.frame&apos;)</span><br><span class="line">str(ML_3u_df)</span><br></pre></td></tr></table></figure>

</details>


<p>在示例代码中，我们也展示了如何对<code>MovieLense</code> 评价矩阵进行预处理（代码如下）。其中<code>MovieLense_c</code> 使得每个用户的平均评价值为0（即对每个用户进行中心化）；<code>MovieLense_c_item</code> 使得每件商品的平均评价值为0；<code>MovieLense_z</code> 使得每个用户的平均评价值为0，评价值的标准差为1（即对每个用户进行标准化）。这里我们还进一步计算了<code>ML_c_rowSums</code> 、<code>ML_c_colSums、ML_z_rowSds</code> 来进行验证。我们还探索了<code>MovieLense_c</code> 和<code>MovieLense_z</code> 中评价值的分布，分别如图7-7和图7-8所示。从图7-7和图7-8中可以看出，与原始的评价值的分布相比，预处理后的评价值分布更加接近于正态分布，而且<code>MovieLense_z</code> 的分布更加接近一些。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MovieLense_c &lt;-normalize(MovieLense, method=&apos;center&apos;)</span><br><span class="line">summary(getRatings(MovieLense_c))</span><br><span class="line">hist(getRatings(MovieLense_c), breaks=50)</span><br><span class="line"># We can check the rowSums of MovieLense_c, all of them are very close to 0.</span><br><span class="line">ML_c_rowSums &lt;-rowSums(MovieLense_c)</span><br><span class="line"></span><br><span class="line">MovieLense_c_item &lt;-normalize(MovieLense, method=&apos;center&apos;, row=F)</span><br><span class="line"># We can also check the colSums of MovieLense_c_item, all of them are </span><br><span class="line"># very close to 0.</span><br><span class="line">ML_c_colSums &lt;-colSums(MovieLense_c_item)</span><br><span class="line"></span><br><span class="line">MovieLense_z &lt;-normalize(MovieLense, method=&apos;Z-score&apos;)</span><br><span class="line">summary(getRatings(MovieLense_z))</span><br><span class="line">hist(getRatings(MovieLense_z), breaks=50)</span><br><span class="line"># We can also check the rowSds (row standard deviation) for</span><br><span class="line"># MovieLense_z, all of them are 1.</span><br><span class="line">ML_z_rowSds &lt;-rowSds(MovieLense_z)</span><br></pre></td></tr></table></figure>

</details>


<p><img src="Image01286.jpg" alt></p>
<p>图7-7 中心化后MovieLense_c中评价值的分布</p>
<p><img src="Image01287.jpg" alt></p>
<p>图7-8 标准化处理后MovieLense_z中评价值的分布</p>
<p>下面讲解如何利用<code>MovieLense</code> 数据构建不同的模型，并在测试集上进行预测，最后调用<code>calcPredictionAccuracy()</code> 函数计算所得模型在测试集上的性能。首先使用<code>evaluationScheme()</code> 函数将<code>MovieLense</code> 数据分为3部分。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ML_e &lt;-evaluationScheme(MovieLense,</span><br><span class="line">　　　　　　　　　　　　 method=&apos;split&apos;,</span><br><span class="line">　　　　　　　　　　　　 train=0.9,</span><br><span class="line">　　　　　　　　　　　　 given = -10,</span><br><span class="line">　　　　　　　　　　　　 goodRating=5)</span><br><span class="line">ML_train &lt;-getData(ML_e, &apos;train&apos;)</span><br><span class="line">ML_test_known &lt;-getData(ML_e, &apos;known&apos;)</span><br><span class="line">ML_test_unknown &lt;-getData(ML_e, &apos;unknown&apos;)</span><br><span class="line">topN &lt;-10</span><br></pre></td></tr></table></figure>

</details>


<p>这里我们就得到了<code>ML_train</code> 、<code>ML_test_known</code> 和<code>ML_test_unknown</code> 。在下面的例子中，我们都使用<code>ML_train</code> 来训练推荐模型，再使用<code>ML_test_known</code> 作为<code>predict()</code> 函数的输入来得到推荐结果，最后比较模型的推荐结果和<code>ML_test_unknown</code> 从而计算推荐模型的性能。</p>
<p>我们构建的第一个模型是<code>recommenderlab</code> 中提供的一个很简单的<code>POPULAR</code> 模型，即从训练集中找出最流行的topN件商品并推荐给测试集中所有的用户。对应的R代码如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">M_RR_p &lt;-Recommender(ML_train, </span><br><span class="line">　　　　　　　　　　　method=&apos;POPULAR&apos;)</span><br><span class="line"># Make item recommendations for users</span><br><span class="line">ML_test_topN_p &lt;-predict(M_RR_p, ML_test_known, </span><br><span class="line">　　　　　　　　　　　　 n=topN, type=&apos;topNList&apos;)</span><br><span class="line"># Explore the returned prediction for the first</span><br><span class="line">ML_test_topN_p_list &lt;-as(ML_test_topN_p, &apos;list&apos;)</span><br><span class="line"># Show the returned topN prediction for the first user in the test set</span><br><span class="line">ML_test_topN_p_list[1]</span><br><span class="line"># Predict ratings</span><br><span class="line">ML_test_rating_p &lt;-predict(M_RR_p, ML_test_known,type=&apos;ratings&apos;)</span><br><span class="line"># Show the predicted rating as a list</span><br><span class="line">ML_test_rating_p_list &lt;-as(ML_test_rating_p, &apos;list&apos;)</span><br><span class="line"># Compute metrics for topN results</span><br><span class="line">ML_metric_topN_p &lt;-calcPredictionAccuracy(ML_test_topN_p,</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　ML_test_unknown,</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　given=-10,</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　goodRating=5)</span><br><span class="line"># compute metrics based on rating prediction</span><br><span class="line"># metrics averaged over all recommendations</span><br><span class="line">ML_metric_rating_p &lt;-calcPredictionAccuracy(ML_test_rating_p,</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　ML_test_unknown)</span><br><span class="line"># metrics averaged by users: we compute metrics for each user</span><br><span class="line">ML_metric_rating_by_user_p &lt;-calcPredictionAccuracy(ML_test_rating_p,</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　　　ML_test_unknown,</span><br><span class="line">　　　　　　　　　　　　　　　　　　　　　　　　　　byUser=T)</span><br></pre></td></tr></table></figure>

</details>


<p>这里我们调用<code>Recommender()</code> 函数来构建推荐模型。在构建<code>POPULAR</code> 模型时，只需要将<code>method</code> 参数设为<code>&#39;POPULAR&#39;</code> 。然后，调用<code>predict()</code> 函数两次，分别将<code>type</code> 参数设为<code>&#39;topNList&#39;</code> 和<code>&#39;ratings&#39;</code> 。对于<code>topNList</code> 的返回结果<code>ML_test_topN_p</code> ，可以将其转化为列表<code>ML_test_topN_p_list</code> 。这里我们使用<code>ML_test_topN_p_list[1]</code> 来直接观察模型的返回结果。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; ML_test_topN_p_list[1]</span><br><span class="line">$&apos;7&apos;</span><br><span class="line"> [1] &quot;Titanic (1997)&quot;　　　　　　 &quot;Schindler&apos;s List (1993)&quot;　 </span><br><span class="line"> [3] &quot;L.A. Confidential (1997)&quot;　 &quot;Good Will Hunting (1997)&quot;　</span><br><span class="line"> [5] &quot;Toy Story (1995)&quot;　　　　　 &quot;Alien (1979)&quot;　　　　　　　</span><br><span class="line"> [7] &quot;Close Shave, A (1995)&quot;　　　&quot;Wrong Trousers, The (1993)&quot;</span><br><span class="line"> [9] &quot;Apt Pupil (1998)&quot;　　　　　 &quot;Lone Star (1996)&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>类似地，也可以将<code>ML_test_rating_p</code> 转化为列表对象<code>ML_test_rating_p_list</code> 以方便查看。对于两种不同的预测结果<code>ML_test_topN_p</code> 和<code>ML_test_rating_p</code> ，可以使用<code>calcPrediction Accuracy()</code> 函数来评价模型的性能。在使用<code>calcPredictionAccuracy()</code> 函数计算评价值预测结果时，还有一个控制参数<code>byUser</code> 。该参数的默认值是<code>F</code> ，表示将所有的用户一起考虑；当其值是<code>T</code> 时，表示单独为每个用户计算结果。在上面的示例程序中，我们为每个用户单独计算了MAE、MSE和RMSE，并保存在<code>ML_metric_rating_by_user_p</code> 中。我们可以直接在R的控制端查看如下结果，其中对于<code>ML_metric_rating_by_user_p</code> ，只查看了前5个用户对应的结果。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; ML_metric_topN_p</span><br><span class="line">　　　　　TP　　　　　 FP　　　　　 FN　　　　　 TN　　precision </span><br><span class="line">2.421053e-01 9.757895e+00 2.168421e+00 1.661832e+03 2.421053e-02 </span><br><span class="line">　　　recall　　　　　TPR　　　　　FPR </span><br><span class="line">1.083676e-01 1.083676e-01 5.837417e-03 </span><br><span class="line">&gt; ML_metric_rating_p</span><br><span class="line">　　 RMSE　　　 MSE　　　 MAE </span><br><span class="line">1.0100247 1.0201499 0.8034736 </span><br><span class="line">&gt; ML_metric_rating_by_user_p[1:5,]</span><br><span class="line">　　　　RMSE　　　 MSE　　　 MAE</span><br><span class="line">7　1.1779959 1.3876744 1.0343581</span><br><span class="line">9　0.7184146 0.5161196 0.5896807</span><br><span class="line">10 0.4465300 0.1993890 0.3825720</span><br><span class="line">21 0.7947457 0.6316208 0.6586477</span><br><span class="line">31 1.0773234 1.1606256 0.8836956</span><br></pre></td></tr></table></figure>

</details>


<p>对于其他类型的推荐模型，在<code>recommenderlab_example.R</code> 文件中我们给出了完整的模型构建和评价过程。这里我们列出构建不同模型时对应的代码：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">M_RR_ibcf &lt;-Recommender(ML_train,</span><br><span class="line">　　　　　　　　　　　　 method=&apos;IBCF&apos;,</span><br><span class="line">　　　　　　　　　　　　 param=list(k=20,</span><br><span class="line">　　　　　　　　　　　　　　　　　　normalize=&apos;center&apos;))</span><br><span class="line">M_RR_ubcf &lt;-Recommender(ML_train,</span><br><span class="line">　　　　　　　　　　　　 method=&apos;UBCF&apos;,</span><br><span class="line">　　　　　　　　　　　　 param=list(method=&apos;cosine&apos;,</span><br><span class="line">　　　　　　　　　　　　　　　　　　nn=30,</span><br><span class="line">　　　　　　　　　　　　　　　　　　normalize=&apos;center&apos;))</span><br><span class="line">M_RR_ubcf2 &lt;-Recommender(ML_train,</span><br><span class="line">　　　　　　　　　　　　　method=&apos;UBCF&apos;,</span><br><span class="line">　　　　　　　　　　　　　param=list(method=&apos;pearson&apos;,</span><br><span class="line">　　　　　　　　　　　　　　　　　　 nn=30,</span><br><span class="line">　　　　　　　　　　　　　　　　　　 sample=F,</span><br><span class="line">　　　　　　　　　　　　　　　　　　 normalize=&apos;center&apos;))</span><br><span class="line">M_RR_svd &lt;-Recommender(ML_train,</span><br><span class="line">　　　　　　　　　　　　method=&apos;SVD&apos;,</span><br><span class="line">　　　　　　　　　　　　param=list(k=15,</span><br><span class="line">　　　　　　　　　　　　　　　　　 maxiter=500,</span><br><span class="line">　　　　　　　　　　　　　　　　　 normalize=&apos;center&apos;))</span><br><span class="line">M_RR_svdf &lt;-Recommender(ML_train,</span><br><span class="line">　　　　　　　　　　　　 method=&apos;SVDF&apos;,</span><br><span class="line">　　　　　　　　　　　　 param=list(k=15,</span><br><span class="line">　　　　　　　　　　　　　　　　　　lambda=0.001,</span><br><span class="line">　　　　　　　　　　　　　　　　　　max_epochs=50,</span><br><span class="line">　　　　　　　　　　　　　　　　　　normalize=&apos;center&apos;))</span><br></pre></td></tr></table></figure>

</details>


<p>这里我们分别构建了一个基于商品的邻域方法模型<code>M_RR_ibcf</code> ，两个基于用户的邻域方法模型<code>M_RR_ubcf</code> 和<code>M_RR_ubcf2</code> ，两个基于矩阵分解的模型<code>M_RR_svd</code> 和<code>M_RR_svdf</code> 。在构建不同的模型时，我们需要将参数<code>method</code> 设为不同的值，如<code>&#39;IBCF&#39;</code> 、<code>&#39;UBCF&#39;</code> 、<code>&#39;SVD&#39;</code> 和<code>&#39;SVDF&#39;</code> 等。对于每种方法，我们再将它对应的参数用一个列表对象组织起来赋给<code>param</code> 参数。在<code>IBCF</code> 中，我们使用参数<code>k</code> 指定邻域的大小，使用参数<code>normalize</code> 指定采用何种方法对评价值进行预处理，选项包括<code>&#39;center&#39;</code> 和<code>&#39;Z-score&#39;</code> 。在<code>UBCF</code> 中，我们使用<code>nn</code> 指定邻域的大小，使用<code>method</code> 指定采用何种方法计算相似度，参数<code>sample</code> 表示是否对用于训练的评价矩阵进行取样，参数<code>normalize</code> 同样表示如何对评价值进行预处理。在<code>SVD</code> 中，参数<code>k</code> 表示矩阵分解中用户和商品的低维表示的维数，参数<code>maxiter</code> 表示计算矩阵分解时算法的最多运行次数，<code>normalize</code> 表示对评价矩阵进行预处理的方法。算法<code>SVDF</code> 与<code>SVD</code> 类似，区别在于<code>SVDF</code> 中使用了梯度下降法。在<code>SVDF</code> 中，参数<code>k</code> 表示低维表示的维数，<code>lambda</code> 表示正则化项的权重，<code>max_epochs</code> 表示梯度下降法运行的次数，<code>normalize</code> 也表示对评价矩阵进行预处理的方法。</p>
<p>上面的<code>MovieLense</code> 是<code>realRatingMatrix</code> 对象。接下来我们考虑<code>binaryRatingMatrix</code> 类型的<code>MSWeb</code> 数据。在<code>recommenderlab_example.R</code> 文件中，我们给出了关于<code>MSWeb</code> 数据完整的模型构建和评价过程，在这里只简单列出构建多个不同推荐模型的代码并进行说明。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">M_BR_p &lt;-Recommender(MSW_train, </span><br><span class="line">　　　　　　　　　　　method=&apos;POPULAR&apos;)</span><br><span class="line">M_BR_ibcf &lt;-Recommender(MSW_train,</span><br><span class="line">　　　　　　　　　　　　 method=&apos;IBCF&apos;,</span><br><span class="line">　　　　　　　　　　　　 param=list(k=20,</span><br><span class="line">　　　　　　　　　　　　　　　　　　method=&apos;Jaccard&apos;,</span><br><span class="line">　　　　　　　　　　　　　　　　　　normalize_sim_matrix=F))</span><br><span class="line">M_BR_ubcf &lt;-Recommender(MSW_train,</span><br><span class="line">　　　　　　　　　　　　 method=&apos;ubcf&apos;,</span><br><span class="line">　　　　　　　　　　　　 param=list(method=&apos;Jaccard&apos;,</span><br><span class="line">　　　　　　　　　　　　　　　　　　nn = 20,</span><br><span class="line">　　　　　　　　　　　　　　　　　　weighted=T))</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，其参数设置与<code>realRatingMatrix</code> 大致相同，只需要根据<code>recommenderlab</code> 包的要求稍微调节一下具体参数即可。</p>
<p>之后我们调用<code>recommenderlab</code> 中的<code>HybridRecommender()</code> 函数将已有的多个推荐模型以线性组合的形式综合在一起。在使用该函数时，需要指定要综合哪些推荐模型，并在参数<code>weights</code> 中指定它们的权重。在下面的例子中，我们综合了3个不同的模型<code>M_RR_ibcf</code> 、<code>M_RR_ubcf</code> 和<code>M_RR_svd</code> ，它们对应的权重分别为0.3、0.2和0.5。得到综合后的模型后，可以使用<code>predict()</code> 函数根据综合后的模型进行预测。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">M_RR_h1 &lt;-HybridRecommender(M_RR_ibcf,</span><br><span class="line">　　　　　　　　　　　　　　 M_RR_ubcf,</span><br><span class="line">　　　　　　　　　　　　　　 M_RR_svd,</span><br><span class="line">　　　　　　　　　　　　　　 weights=c(0.3, 0.2, 0.5))</span><br><span class="line">ML_test_topN_h1 &lt;-predict(M_RR_h1, ML_test_known,</span><br><span class="line">　　　　　　　　　　　　　 n=topN, type=&apos;topNList&apos;)</span><br><span class="line">ML_test_rating_h1 &lt;-predict(M_RR_h1, ML_test_known,type=&apos;ratings&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>最后，我们利用<code>recommenderlab</code> 提供的<code>evaluate()</code> 函数来比较不同的推荐模型。我们可以利用上面介绍的<code>Recommender()</code> 、<code>predict()</code> 和<code>calcPredictionAccuracy()</code> 等诸函数来逐一构建并比较多个不同的模型。但是利用提供的<code>evaluate()</code> 函数，可以更加直接和方便地比较多个不同的模型。在下面的例子中，我们使用<code>Jester5k</code> 数据集中的前1000个用户的数据来训练和比较不同的模型。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">J_e &lt;-evaluationScheme(Jester5k[1:1000], </span><br><span class="line">　　　　　　　　　　　　method=&quot;split&quot;, </span><br><span class="line">　　　　　　　　　　　　train = .9,</span><br><span class="line">　　　　　　　　　　　　k=1, </span><br><span class="line">　　　　　　　　　　　　given=-5, </span><br><span class="line">　　　　　　　　　　　　goodRating=5)</span><br><span class="line">algorithms2compare &lt;-list(</span><br><span class="line">　&quot;random items&quot; = list(name=&quot;RANDOM&quot;, param=NULL),</span><br><span class="line">　&quot;popular items&quot; = list(name=&quot;POPULAR&quot;, param=NULL),</span><br><span class="line">　&quot;user-based CF&quot; = list(name=&quot;UBCF&quot;, param=list(nn=50)),</span><br><span class="line">　&quot;item-based CF&quot; = list(name=&quot;IBCF&quot;, param=list(k=50)),</span><br><span class="line">　&quot;SVD approximation&quot; = list(name=&quot;SVD&quot;, param=list(k = 50))</span><br><span class="line">)</span><br><span class="line">J_results_topN &lt;-evaluate(J_e, algorithms2compare, </span><br><span class="line">　　　　　　　　　　　　　 type = &quot;topNList&quot;,</span><br><span class="line">　　　　　　　　　　　　　 n=c(1, 3, 5, 10, 15, 20))</span><br><span class="line">J_results_topN</span><br><span class="line">names(J_results_topN)</span><br><span class="line">J_results_topN[[&quot;user-based CF&quot;]]</span><br><span class="line">plot(J_results_topN, annotate=c(1,2,4), legend=&quot;bottomright&quot;)</span><br><span class="line">plot(J_results_topN, &quot;prec/rec&quot;, annotate=4, legend=&quot;topleft&quot;)</span><br><span class="line"># Compare ratings</span><br><span class="line">J_results_rating &lt;-evaluate(J_e, algorithms2compare, type = &quot;ratings&quot;)</span><br><span class="line">J_results_rating</span><br><span class="line">plot(J_results_rating)</span><br></pre></td></tr></table></figure>

</details>


<p>在上面的代码中，首先使用<code>evaluationScheme()</code> 函数将数据分为3个子集。然后，将要比较的多个模型对应的参数用一个列表<code>algorithms2compare</code> 表示，该列表的每个元素也是一个列表，对应一个模型包含的参数，其中<code>name</code> 是算法的类型，<code>param</code> 是该算法对应的具体参数。接着直接调用<code>evaluate()</code> 函数就可以按照<code>evaluationScheme()</code> 中的数据划分来训练、比较不同的推荐模型。在使用<code>evaluate()</code> 函数时，可以指定<code>type</code> 参数为<code>&#39;topNList&#39;</code> 或<code>&#39;ratings&#39;</code> ，表示以何种方式调用<code>predict()</code> 函数。当<code>type</code> 参数为<code>&#39;topNList&#39;</code> 时，还可以指定参数<code>n</code> ，表示将对诸模型返回的推荐结果的前 _n_ 个分别计算评价指标并比较。在上面的代码中，我们将<code>n</code> 设为<code>c（1,3,5,10,15,20），</code> 表示将对这6种不同的情况进行比较。最后，可以调用<code>plot()</code> 函数来直接可视化不同的模型的结果。图7-9显示了5种不同的算法在<code>n</code> 等于1、3、5、10、15、20这6种情况下的ROC曲线［见图7-9（a）］，以及召回率-精确率曲线［见图7-9（b）］的比较。图7-10则给出了在<code>Jester5k</code> 数据上不同算法的RMSE、MSE和MAE比较。</p>
<p><img src="Image01288.jpg" alt></p>
<p>图7-9 不同算法在<code>Jester5k</code> 数据集上的比较</p>
<p><img src="Image01289.jpg" alt></p>
<p>图7-10 在<code>Jester5k</code> 数据上不同算法的RMSE、MSE和MAE比较</p>
<h2 id="7-6-推荐算法的评价和选取"><a href="#7-6-推荐算法的评价和选取" class="headerlink" title="7.6 推荐算法的评价和选取"></a>7.6 推荐算法的评价和选取</h2><p>在本节中，我们回顾一下上面介绍的各种推荐算法，并讨论在实际中如何选取合适的算法。这里我们着重讨论协同过滤的推荐算法。</p>
<p>在多次关于推荐问题的数据挖掘比赛中，如Netflix Prize 、KDD Cup等，基于矩阵分解的推荐算法都取得了良好的表现。对于很多数据，基于矩阵分解的算法都能取得较好的效果，值得一试。但是我们也要指出，基于矩阵分解的算法并不一定适用于所有的数据。在Netflix Prize中，采用的是RMSE作为算法的评价标准。如果采用不同的标准，那么基于矩阵分解的算法未必是最优的算法。此外，基于矩阵分解的算法对于参数比较敏感。在实际使用基于矩阵分解的算法时，我们要根据数据的具体特征仔细地调节参数。很多时候，在部署推荐算法时，随着时间的变化，数据一般也会发生变化。因此，在实际部署的过程中，我们要根据数据的变化及时调整参数，从而取得较好的性能。</p>
<p>与基于矩阵分解的推荐算法相比，基于邻域的推荐算法简单直观，易于实现。特别是基于商品的推荐算法，对于推荐结果的解释也很容易。在采用基于邻域的推荐算法时，一个非常实际问题是：是采用基于用户的邻域推荐算法还是采用基于商品的邻域推荐算法？</p>
<p>其中一个影响算法选择的因素是推荐结果的准确性。很多时候，我们需要考虑用户数目和商品数目的对比。在基于用户的方法中，我们依赖于用户-用户之间的相似度；而在基于商品的方法中，我们依赖于商品-商品之间的相似度。通过前面关于相似度显著性的讨论我们可以知道，相似度的计算存在着“可靠性”的问题。在基于邻域的推荐系统中，少量“可靠”的相似用户或者商品的作用远大于大量“不可靠”的用户或者商品的作用。因此，如果用户数目远大于商品数目，一般而言，商品-商品之间的相似度度量会更可靠一些。此外，基于商品的推荐算法在商品-商品相似度的基础上，利用每个用户以往对其他商品的评价情况来推断该用户对某一商品的喜好程度。简而言之，就是利用用户自己的信息来推断自己的喜好。而基于用户的推荐算法则是利用其他用户对于某一商品的喜好程度来推断该用户对于这一商品的喜好程度，而其他用户的评价习惯等可能与该用户不完全一样，导致在推断时可能存在较大的误差，从而得到较差的推荐结果。因此，在很多实际问题中，基于商品的推荐算法更容易得到更好的推荐结果。当然，基于商品的推荐算法并不是绝对好于基于用户的推荐算法。在实际中，很多情况下基于用户的算法要优于基于商品的算法。我们需要根据实际问题的需要和数据的具体特征选择相应的算法。</p>
<p>另一个不同点是基于商品的方法通常给出比较稳妥但是没有太多新意的推荐，而基于用户的方法则会给出一些新颖甚至不是那么“安全”的推荐。</p>
<p>在基于商品的方法中，我们估计用户 _u_ 对商品 _i_ 的喜好程度 _r ui _ 时，首先考虑用户 _u_ 评价过的所有商品，从这些商品中选出与商品 _i_ 最相似的 _k_ 件商品，根据用户对这些商品的喜好来估计 _r ui _ 。因此，在推荐新商品时，推荐系统更倾向于推荐那些与用户已经熟悉的商品相似的商品。例如，在Netflix Prize的电影推荐中，使用基于商品的邻域方法更倾向于推荐与用户看过的电影相似的电影，如相同的类型、包含同样的演员等。这样的推荐一般来说都是合理的，但是很难向用户推荐比较“新”的商品。</p>
<p>而在基于用户的邻域方法中，则很有可能推荐一些更加“新颖”的商品。这里举一个简单的例子，假设一个用户 _u_ 喜欢不同类型的电影，而用户 _v_ 目前只观看过一种类型的电影，如果用户 _u_ 和用户 _v_ 很相似的话，使用基于用户的方法，用户 _u_ 喜欢的电影可以推荐给用户 _v_ 。</p>
<p>我们还可以从“可解释性”角度来进一步对比这两种方法。基于商品的方法一般给出的推荐都是比较安全的，并且很好解释。对于用户来说，基于商品的方法所使用的数据主要是基于自己的过往数据，非常直观。例如，在电影推荐中，对用户 _u_ 推荐典型电影M1可以很容易解释：因为用户 _u_ 之前看过与M1相似度很高的若干电影M2、M3等。而基于用户的方法，则不是那么直观，因为每个用户并不了解其他用户的历史数据。</p>
<p>还有一个因素是计算复杂度。一般来讲，在很多实际问题中，用户的数目远大于商品的数目。由于基于商品的推荐算法只需要计算商品-商品的相似度，因此需要的计算资源会较少。衡量计算复杂度的另一个重要因素是系统的更新速度。注意，当我们计算同时评价过同一商品的两个用户之间的相似度时，他们同时评价过的商品数目一般而言不是很多。而我们计算两件商品之间的相似度时，同时评价过这两件商品的用户数目一般要大得多。因此，当新增一些评价值时，用户之间的相似度值可能会发生很大变化，而商品之间的相似度则基本上不受影响。此外，在很多实际的推荐系统中，用户的增加是很频繁的，而商品的增加则不是那么频繁。因此，用户-用户相似度要比商品-商品相似度的更新频率高得多。在这种情况下，基于商品的推荐算法相应的计算复杂度要低很多。</p>
<p>这里我们还特别指出评价值中商品的长尾分布对基于用户的邻域推荐算法的影响。在计算用户之间的相似度时，主要依赖于评价信息。这样的话，那些流行的商品的评价信息就主导了用户相似度的计算。换言之，我们基本上是利用那些流行的商品来确定用户之间是否相似。在实际中，虽然用户 _u_ 和用户 _v_ 对于那些流行商品的喜好一致，但是很有可能他们对于那些不流行的商品的喜好完全不同。</p>
<p>总之，与前面讨论的分类和回归算法类似，合理的算法的选择依赖于数据的具体特征。因此，在实际使用中，读者需要根据实际需要灵活选择算法。</p>
<hr>
<p>① <a href="http://www.netflixprize.com/" target="_blank" rel="noopener">http://www.netflixprize.com/</a></p>
<p>② <a href="https://azure.microsoft.com/en-us/documentation/articles/machine-learning-recommendation-api-documentation/" target="_blank" rel="noopener">https://azure.microsoft.com/en-us/documentation/articles/machine-learning-recommendation-api-documentation/</a></p>
<p>③ <a href="https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine" target="_blank" rel="noopener">https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine</a></p>
<p>④ <a href="https://github.com/cran/recommenderlab" target="_blank" rel="noopener">https://github.com/cran/recommenderlab</a></p>
<p>⑤ 手册：<a href="https://cran.r-project.org/web/packages/recommenderlab/recommenderlab.pdf" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/recommenderlab/recommenderlab.pdf</a> 。简介：<a href="https://cran.r-project.org/%20web/packages/recommenderlab/vignettes/recommenderlab.pdf" target="_blank" rel="noopener">https://cran.r-project.org/ web/packages/recommenderlab/vignettes/recommenderlab.pdf</a> 。</p>
<h1 id="第8章-排序学习"><a href="#第8章-排序学习" class="headerlink" title="第8章 排序学习"></a>第8章 排序学习</h1><h2 id="8-1-排序学习简介"><a href="#8-1-排序学习简介" class="headerlink" title="8.1 排序学习简介"></a>8.1 排序学习简介</h2><p>在本章中，我们主要讨论排序学习 （learning to rank）。排序学习在很多领域有着广泛的应用，如信息检索 （information retrieval，IR）、自然语言处理 （natural language processing，NLP）。最常见的例子是文档检索 （document retrieval）。本章主要从机器学习的角度讨论如何解决排序问题。</p>
<p>我们以文档检索为例说明什么是排序学习。在文档检索中，我们有一个文档集合<img src="Image01290.gif" alt> 。用户提供一个查询 _q_ （一般由若干词组成），然后从文档集合中返回最相关的文档。根据查询 _q_ 中包含的词，找出那些含有这些词的文档，并将这些文档按照与查询的相似程度排好序，最后选择相似度最高的若干文档返回，即作为查询结果。图8-1描述了文档检索的过程。</p>
<p><img src="Image01291.jpg" alt></p>
<p>图8-1 文档检索</p>
<p>在这个文档排序的例子中，需要估计排序模型<img src="Image01292.gif" alt> ，这里 _q_ 是查询， _d_ 是文档， _f_ 是我们要学习得到的排序模型。在很多传统的信息检索模型中， _f_ 都是通过一些经验公式而不是学习得到的。例如，在最简单的情况下，可以将 _q_ 也视为一个较小的文档，利用词频-逆文档频率（TF-IDF）为查询和文档构建相应的特征，并将查询 _q_ 和文档 _d_ 之间的相似度作为<img src="Image01292.gif" alt> 。使用经验公式的缺点是很难根据数据和问题的具体特性采用针对性的方法以提高性能。</p>
<p>通过使用机器学习，可以从数据中学习出模型<img src="Image01292.gif" alt> 。现在，基于机器学习的模型在排序问题中越来越流行，其原因如下：</p>
<p>（1）使用机器学习方法可以考虑多种不同的信号。例如，在网络搜索中，可以将PageRank算法和基于文档的排序方法很好地结合起来。而在传统的信息检索方法中，很难同时引入多种数据。</p>
<p>（2）随着数据越来越大，很多常用的信息检索方法难以适用。而机器学习的方法能够从海量数据中自主学习出模型，因此受到了广泛的欢迎。而且在实际问题中，数据经常发生变化，在这种情况下，我们经常需要根据数据调整模型。在这种情况下，机器学习方法具有显著的优势。</p>
<p>与前面讨论的分类问题和回归问题类似，我们也将排序学习转化为监督型学习 （supervised learning）的任务。因此，在排序学习中，我们也有训练数据和测试数据。但是与分类和回归问题不同，排序学习的训练集中包括查询和文档。每个查询和若干个文档相关联。在训练集中，文档和查询的相关度也给出。这里的相关度可以用多种方式来表示。例如，可以是相关或者不相关，也可以是相关的程度，如从1到5的值。</p>
<p>与前面的分类或者回归问题不同，排序学习中的一个重要的概念是查询。用户首先提供查询信息，然后系统根据用户的查询，从对象集合中返回与用户查询最匹配的若干对象。例如，在使用搜索引擎时，对象集合就是整个互联网上的网页的集合。当用户在expedia.com上搜索航班时，这里的对象集合就是所有航班的集合；当用户在expedia.com上搜索旅馆时，这里的对象集合就是所有旅馆的集合。</p>
<p>查询也是排序问题和推荐问题的区别之一：在推荐问题中，用户没有查询，我们通过考察用户的历史信息来给用户进行推荐；而在排序问题中，我们要根据用户的查询，找出最匹配其输入查询的结果。</p>
<h3 id="8-1-1-解决排序问题的基本思路"><a href="#8-1-1-解决排序问题的基本思路" class="headerlink" title="8.1.1 解决排序问题的基本思路"></a>8.1.1 解决排序问题的基本思路</h3><p>使用机器学习方法解决排序问题的基本思路是将它转化为一个监督型学习问题。具体地讲，就是考虑训练数据集，并最小化损失函数在训练集上的经验损失<br>（empirical loss）。这种最小化损失函数的方法和我们在回归问题和分类问题上的处理方法是一致的。</p>
<p>但是，与前面的回归或者分类问题的处理相比，排序问题主要有两点不同。</p>
<p>（1）我们需要显式地为每个查询-文档对<img src="Image01293.gif" alt> 构造特征<img src="Image01294.gif" alt> ，这里<img src="Image01295.gif" alt> 是一个与查询<img src="Image01296.gif" alt> 对应的文档。</p>
<p>（2）在排序问题中，由于算法评价标准不同，因此不能直接使用回归或者分类问题中使用的损失函数。</p>
<p>在排序问题中采用的算法评价指标通常都是不连续和不可导的。因此，这些指标对应的损失函数也是不连续和不可导的，这使得很多数值优化算法无法直接应用。在这种情况下，通常采用一些近似函数来逼近那些算法评价指标。进一步，按照排序算法的具体实现方式，可以分为如下3类。</p>
<ul>
<li>逐点（pointwise）方法 ：在逐点方法中，每次只考虑一个查询和一个文档，以及对应的评分或者相关度；</li>
<li>逐对（pairwise）方法 ：在逐对方法中，每次考虑同一查询对应的一对文档和它们对应的相关度的差；</li>
<li>逐列（listwise）方法 ：在逐列方法中，每次考虑一个查询对应的一个文档序列和它们对应的排序结果。</li>
</ul>
<p>基本上，逐点方法是较早提出的适用于排序问题的算法，而且很多算法都是从已有的分类和回归算法借鉴过来的。而逐对和逐列方法是后期发展出来的专门针对排序问题的算法。</p>
<h3 id="8-1-2-构造特征"><a href="#8-1-2-构造特征" class="headerlink" title="8.1.2 构造特征"></a>8.1.2 构造特征</h3><p>在使用机器学习算法解决排序问题时，核心问题是研究查询和文档之间的关系。因此，需要给查询-文档对构建相应的特征。具体来说，我们要为每个查询-文档对<img src="Image01293.gif" alt> 构造特征<img src="Image01294.gif" alt> ，这里<img src="Image01294.gif" alt> 是一个向量：</p>
<p><img src="Image01297.gif" alt></p>
<p>（8-1）</p>
<p>这里用 _φ_ 表示构造的函数。</p>
<p>在实际中，我们有多种方法可以构造特征。例如，可以使用信息检索中传统的BM25模型的输出作为一个特征，也可以使用PageRank对于网站的权重作为一个特征。这里我们介绍BM25模型的输出和一些在文档检索中常用的特征。</p>
<h4 id="1．BM25模型"><a href="#1．BM25模型" class="headerlink" title="1．BM25模型"></a>1．BM25模型</h4><p>BM25模型是信息检索中的经典模型，这里BM是Best Matching（最优匹配）的简写。BM25也称为Okapi BM25，这里Okapi是由伦敦城市大学开发的世界上第一个使用该模型的信息检索系统的名字。BM25事实上不是一个单一的模型，可以认为是一组排序模型；而模型之间具有不同的组成部分和参数。下面我们简单介绍其中一个流行的模型。</p>
<p>对于查询 _q_ ，假设其包含的项（或者词）为<img src="Image01298.gif" alt> 。在BM25算法中，我们计算 _q_ 和文档 _d_ 的相似度为：</p>
<p><img src="Image01299.gif" alt></p>
<p>（8-2）</p>
<p>这里<img src="Image01300.gif" alt> 是项<img src="Image01301.gif" alt> 出现在文档 _d_ 中的频率，<img src="Image01302.gif" alt> 是文档 _d_ 的长度（可用文档所包含的词的总数度量）， _avdl_ 是文档集合中文档的平均长度，<img src="Image01303.gif" alt> 和 _b_ 是控制参数，<img src="Image01304.gif" alt> 是项<img src="Image01301.gif" alt> 对应的逆文档频率 （Inverse Document Frequency，IDF）权重：</p>
<p><img src="Image01305.gif" alt></p>
<p>（8-3）</p>
<p>这里 _N_ 是所有文档的总数，<img src="Image01306.gif" alt> 是其中包含项<img src="Image01301.gif" alt> 的文档数目。在信息检索中，对于<img src="Image01307.gif" alt> 和<img src="Image01304.gif" alt> 都有一些不同的定义，我们这里介绍的都是最简单的定义。对于控制参数<img src="Image01303.gif" alt> 和 _b_ ，一般而言，需要根据数据具体调节。一般的推荐值为<img src="Image01308.gif" alt> 。</p>
<h4 id="2．其他特征"><a href="#2．其他特征" class="headerlink" title="2．其他特征"></a>2．其他特征</h4><p>表8-1总结了一些适用于文档检索的常用特征，其中<img src="Image01309.gif" alt> 表示查询 _q_ 和文档 _d_ 相交的词 _w_ ，<img src="Image01310.gif" alt> 表示 _w_ 在 _d_ 中出现的词频，<img src="Image01311.gif" alt> 表示 _w_ 的逆文档频率，<img src="Image01302.gif" alt> 表示文档 _d_ 的长度。对于其他问题，我们可以类似地建立相应的特征。</p>
<p>表8-1 适用于文档检索的常用特征</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特 征 编 号</th>
<th>特 征 定 义  </th>
</tr>
</thead>
<tbody>
<tr>
<td> 1</td>
<td><img src="Image01312.gif" alt>  </td>
</tr>
<tr>
<td> 2</td>
<td><img src="Image01313.gif" alt>  </td>
</tr>
<tr>
<td> 3</td>
<td><img src="Image01314.gif" alt>  </td>
</tr>
<tr>
<td> 4</td>
<td><img src="Image01315.gif" alt>  </td>
</tr>
<tr>
<td> 5</td>
<td><img src="Image01316.gif" alt>  </td>
</tr>
<tr>
<td> 6</td>
<td><img src="Image01317.gif" alt>  </td>
</tr>
<tr>
<td> 7</td>
<td><img src="Image01318.gif" alt>  </td>
</tr>
<tr>
<td> 8</td>
<td><img src="Image01319.gif" alt>  </td>
</tr>
</tbody>
</table>
</div>
<h3 id="8-1-3-获取相关度分数"><a href="#8-1-3-获取相关度分数" class="headerlink" title="8.1.3 获取相关度分数"></a>8.1.3 获取相关度分数</h3><p>注意，在训练集中，对于查询-文档对<img src="Image01320.gif" alt> ，我们已知相关度<img src="Image01321.gif" alt> 。在实际中，通常有两种方式获得。第一种是人工标注的方法。这种方法适用于数据规模较小的情况。当数据规模较大时难以实行。第二种方式是间接方法。例如，我们可以通过搜索的日志文件来得到。例如，在网站expedia.com上，用户搜索某地的旅馆信息后，expedia.com返回一系列结果。在实际中，用户会点击一部分旅馆，显示对这些旅馆感兴趣。如果用户最终预定了某一旅馆，则表示用户对该旅馆的各方面包括位置、价格等都比较满意。另外，如果某些旅馆虽然排在返回结果的前几名，但是用户都没有点击，则反映了用户对这些旅馆没有兴趣。在这个例子中，我们可以得到如下的排序结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">预定的旅馆&gt;点击但没有预定的旅馆&gt;没有点击的旅馆</span><br></pre></td></tr></table></figure>
<p>在本章中，我们主要使用文档检索来讨论算法。一方面，文档检索是排序问题中的一个常见问题；另一方面，文档检索中场景比较简单，利于我们讨论算法的原理。接下来，我们首先介绍本章使用的数学符号，然后在后续章节中，介绍排序算法的评价指标，并逐一介绍逐点方法、逐对方法和逐列方法。</p>
<h3 id="8-1-4-数学符号"><a href="#8-1-4-数学符号" class="headerlink" title="8.1.4 数学符号"></a>8.1.4 数学符号</h3><p>查询一般用 _q_ 表示，并将查询的集合记为 _Q_ 。我们用 _d_ 表示文档，用 _D_ 表示文档的集合。在训练集中的查询集合记为<img src="Image01322.gif" alt> 。对于查询<img src="Image01296.gif" alt> ，我们将训练集中与<img src="Image01296.gif" alt> 对应的文档集合记为<img src="Image01323.gif" alt> 。注意，对于查询<img src="Image01296.gif" alt> ，我们有<img src="Image01324.gif" alt> 个文档与之对应，其中有些文档与<img src="Image01296.gif" alt> 相关，有些则不相关。在文档集合<img src="Image01325.gif" alt> 中，我们将所有与<img src="Image01296.gif" alt> 相关的文档的总数记为<img src="Image01326.gif" alt> 。</p>
<p>对于训练集中的查询-文档对{ _q i _ , _d i _ , _j_ }，其对应的相关度为<img src="Image01327.gif" alt> ，我们把为该查询-文档对构建的特征记为<img src="Image01294.gif" alt> 。相关度<img src="Image01321.gif" alt> 可以为布尔型，也可以使用多个不同的分数表示相关程度。在本章中，我们假设这个分数越高，查询和文档的相关度越高。我们把查询<img src="Image01296.gif" alt> 对应的所有文档的相关度分数记为向量的形式<img src="Image01328.gif" alt> 。举例来说，假设对于查询<img src="Image01329.gif" alt> 有3个对应文档<img src="Image01330.gif" alt> ，其中只有第二个文档<img src="Image01331.gif" alt> 与<img src="Image01329.gif" alt> 相关，则对应的相关度向量可设置为<img src="Image01332.gif" alt> 。</p>
<p>在逐对方法中，我们要考察每个查询<img src="Image01296.gif" alt> 对应的所有文档对。我们将查询<img src="Image01296.gif" alt> 所对应的所有文档对的集合记为<img src="Image00665.gif" alt> 。对于训练集中的任意文档对(<img src="Image01333.gif" alt> )，我们假设第一项 _d i _ , _u_ 比第二项 _d i _ , _v_ 更相关，记为<img src="Image01334.gif" alt> 。在前一个例子中，<img src="Image01329.gif" alt> 有3个对应文档<img src="Image01330.gif" alt> ，其中只有第二个文档<img src="Image01331.gif" alt> 与<img src="Image01329.gif" alt> 相关，则对应的文档对集合为<img src="Image01335.gif" alt> 。</p>
<p>我们把排序模型记为 _f_ 。一般而言，我们利用特征<img src="Image01336.gif" alt> 作为输入。 _f_ 的输出可以有多种，其中最常用的是<img src="Image01337.gif" alt> 是查询<img src="Image01296.gif" alt> 与文档<img src="Image01295.gif" alt> 的相关度。根据排序模型 _f_ ，我们可以将与<img src="Image01296.gif" alt> 对应的所有文档<img src="Image01338.gif" alt> 按照输出的相关度从高到低排列好输出。如果我们将文档集合<img src="Image01325.gif" alt> 中的文档用整数<img src="Image01339.gif" alt> 来标记，那么利用模型 _f_ 就会得到一个关于<img src="Image01339.gif" alt> 的排列 。我们将<img src="Image01339.gif" alt> 的所有排列的集合记为<img src="Image01340.gif" alt> 。对于某一排列<img src="Image01341.gif" alt> ，我们记<img src="Image01342.gif" alt> 为排列 _π_ 中的第 _r_ 个文档。例如，对于排列<img src="Image01339.gif" alt> ，我们有<img src="Image01343.gif" alt> ，这里<img src="Image01344.gif" alt> 。对于排列<img src="Image01345.gif" alt> ，我们有<img src="Image01346.gif" alt> ，这里<img src="Image01344.gif" alt> 。对于查询<img src="Image01296.gif" alt> ，不失一般性，我们将其所对应的所有文档的真实排序记为<img src="Image01347.gif" alt> 。</p>
<h2 id="8-2-排序算法的评价"><a href="#8-2-排序算法的评价" class="headerlink" title="8.2 排序算法的评价"></a>8.2 排序算法的评价</h2><p>鉴于排序问题的特殊性，我们不能直接使用分类或者回归问题中的算法评价指标。在本节，我们介绍常用的评价排序算法的度量或者指标，包括MAP （mean average precision）、DCG （discounted cumulative gain）和NDCG （normalized discounted cumulative gain）。基本原理是：首先对于每个查询，我们比较排序模型返回的结果和真实结果，并计算相应的度量；在将这些度量对所有的查询平均后，我们得到最后的评价结果。下面依次详细介绍MAP、DCG和NDCG。</p>
<h3 id="8-2-1-MAP"><a href="#8-2-1-MAP" class="headerlink" title="8.2.1 MAP"></a>8.2.1 MAP</h3><p>MAP 是信息检索中常用的一个评价排序模型的度量。在MAP中，我们假设文档与查询的相关度为0或者1。计算MAP时，我们首先计算测试集中每个查询对应的平均精度<br>（average precision, AP），然后计算所有查询的AP的平均值作为最终的MAP。</p>
<p>首先，计算对于查询 _q i _ 所得的排序结果中前 _k_ 个文档的精度 （precision at _k_ ， _P@k_ ）：</p>
<p><img src="Image01348.gif" alt></p>
<p>（8-4）</p>
<p>这里<img src="Image01349.gif" alt> 是模型返回结果前 _k_ 个文档中与查询 _q i _ 真正相关的文档数目。</p>
<p>在<img src="Image01350.gif" alt> 的基础上，可以计算查询 _q i _ 所对应的平均精度 （average precision, AP），其定义为：</p>
<p><img src="Image01351.gif" alt></p>
<p>（8-5）</p>
<p>这里<img src="Image01352.gif" alt> 是对于查询 _q i _ 我们考察的文档总数（如排序算法为查询 _q i _ 所返回的文档总数），<img src="Image01326.gif" alt> 是其中与 _q i _ 相关联的文档总数。 _l k _ 表示模型所得到的排序结果中第 _k_ 位的文档是否与查询 _q i _ 相关。如果相关，<img src="Image01353.gif" alt> ，否则<img src="Image01354.gif" alt> 。换言之，我们只考虑返回结果中那些真正相关的文档所对应位置的<img src="Image01350.gif" alt> 。</p>
<p>最后，在为每个查询 _q i _ 计算<img src="Image01355.gif" alt> 后，我们计算所有<img src="Image01355.gif" alt> 的平均值，得到MAP：</p>
<p><img src="Image01356.gif" alt></p>
<p>（8-6）</p>
<p>下面我们使用一个实际例子来说明如何计算MAP。</p>
<p><strong>例8-1</strong> 在这个例子中，假设我们有一个查询，其对应8个文档，编号为1~8。我们假设有两个排序模型M1和M2，其排序结果分别为87654321和12345678。根据表8-2，其中与查询相关的文档为1、6和7。我们分别计算这两个模型对应的MAP。为了简化讨论，在这个例子中我们假设只有一个查询，因此只需要计算该查询对应的AP。</p>
<p>表8-2 计算MAP示例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>文 档 编 号</th>
<th>真实相关度</th>
<th>排序模型M1</th>
<th>排序模型M2  </th>
</tr>
</thead>
<tbody>
<tr>
<td>   1</td>
<td>1</td>
<td>8</td>
<td>1  </td>
</tr>
<tr>
<td>   2</td>
<td>0</td>
<td>7</td>
<td>2  </td>
</tr>
<tr>
<td>   3</td>
<td>0</td>
<td>6</td>
<td>3  </td>
</tr>
<tr>
<td>   4</td>
<td>0</td>
<td>5</td>
<td>4  </td>
</tr>
<tr>
<td>   5</td>
<td>0</td>
<td>4</td>
<td>5  </td>
</tr>
<tr>
<td>   6</td>
<td>1</td>
<td>3</td>
<td>6  </td>
</tr>
<tr>
<td>   7</td>
<td>1</td>
<td>2</td>
<td>7  </td>
</tr>
<tr>
<td>   8</td>
<td>0</td>
<td>1</td>
<td>8  </td>
</tr>
</tbody>
</table>
</div>
<p>   表8-3展示了模型M1对应的AP的计算过程。在表8-3中，对于每个 _k_ ，首先计算对应的 _P@k_ ，然后计算对应的<img src="Image01357.gif" alt> ，最后将所有的<img src="Image01358.gif" alt> 累加得到：</p>
<p><img src="Image01359.gif" alt></p>
<p>注意，一共有3个文档相关，因此<img src="Image01360.gif" alt> ，则对应的 _AP_ 为：</p>
<p><img src="Image01361.gif" alt></p>
<p>表8-3 模型M1对应AP的计算过程</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>_k_</th>
<th>文档编号</th>
<th>_l k _</th>
<th>M1排序</th>
<th>_P_ @ _k_</th>
<th>_P_ @ _k×l k _  </th>
</tr>
</thead>
<tbody>
<tr>
<td>   1</td>
<td>8</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0  </td>
</tr>
<tr>
<td>   2</td>
<td>7</td>
<td>1</td>
<td>2</td>
<td>0.5</td>
<td>0.5  </td>
</tr>
<tr>
<td>   3</td>
<td>6</td>
<td>1</td>
<td>3</td>
<td>0.667</td>
<td>0.667  </td>
</tr>
<tr>
<td>   4</td>
<td>5</td>
<td>0</td>
<td>4</td>
<td>0.5</td>
<td>0  </td>
</tr>
<tr>
<td>   5</td>
<td>4</td>
<td>0</td>
<td>5</td>
<td>0.4</td>
<td>0  </td>
</tr>
<tr>
<td>   6</td>
<td>3</td>
<td>0</td>
<td>6</td>
<td>0.333</td>
<td>0  </td>
</tr>
<tr>
<td>   7</td>
<td>2</td>
<td>0</td>
<td>7</td>
<td>0.286</td>
<td>0  </td>
</tr>
<tr>
<td>   8</td>
<td>1</td>
<td>1</td>
<td>8</td>
<td>0.375</td>
<td>0.375  </td>
</tr>
</tbody>
</table>
</div>
<p>   相应地，表8-4展示了模型M2对应的MAP的计算过程。将所有的 _P_ @ _k_ × _l k _ 累加得到：</p>
<p><img src="Image01362.gif" alt></p>
<p>注意<img src="Image01360.gif" alt> ，因此对应的 _AP_ 为：</p>
<p><img src="Image01363.gif" alt></p>
<p>表8-4 模型M2对应AP的计算过程</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>_k_</th>
<th>文档编号</th>
<th>_l k _</th>
<th>M2排序</th>
<th>_P_ @ _k_</th>
<th>_P_ @ _k×l k _  </th>
</tr>
</thead>
<tbody>
<tr>
<td>   1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1  </td>
</tr>
<tr>
<td>   2</td>
<td>2</td>
<td>0</td>
<td>2</td>
<td>0.5</td>
<td>0  </td>
</tr>
<tr>
<td>   3</td>
<td>3</td>
<td>0</td>
<td>3</td>
<td>0.333</td>
<td>0  </td>
</tr>
<tr>
<td>   4</td>
<td>4</td>
<td>0</td>
<td>4</td>
<td>0.25</td>
<td>0  </td>
</tr>
<tr>
<td>   5</td>
<td>5</td>
<td>0</td>
<td>5</td>
<td>0.2</td>
<td>0  </td>
</tr>
<tr>
<td>   6</td>
<td>6</td>
<td>1</td>
<td>6</td>
<td>0.333</td>
<td>0.333  </td>
</tr>
<tr>
<td>   7</td>
<td>7</td>
<td>1</td>
<td>7</td>
<td>0.429</td>
<td>0.429  </td>
</tr>
<tr>
<td>   8</td>
<td>8</td>
<td>0</td>
<td>8</td>
<td>0.375</td>
<td>0  </td>
</tr>
</tbody>
</table>
</div>
<p>   从模型M1和M2的对比可以看出，虽然M1把第2、3位的文档都排对了，但是M2把最重要的第1位的文档选对了，因此M2对应的MAP更大一些。</p>
<h3 id="8-2-2-DCG"><a href="#8-2-2-DCG" class="headerlink" title="8.2.2 DCG"></a>8.2.2 DCG</h3><p>在前面的MAP中，我们只考虑了返回的文档是否与查询相关。在实际问题中，我们有时需要考虑不同的相关度。在前面那个关于expedia.com的例子中，我们就考虑了多种情况。</p>
<p>（1）用户点击了返回的旅馆链接并预定了旅馆；</p>
<p>（2）用户仅点击但是并没有预定旅馆；</p>
<p>（3）用户没有点击旅馆链接并且没有预定旅馆。</p>
<p>在这个例子中，对于不同的情况，我们应该给予不同的权值。例如，对于情况1，我们可以给予5分，对于情况2，我们可以给1分，对于情况3，我们可以给0分。此外，我们还要根据上述几种情况在返回结果中的位置给予不同的权重。DCG综合考虑了返回的序列中每个文档与查询的相关度，并对位置信息直接予以考虑。</p>
<p>与MAP类似，在计算DCG时，首先计算每个查询对应的DCG，然后对所有查询求平均。假设对于查询 _q i _ 模型返回的排列结果是<img src="Image01364.gif" alt> ，那么位置 _k_ 的DCG定义为：</p>
<p><img src="Image01365.gif" alt></p>
<p>（8-7）</p>
<p>这里我们顺次考虑排序结果<img src="Image01364.gif" alt> 中的第<img src="Image01366.gif" alt> 个文档，<img src="Image01367.gif" alt> 表示在排列<img src="Image01347.gif" alt> 中排在第 _j_ 位的文档，<img src="Image01368.gif" alt> 表示该文档对应的权值，而<img src="Image01369.gif" alt> 表示由位置 _j_ 决定的权值。对于<img src="Image01368.gif" alt> ，我们可以根据实际问题的性质给予不同的权值。一种常用的关于<img src="Image01368.gif" alt> 的计算公式为：</p>
<p><img src="Image01370.gif" alt></p>
<p>（8-8）</p>
<p>这里<img src="Image01371.gif" alt> 表示排列<img src="Image01347.gif" alt> 中排在第 _j_ 位的文档与查询<img src="Image01296.gif" alt> 的相关度。例如，如果相关，则<img src="Image01372.gif" alt> ，否则为0。此外，<img src="Image01371.gif" alt> 也可以取其他值，例如，在实际问题中可以根据文档与查询不同的相关度，设定取值范围为{3,2,1,0}。在前面的关于expedia.com的例子中，取值范围为{5,1,0}。</p>
<p>下面是一个常见的<img src="Image01369.gif" alt> 计算公式：</p>
<p><img src="Image01373.gif" alt></p>
<p>（8-9）</p>
<p><strong>例8-2</strong> 在表8-5所示的这个例子中，我们使用前面一个例子中的M1模型的输出来计算 _DCG@k_ 。这里我们假设<img src="Image01374.gif" alt> 或者0，<img src="Image01369.gif" alt> 的计算使用式（8-9）。</p>
<p>表8-5 模型M1对应DCG的计算过程</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>_k_</th>
<th>文档编号</th>
<th><img src="Image01375.gif" alt></th>
<th>M1排序</th>
<th><img src="Image01376.gif" alt></th>
<th>_η_ ( _j_ )</th>
<th>_DCG_ @ _k_  </th>
</tr>
</thead>
<tbody>
<tr>
<td>   1</td>
<td>8</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1.000</td>
<td>0  </td>
</tr>
<tr>
<td>   2</td>
<td>7</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>0.6309</td>
<td>0.6309  </td>
</tr>
<tr>
<td>   3</td>
<td>6</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>0.5</td>
<td>1.1309  </td>
</tr>
<tr>
<td>   4</td>
<td>5</td>
<td>0</td>
<td>4</td>
<td>0</td>
<td>0.4307</td>
<td>1.1309  </td>
</tr>
<tr>
<td>   5</td>
<td>4</td>
<td>0</td>
<td>5</td>
<td>0</td>
<td>0.3869</td>
<td>1.1309  </td>
</tr>
<tr>
<td>   6</td>
<td>3</td>
<td>0</td>
<td>6</td>
<td>0</td>
<td>0.3562</td>
<td>1.1309  </td>
</tr>
<tr>
<td>   7</td>
<td>2</td>
<td>0</td>
<td>7</td>
<td>0</td>
<td>0.3333</td>
<td>1.1309  </td>
</tr>
<tr>
<td>   8</td>
<td>1</td>
<td>1</td>
<td>8</td>
<td>1</td>
<td>0.3155</td>
<td>1.4464  </td>
</tr>
</tbody>
</table>
</div>
<p>   在这个例子中，如果我们考虑<img src="Image01377.gif" alt> ，则有 _DCG_ @8=1.4464。在我们提供的R程序ranking_metrics.R中，也提供了计算DCG的函数。</p>
<h3 id="8-2-3-NDCG"><a href="#8-2-3-NDCG" class="headerlink" title="8.2.3 NDCG"></a>8.2.3 NDCG</h3><p>在DCG的基础上，可以将其标准化到0和1之间，成为NDCG。NDCG是目前在排序学习中应用非常广泛的一种度量。与MAP和DCG类似，我们也是先计算每个查询<img src="Image01296.gif" alt> 对应的NDCG，然后对所有的查询求平均值得到最后的结果。在计算关于查询<img src="Image01296.gif" alt> 的 _NDCG@k_ 时，核心是先计算 _DCG@k_ ，然后寻找使得 _DCG@k_ 最大的排列，计算相应的 _DCG@k_ ，记为<img src="Image01378.gif" alt> ，那么NDCG的定义为：</p>
<p><img src="Image01379.gif" alt></p>
<p>（8-10）</p>
<p>根据NDCG的定义，我们知道其值在0和1之间。</p>
<p><strong>例8-3</strong> 在下面的这个例子中，我们仍旧使用前面一个例子中的M1模型的输出来计算 _DCG@k_ 。我们只需要计算最好的排序结果对应的 _DCG@k_ 即可。最好的排序结果对应的相关度序列应该为1,1,1,0,0,0,0,0。其对应的 _DCG_ @8=2.1309。因此，用M1模型的输出来计算 _NDCG@_ 8的结果如下：</p>
<p><img src="Image01380.gif" alt></p>
<p>本书附属的R程序ranking_metrics.R中也提供了直接计算NDCG的函数。</p>
<h3 id="8-2-4-讨论"><a href="#8-2-4-讨论" class="headerlink" title="8.2.4 讨论"></a>8.2.4 讨论</h3><p>从上面讨论的若干排序算法评价度量来看，它们基本上满足如下性质。</p>
<p>（1）基本上所有的指标都是先对每个查询计算，然后对测试集中所有的查询进行平均，进而得到最终结果。因此，每个查询都被平等对待。</p>
<p>（2）所有这些指标都是基于位置的。换言之，就是在计算度量时考虑了返回结果中文档在不同位置的权重。</p>
<p>注意，在排序学习中，所有算法评价指标都是基于位置的。但是从数学上来讲，这些评价指标都是很难直接优化的。这里我们考虑如下情况：假设我们的排序模型 _f_ 返回了两个文档<img src="Image01381.gif" alt> 、<img src="Image00437.gif" alt> 及其对应的分数<img src="Image01382.gif" alt> 、<img src="Image01383.gif" alt> ，这里<img src="Image01384.gif" alt> &gt;<img src="Image01385.gif" alt> ，<img src="Image01386.gif" alt> 、<img src="Image01387.gif" alt> 分别是文档<img src="Image01381.gif" alt> 、<img src="Image01388.gif" alt> 对应的特征。我们将<img src="Image01385.gif" alt> 逐渐增大，但是在<img src="Image01385.gif" alt> 大于<img src="Image01384.gif" alt> 之前，排序模型的返回结果是一样的。也就是说，在<img src="Image01385.gif" alt> 增大到<img src="Image01384.gif" alt> 的过程中，算法的性能指标没有任何变化。但是一旦<img src="Image01389.gif" alt> &gt;<img src="Image01390.gif" alt> ，算法的性能指标会发生很大变化。这意味着如果我们直接最小化这些度量的话，这些度量都是不连续且不可导的。而我们在机器学习中常用的策略是通过计算导数来最小化目标函数从而求出最优解。因此，在排序学习中，我们需要新的办法来最优化算法的性能。在下面的章节中，我们将分别介绍逐点方法、逐对方法和逐列方法，并重点讨论较为常用的逐对方法以及在R中的实际使用。</p>
<h2 id="8-3-逐点方法"><a href="#8-3-逐点方法" class="headerlink" title="8.3 逐点方法"></a>8.3 逐点方法</h2><p>逐点方法 是解决排序问题最简单直接的方法。在逐点方法中，我们将排序问题转化为回归或者分类问题。这样前面讨论的那些适用于回归或者分类的算法就可以直接使用了。按照转化问题的类型，逐点方法可以进一步分为以下3种。</p>
<p>（1）基于回归 的方法；</p>
<p>（2）基于分类 的方法；</p>
<p>（3）基于有序回归 的方法。</p>
<p>在基于回归的方法中，我们直接将每个查询-文档对<img src="Image01293.gif" alt> 对应的相关度<img src="Image01321.gif" alt> 作为实数型的输出来构建模型。在基于分类的方法中，我们将<img src="Image01321.gif" alt> 的每个不同值视为一个不同的类标<br>（class label），从而构建一个分类模型。注意，在此方法中，我们并不考虑<img src="Image01321.gif" alt> 之间的顺序关系。在基于有序回归的方法中，我们将<img src="Image01321.gif" alt> 看成不同的类，但是类之间存在顺序关系。</p>
<p>将前面讨论的回归或者分类算法应用于排序问题时应注意以下两点。</p>
<p>（1）输入数据不同。我们需要显式地为每个查询-文档对<img src="Image01293.gif" alt> 构造特征<img src="Image01294.gif" alt> 。</p>
<p>（2）在排序问题中，为了得到最优性能，一般不能直接使用回归或者分类问题中使用的损失函数。我们需要更加适用于排序问题的损失函数。</p>
<p>在本节中，我们介绍两种基于SVM的逐点排序算法以帮助读者理解逐点方法的基本原理。在此基础上，我们进一步讨论逐点排序算法的优缺点。</p>
<p>在下面的讨论中，假设我们有 _m_ 个查询<img src="Image01322.gif" alt> 。对于查询<img src="Image01296.gif" alt> ，我们将训练集中与<img src="Image01296.gif" alt> 对应的文档集合记为<img src="Image01323.gif" alt> ，其相应的相关度记为<img src="Image01391.gif" alt> 。将查询-文档对<img src="Image01293.gif" alt> 构建的特征记为<img src="Image01392.gif" alt> 。</p>
<h3 id="8-3-1-基于SVM的逐点排序方法"><a href="#8-3-1-基于SVM的逐点排序方法" class="headerlink" title="8.3.1 基于SVM的逐点排序方法"></a>8.3.1 基于SVM的逐点排序方法</h3><p>在将SVM推广到逐点排序方法的过程中，有两种思路：第一种思路称为定间隔策略 （fixed margin strategy），第二种策略称为“间隔和”策略<br>（sum of margins strategy）。下面我们分别予以讨论。</p>
<p>在讨论新的算法之前，我们简单回顾一下标准的SVM算法。在标准的SVM中，我们要尽量使得每个样本被正确地分类。回忆一下在基本的SVM中，对于第 _i_ 个样本<img src="Image00730.gif" alt> 以及对应的类标<img src="Image01393.gif" alt> ，我们构建了如下的约束条件：</p>
<p><img src="Image01394.gif" alt></p>
<p>（8-11）</p>
<p>这里我们使用松弛变量<img src="Image01395.gif" alt> 来表示样本<img src="Image00730.gif" alt> 是否被正确分类以及被错误分类时错误的程度。注意，在这里我们稍稍“滥用”了一下符号，使用了<img src="Image00730.gif" alt> 、<img src="Image00540.gif" alt> 、<img src="Image00823.gif" alt> 、 _b_ 等。</p>
<p>在排序问题中，我们也可以采取类似的思路，为排序问题构建类似的约束条件。在Shashua和Levin提出的适用于排序问题的SVM算法[29] 中，需要求解如下的优化问题：</p>
<p><img src="Image01396.jpg" alt></p>
<p>（8-12）</p>
<p>这里我们假设相关度<img src="Image01321.gif" alt> 有 _r_ 个不同的值<img src="Image01397.gif" alt> ，值越大，相关度越高。与标准的SVM类似，我们在这里要求解一个线性函数<img src="Image01398.gif" alt> 。但是与标准SVM不同的是，在标准SVM中我们只有两个类；而在排序问题中我们有 _r_ 个类标<img src="Image01399.gif" alt> ，且它们之间存在偏序关系。为了处理该偏序关系，我们引入了 _r_<br>-1个不同的截距<img src="Image01400.gif" alt> ，对应 _r_<br>-1个平行的超平面<img src="Image01401.gif" alt> 。在排序问题中，我们将整个输入空间用 _r_ -1个平行的超平面来划分，使得类标为 _k_ 的查询-文档对的特征<img src="Image01294.gif" alt> 正好被映射到<img src="Image01402.gif" alt> 和<img src="Image01403.gif" alt> 之间。类似地，我们引入松弛变量<img src="Image01404.gif" alt> 和<img src="Image01405.gif" alt> 来处理不能被映射到指定区间所引起的惩罚。此外，与标准的SVM类似，我们也用<img src="Image01406.gif" alt> 来控制模型的复杂度。将这些综合起来，我们使用<img src="Image01406.gif" alt> 和所有松弛变量的和作为最终的目标函数。</p>
<p>第二种策略称为“间隔和”策略。在这种策略中，我们为每个类增加一个新变量<img src="Image01407.gif" alt> ，这样使得类标为 _k_ 的查询-文档对的特征<img src="Image01294.gif" alt> 被映射到超平面<img src="Image01408.gif" alt><br>（下界）和超平面<img src="Image01409.gif" alt><br>（上界）之间，如图8-2所示。</p>
<p><img src="Image01410.jpg" alt></p>
<p>图8-2 “间隔和”策略原理</p>
<p>我们希望<img src="Image01294.gif" alt> 在映射之后，不同类对应的空间之间的间隔尽量大。例如，在图8-2中，我们希望Class 1和Class 2在投影之后的间隔<img src="Image01411.gif" alt> 尽量大。一般来讲，我们要求<img src="Image01412.gif" alt> 的和最小（这就是该方法称为“间隔和”策略的原因）。另外，我们也引入松弛变量以处理那些不能被正确分类的样本。因此，在如下的优化问题中，我们要最小化所有<img src="Image01412.gif" alt> 以及所有松弛变量的和：</p>
<p><img src="Image01413.jpg" alt></p>
<p>（8-13）</p>
<h3 id="8-3-2-逐点方法讨论"><a href="#8-3-2-逐点方法讨论" class="headerlink" title="8.3.2 逐点方法讨论"></a>8.3.2 逐点方法讨论</h3><p>事实上，逐点方法 可以认为是分类或者回归问题在排序问题上的简单推广。当类别数<img src="Image01414.gif" alt> 时，可以将逐点方法直接转化为分类问题来解决。另外，逐点方法没有更多地挖掘排序问题本身的特性。例如，在逐点方法中，我们忽视了同一查询<img src="Image01296.gif" alt> 所对应的多个文档<img src="Image01338.gif" alt> 之间的关系。</p>
<p>事实上，在排序问题中，我们更关注的是文档之间的相互位置而不是准确估计文档和查询之间的相关度。当然，如果我们能够准确估计相关度，也能得到较好的排序结果；但是，从解决问题的角度讲，这要求我们首先解决一个更难的问题，然后利用得到的答案构建排序结果。在实际中，我们一般青睐更直接的方法。</p>
<p>在前面讨论的诸多评价排序算法性能的标准中，基本上所有的标准都和查询（query）以及返回文档对应的排序位置<br>（position）直接相关。这些标准都是先对每个查询计算，然后对所有的查询计算平均值。在对每个查询计算时，我们都考虑了返回的文档序列中每个文档的位置。因此，在解决排序问题时，有以下两个核心思想：（1）直接考虑排序结果中每个文档的位置信息，特别是前面几个位置；（2）直接考虑查询，包括每个查询所对应的文档以及它们之间的相互关系。</p>
<p>在逐点方法中，我们没有在查询层次考虑模型优化。具体而言，就是对于每个查询，没有直接考虑和它相关的所有文档之间的相互关系。在逐点方法中，我们将所有的查询和相应的文档放在一起考虑。因此，如果有的查询对应很多的文档，则这样的查询就会在最终模型中造成很大的影响，而那些只对应较少文档的查询则被忽视。此外，我们没有直接考虑文档在返回结果中的位置。虽然我们使用了相关度分数来引入位置信息，但毕竟不是最直接的方式。</p>
<p>考虑到排序问题的特殊性，逐点方法并不是一种理想的方法。事实上，在实际中，逐点方法的使用也不多。因此，本节的主要目的是通过介绍两种SVM的变体来说明逐点方法的基本思想。</p>
<p>为了弥补逐点方法的不足，下面我们介绍逐对和逐列方法。在逐对方法中，我们显式地考虑了每个查询对应的文档两两之间的关系。在逐列方法中，我们显式地考虑了排序算法返回的整个排序序列。由于这两种方法直接考虑了每个查询以及对应文档之间的关系，因此能够获得更好的效果。</p>
<h2 id="8-4-逐对方法"><a href="#8-4-逐对方法" class="headerlink" title="8.4 逐对方法"></a>8.4 逐对方法</h2><p>在逐对方法 中，我们关注的是文档之间的相对排序：对于给定的文档<img src="Image01296.gif" alt> ，如果我们有两个文档，到底哪个文档更加相关一些？在理想情况下，如果我们对于每个这样的问题都回答正确，就得到了一个完美的排序模型。</p>
<p>因此，我们可以很自然地将排序问题转化为分类问题：对于给定的查询<img src="Image01296.gif" alt> 和对应文档对<img src="Image01415.gif" alt> ，如果文档<img src="Image01416.gif" alt> 比<img src="Image01417.gif" alt> 更相关，则可以视为正类；否则视为负类。这样就可以利用分类算法来求解。但是要注意输入数据不是一个文档，而是一对文档。此外，与传统的分类算法类似，我们也可以在解决新的分类问题时选取不同的损失函数，如Hinge损失函数 、指数损失函数 、交叉熵损失函数 等。</p>
<p>当我们将排序问题转化为输入数据是一对文档的分类问题时，很多算法都将最小化被错误分类的文档对的总数作为优化目标。但是，要注意最小化被错分的文档对的数目并不等于优化排序问题中的一些评价指标，如NDCG。在本节中，我们介绍在处理逐对数据时直接优化排序算法评价指标的算法，如LambdaRank和LambdaMART算法。</p>
<p>在实践中，逐对算法是较常用和有效的排序算法，因此也是本章我们讨论的重点。此外，在实际中我们更容易得到适用于逐对方法的训练数据。事实上，对于真实的排序结果 ，很多时候我们并不知道一个完整的排序结果序列。在很多问题中训练数据以如下形式表示：对于查询<img src="Image01296.gif" alt> ，我们知道文档<img src="Image01416.gif" alt> 应该排在文档<img src="Image01417.gif" alt> 之前。</p>
<p>本节首先介绍SVM的两种变形算法，并阐述逐对方法的一些基本原理，然后介绍基于人工神经网络的RankNet算法以及它的两种变形：LambdaRank和LambdaMART。其中LambdaMART算法在多次数据挖掘竞赛中都取得了非常优秀的成绩，因此在本节中我们将予以重点介绍。</p>
<h3 id="8-4-1-Ranking-SVM算法"><a href="#8-4-1-Ranking-SVM算法" class="headerlink" title="8.4.1 Ranking SVM算法"></a>8.4.1 Ranking SVM算法</h3><p>在Ranking SVM算法中，我们将标准的SVM算法推广到逐对方法中。假设对于查询<img src="Image01296.gif" alt> ，我们把它对应的任意文档对<img src="Image01415.gif" alt> 所对应的特征对记为<img src="Image01418.gif" alt> ，并把对应的类标记为<img src="Image01419.gif" alt> 。如果文档<img src="Image01416.gif" alt> 比文档<img src="Image01417.gif" alt> 与查询<img src="Image01296.gif" alt> 更相关，即<img src="Image01420.gif" alt> ，则<img src="Image01421.gif" alt><br>；如果<img src="Image01422.gif" alt> ，则<img src="Image01423.gif" alt> 。</p>
<p>在Ranking SVM中，假设模型表示为<img src="Image01424.gif" alt> ，则我们要求解如下的优化问题：</p>
<p><img src="Image01425.jpg" alt></p>
<p>（8-14）</p>
<p>与前面将标准SVM推广到逐点方法中类似，我们通过构建约束条件来为每对文档分类。对于查询<img src="Image01296.gif" alt> 和相应的文档对所对应的特征向量<img src="Image01418.gif" alt> ，如果<img src="Image01416.gif" alt> 比<img src="Image01417.gif" alt> 与<img src="Image01296.gif" alt> 更相关，我们希望投影后<img src="Image01426.gif" alt> 比<img src="Image01427.gif" alt> 更大。类似地，我们也引入了松弛变量<img src="Image01428.gif" alt> 。但要注意，这里我们使用的也是Hinge损失函数。根据前面的约束条件，假设<img src="Image01421.gif" alt> ，即<img src="Image01429.gif" alt> 比<img src="Image01430.gif" alt> 与<img src="Image01296.gif" alt> 更相关。如果<img src="Image01426.gif" alt> 与<img src="Image01427.gif" alt> 的差大于或等于1，则惩罚项为0；否则，计算<img src="Image01431.gif" alt> 作为惩罚项。最终的目标函数是所有松弛变量和<img src="Image01406.gif" alt> （该项对应着模型的复杂度）的和。注意，在约束条件中，我们只考虑了满足<img src="Image01334.gif" alt> 的文档对，以避免重复考虑同一个文档对。</p>
<h3 id="8-4-2-IR-SVM算法"><a href="#8-4-2-IR-SVM算法" class="headerlink" title="8.4.2 IR-SVM算法"></a>8.4.2 IR-SVM算法</h3><p>在讨论逐点算法的缺点时，我们提到如果查询对应的文档集是不平衡的话，会导致那些文档数更多的查询更多地影响算法。在极端的情况下，很多对应少量文档的查询可能对所得的模型几乎没有影响，而模型的参数更多是由那些对应文档数多的查询决定。在逐对方法中，这个问题同样存在，而且还进一步恶化。在Ranking SVM中，对于查询<img src="Image01296.gif" alt> 有<img src="Image01324.gif" alt> 个不同的文档，那么满足条件<img src="Image01432.gif" alt><br>(即<img src="Image01421.gif" alt> )的文档对<img src="Image01433.gif" alt> 的数目的数量级为<img src="Image01434.gif" alt> 。在这种情况下，不平衡的情况会更加恶化，使得逐对方法中模型的学习会更加依赖那些对应文档数较多的查询。</p>
<p>为了解决查询间的不平衡问题，一种解决方案就是在求解优化问题时，将所得的经验损失函数对每个查询贡献的部分进行修正，使得经验损失函数不被那些对应文档数目较多的查询所支配。简而言之，就是对损失函数进行查询级别的标准化<br>（query level normalization）。具体而言，就是将损失函数中每个查询对应的项除以该查询所对应的文档对的数目。这样，我们就可以在Ranking SVM的基础上得到新的算法，称为IR-SVM。将查询<img src="Image01296.gif" alt> 所有满足条件<img src="Image01432.gif" alt> 的文档对<img src="Image01433.gif" alt> 的数目记为<img src="Image01435.gif" alt> ，在IR-SVM中，我们求解的优化问题如下所示：</p>
<p><img src="Image01436.jpg" alt></p>
<p>（8-15）</p>
<p>大量实验结果表明，IR-SVM性能比Ranking SVM有了显著提高，这就说明了在查询层次进行标准化处理的重要性。</p>
<h3 id="8-4-3-RankNet算法"><a href="#8-4-3-RankNet算法" class="headerlink" title="8.4.3 RankNet算法"></a>8.4.3 RankNet算法</h3><p>RankNet算法[30] 是较早在工业界被实际部署的算法之一。在RankNet算法中，我们考虑所有的文档对，并使用交叉熵 （cross entropy）损失函数来度量损失以最小化被错分的文档对数目。为了最小化交叉熵损失函数，RankNet构建了一个人工神经网络，并使用随机梯度下降 算法来求解其中的参数。下面我们详细介绍RankNet算法，并介绍其对应的损失函数。由于本书没有涉及人工神经网络相关的讨论，因此省略该算法的具体实现，主要讨论RankNet的原理及其中的关键步骤，即求解导数。在RankNet算法的基础上，我们进一步讨论LambdaRank和LambdaMART算法及其具体实现，并介绍在R中如何使用LambdaMART算法。</p>
<p>对于查询<img src="Image01296.gif" alt> 以及给定的文档对<img src="Image01415.gif" alt> ，我们用<img src="Image01437.gif" alt> 表示在真实情况中，查询<img src="Image01296.gif" alt> 对应的文档<img src="Image01416.gif" alt> 排在<img src="Image01417.gif" alt> 之前的概率：</p>
<p><img src="Image01438.jpg" alt></p>
<p>（8-16）</p>
<p>与前面讨论的算法类似，我们使用类标<img src="Image01419.gif" alt> 来表示对于查询<img src="Image01296.gif" alt> ，文档<img src="Image01416.gif" alt> 和<img src="Image01417.gif" alt> 之间的真实排序关系。</p>
<p>根据<img src="Image01437.gif" alt> 和<img src="Image01419.gif" alt> 的定义，有：</p>
<p><img src="Image01439.gif" alt></p>
<p>（8-17）</p>
<p>这里我们将排序模型记为 _f_ ，模型参数记为 <strong>_w_ </strong> 。在RankNet算法中，采用的模型是人工神经网络， <strong>_w_ </strong> 就是人工神经网络的参数。这里为了书写方便，我们将模型 _f_ 对于<img src="Image01440.gif" alt> 的输出<img src="Image01441.gif" alt> 记为<img src="Image01442.gif" alt> ，即<img src="Image01443.gif" alt> 。利用<img src="Image01441.gif" alt> 和<img src="Image01444.gif" alt> ，我们可以定义在给定模型 _f_ 的情况下，对于文档对<img src="Image01415.gif" alt> ，文档<img src="Image01416.gif" alt> 排在文档<img src="Image01417.gif" alt> 之前的概率<img src="Image01445.gif" alt> ：</p>
<p><img src="Image01446.gif" alt></p>
<p>（8-18）</p>
<p>这里我们使用了sigmoid函数<img src="Image01447.gif" alt> 来将<img src="Image01448.gif" alt> 转化为概率，其中<img src="Image00213.gif" alt> 是控制参数。这里概率的定义与逻辑回归类似。</p>
<p>根据前面的定义，我们可以将交叉熵 （cross-entropy）损失函数表示为：</p>
<p><img src="Image01449.gif" alt></p>
<p>（8-19）</p>
<p>注意，这里我们只考虑了<img src="Image01450.gif" alt> 、<img src="Image01416.gif" alt> 、<img src="Image01417.gif" alt> 。如果我们考虑了训练集中所有的 _i_ 、 _u_ 、 _v_ 的组合，则得到整个训练集对应的交叉熵损失函数。</p>
<p>接下来我们要将函数 _f_ 代入损失函数中并简化。不失一般性，这里我们将<img src="Image01445.gif" alt> 写为<img src="Image01451.gif" alt> 。首先我们注意到</p>
<p><img src="Image01452.gif" alt></p>
<p>（8-20）</p>
<p><img src="Image01453.gif" alt></p>
<p>（8-21）</p>
<p>我们将<img src="Image01454.gif" alt> 简记为<img src="Image01455.gif" alt> ，并简化如下：</p>
<p><img src="Image01456.gif" alt></p>
<p><img src="Image01457.gif" alt></p>
<p><img src="Image01458.gif" alt></p>
<p><img src="Image01459.gif" alt></p>
<p>（8-22）</p>
<p>注意，当<img src="Image01460.gif" alt><br>（即<img src="Image01432.gif" alt> ）时，有：</p>
<p><img src="Image01461.gif" alt></p>
<p>（8-23）</p>
<p>这样我们得到了<img src="Image01462.gif" alt> 关于<img src="Image01442.gif" alt> 、<img src="Image01463.gif" alt> 的表达式。根据该公式，我们可以计算<img src="Image01464.gif" alt> 关于<img src="Image01442.gif" alt> 的偏导数<img src="Image01465.gif" alt> 。事实上，在RankNet算法中，我们使用随机梯度下降算法来求解参数 <strong>_w_ </strong> 。在推荐算法中，我们已经介绍了随机梯度算法的基本原理，这里我们直接予以应用。</p>
<p>在使用随机梯度下降算法求解模型参数 <strong>_w_ </strong> 时，我们需要计算<img src="Image01466.gif" alt> ，即损失函数对于参数 <strong>_w_ </strong> 的偏导数。利用该偏导数<img src="Image01466.gif" alt> ，我们可以逐步更新模型的参数 <strong>_w_ </strong> 。在RankNet算法中，我们使用人工神经网络来构建模型 _f_ ，因此我们知道参数 <strong>_w_ </strong> 和模型输出<img src="Image01443.gif" alt> 的关系，且偏导数<img src="Image01467.gif" alt> 也是比较容易计算的。这样我们可以利用链式规则得到<img src="Image01466.gif" alt> 。在本书中，我们省略了关于人工神经网络的讨论，因此这里也不深入讨论<img src="Image01467.gif" alt> 的计算了。我们主要讨论如何计算偏导数<img src="Image01465.gif" alt> 和<img src="Image01468.gif" alt> ，其具体计算过程如下：</p>
<p><img src="Image01469.gif" alt></p>
<p>（8-24）</p>
<p>类似地，可以得到：</p>
<p><img src="Image01470.gif" alt></p>
<p>（8-25）</p>
<p>因此，</p>
<p><img src="Image01471.gif" alt></p>
<p>（8-26）</p>
<p>从式（8-26）可以看出，当我们利用文档对<img src="Image01432.gif" alt> 来计算导数时，<img src="Image01465.gif" alt> 与<img src="Image01468.gif" alt> 互为相反数。</p>
<p>利用链式规则，可以计算<img src="Image01464.gif" alt> 对模型参数 <strong>_w_ </strong> 的偏导数：</p>
<p><img src="Image01472.gif" alt></p>
<p><img src="Image01473.gif" alt></p>
<p>（8-27）</p>
<p>这里<img src="Image01474.gif" alt> 定义为：</p>
<p><img src="Image01475.gif" alt></p>
<p>（8-28）</p>
<p>因此，对于<img src="Image01462.gif" alt> ，在随机梯度下降算法中，我们可使用如下公式来更新模型的参数 <strong>_w_
</strong> ：</p>
<p><img src="Image01476.gif" alt></p>
<p>（8-29）</p>
<p>这里 <img src="Image01477.gif" alt> &gt;0是随机梯度下降算法中的学习率（learning rate）。</p>
<h4 id="RankNet算法的Mini-batch实现"><a href="#RankNet算法的Mini-batch实现" class="headerlink" title="RankNet算法的Mini-batch实现"></a>RankNet算法的Mini-batch实现</h4><p>上面的随机梯度下降算法对于训练集中每个查询对应的每个文档对都要更新一次模型参数 <strong>_w_ </strong> ，在实际应用中效率很低。为了进一步提高算法的效率，RankNet的作者提出了Mini-batch算法 。其基本思想是将每个查询<img src="Image01296.gif" alt> 对应的所有文档对放在一起考虑，一并更新模型的参数 <strong>_w_ </strong> 。</p>
<p>对于训练集中的所有文档对，我们假设第一项应该排在第二项之前，这样我们在讨论<img src="Image01478.gif" alt> 时都有<img src="Image01432.gif" alt> 。对参数 <strong>_w_ </strong> 的更新公式可以写为：</p>
<p><img src="Image01479.gif" alt></p>
<p>（8-30）</p>
<p>接下来考虑简化累加项。考虑累加项中涉及<img src="Image01480.gif" alt> 的项，分为两种情况：（1）文档<img src="Image01416.gif" alt> 排在另一文档之前；（2）文档<img src="Image01416.gif" alt> 排在另一文档之后。在第一种情况中，如果文档<img src="Image01416.gif" alt> 排在另一文档<img src="Image01417.gif" alt> 之前，其和为：</p>
<p><img src="Image01481.gif" alt></p>
<p>（8-31）</p>
<p>在第二种情况中，如果文档<img src="Image01482.gif" alt> 排在另一文档<img src="Image01483.gif" alt> 之后，其和为：</p>
<p><img src="Image01484.gif" alt></p>
<p>（8-32）</p>
<p>将最后的累加中所有涉及<img src="Image01474.gif" alt> 的项记为<img src="Image01485.gif" alt> ：</p>
<p><img src="Image01486.gif" alt></p>
<p>（8-33）</p>
<p>对参数 <strong>_w_ </strong> 的更新公式可以简写为：</p>
<p><img src="Image01487.gif" alt></p>
<p>（8-34）</p>
<p>利用式（8-34），我们对于查询<img src="Image01450.gif" alt> 所对应的所有文档集合<img src="Image01325.gif" alt> 中的每个文档<img src="Image01416.gif" alt> 计算相应的<img src="Image01485.gif" alt> ，并计算<img src="Image01488.gif" alt> ，之后只需要更新参数 <strong>_w_ </strong> 一次即可。注意，这里的<img src="Image01467.gif" alt> 与具体的模型有关系，但是一般来讲较容易计算。这样的话，对于每个查询，我们在计算累加项后只需要更新一次参数即可。在RankNet中，由于每次更新人工神经网络的参数比较耗费时间，因此这种Mini-batch的随机梯度下降算法在实际计算中能够显著提高计算速度。在原始的随机梯度下降算法中，如果对于每个文档对更新一下模型参数，则训练时间是每个查询对应文档总数的二次函数；而使用Mini-batch算法之后，时间复杂度大致降为每个查询对应文档总数的线性函数。同时，Mini-batch算法也是LambdaRank算法 的基础，下文会详细讨论。</p>
<h3 id="8-4-4-LambdaRank算法"><a href="#8-4-4-LambdaRank算法" class="headerlink" title="8.4.4 LambdaRank算法"></a>8.4.4 LambdaRank算法</h3><p>在RankNet算法中，我们直接优化了被错分的文档对的数目（严格地讲，只是被错分的文档对的数目的一个近似）。而在实际工作中，往往需要直接优化我们前面讨论的那些适用于排序问题的评价指标，如NDCG。事实上，直接最小化被错分的文档对的数目并不等于优化对应的指标。下面我们以NDCG为例，给出一个具体的例子。</p>
<p>在这个例子中，我们给出了一个查询的两个不同的排序结果，如图8-3所示。在这个例子中，返回的结果中一共有16个文档。其中，深灰色代表对应的文档是真正和查询相关的，而浅灰色的则代表事实上和查询不相关的文档。在图8-3（a）所示的排序结果中，第1个和第15个文档是和查询真正相关的，因此被错分的文档对的数目是13；在图8-3（b）所示的排序结果中，第4个和第10个文档是和查询真正相关的，因此被错分的文档对的数目是11。但通过简单计算NDCG可以知道，这两个排序结果对应的NDCG是不同的。在NDCG中，我们更加重视返回的文档序列中最前面的结果，因此图8-3（a）的例子对应的NDCG更高。从直观上讲，在实际中我们更希望排序算法返回的最前面的结果是与用户查询相关的，因此这个例子也显示了直接优化被错分的文档对的数目在实际中并不一定是最优的选择。</p>
<p><img src="Image01489.jpg" alt></p>
<p>图8-3 不同排序算法评价标准的比较</p>
<p>为了直接、有效地优化NDCG，在RankNet算法的基础上，原作者又提出了LambdaRank 算法[31] 。注意，在RankNet中采用随机梯度下降算法求解问题时，每一步只需要根据训练数据计算梯度值并更新模型参数。在LambdaRank 算法中，核心还是利用梯度信息来更新模型的参数，但是最重要的一点是直接在梯度中引入了NDCG相关的信息。</p>
<p>在前面的RankNet算法中，我们在计算梯度的过程中引入了<img src="Image01478.gif" alt> 。回忆一下，在RankNet算法中<img src="Image01478.gif" alt> 可以使用如下公式计算：</p>
<p><img src="Image01490.gif" alt></p>
<p>（8-35）</p>
<p>对于训练集中的所有文档对，我们假设第一项应该排在第二项之前，即<img src="Image01491.gif" alt> 。这样的话，我们直接可以得到<img src="Image01421.gif" alt> 。这样可以将<img src="Image01478.gif" alt> 的计算公式简写为：</p>
<p><img src="Image01492.gif" alt></p>
<p>（8-36）</p>
<p>在使用随机梯度算法求解RankNet算法时，我们可以将<img src="Image01474.gif" alt> 视为从文档对<img src="Image01432.gif" alt> 提取的有用信息。在实际中，人们发现，如果在上面的<img src="Image01478.gif" alt> 中简单乘以NDCG的变化量（记为<img src="Image01493.gif" alt> ），可以得到更好的排序结果。这里<img src="Image01494.gif" alt> 是我们将当前模型 _f_ 所得排序结果中文档<img src="Image01416.gif" alt> 和<img src="Image01417.gif" alt> 交换位置（保持其他文档的排序位置不变）所引起的NDCG的变化的绝对值。这样，在LambdaRank算法中，我们使用如下公式来计算新的<img src="Image01478.gif" alt> ，记为<img src="Image01495.gif" alt> ：</p>
<p><img src="Image01496.gif" alt></p>
<p>（8-37）</p>
<p>在<img src="Image01495.gif" alt> 的基础上，可以认为存在一个新的损失函数<img src="Image01497.gif" alt> ，使得</p>
<p><img src="Image01498.gif" alt></p>
<p>（8-38）</p>
<p>这里<img src="Image01499.gif" alt> 是<img src="Image01497.gif" alt> 对应于文档对<img src="Image01432.gif" alt> 的部分。由于在LambdaRank算法中我们仅仅改变了梯度的计算，因此仍然可以采用随机梯度下降算法来优化模型的参数。具体来说，就是使用如下公式来更新模型的参数<br><strong>_w_ </strong> ：</p>
<p><img src="Image01500.gif" alt></p>
<p>（8-39）</p>
<p>类似地，我们也可以推导出LambdaRank中的对应的Mini-batch算法。将文档<img src="Image01482.gif" alt> 所有对应的<img src="Image01501.gif" alt> 累加在一起，可以得到<img src="Image01502.gif" alt> ：</p>
<p><img src="Image01503.gif" alt></p>
<p><img src="Image01504.gif" alt></p>
<p><img src="Image01505.gif" alt></p>
<p>（8-40）</p>
<p>这样，使用<img src="Image01506.gif" alt> 就综合考虑了当查询为<img src="Image01296.gif" alt> 时所有涉及文档<img src="Image01416.gif" alt> 的文档对相关的信息。使用<img src="Image01502.gif" alt> 我们就得到了LambdaRank算法对应的Mini-batch算法。其中，参数 <strong>_w_ </strong> 的更新公式可以简写为：</p>
<p><img src="Image01507.gif" alt></p>
<p>（8-41）</p>
<h3 id="8-4-5-LambdaMART算法"><a href="#8-4-5-LambdaMART算法" class="headerlink" title="8.4.5 LambdaMART算法"></a>8.4.5 LambdaMART算法</h3><p>我们知道，LambdaRank算法基于RankNet，而LambdaMART算法[32] 可以认为是将梯度提升 算法和LambdaRank算法结合的产物。LambdaMART算法多次在机器学习竞赛中取得优异的成绩，如2010年的Yahoo! Learning to Rank Challenge，以及2013年由ICDM会议和Expedia公司共同组织的竞赛 ① ，获胜方案都使用了LambdaMART。</p>
<p>在LambdaMART算法中，我们利用LambdaRank算法中的梯度来构造对应的损失函数。在给定损失函数之后，我们使用提升决策树 （boosted tree），即一系列决策树的加权和作为最终模型。在提升决策树中，我们顺次构建多个决策树，希望新的决策树能够弥补已有的决策树的“不足”。在实现上，我们使用梯度提升算法来学习这些决策树。梯度提升算法的基本思想与数值优化中的梯度下降算法 类似。在梯度提升算法中，我们每次都根据已有的模型的预测结果，构建新的目标值以更好地分类那些“难分类”的样本。具体来说，对于样本 <strong>_x_ </strong> i ，在构建第 _j_ 个学习器时，我们用 _t ij _ 来表示对应的目标值。通过不停地改变 _t ij _ ，使得综合后的学习器能够更好地处理样本<br><strong>_x_ </strong> i 。在梯度提升算法中， _t ij _ 是损失函数关于当前模型预测值的负梯度。关于提升树和梯度提升算法的具体介绍可参见第9章相关内容。在第9章中，我们将详细介绍梯度提升算法。在本节，我们着重讨论如何计算梯度和步长。读者可以先阅读第9章的相关章节再阅读本节。</p>
<p>可以简单地进行理解，在LambdaMART算法中，对于查询<img src="Image01450.gif" alt> ，我们考虑的损失函数为：</p>
<p><img src="Image01508.gif" alt></p>
<p>（8-42）</p>
<p>式中<img src="Image01509.gif" alt> 定义为：</p>
<p><img src="Image01510.gif" alt></p>
<p>（8-43）</p>
<p>这里<img src="Image01511.gif" alt> 是我们将文档<img src="Image01416.gif" alt> 和<img src="Image01417.gif" alt> 交换位置（而其他文档位置不变）后所引起的评价指标的变化。例如，采用NDCG时，<img src="Image01512.gif" alt> 。此外，与前面的假设相同，对于训练集中的所有文档对，我们假设第一项应该排在第二项之前，即<img src="Image01432.gif" alt> 。</p>
<p>将所有的查询都考虑，则得到最终的损失函数：</p>
<p><img src="Image01513.gif" alt></p>
<p>（8-44）</p>
<p>可以看出，<img src="Image01514.gif" alt> 与前面讨论的RankNet算法和LambdaRank算法都有一定的相似之处。事实上，如果我们进一步计算<img src="Image01515.gif" alt> 对于当前评分<img src="Image01442.gif" alt> 的一阶导数和二阶导数的话，它们之间的联系会更加清楚。</p>
<p>首先我们计算<img src="Image01516.gif" alt> 对<img src="Image01517.gif" alt> 的偏导数：</p>
<p><img src="Image01518.gif" alt></p>
<p>（8-45）</p>
<p><img src="Image01519.gif" alt></p>
<p>（8-46）</p>
<p>这里记<img src="Image01520.gif" alt> 为：</p>
<p><img src="Image01521.gif" alt></p>
<p>（8-47）</p>
<p>接下来我们求解<img src="Image01514.gif" alt> 对于当前评分<img src="Image01442.gif" alt> 的一阶导数。注意，对于文档<img src="Image01416.gif" alt> ，我们要考虑在真实数据的文档对中，文档<img src="Image01416.gif" alt> 排在另一文档之前还是之后。这样，我们有：</p>
<p><img src="Image01522.gif" alt></p>
<p>（8-48）</p>
<p>比较<img src="Image01523.gif" alt> 和前面LambdaRank算法中的<img src="Image01502.gif" alt> ，将上式中的<img src="Image01524.gif" alt> 替换为<img src="Image01494.gif" alt> ，我们可以发现<img src="Image01525.gif" alt> 。换言之，在LambdaMART算法中，我们使用了与LambdaRank算法中类似的导数以处理不同的评价标准。这里我们也把<img src="Image01526.gif" alt> 记为<img src="Image01527.gif" alt> 。</p>
<p>在这里讨论的梯度提升算法中，我们使用牛顿近似（即二阶泰勒展式）来拟合函数<img src="Image01515.gif" alt> 以求步长从而加速收敛（具体原理可参见9.5.1节）。在二阶泰勒展式中，我们需要计算一阶导数<img src="Image01526.gif" alt> 和二阶导数<img src="Image01528.gif" alt> 。前面我们已经给出了计算<img src="Image01526.gif" alt> 的公式，下面给出计算<img src="Image01528.gif" alt> 的具体公式。我们首先计算<img src="Image01529.gif" alt> 对<img src="Image01442.gif" alt> 和<img src="Image01463.gif" alt> 的导数：</p>
<p><img src="Image01530.gif" alt></p>
<p>（8-49）</p>
<p><img src="Image01531.gif" alt></p>
<p>（8-50）</p>
<p>因此，可以计算二阶导数<img src="Image01532.gif" alt> 如下：</p>
<p><img src="Image01533.gif" alt></p>
<p><img src="Image01534.gif" alt></p>
<p>（8-51）</p>
<p>在得到一阶导数和二阶导数之后，在梯度提升算法中我们可以计算相应的步长。在构建的第 _k_ 棵决策树对应的第 _l_ 个叶结点时，考虑该结点对应的数据<img src="Image01440.gif" alt> ，我们可以使用如下公式来计算采用牛顿近似时的步长<img src="Image01535.gif" alt> ：</p>
<p><img src="Image01536.gif" alt></p>
<p>（8-52）</p>
<p>这里我们使用<img src="Image01537.gif" alt> 表示第 _k_ 棵决策树对应的第 _l_ 个叶结点所对应的数据的集合。</p>
<p>在LambdaMART 算法中，我们使用梯度提升算法来构建模型，具体步骤见算法8-1。</p>
<p>算法8-1 LambdaMART算法</p>
<blockquote>
<p>输入：决策树的总数<img src="Image01538.gif" alt> &gt; ，每棵树可允许的最大叶结点数<img src="Image01539.gif" alt> ，学习率 _η_ ，总查询数 _m_ &gt; &gt; 具体步骤： &gt; &gt; for all <img src="Image01429.gif" alt> do &gt; &gt; &gt; <img src="Image01540.gif" alt> &gt; &gt; end for &gt; &gt; for _k_ = 1:<img src="Image01541.gif" alt> do &gt; &gt; for all <img src="Image01429.gif" alt> do &gt; &gt; 计算梯度<img src="Image01542.gif" alt> ：<img src="Image01543.gif" alt> &gt; &gt; 将<img src="Image01542.gif" alt> 作为第 _i_ &gt; 个样本的新的目标值<img src="Image01544.gif" alt> &gt; &gt; 计算二阶导数<img src="Image01528.gif" alt> &gt; &gt; end for &gt; &gt; 根据训练数据<img src="Image01545.gif" alt> &gt; 构建一个有<img src="Image01539.gif" alt> 个叶结点的决策树 &gt; &gt; for _l_ = 1: _L_ max do &gt; &gt; 为第 _l_ 个叶结点计算牛顿步长<img src="Image01546.gif" alt> &gt; &gt; end for &gt; &gt; for all <img src="Image01429.gif" alt> do &gt; &gt; 更新函数 _f_ &gt; ：<img src="Image01547.gif" alt> &gt; &gt; end for &gt; &gt; end for</p>
</blockquote>
<p>在该算法中，我们首先建立一个基本模型<img src="Image01548.gif" alt> 。如果在实际中没有基本模型，可以直接设置<img src="Image01549.gif" alt> 。注意，算法中的 _I_ ()函数可参见式（9-51）。</p>
<p>这里我们再简单讨论一下LambdaRank算法和LambdaMART算法的区别。首先它们使用的模型不同。在LambdaRank算法中我们使用人工神经网络，而在LambdaMART算法中我们使用一系列决策树。其次，在具体的算法中，模型中参数更新的方式也不一样。在LambdaRank算法中，我们每次处理查询 _q i _ 对应的一个文档对或者所有文档对。根据这部分数据，我们使用随机梯度下降算法来更新模型中所有的参数。而在LambdaMART算法中，我们使用梯度提升算法通过生成一系列决策树来构建模型。在每棵新决策树的生成过程中，我们使用对应于每个结点的所有数据来决定下面的结点如何生成。因此，一般而言，在决策树的一个结点的进一步划分过程中，我们可以使用该结点所对应的文档对。并且在这里我们只是考虑当前决策树的当前结点如何进一步划分，而不用考虑其他决策树。</p>
<h4 id="gbm包和LambdaMART算法"><a href="#gbm包和LambdaMART算法" class="headerlink" title="gbm包和LambdaMART算法"></a>gbm包和LambdaMART算法</h4><p>在R中，<code>gbm</code> 包 ② 提供了LambdaMART算法的实现。我们会在第9章中详细介绍<code>gbm</code> 包的使用方法。这里我们简单讨论一下如何使用<code>gbm</code> 包中的LambdaMART算法。</p>
<p>使用<code>gbm</code> 包时，我们需要指定如下参数：</p>
<ul>
<li>损失函数（参数<code>distribution</code> ）；</li>
<li>决策树的数目（参数<code>n.trees</code> ）；</li>
<li>决策树的内结点的数目（参数<code>interaction.depth</code> ）。注意，指定每棵决策树中内结点的数目时，也决定了叶结点的最大数目；</li>
<li>学习率（参数<code>shrinkage</code> ）。</li>
</ul>
<p>上面的参数是最重要的几个参数。除此之外，还有如下参数。</p>
<ul>
<li>子取样 的比例（参数<code>bag.fraction</code> ）。</li>
<li>划分结点时要求新的叶结点所拥有的最少样本数（参数<code>n.minobsinnode</code> ）。</li>
<li>训练时使用的数据比例（参数<code>train.fraction</code> ）。</li>
</ul>
<p>在<code>gbm</code> 包中，我们需要通过设置参数<code>&#39;distribution&#39;</code> 来指定相应的损失函数。如果要在<code>gbm</code> 中使用LambdaMART算法，则需要将<code>&#39;distribution&#39;</code> 设为列表形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(name=&quot;pairwise&quot;,group=...,metric=...,max.rank=...)</span><br></pre></td></tr></table></figure>
<p>这里的<code>metric</code> 是我们选择的排序算法性能的评价指标。在<code>gbm</code> 包中，我们可以选择：</p>
<ul>
<li>NDCG；</li>
<li>MAP；</li>
<li>MRR（Mean Reciprocal Rank），注意这里的MRR是排序最高的正类对象所对应的MRR。</li>
<li>Concordance：考察所有的文档对，Concordance返回模型能够正确处理的文档对的比例。实际上就是在文档对分类问题上的AUC。</li>
</ul>
<p>在使用LambdaMART算法时，<code>gbm</code> 要求输入数据是数据框。在输入的数据框中，有一列表示该行数据关于其所对应的查询的信息，如查询的<code>id</code> 。这里我们使用<code>group</code> 参数指定数据框中的哪一列或者哪些列对应查询。</p>
<p>由于NDCG在排序问题中的广泛使用，因此，在<code>gbm</code> 实现的LambdaMART算法中，NDCG是默认的评价指标。注意，在<code>gbm</code> 包的LambdaMART算法的实现中，在使用NDCG和Concordance标准时，输入数据对应的目标值可以是任何值；但是当使用MAP和MRR时，目标值只能是0或者1。</p>
<p>在计算NDCG和MRR时，我们可以设置参数<code>max.rank</code> ，这样就只考虑排序结果最前面的<code>max.rank</code> 个返回结果。如果<code>max.rank</code> 不设置，则整个返回结果都考虑。</p>
<p>下面这段程序显示了我们如何使用<code>gbm</code> 来优化NDCG。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">gbm.ndcg &lt;-gbm(Y~X1+X2+X3,　　　　　# formula</span><br><span class="line">　　　　　　　　data=data.train,　　 # dataset</span><br><span class="line">　　　　　　　　distribution=list(　 # loss function: </span><br><span class="line">　　　　　　　　　　name=&apos;pairwise&apos;, # pairwise</span><br><span class="line">　　　　　　　　　　metric=&quot;ndcg&quot;,　 # ranking metric: NDCG</span><br><span class="line">　　　　　　　　　　group=&apos;query&apos;),　# column indicating query groups</span><br><span class="line">　　　　　　　　n.trees=2000,　　　　# number of trees</span><br><span class="line">　　　　　　　　interaction.depth=3　# number of internal nodes </span><br><span class="line">　　　　　　　　shrinkage=0.005,　　 # learning rate</span><br><span class="line">　　　　　　　　bag.fraction = 0.5,　# subsampling fraction</span><br><span class="line">　　　　　　　　train.fraction = 1,　# fraction of data for training</span><br><span class="line">　　　　　　　　n.minobsinnode = 10, # minimum number of obs for split</span><br><span class="line">　　　　　　　　keep.data=TRUE,　　　# store copy of input data in model </span><br><span class="line">　　　　　　　　cv.folds=5)　　　　　# number of cross validation folds</span><br></pre></td></tr></table></figure>

</details>


<p>在这个例子中，我们假设输入数据保存在数据框<code>data.train</code> 中，并且我们希望使用列<code>X1</code> 、<code>X2</code> 、<code>X3</code> 来预测<code>Y</code> ，同时我们指定列<code>query</code> 来表示查询。具体来说，就是将<code>distribution</code> 参数设置为如下形式来使用<code>gbm</code> 实现的LambdaMART算法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distribution=list(name=&apos;pairwise&apos;, metric=&quot;ndcg&quot;, group=&apos;query&apos;)</span><br></pre></td></tr></table></figure>
<p>在本书附带的R程序gbm_ranking_example.R中，我们详细介绍了如何使用<code>gbm</code> 来优化NDCG和其他指标，感兴趣的读者可以直接运行该程序。</p>
<h2 id="8-5-逐列方法"><a href="#8-5-逐列方法" class="headerlink" title="8.5 逐列方法"></a>8.5 逐列方法</h2><p>在逐列方法 中，我们考察排序模型返回的整个序列。根据逐列方法的具体实现，基本上可分为两类。在第一类中，我们要直接输出每个查询和对应的文档之间的相关度。在输出的相关度的基础上，我们得到模型对于文档的排序结果，并比较真实的排序结果。通常我们定义损失函数并优化模型。一般来讲，这里的损失函数都是前面所讨论的关于排序算法的指标的近似函数或者上界。在第二类逐列方法中，对于每个查询，我们直接输出文档的排序结果序列，然后将该排序结果与真实排序进行比对，计算相应的损失函数并优化模型。注意，在第二类方法中，我们不需要知道查询和对应文档具体的相似度，只需要知道每个查询模型输出的排序结果即可。</p>
<p>为什么这里的损失函数都是评价标准的近似函数或者上界呢？原因在于排序算法中的评价标准（如NDCG和MAP）都涉及排序结果中每个文档的具体位置，这些都是不连续而且不可导的，直接优化的求解难度很大。一般来说，在机器学习中，我们使用的损失函数都是连续且可导的，这样就可以使用数值优化中的很多优化算法来最小化损失函数。</p>
<p>与逐点和逐对方法不同，逐列方法直接从所得模型的排序结果入手。换言之，逐列方法考虑了排序模型对于每个查询<img src="Image01296.gif" alt> 返回的整个排序序列并优化。与逐点方法相比，逐列方法和逐对方法类似，也是对每个查询单独考虑。比较常见的逐列方法包括ListNet、ListMLE、AdaRank、<img src="Image01550.gif" alt> 、Soft Rank等。在本节中，我们着重介绍逐列方法的基本思想。在前面章节中，我们讨论了SVM在逐点和逐对方法中的扩展，在本节我们着重介绍SVM在逐列方法中的扩展<img src="Image01550.gif" alt> 算法，这样有利于读者比较SVM算法在不同方法中的不同扩展形式。</p>
<h3 id="8-5-1-SVMmap-算法"><a href="#8-5-1-SVMmap-算法" class="headerlink" title="8.5.1 SVMmap 算法"></a>8.5.1 SVMmap 算法</h3><p>在LambdaMART算法中，我们以NDCG为例来说明如何优化排序算法中的评价指标。在SVMmap 算法 中，我们直接优化MAP ③ 。具体来说，我们考虑排序模型输出的排序文档的整个序列。将输出的文档序列与真实的排序序列相比较并优化其差值，从而优化MAP。</p>
<p>在具体介绍算法的细节之前，我们首先介绍一下在<img src="Image01550.gif" alt> 算法中要用到的符号和基本假设。对于查询<img src="Image01296.gif" alt> 和其对应的文档<img src="Image01295.gif" alt> ，我们首先构建相应的特征向量<img src="Image01294.gif" alt> 。为了简化讨论，我们假设相关度为0或者1。对于模型的输出，我们通过计算MAP来衡量模型的好坏。与前面讨论的SVM模型的变体类似，我们假设要求一个线性模型<img src="Image01337.gif" alt></p>
<p><img src="Image01551.gif" alt></p>
<p>（8-53）</p>
<p>使得通过线性变换之后MAP最大。换言之，我们对特征<img src="Image01552.gif" alt> 求解一个投影 <strong>_w_ </strong> ，使得投影后所得的MAP最大。</p>
<p>首先我们讨论查询<img src="Image01296.gif" alt> 。将与<img src="Image01296.gif" alt> 对应的文档<img src="Image01338.gif" alt> 中与<img src="Image01296.gif" alt> 相关的文档的集合记为<img src="Image01553.gif" alt> ，将其中与<img src="Image01296.gif" alt> 不相关的文档集合记为<img src="Image01554.gif" alt> 。从直观上讲，在投影后，如果<img src="Image01553.gif" alt> 中的文档<img src="Image01416.gif" alt> 对应的<img src="Image01555.gif" alt> 比<img src="Image01556.gif" alt> 中的文档<img src="Image01417.gif" alt> 对应的<img src="Image01427.gif" alt> 大，我们就能将两类文档分开。进一步分析，我们希望投影后我们选取的某项标准能够最大。在<img src="Image01550.gif" alt> 算法中，我们希望MAP最大。</p>
<p>对于查询<img src="Image01296.gif" alt> 的一个给定的排序结果 _π_ ，我们定义一个函数<img src="Image01557.gif" alt> 来描述投影 <strong>_w_ </strong> 的好坏：</p>
<p><img src="Image01558.gif" alt></p>
<p>（8-54）</p>
<p>这里我们用<img src="Image00474.gif" alt> 表示查询<img src="Image01296.gif" alt> 代表的所有输入数据，函数<img src="Image01559.gif" alt> 的定义如下：</p>
<p><img src="Image01560.gif" alt></p>
<p>（8-55）</p>
<p>在排序 _π_ 中，如果文档<img src="Image01482.gif" alt> 排在文档<img src="Image01417.gif" alt> 之前，则<img src="Image01561.gif" alt> ；如果文档<img src="Image01416.gif" alt> 排在文档<img src="Image01417.gif" alt> 之后，则<img src="Image01562.gif" alt> 。注意，文档<img src="Image01416.gif" alt> 是事实上与查询<img src="Image01296.gif" alt> 相关的文档，而文档<img src="Image01417.gif" alt> 是与查询<img src="Image01296.gif" alt> 不相关的文档。我们可以将函数<img src="Image01563.gif" alt> 进一步表示为</p>
<p><img src="Image01564.gif" alt></p>
<p>（8-56）</p>
<p>根据函数<img src="Image01563.gif" alt> 的定义，我们知道<img src="Image01563.gif" alt> 实际计算的是所有(相关，不相关)文档对根据在 _π_ 中的排序位置在投影之后差的平均值。直观地讲，对于一个好的投影 <strong>_w_ </strong> ，<img src="Image01563.gif" alt> 的值越大越好。与前面讨论的LambdaRank算法和LambdaMART算法类似，问题的难点是如何将MAP引入到优化问题中。</p>
<p>在<img src="Image01550.gif" alt> 算法中，我们将错误排序 _π_ 对应的AP值<img src="Image01565.gif" alt> 引入到优化问题的约束条件中，从而达到最优化MAP的目的。具体而言，我们求解如下的优化问题：</p>
<p><img src="Image01566.gif" alt></p>
<p>（8-57）</p>
<p>在这个优化问题中，我们考虑了所有的 _m_ 个查询。这里<img src="Image01567.gif" alt> 是<img src="Image01296.gif" alt> 对应的文档集<img src="Image01338.gif" alt> 所有排序结果的集合，<img src="Image01347.gif" alt> 是<img src="Image01296.gif" alt> 对应的文档集的真实排序， _C_ &gt;0是控制参数，<img src="Image00823.gif" alt> 是松弛变量。在这个优化问题的目标函数中，<img src="Image00816.gif" alt> 对应模型的复杂度，第二项中<img src="Image01568.gif" alt> 则对应于由MAP引起的Hinge损失函数， _C_ &gt;0则是控制第一项和第二项权重的参数。<img src="Image01569.gif" alt> 是对于 _q i _ 所有错误排序结果的集合，<img src="Image01570.gif" alt> 表示对于查询 _q i _ 在真实排序是<img src="Image01347.gif" alt> 的情况下，当前排序为 _π_ 所对应的AP。因此，<img src="Image01571.gif" alt> 就是在真实排序是<img src="Image01364.gif" alt> 的情况下当前排序为 _π_ 时所带来的关于MAP的损失。</p>
<p>在上面的优化问题中，可以将约束条件写.为：</p>
<p><img src="Image01572.gif" alt></p>
<p>（8-58）</p>
<p>换言之</p>
<p><img src="Image01573.gif" alt></p>
<p>（8-59）</p>
<p>在理想情况下，我们希望这里错误的排序 _π_ 与真实排序<img src="Image01364.gif" alt> 越接近越好。这就意味着：第一，我们希望<img src="Image01570.gif" alt> 尽量大，即<img src="Image01571.gif" alt> 尽量小；第二，我们希望<img src="Image01574.gif" alt> 越小越好。根据前面的讨论，好的排序对应更大的<img src="Image01557.gif" alt> 。在理想情况下，真实的排序<img src="Image01347.gif" alt> 对应的<img src="Image01575.gif" alt> 值比所有错误的排序 _π_ 对应的<img src="Image01575.gif" alt> 值都高。如果一个错误的排序 _π_ 对应的<img src="Image01575.gif" alt> 值比真实的排序<img src="Image01364.gif" alt> 对应的<img src="Image01576.gif" alt> 值更高的话，说明当前的 <strong>_w_ </strong> 值不理想。在这种情况下，我们引入对应的惩罚项<img src="Image01577.gif" alt> 。</p>
<p>在约束条件中，如果<img src="Image01578.gif" alt> ，则<img src="Image01579.gif" alt> ；如果错误排序 _π_ 很糟糕，则<img src="Image01580.gif" alt> 很大，或者，如果<img src="Image01581.gif" alt> &gt;<img src="Image01557.gif" alt> ，则会导致<img src="Image00823.gif" alt> 很大。因此，在这个优化问题中，我们最小化惩罚项<img src="Image00823.gif" alt> 之和。事实上，我们可以严格地证明<img src="Image01582.gif" alt> 是由MAP导致的损失函数的上界，有兴趣的读者可以参考相关文献[33]。</p>
<p>注意，对于训练集中的每个查询对应的数据，对于每种可能的错误排序结果，我们都有一系列的约束条件。我们知道对于查询<img src="Image01296.gif" alt> 相应的文档<img src="Image01338.gif" alt> ，错误的排序结果的数目是<img src="Image01324.gif" alt> 的指数级，因此在前面的优化问题中，我们也有<img src="Image01324.gif" alt> 指数级的约束条件，而且这还只是涉及查询<img src="Image01296.gif" alt> 相关的部分。</p>
<p>为了有效地求解所得的优化问题，我们使用割平面算法 （cutting plane algorithm）来求解。为了提高算法的效率，解决约束条件过多的问题，在算法的每一步我们都只考虑那些最重要的约束条件。从直观上讲，要得到好的 <strong>_w_
</strong> ，我们需要集中在那些使得<img src="Image00823.gif" alt> 较大的约束条件，并在求解时重点考虑这些约束条件以加快计算。我们使用一个重点错误排序的集合<img src="Image01583.gif" alt> 来简化计算：对于每个查询<img src="Image01296.gif" alt> ，我们用<img src="Image01584.gif" alt> 来记录当前查询<img src="Image01296.gif" alt> 应考虑的重点错误排序的集合，而<img src="Image01584.gif" alt> 中只包含当前使得对应的<img src="Image00823.gif" alt> 较大的重点错误排序。<img src="Image01583.gif" alt> 表示所有应考虑的重点错误排序的集合，即<img src="Image01585.gif" alt> 。在算法开始时，<img src="Image01583.gif" alt> 是空集；在算法的每个循环中，我们根据当前的 <strong>_w_ </strong> 来更新<img src="Image01584.gif" alt> 以及相应的<img src="Image01583.gif" alt> ，直到算法收敛到一个局部最优解。具体来说，对于查询<img src="Image01296.gif" alt> 的某个错误的排序结果<img src="Image01586.gif" alt> ，我们可以定义<img src="Image01586.gif" alt> 导致的约束条件的违反程度如下：</p>
<p><img src="Image01587.gif" alt></p>
<p>（8-60）</p>
<p><img src="Image01588.gif" alt> 越大，说明导致的<img src="Image00823.gif" alt> 越大，对应约束条件的优先级越高。算法的具体步骤见算法8-2。</p>
<p>算法8-2 SVMmap 算法</p>
<blockquote>
<p>输入：训练集<img src="Image01589.gif" alt> ，控制参数 _C_ , _t_ &gt; &gt; 对所有的<img src="Image01590.gif" alt> &gt; ，设置<img src="Image01591.gif" alt> &gt; &gt; repeat &gt; &gt; for <img src="Image01592.gif" alt> do &gt; &gt; 计算<img src="Image01593.gif" alt> &gt; &gt; 计算<img src="Image01594.gif" alt> &gt; &gt; if _H_ (<img src="Image01595.gif" alt> &gt; )&gt; _ε i _ + _t_ then &gt; &gt; <img src="Image01596.gif" alt> &gt; &gt; <img src="Image01597.gif" alt> &gt; &gt; 使用当前的<img src="Image01583.gif" alt> ，利用割平面算法求解式（8-57）中的优化问题 &gt; &gt; endif &gt; &gt; endfor &gt; &gt; until 所有的<img src="Image01584.gif" alt> 都不变</p>
</blockquote>
<p>根据以上算法的具体步骤，在每次循环中，我们要对每个查询<img src="Image01296.gif" alt> 计算<img src="Image01598.gif" alt> 。如果我们遍历所有可能的<img src="Image01599.gif" alt> 的话，算法的效率会受到影响。这里我们利用MAP的具体性质以加快计算。具体而言，就是对于查询<img src="Image01296.gif" alt> 对应的文档的排序结果，我们并不需要关心每个位置对应哪个文档，而只需要关心该位置对应的文档是否相关即可。对于一个排序结果，即使我们交换两个相关文档的位置，也并不会改变AP的值。在<img src="Image01550.gif" alt> 算法中，利用该性质，将考虑查询<img src="Image01296.gif" alt> 对应的约束条件的复杂度降为<img src="Image01600.gif" alt> 。由于本节着重说明算法的原理而非具体实现，因此我们就省略加快计算的具体细节了。这里参数 _t_ &gt; 0是我们用来判断是否将<img src="Image01601.gif" alt> 加入<img src="Image01584.gif" alt> 的容许参数。</p>
<p>注意，在<img src="Image01602.gif" alt> 的定义中，我们实际上考虑了<img src="Image01338.gif" alt> 中所有(相关，不相关)文档对之间的关系，因此<img src="Image01550.gif" alt> 算法与逐对方法也有相似之处。但是<img src="Image01550.gif" alt> 算法在优化问题中进一步考虑了所有不同的排序结果，在这个意义上该算法是一种典型的逐列方法。</p>
<p><img src="Image01550.gif" alt> 算法目前已经成功地推广到其他算法评价标准，包括NDCG和MRR。其基本思想与<img src="Image01550.gif" alt> 算法相同，核心就是在约束条件中引入NDCG和MRR，并利用NDCG和MRR的性质以快速找出使得<img src="Image00823.gif" alt> 最大的约束条件。由于篇幅所限，本节的一些内容我们就不展开讨论了。</p>
<h3 id="8-5-2-讨论"><a href="#8-5-2-讨论" class="headerlink" title="8.5.2 讨论"></a>8.5.2 讨论</h3><p>与逐点方法和逐对方法相比，逐列方法直接考虑了排序算法的整个输出，并优化对应的评价标准（如MAP等）。逐列方法更直接地考虑了影响排序算法性能的因素。例如，排序结果中的位置信息被明确考虑了，同时也考虑了每个查询对应的文档之间的相互关系。从理论上讲，逐列方法应该更易于取得较高的性能。但是另一方面，在实际使用中要注意逐列方法的计算复杂度较高。</p>
<hr>
<p>① 在1.3.4节中我们对相关问题进行了讨论。</p>
<p>② <a href="https://cran.r-project.org/web/packages/gbm/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/gbm/index.html</a></p>
<p>③ SVMmap 算法可以修改，使之能够优化NDCG等其他标准，但是在本节中我们主要讨论如何优化MAP。</p>
<h1 id="第9章-集成学习"><a href="#第9章-集成学习" class="headerlink" title="第9章 集成学习"></a>第9章 集成学习</h1><h2 id="9-1-集成学习简介"><a href="#9-1-集成学习简介" class="headerlink" title="9.1 集成学习简介"></a>9.1 集成学习简介</h2><p>作为本书的最后一章，我们主要介绍集成学习。首先我们介绍集成学习的基本思想，之后讨论几种常用的集成学习的方法，包括bagging、boosting 和stacking 。对于bagging和boosting，我们进一步介绍几种在实际中非常有效的算法，包括随机森林、AdaBoost及提升决策树。</p>
<p>集成学习的核心是构建多个不同的模型，并将这些模型聚合起来从而提高模型的性能。集成学习适用于机器学习的几乎所有领域，包括我们前面讨论过的回归、分类、推荐和排序等。在集成学习中，构建的一系列模型称为基学习器。通过使用不同的策略，我们可将这些基学习器聚合起来。在集成学习中，通常并不要求每个基学习器的性能特别好。在很多典型的集成学习算法中，我们只要求每个基学习器要好于随机猜测<br>（random guess）。例如，在两类分类问题中，我们要求基学习器的准确率大于0.5即可。一般而言，这种基学习器也称为弱学习器 （weak learner）。在集成学习中，重点是使训练得到的基学习器满足多样性<br>（diversity）的要求，这样将多个基学习器聚合起来时，我们能够有效地提高性能。从直观上看，虽然每个基学习器都犯错，但是如果它们在犯不同的错误，那么将它们聚合起来之后犯错的可能性会很低。反之，如果基学习器比较相似，则通过集成学习提高性能的幅度较小，甚至可能带来过拟合<br>（over-fitting）的问题而导致性能降低。一个极端的例子是，如果我们得到多个完全一样的模型，那么将它们聚合起来不会得到任何提升。</p>
<p>由于集成学习有效地考虑了多个不同的模型，一般而言能够获得较好的性能，因此在很多注重算法性能的场合，集成学习一般是首选。例如，在很多数据挖掘的竞赛中，获胜的算法一般都是使用集成学习将多个模型聚合而成。</p>
<p>根据基学习器的生成策略，集成学习的方法可以分为两类：（1）并行方法 （parallel method），以bagging为主要代表；（2）顺序方法<br>（sequential method），以boosting为主要代表。</p>
<p>在并行方法中，我们同时构建多个基学习器，并利用这些基学习器的独立性以提高最后模型的性能。在集成学习中，我们要尽量构建独立的学习器，以使得它们犯错也相互独立。当然，在实际中我们很难得到完全相互独立的基学习器。此外，我们一般只有一个训练集。如果都从同一训练集得到多个不同的模型的话，它们之间很难做到相互独立。因此，在实践中，我们通常通过引入随机性来尽量构造相互独立的模型。并行方法的一大优点是利于并行计算，这样我们可以显著地降低训练模型所需要的时间。</p>
<p>在顺序方法中，我们顺次构建多个学习器。当我们构建后面的学习器时，希望后面的学习器能够避免前面学习器的错误，从而提高聚合后的性能。因此，在顺序方法中，我们要利用基学习器的相关性。与并行方法不同，在顺序方法中，我们需要逐个建立新的学习器，较难利用并行性以缩短训练时间。</p>
<p>与单个模型相比，集成学习的缺点包括：（1）计算复杂度较大。因为在集成学习中需要训练多个模型，所以计算复杂度会有较大程度的提高；（2）一般而言，所得的模型很难解释。如单个决策树模型很容易解释，而由多个决策树组成的随机森林<br>（random forest）却不大容易解释。</p>
<p>在本章中，我们首先介绍顺序方法中的bagging，并介绍bagging的一个典型应用：随机森林；然后我们介绍顺序方法中的boosting，并介绍boosting的两个典型应用：AdaBoost和梯度提升算法；最后，我们介绍stacking的基本思想及应用。</p>
<p>在下面的讨论中，我们一般以分类问题为例进行讨论。本章所讨论的集成学习方法都可以简单修改使之能适用于更广泛的问题，如回归问题、排序和推荐问题等。这也是我们将集成学习放到最后一章讨论的原因之一。</p>
<p>本章中我们将训练集表示为<img src="Image01603.gif" alt> ，这里<img src="Image00501.gif" alt> 、<img src="Image00930.gif" alt> 。在集成学习中，我们通常构建 _m_ 个弱学习器，记为<img src="Image01604.gif" alt> 。一般而言，我们通常考虑多个弱学习器的线性组合作为最终的模型。在聚合 _m_ 个弱学习器时，将<img src="Image01605.gif" alt> 的权重记为<img src="Image00523.gif" alt> 。这样，最终的模型记为：</p>
<p><img src="Image01606.gif" alt></p>
<p>（9-1）</p>
<p>在分类问题中，假设输出是{−1,1}，则可将最终的模型记为：</p>
<p><img src="Image01607.gif" alt></p>
<p>（9-2）</p>
<p>这里sign是符号函数。在集成学习中，一般而言，我们需要从训练集中学习出 _m_ 个学习器<img src="Image01608.gif" alt> ，同时也需要学习<img src="Image01609.gif" alt> 。</p>
<h2 id="9-2-bagging简介"><a href="#9-2-bagging简介" class="headerlink" title="9.2 bagging简介"></a>9.2 bagging简介</h2><p>在本节中，我们讨论并行集成学习算法的一个典型例子——bagging。在下一节中我们介绍最常用的bagging算法随机森林。</p>
<p>bagging是Bootstrap AGGregatING的缩写。简而言之，就是通过bootstrap取样（可重复取样）的方法构造多个不同的训练集。之后在每个训练集上训练相应的基学习器，最后将这些基学习器聚合起来得到最终的模型。从定义来看，bagging中有两个重要的部分：（1）bootstrap取样；（2）模型聚合<br>（aggregation）。下面我们分别予以介绍。</p>
<p>首先我们讨论bootstrap取样。在前面的讨论中，我们强调了基学习器相互独立的重要性：虽然每个基学习器的性能较弱，但是如果它们都是从不同的角度犯错，那么将它们聚合起来的性能也许会有较大提升。在实际中，在给定一个训练集的情况下，如何尽可能地建立相对独立的基学习器呢？基本上我们只有两种选择：（1）在训练每个基学习器的时候使用不同的训练集；（2）在训练集相同或者相近的情况下，使用不同的学习算法来训练不同的基学习器。如果我们选择使用不同的训练集，一种可能性是将整个训练集划分为多个不同的子集，然后在每个子集上训练不同的基学习器。这个方法虽然保证了这些基学习器是从不同的训练集上得到的，但是由于每个子集都显著小于原来的训练集，使得构建的基学习器可能遗漏了原始训练集中的一些关键信息。同时，每个子集和整个训练集的分布可能存在差异。因此，这样得到的基学习器的性能会受到影响。</p>
<p>因此，这里的问题是，既要利用训练集中更多的样本，又要同时尽量保证不同的训练集的独立性。为了解决这个问题，人们提出了bootstrap取样<br>（bootstrap sampling）。简而言之，bootstrap取样就是使用可重复取样 （sampling with replacement）的方法，从样本数为 _n_ 的数据集中取出 _n_ 个样本。除了一种极端情况（就是bootstrap取样得到了整个原来的数据集），使用bootstrap取样得到的 _n_ 个样本中都有重复样本，同时也有原来数据集中的样本没有被取到。注意，在可重复取样中，我们假设每个样本被选中的概率是一样的。在bagging中，对于原始的训练集，我们使用bootstrap取样 _m_ 次，选取出 _m_ 个样本集。在每个样本集上，我们构建相应的基学习器。这样就得到了 _m_ 个不同的学习器。</p>
<p>bagging的另一个重要步骤是模型聚合。在bagging中，我们通常采用比较简单的方法来聚合多个模型。对于分类问题，我们采用投票 （voting）的方法将 _m_ 个基学习器的分类结果中出现最多的一个作为最终的分类结果；对于回归问题，我们直接取 _m_ 个基学习器的输出的平均值。注意，使用bagging还可以处理多类分类问题，只需要基学习器能够处理多类问题即可。</p>
<p>算法9-1给出了bagging在分类和回归问题中的基本流程。其中 _mode_ 函数计算统计中的众数（mode），<img src="Image01610.gif" alt> 表示从<img src="Image01611.gif" alt> 中选取出现频率最高的类标作为最终的分类结果。</p>
<p>算法9-1 bagging的基本流程</p>
<blockquote>
<p>1. for _j_ =1: _m_ &gt; &gt; 1.1 产生一个bootstrap取样的样本集<img src="Image01612.gif" alt> &gt; &gt; 1.2 在<img src="Image01612.gif" alt> &gt; 上训练一个基学习器<img src="Image01613.gif" alt> &gt; &gt; 2. 聚合 _m_ 个模型： &gt; &gt; 2.1 对于分类问题： &gt; &gt; <img src="Image01614.gif" alt> &gt; &gt; （9-3） &gt; &gt; 2.2 对于回归问题： &gt; &gt; <img src="Image01615.gif" alt> &gt; &gt; （9-4）</p>
</blockquote>
<p>bagging的另一个优点：对于每个基学习器<img src="Image01616.gif" alt> ，我们可以利用不在训练集<img src="Image01612.gif" alt> 中的样本（out-of-bag sample，OOB样本）来估计基学习器的性能。在训练模型的过程中，由于我们从未接触过OOB样本，因此它们是天然的检验算法性能的数据。</p>
<p>这里简单介绍如何利用OOB样本来估计bagging在分类问题中的正确率。在后面讨论bagging的具体例子随机森林时，我们将进一步讨论如何使用OOB样本来估计变量的重要性。</p>
<p>对于每个样本<img src="Image01617.gif" alt> ，我们首先可以找出所有没有利用<img src="Image01617.gif" alt> 训练的基学习器<img src="Image01618.gif" alt> ，然后利用这些基学习器计算<img src="Image01616.gif" alt> ，并使用投票的方法聚合这些<img src="Image01616.gif" alt> 得到最终的输出。对于所有的 _n_ 个样本，我们都可重复这一过程。最后，将这些样本的预测值和真实类别相比较即可算出相应的正确率。</p>
<p>虽然一次bootstrap取样能够得到 _n_ 个样本（这里 _n_ 是原始训练集中的样本数目），但是在理想情况下，只能得到大约63.2%的原始样本。下面我们详细解释为什么只能得到大约63.2%的原始样本。这一段介绍不影响读者学习bagging，不感兴趣的读者可以跳过。</p>
<p>考虑样本<img src="Image01619.gif" alt> ，其在 _n_ 次取样中都没有被选中的概率<img src="Image01620.gif" alt> 为：</p>
<p><img src="Image01621.gif" alt></p>
<p>（9-5）</p>
<p>因此，<img src="Image00474.gif" alt> 被选中至少一次的概率<img src="Image01622.gif" alt> 为：</p>
<p><img src="Image01623.gif" alt></p>
<p>（9-6）</p>
<p>当 _n_ 趋近于无穷大时，有：</p>
<p><img src="Image01624.gif" alt></p>
<p>（9-7）</p>
<p>因此</p>
<p><img src="Image01625.gif" alt></p>
<p>（9-8）</p>
<p>因此，如果我们考虑期望值的话，当 _n_ 很大时，就会有大约63.2%的原始样本在一次bootstrap取样中被选中。</p>
<p><strong>讨论</strong></p>
<p>在回归分析中，我们讨论了偏差-方差权衡 （bias-variance tradeoff）。我们知道，模型 （model）也有偏差 （bias）和方差<br>（variance）。从直观上看，对于一组给定的训练数据，如果某一算法能够很好地拟合这组训练数据，则该模型的偏差较低；但是如果训练数据发生了一些变动，而导致算法产生的模型也发生了较大变动的话，则该模型的方差较高。典型的例子就是决策树和人工神经网络。虽然它们都能很好地拟合训练集，但是对于训练集过于敏感。下面我们首先以随机变量为例讨论多个变量平均之后的方差，然后将其推广到模型以说明模型多样性的重要性。</p>
<p>我们知道，假设我们有 _m_ 个独立同分布的随机变量，它们的方差是<img src="Image00635.gif" alt> ，则平均之后的方差为<img src="Image01626.gif" alt> 。简而言之，取平均值可以降低方差。注意，在bagging中，实际上不可能构造出完全独立的基学习器。如果我们将bagging中构造的每个模型视为一个随机变量的话，则这些随机变量都不是相互独立的。</p>
<p>下面我们考虑稍微一般的情况：考虑 _m_ 个随机变量同分布但不相互独立，并假设两两之间的相关系数为<img src="Image01627.gif" alt> &gt;0，则平均后的方差为：</p>
<p><img src="Image01628.gif" alt></p>
<p>（9-9）</p>
<p>从式（9-9）可以看出，当 _m_ 增加时，式（9-9）中的第二项减少，但是第一项不受 _m_ 的影响。为了清楚地了解 _ρ_ 对平均后的方差的影响，可将式（9-9）写为：</p>
<p><img src="Image01629.gif" alt></p>
<p>（9-10）</p>
<p>从式（9-10）可以看出， _ρ_ 越小，平均之后的方差就越小。回到模型的讨论，如果所构建的模型相互越独立，则平均之后的模型对应的方差就越小。</p>
<p>这里我们给出随机变量平均后的方差的具体推导过程。不感兴趣的读者可以直接跳过这部分的推导。假设我们有未知变量<img src="Image01630.gif" alt> 满足<img src="Image01631.gif" alt> ，并且当<img src="Image01632.gif" alt> 时，对应的相关系数<img src="Image01633.gif" alt> ，则协方差<img src="Image01634.gif" alt> 可写为：</p>
<p><img src="Image01635.gif" alt></p>
<p>（9-11）</p>
<p><img src="Image01636.gif" alt> 的平均值<img src="Image01637.gif" alt> 的方差可计算如下：</p>
<p><img src="Image01638.gif" alt></p>
<p>（9-12）</p>
<p>在集成学习中，可以将模型的方差类比为随机变量的方差。通过平均多个模型，也能降低模型的方差，从而得到更好的模型。bagging通过人为的办法，构建了多个模型，最后通过平均的方法，降低模型的方差。通过上面的讨论可知，要尽量发挥bagging的功效，在构造基学习器时要尽量构造相互独立的基学习器，这样最后平均得到的模型的方差就会小。另外，bagging对于那些高方差 、低偏差 （high-variance，low-bias）的基学习器非常有效。基本上基学习器对训练数据越敏感，bagging之后的效果就越好。但是如果基学习器比较稳定，换言之就是方差较小，在这种情况下，bagging所能取得的提升则非常有限。此外，bagging的优点是非常适合并行训练多个算法，可有效地处理大数据。</p>
<h2 id="9-3-随机森林"><a href="#9-3-随机森林" class="headerlink" title="9.3 随机森林"></a>9.3 随机森林</h2><p>在本节中，我们讨论bagging的一个实际应用算法——随机森林 （random forest）。随机森林使用了bagging的基本思想来训练一系列决策树。但是随机森林又根据决策树的特点做了很多改进，使得所构建的决策树尽量没有相关性，从而显著提高最终的性能。在很多实际问题中，随机森林都能取得很好的效果，同时由于其控制参数易于选择，因此使得随机森林成为实际中非常受欢迎的算法，广泛应用于各类实际问题中。</p>
<p>在讨论随机森林之前，我们先回顾一下决策树。决策树能够有效地获取多个变量之间的相互关系。同时，如果决策树足够深，那么它能够有效地降低模型的偏差。通过前面章节关于决策树的讨论我们知道，较深的决策树虽然能够降低模型的偏差，但是很容易导致过拟合。对于不同的数据集，得到的决策树可以存在较大差异。换言之，决策树的方差较大，因此是使用bagging技巧的好对象。</p>
<h3 id="9-3-1-训练随机森林的基本流程"><a href="#9-3-1-训练随机森林的基本流程" class="headerlink" title="9.3.1 训练随机森林的基本流程"></a>9.3.1 训练随机森林的基本流程</h3><p>在本节中，我们介绍训练随机森林的基本思想和流程。随机森林的基本思想是利用bagging构建很多决策树。通过前一节的讨论我们知道，为了提高bagging的效果，我们需要构建尽量独立的基学习器。与简单的bagging相比，随机森林在构建每棵决策树时，通过引入一些随机信息，有效地降低了各个基学习器的相关度。</p>
<p>具体来说，在构建决策树时，我们需要在每步选择最优的变量。在标准的构建决策树的算法中，我们需要考虑当前所有可选的变量；但是在随机森林中，我们从所有可选的 _d_ 个变量中随机地取出<img src="Image01639.gif" alt> 个变量，然后从中选取出最优的变量。从直观上讲，降低<img src="Image01639.gif" alt> 的值虽然降低了单个决策树在其对应的训练集上的表现，但是它能够降低所构建的不同的决策树的相关度，从而能够降低最终平均后的模型的方差。训练随机森林的具体步骤参见算法9-2。</p>
<p>算法9-2 训练随机森林的具体步骤</p>
<blockquote>
<p>1. for _j_ =1: _m_ &gt; &gt; 1.1 产生一个bootstrap取样的样本集<img src="Image01612.gif" alt> &gt; &gt; 1.2 在<img src="Image01612.gif" alt> 上训练一个决策树<img src="Image01640.gif" alt> &gt; 。在生成决策树的过程中，对于每个对应的样本数大于<img src="Image01641.gif" alt> 的叶结点做如下处理： &gt; &gt; 1.2.1 从所有可选的 _d_ 个变量中随机选择<img src="Image01381.gif" alt> 个变量 &gt; &gt; 1.2.2 从这<img src="Image01381.gif" alt> 个变量中选择导致最优划分的变量 &gt; &gt; 1.2.3 将该结点根据选择的最优变量划分为两个子结点 &gt; &gt; 1.2.4 重复该过程，直到所有的叶结点对应的样本数都小于或等于<img src="Image01641.gif" alt> &gt; &gt; 2. 聚合 _m_ 棵决策树<img src="Image01642.gif" alt> ： &gt; &gt; 2.1 对于分类问题： &gt; &gt; <img src="Image01643.gif" alt> &gt; &gt; （9-13） &gt; &gt; 2.2 对于回归问题： &gt; &gt; <img src="Image01644.gif" alt> &gt; &gt; （9-14）</p>
</blockquote>
<p>在随机森林中，我们要构建一系列决策树。在构建每个决策树时，首先使用bootstrap取样得到一个样本数为 _n_ 的可重复样本。然后利用这个样本集，构建一棵决策树。在构建决策树的每一步，我们都是按照上面所讨论的，先随机取出 _d_ 1 个变量，再选取出最优的变量。最后我们将所有模型的输出取平均（回归问题）或者取出最多的类别（分类问题）作为最终的输出。</p>
<h3 id="9-3-2-利用随机森林估计变量的重要性"><a href="#9-3-2-利用随机森林估计变量的重要性" class="headerlink" title="9.3.2 利用随机森林估计变量的重要性"></a>9.3.2 利用随机森林估计变量的重要性</h3><p>在实际使用机器学习算法时，通常数据中都含有噪声或者冗余数据。因此，如果模型能够直接告诉我们各个变量的重要性，就可以剔除这些噪声和冗余数据。这样一方面能够提高模型的性能，另一方面能够降低计算复杂度。</p>
<p>在随机森林中，有两种方法可以用来确定每个变量的重要性：第一种方法是利用决策树的性质来计算变量的重要性；第二种方法是利用OOB样本来估计变量的重要性。在实际使用中，第二种方法的应用更为广泛。</p>
<p>在第一种方法中，在构建每棵决策树时，可以计算每个结点对应的变量所导致的信息增益 。把所有的决策树都聚合起来，就可以计算每个变量所导致的信息增益。利用这个信息增益，可计算每个变量的重要性：信息增益越大，变量的重要性越高。</p>
<p>在第二种方法中，可以使用OOB样本来确定每个变量的重要性。在考虑第 _j_ 棵树<img src="Image01645.gif" alt> 时，我们将所有没有包含在第 _j_ 次bootstrap取样集<img src="Image01612.gif" alt> 中的OOB样本集记为<img src="Image01646.gif" alt> 。我们可将<img src="Image01646.gif" alt> 作为检验集<br>（validation set），计算<img src="Image01640.gif" alt> 在<img src="Image01646.gif" alt> 上的性能<img src="Image01647.gif" alt> ，如分类中的准确率或者回归中的RMSE。当考虑第 _k_ 个变量的重要性时，我们将<img src="Image01646.gif" alt> 中所有样本的第 _k_ 个变量的值打乱以得到一个随机排列。这样我们得到一个新的数据集<img src="Image01648.gif" alt> ，而且<img src="Image01648.gif" alt> 与<img src="Image01646.gif" alt> 的区别在于，<img src="Image01648.gif" alt> 中第 _k_ 个变量的值是随机赋予的，并没有明确的意义。我们将<img src="Image01649.gif" alt> 作为一个新的检验数据集，并计算<img src="Image01640.gif" alt> 在<img src="Image01648.gif" alt> 上的性能<img src="Image01650.gif" alt> 。一般而言，<img src="Image01647.gif" alt> 要好于<img src="Image01650.gif" alt> ，我们将<img src="Image01647.gif" alt> 与<img src="Image01650.gif" alt> 的差作为第 _k_ 个变量的重要性。注意，在随机森林中，我们有 _m_ 棵树，将所有 _m_ 棵树<img src="Image01647.gif" alt> 与<img src="Image01650.gif" alt> 的差平均起来，就可以作为第 _k_ 个变量的重要性的度量。以分类问题为例，假设我们选定性能度量为准确率，则一般而言有<img src="Image01651.gif" alt> ，那么第 _k_ 个变量的重要性可以使用式（9-15）来计算：</p>
<p><img src="Image01652.gif" alt></p>
<p>（9-15）</p>
<p>这种通过随机排列变量值来估计变量重要性的方式，目前也有了很多应用。在微软公司最新推出的Azure Machine Learning ① 中就有相应的模块来通过此法估计变量的重要性 ② 。</p>
<h3 id="9-3-3-随机森林的实际使用"><a href="#9-3-3-随机森林的实际使用" class="headerlink" title="9.3.3 随机森林的实际使用"></a>9.3.3 随机森林的实际使用</h3><p>在实际使用随机森林时，一般需要确定如下参数。</p>
<ul>
<li>决策树的数目 _m_ ；</li>
<li>每棵决策树的大小，由决策树叶结点所能包含的样本数的最大值决定；</li>
<li>每次选取最佳变量时随机选取的变量数<img src="Image01381.gif" alt> 。</li>
</ul>
<p>这里我们首先讨论选取这些参数的一般原则，然后通过介绍R中的<code>randomForest</code> 包来具体介绍随机森林的使用。</p>
<p>对于<img src="Image01381.gif" alt> 的选取和决策树的大小，随机森林的发明者Leo Breiman提出了如下建议。</p>
<ul>
<li>在分类问题中，可以将<img src="Image01381.gif" alt> 的默认值设为<img src="Image01653.gif" alt> ，同时决策树的结点至少包括1个样本。</li>
<li>在回归问题中，可以将<img src="Image01381.gif" alt> 的默认值设为<img src="Image01654.gif" alt> ，同时决策树的结点至少包括5个样本。</li>
</ul>
<p>对于<img src="Image01381.gif" alt> 的值，Breiman提出可以试验默认值、默认值的一半以及默认值的两倍，并从中挑选最优值。在很多数据中，随着<img src="Image01381.gif" alt> 的变化，随机森林的性能变化并不是非常剧烈。在一些数据集中，甚至将<img src="Image01381.gif" alt> 设为1都能取得良好的效果。然而，如果数据集的维数 _d_ 较大但是其中很多是无用噪声的话，在建模中只有少数的特征是真正有用的。在这种情况下，我们需要选取较大的<img src="Image01381.gif" alt> 值，这样那些有用的特征才能够在构建决策树时被选到。</p>
<p>在实际中，如果不是数据集特别小或者问题特别简单，那么上百棵决策树是必需的。一般而言，随着变量的增加，若要取得较好的性能，就需要增加随机森林中决策树的数目。具体来说，我们可以简单测试当前决策树的数目是否足够：比较当前已经得到的所有决策树及其一个部分。比如，目前有500棵决策树，我们可以选取前面的450棵决策树，比较500棵决策树组成的随机森林模型和450棵决策树组成的随机森林模型。换言之，我们利用两个不同的随机森林模型来看它们的预测值是否有较大的区别。如果区别较大，则意味着我们要继续增加决策树的数目。我们的目标是保证决策树的数目足够多，直到随机森林的性能比较稳定为止。</p>
<h4 id="R程序以及介绍"><a href="#R程序以及介绍" class="headerlink" title="R程序以及介绍"></a>R程序以及介绍</h4><p>下面介绍R中非常流行的软件包<code>randomForest</code> ③ 。我们提供了程序文件randomForest_ classification_example.R 和 randomForest_regression_example.R用来介绍如何应用随机森林解决分类问题和回归问题。<code>randomForest</code> 包中的核心函数是<code>randomForest</code> 函数。使用该函数，可以构建一个随机森林模型。该函数的重要参数包括以下几个。</p>
<ul>
<li><code>data</code> ：训练数据。</li>
<li>公式：用来指定哪一列是目标变量，哪些列是自变量。</li>
<li><code>ntree</code> ：随机森林中决策树的数目。</li>
<li><code>mtry</code> ：每次选取最佳变量时随机选取的变量数 _d_ 1 。</li>
<li><code>importance</code> ：取值为<code>TRUE</code> 或者<code>FALSE</code> ，表示是否计算变量的重要性。</li>
</ul>
<p>其他一些函数如下。</p>
<ul>
<li><code>combine</code> ：合并多个随机森林模型得到一个最终的随机森林模型。</li>
<li><code>grow</code> ：对于当前的随机森林模型，继续生成新的决策树。</li>
<li><code>i</code> mportance：显示变量的重要性（文本形式）。</li>
<li><code>varImpPlot</code> ：作图显示变量的重要性（图像形式）。</li>
</ul>
<p>注意，<code>randomForest</code> 包的<code>randomForest</code> 函数中有一个参数是<code>importance</code> ，同时<code>randomForest</code> 包中也有一个函数为<code>importance</code> 。</p>
<p>在下面的例子中，我们着重讲解randomForest_regression_example.R，以说明如何使用<code>randomForest</code> 包。在该文件中，我们利用<code>randomForest</code> 包，完成以下任务：</p>
<ul>
<li>导入数据和进行简单的数据分析。</li>
<li>将整个数据集分为训练集和测试集。</li>
<li>作为参照，训练一个决策树模型。</li>
<li>使用不同的参数训练两个不同的随机森林模型。</li>
<li>首先生成3个不同的随机森林模型，然后将它们合并成为一个随机森林模型。</li>
<li>首先生成一个随机森林模型，然后不断增加其中决策树的数目。</li>
<li>使用随机森林研究变量的重要性。</li>
</ul>
<p>下面是具体的R代码。注意，我们首先要检查<code>randomForest</code> 包有没有安装，如果没有安装，则首先安装该包。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line"># Check required package is installed or not. If not, install it.</span><br><span class="line">randomForest.installed &lt;-&apos;randomForest&apos; %in% rownames(installed.packages())</span><br><span class="line">if (randomForest.installed) &#123;</span><br><span class="line">　print(&quot;the randomForest package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the randomForest package first...&quot;)</span><br><span class="line">　install.packages(&apos;randomForest&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(&apos;randomForest&apos;)</span><br><span class="line"></span><br><span class="line">#========================</span><br><span class="line"># Step 1. Load the data and simple exploration</span><br><span class="line"># load the mtcars data</span><br><span class="line">data(mtcars)</span><br><span class="line">D &lt;-mtcars</span><br><span class="line"></span><br><span class="line"># show the type for each col</span><br><span class="line">for(i in 1:ncol(D)) &#123;</span><br><span class="line">　msg &lt;-paste(&apos;col &apos;, i, &apos; and its type is &apos;, class(D[,i]))</span><br><span class="line">　print(msg)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Step 2. Split the data into training and test sets</span><br><span class="line"># Randomly split the whole data set into a training and a test data set</span><br><span class="line"># After spliting, we have the training set: (X_train, y_train)</span><br><span class="line"># and the test data set: (X_test, y_test)</span><br><span class="line">train_ratio &lt;-0.7</span><br><span class="line">n_total &lt;-nrow(D)</span><br><span class="line">n_train &lt;-round(train_ratio * n_total)</span><br><span class="line">n_test &lt;-n_total -n_train</span><br><span class="line">set.seed(42)</span><br><span class="line">list_train &lt;-sample(n_total, n_train)</span><br><span class="line">D_train &lt;-D[list_train,]</span><br><span class="line">D_test &lt;-D[-list_train,]</span><br><span class="line"></span><br><span class="line">y_train &lt;-D_train$mpg</span><br><span class="line">y_test &lt;-D_test$mpg</span><br><span class="line"></span><br><span class="line"># Step 3. Benchmark: train a single decision tree using rpart</span><br><span class="line">library(&apos;rpart&apos;)</span><br><span class="line">M_rpart1 &lt;-rpart(mpg~., data = D_train)</span><br><span class="line">print(&apos;show the summary of the trained model&apos;)</span><br><span class="line">summary(M_rpart1)</span><br><span class="line"></span><br><span class="line"># Compute the performance on the training and test data sets</span><br><span class="line">y_test_pred_rpart1 &lt;-predict(M_rpart1, D_test)</span><br><span class="line">rmse_test_rpart1 &lt;-sqrt(sum((y_test -y_test_pred_rpart1)^2) / n_test)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_test_rpart1 = &apos;, rmse_test_rpart1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_train_pred_rpart1 &lt;-predict(M_rpart1, D_train)</span><br><span class="line">rmse_train_rpart1 &lt;-sqrt(sum((y_train -y_train_pred_rpart1)^2) / n_train)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_train_rpart1 = &apos;, rmse_train_rpart1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># Step 4. train 2 randome forest models using different parameters</span><br><span class="line"># Step 4.1 Train a random forest model using default parameters</span><br><span class="line"># the default number of trees is 500</span><br><span class="line">M_randomForest1 &lt;-randomForest(mpg~., data = D_train)</span><br><span class="line">print(&apos;show the summary of the trained model&apos;)</span><br><span class="line">summary(M_randomForest1)</span><br><span class="line"># We can get mean of squared residuals using the print function directlty</span><br><span class="line">print(M_randomForest1)</span><br><span class="line"># We can check the number of trees of the trained model</span><br><span class="line">ntree1 &lt;-M_randomForest1$ntree</span><br><span class="line">msg &lt;-paste0(&apos;number of trees in M_randomForest1 = &apos;, ntree1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># Get the prediction on the test set and compute the RMSE</span><br><span class="line">y_test_pred_rf1 &lt;-predict(M_randomForest1, D_test)</span><br><span class="line">rmse_test_rf1 &lt;-sqrt(sum((y_test -y_test_pred_rf1)^2) / n_test)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_test_rf1 = &apos;, rmse_test_rf1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_train_pred_rf1 &lt;-predict(M_randomForest1, D_train)</span><br><span class="line">rmse_train_rf1 &lt;-sqrt(sum((y_train -y_train_pred_rf1)^2) / n_train)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_train_rf1 = &apos;, rmse_train_rf1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># We train a second model using fewer decision trees, and control the </span><br><span class="line"># complexity of each tree.</span><br><span class="line">M_randomForest2 &lt;-randomForest(mpg~., data = D_train, ntree=50, mtry=3)</span><br><span class="line">print(&apos;show the summary of the trained model&apos;)</span><br><span class="line">summary(M_randomForest2)</span><br><span class="line">print(M_randomForest2)</span><br><span class="line"># We can check the number of trees of the trained model</span><br><span class="line">ntree2 &lt;-M_randomForest2$ntree</span><br><span class="line">msg &lt;-paste0(&apos;number of trees in M_randomForest2 = &apos;, ntree2)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># Get the prediction on the test set and compute the RMSE</span><br><span class="line">y_test_pred_rf2 &lt;-predict(M_randomForest2, D_test)</span><br><span class="line">rmse_test_rf2 &lt;-sqrt(sum((y_test -y_test_pred_rf2)^2) / n_test)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_test_rf2 = &apos;, rmse_test_rf2)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_train_pred_rf2 &lt;-predict(M_randomForest2, D_train)</span><br><span class="line">rmse_train_rf2 &lt;-sqrt(sum((y_train -y_train_pred_rf2)^2) / n_train)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_train_rf2 = &apos;, rmse_train_rf2)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># Step 5. Combine 3 random forest models together using the combine function</span><br><span class="line"># Train 3 random forest models</span><br><span class="line">M_rf_base1 &lt;-randomForest(mpg~., data = D_train, ntree = 15)</span><br><span class="line">M_rf_base2 &lt;-randomForest(mpg~., data = D_train, ntree = 20)</span><br><span class="line">M_rf_base3 &lt;-randomForest(mpg~., data = D_train, ntree = 10)</span><br><span class="line"># Combine these 3 models just trained</span><br><span class="line">M_rf_comb &lt;-combine(M_rf_base1, M_rf_base2, M_rf_base3)</span><br><span class="line">print(M_rf_comb)</span><br><span class="line"></span><br><span class="line"># compute the performance on the test data set</span><br><span class="line">y_test_pred_rf_base1 &lt;-predict(M_rf_base1, D_test)</span><br><span class="line">rmse_test_rf_base1 &lt;-sqrt(sum((y_test -y_test_pred_rf_base1)^2) / n_test)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_test_rf_base1 = &apos;, rmse_test_rf_base1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_test_pred_rf_base2 &lt;-predict(M_rf_base2, D_test)</span><br><span class="line">rmse_test_rf_base2 &lt;-sqrt(sum((y_test -y_test_pred_rf_base2)^2) / n_test)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_test_rf_base2 = &apos;, rmse_test_rf_base2)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_test_pred_rf_base3 &lt;-predict(M_rf_base3, D_test)</span><br><span class="line">rmse_test_rf_base3 &lt;-sqrt(sum((y_test -y_test_pred_rf_base3)^2) / n_test)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_test_rf_base3 = &apos;, rmse_test_rf_base3)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_test_pred_rf_comb &lt;-predict(M_rf_comb, D_test)</span><br><span class="line">rmse_test_rf_comb &lt;-sqrt(sum((y_test -y_test_pred_rf_comb)^2) / n_test)</span><br><span class="line">msg &lt;-paste0(&apos;rmse_test_rf_comb = &apos;, rmse_test_rf_comb)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># Step 6. Continuously grow a random forest by training more trees, and</span><br><span class="line"># plot the RMSE of training and test as the number of trees increases.</span><br><span class="line"></span><br><span class="line">ntree_list &lt;-1:200</span><br><span class="line">ntree_length &lt;-length(ntree_list)</span><br><span class="line">rmse_train_list &lt;-rep(0, ntree_length)</span><br><span class="line">rmse_test_list &lt;-rep(0, ntree_length)</span><br><span class="line">for (i in 1:ntree_length) &#123;</span><br><span class="line">　# Build the random forest model based on the existing random forest model</span><br><span class="line">　if (i==1) &#123;</span><br><span class="line">　　M_rf_base &lt;-randomForest(mpg~., data = D_train, ntree = ntree_list[1])</span><br><span class="line">　&#125;else &#123;</span><br><span class="line">　　ntree_delta &lt;-ntree_list[i] -ntree_list[i-1]</span><br><span class="line">　　M_rf_base &lt;-grow(M_rf_base, ntree_delta)</span><br><span class="line">　&#125;</span><br><span class="line">　# Compute rmse on training and test data set</span><br><span class="line">　y_train_pred_rfi &lt;-predict(M_rf_base, D_train)</span><br><span class="line">　y_test_pred_rfi &lt;-predict(M_rf_base, D_test) </span><br><span class="line">　rmse_train_rfi &lt;-sqrt(sum((y_train -y_train_pred_rfi)^2) / n_train)</span><br><span class="line">　rmse_test_rfi &lt;-sqrt(sum((y_test -y_test_pred_rfi)^2) / n_test)</span><br><span class="line">　rmse_train_list[i] &lt;-rmse_train_rfi</span><br><span class="line">　rmse_test_list[i] &lt;-rmse_test_rfi</span><br><span class="line">&#125;</span><br><span class="line"># Plot the training and test RMSE</span><br><span class="line">y_min &lt;-min(min(rmse_test_list), min(rmse_train_list)) -0.1</span><br><span class="line">y_max &lt;-max(max(rmse_test_list), max(rmse_train_list)) + 0.1</span><br><span class="line">plot(range(ntree_list), c(y_min, y_max), type=&apos;n&apos;, xlab=&apos;ntree&apos;, ylab=&apos;RMSE&apos;)</span><br><span class="line">lines(ntree_list, rmse_train_list, type=&apos;l&apos;, lty=1, col=&apos;black&apos;)</span><br><span class="line">lines(ntree_list, rmse_test_list, type=&apos;l&apos;, lty=2, col=&apos;red&apos;)</span><br><span class="line">legend_char_list &lt;-c(&apos;training data&apos;, &apos;test data&apos;)</span><br><span class="line"># We can use locator(1) to specify the legend position by clicking the mouse</span><br><span class="line">#legend(locator(1), legend_char_list, cex=1.2, col=c(&apos;red&apos;, &apos;black&apos;), lty=c(1,2))</span><br><span class="line"># Or we can specify the legend position directly</span><br><span class="line">legend(&quot;topright&quot;, legend_char_list, cex=1.2, col=c(&apos;red&apos;, &apos;black&apos;), lty=c(1,2))</span><br><span class="line"></span><br><span class="line"># Step 7. Check the importance of all variables</span><br><span class="line"># We train a new random forest model consisting of 100 trees</span><br><span class="line">M_rf_imp &lt;-randomForest(mpg~., data = D_train, ntree = 100, importance = T)</span><br><span class="line">print(&apos;we show the variable importance using OOB samples&apos;)</span><br><span class="line">var_imp1 &lt;-importance(M_rf_imp, type=1)</span><br><span class="line">print(var_imp1)</span><br><span class="line"></span><br><span class="line">print(&apos;we show the variable importance using tree node impurity/MSE descrease&apos;)</span><br><span class="line">var_imp2 &lt;-importance(M_rf_imp, type=2)</span><br><span class="line">print(var_imp2)</span><br><span class="line"></span><br><span class="line"># Plot the importance of all variables</span><br><span class="line">varImpPlot(M_rf_imp, main = &apos;Variable importance of M_rf_imp&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>然后对数据做了一个简单的检查，即检查每列的类型，其输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;col　1　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　2　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　3　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　4　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　5　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　6　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　7　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　8　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　9　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　10　and its type is　numeric&quot;</span><br><span class="line">[1] &quot;col　11　and its type is　numeric&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>我们将数据按照70%和30%的比例分为训练集和测试集，并分别保存在数据框<code>D_train</code> 和<code>D_test</code> 中。</p>
<p>接着我们使用<code>rpart</code> 软件包构建了一个决策树的回归模型，其输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;show the summary of the trained model&quot;</span><br><span class="line">Call:</span><br><span class="line">rpart(formula = mpg ~ ., data = D_train)</span><br><span class="line">　n= 22 </span><br><span class="line"></span><br><span class="line">　　　　 CP nsplit rel error　　xerror　　　xstd</span><br><span class="line">1 0.6332989　　　0 1.0000000 1.1516657 0.3243608</span><br><span class="line">2 0.0100000　　　1 0.3667011 0.7476655 0.1917382</span><br><span class="line"></span><br><span class="line">Variable importance</span><br><span class="line">　hp　 wt　cyl disp drat qsec </span><br><span class="line">　20　 18　 16　 16　 14　 14 </span><br><span class="line"></span><br><span class="line">Node number 1: 22 observations,　　complexity param=0.6332989</span><br><span class="line">　mean=19.97273, MSE=40.49198 </span><br><span class="line">　left son=2 (12 obs) right son=3 (10 obs)</span><br><span class="line">　Primary splits:</span><br><span class="line">　　　hp　 &lt; 116.5 to the right, improve=0.6332989, (0 missing)</span><br><span class="line">　　　wt　 &lt; 3.325 to the right, improve=0.6219046, (0 missing)</span><br><span class="line">　　　cyl　&lt; 5　　 to the right, improve=0.6198157, (0 missing)</span><br><span class="line">　　　disp &lt; 163.8 to the right, improve=0.5950779, (0 missing)</span><br><span class="line">　　　drat &lt; 3.91　to the left,　improve=0.4739783, (0 missing)</span><br><span class="line">　Surrogate splits:</span><br><span class="line">　　　wt　 &lt; 3.325 to the right, agree=0.955, adj=0.9, (0 split)</span><br><span class="line">　　　cyl　&lt; 5　　 to the right, agree=0.909, adj=0.8, (0 split)</span><br><span class="line">　　　disp &lt; 163.8 to the right, agree=0.909, adj=0.8, (0 split)</span><br><span class="line">　　　drat &lt; 3.655 to the left,　agree=0.864, adj=0.7, (0 split)</span><br><span class="line">　　　qsec &lt; 18.25 to the left,　agree=0.864, adj=0.7, (0 split)</span><br><span class="line"></span><br><span class="line">Node number 2: 12 observations</span><br><span class="line">　mean=15.35, MSE=8.8225 </span><br><span class="line"></span><br><span class="line">Node number 3: 10 observations</span><br><span class="line">　mean=25.52, MSE=22.0796 </span><br><span class="line"></span><br><span class="line">[1] &quot;rmse_test_rpart1 = 3.60306119848109&quot;</span><br><span class="line">[1] &quot;rmse_train_rpart1 = 3.85336924592681&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，所得的决策树模型在测试集和训练集上的RMSE分别约为3.60和3.85。</p>
<p>之后在第4步中我们构建决策树模型。在第一个随机森林模型中，全部采用默认的参数值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_randomForest1 &lt;-randomForest(mpg~., data = D_train)</span><br></pre></td></tr></table></figure>
<p>随后我们使用<code>M_randomForest1$ntree</code> 检查一共构建了多少决策树。在第二个随机森林模型中，我们指定构建50棵决策树，并将<code>mtry</code> 设为3：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_randomForest2 &lt;-randomForest(mpg~., data = D_train, ntree=50, mtry=3)</span><br></pre></td></tr></table></figure>
<p>我们使用<code>print</code> 函数输出了这两个模型的一些信息：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;show the summary of the trained model&quot;</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line"> randomForest(formula = mpg ~ ., data = D_train) </span><br><span class="line">　　　　　　 Type of random forest: regression</span><br><span class="line">　　　　　　　　　　 Number of trees: 500</span><br><span class="line">No. of variables tried at each split: 3</span><br><span class="line"></span><br><span class="line">　　　　Mean of squared residuals: 6.648723</span><br><span class="line">　　　　　　　　 % Var explained: 83.58</span><br><span class="line">[1] &quot;number of trees in M_randomForest1 = 500&quot;</span><br><span class="line">[1] &quot;rmse_test_rf1 = 1.94172203933111&quot;</span><br><span class="line">[1] &quot;rmse_train_rf1 = 1.29780945437018&quot;</span><br><span class="line">[1] &quot;show the summary of the trained model&quot;</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line"> randomForest(formula = mpg ~ ., data = D_train, ntree = 50, mtry = 3) </span><br><span class="line">　　　　　　 Type of random forest: regression</span><br><span class="line">　　　　　　　　　 Number of trees: 50</span><br><span class="line">No. of variables tried at each split: 3</span><br><span class="line"></span><br><span class="line">　　　　 Mean of squared residuals: 8.865426</span><br><span class="line">　　　　　　　　 % Var explained: 78.11</span><br><span class="line">[1] &quot;number of trees in M_randomForest2 = 50&quot;</span><br><span class="line">[1] &quot;rmse_test_rf2 = 1.86589255022898&quot;</span><br><span class="line">[1] &quot;rmse_train_rf2 = 1.47045984929335&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>在第5步中，我们构建了<code>M_rf_base1</code> 、<code>M_rf_base2</code> 和<code>M_rf_base3</code> 随机森林模型，然后调用<code>combine</code> 函数将这3个模型聚合起来得到一个随机森林模型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_rf_comb &lt;-combine(M_rf_base1, M_rf_base2, M_rf_base3)</span><br></pre></td></tr></table></figure>
<p>在第6步中，我们首先构建只有一棵决策树的随机森林模型，然后使用<code>grow</code> 函数向该随机森林模型中不断添加新的决策树。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_rf_base &lt;-grow(M_rf_base, ntree_delta)</span><br></pre></td></tr></table></figure>
<p>图9-1显示了随着决策树数目的增加，训练集和测试集上的RMSE的变化。从图9-1中可以看出，当决策树刚开始增加时，训练集和测试集上的RMSE都开始减小。但是当决策树的数目继续增加时，训练集和测试集上的RMSE都在波动一下后保持平稳，说明进一步增加决策树的数量不会进一步提高模型的性能。</p>
<p><img src="Image01655.jpg" alt></p>
<p>图9-1 在随机森林模型中增加决策树的数目导致的训练集和测试集上RMSE的变化情况</p>
<p>在第7步中我们考虑了随机森林模型中变量的重要程度。在构建随机森林模型时，首先必须将参数<code>importance</code> 设为<code>TRUE</code> 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_rf_imp &lt;-randomForest(mpg~., data = D_train, ntree = 100, importance = T)</span><br></pre></td></tr></table></figure>
<p>然后可以使用<code>importance</code> 函数得到该模型中变量的重要程度。通过将参数<code>type</code> 设为1（使用OOB样本来估计）或者2（使用决策树中MSE或者不纯洁程度来度量），我们能够得到不同的重要程度度量。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var_imp1 &lt;-importance(M_rf_imp, type=1)</span><br><span class="line">var_imp2 &lt;-importance(M_rf_imp, type=2)</span><br></pre></td></tr></table></figure>

</details>


<p>其输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;we show the variable importance using OOB samples&quot;</span><br><span class="line">　　　%IncMSE</span><br><span class="line">cyl　4.596421</span><br><span class="line">disp 4.250417</span><br><span class="line">hp　 6.674625</span><br><span class="line">drat 1.616390</span><br><span class="line">wt　 6.409588</span><br><span class="line">qsec 2.719904</span><br><span class="line">vs　 1.652735</span><br><span class="line">am　 1.483702</span><br><span class="line">gear 1.306683</span><br><span class="line">carb 1.862388</span><br><span class="line">[1] &quot;we show the variable importance using tree node impurity/MSE descrease&quot;</span><br><span class="line">　　 IncNodePurity</span><br><span class="line">cyl　　　96.819302</span><br><span class="line">disp　　160.206895</span><br><span class="line">hp　　　221.200911</span><br><span class="line">drat　　 59.407580</span><br><span class="line">wt　　　196.157244</span><br><span class="line">qsec　　 21.761652</span><br><span class="line">vs　　　 17.399410</span><br><span class="line">am　　　　9.937528</span><br><span class="line">gear　　　7.189293</span><br><span class="line">carb　　 21.664176</span><br></pre></td></tr></table></figure>

</details>


<p>最后我们使用函数<code>varImpPlot</code> 直接绘出变量的重要程度，如图9-2所示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">varImpPlot(M_rf_imp, main = &apos;Variable importance of M_rf_imp&apos;)</span><br></pre></td></tr></table></figure>
<p><img src="Image01656.jpg" alt></p>
<p>图9-2 随机森林模型<code>M_rf_imp</code> 中使用两种不同的方法所得的变量的重要程度</p>
<h2 id="9-4-boosting简介"><a href="#9-4-boosting简介" class="headerlink" title="9.4 boosting简介"></a>9.4 boosting简介</h2><p>boosting是近年来机器学习领域比较热门的一个方向。boosting的本意是“提高”。与bagging中并行构建很多基学习器相比，boosting顺次建立一系列基学习器。在构建新的基学习器时，boosting通过分析当前已经建立的基学习器，如分析以前的基学习器错误分类的训练样本，寻找改进的方向，来构建新的基学习器。利用boosting，可以显著地提高传统算法的性能。</p>
<p>boosting的基本思想虽然简单，但是事实上涉及的数学基础较多。在本章中，我们首先以AdaBoost为例来介绍boosting。在下一节我们将介绍广泛应用的提升决策树<br>（boosted tree）和对应的梯度提升 （gradient boosting）算法。</p>
<p>在很多文献中，AdaBoost都是作为boosting的一个典型例子来介绍的。在下面的讨论中，我们以分类问题为例介绍AdaBoost的基本思想，其在回归问题中的使用可以类似地推导出。</p>
<p>AdaBoost的基本思想：首先构造一个弱分类器 （weak classifier）。根据该分类器在训练集上的分类结果，我们给训练集中的每个样本赋予一个权值<br>（weight）：如果该样本被正确分类，则权值较小；如果该样本被错误分类，则权值很大。之后我们考虑每个样本的权值以构建第二个弱分类器，使得权值较大的样本能够尽量被正确分类。换言之，在构建新的分类器时，我们尽量多考虑那些在前面被错分的样本。根据前面两个弱分类器的结果，我们可以更新每个样本的权值，从而构建第三个弱分类器。通过重复该过程，我们可以构建多个弱分类器，从而最终得到较好的分类效果。从直观上讲，后面的弱分类器集中处理前面被错分的样本，这样使得分类器犯的错误各不相同，因此聚合之后能够得到较好的效果。</p>
<p>在介绍AdaBoost算法之前，我们先介绍boosting的基本思想和指数损失函数 （exponential loss function）。在此基础上，我们给出AdaBoost的具体步骤，最后介绍如何在R中使用AdaBoost。</p>
<h3 id="9-4-1-boosting和指数损失函数"><a href="#9-4-1-boosting和指数损失函数" class="headerlink" title="9.4.1 boosting和指数损失函数"></a>9.4.1 boosting和指数损失函数</h3><p>在boosting中，我们所得的模型<img src="Image01657.gif" alt> 是顺次构建的多个模型的聚合：</p>
<p><img src="Image01658.gif" alt></p>
<p>（9-16）</p>
<p>在式（9-16）中，我们有 _m_ 个基学习器<img src="Image01659.gif" alt> ，这里 _<strong>x</strong> _ 表示输入，<img src="Image01660.gif" alt> 表示第 _j_ 个模型对应的参数，<img src="Image00523.gif" alt> 是模型<img src="Image01661.gif" alt> 对应的权重。注意，在第<img src="Image01662.gif" alt> 步时，我们所得的模型<img src="Image01663.gif" alt> 为：</p>
<p><img src="Image01664.gif" alt></p>
<p>（9-17）</p>
<p>在boosting中，我们顺次构建这些模型<img src="Image01665.gif" alt> ，同时<img src="Image01665.gif" alt> 一般是一类函数，且比较简单。例如，在我们接下来要讨论的提升决策树中，每个<img src="Image01665.gif" alt> 都是由一棵决策树来确定的。</p>
<p>在机器学习中，我们通常最小化模型<img src="Image00504.gif" alt> 在训练集<img src="Image01666.gif" alt> 上的损失函数来学习出模型的参数，这里<img src="Image01667.gif" alt> 。换言之，我们要求解如下的优化问题：</p>
<p><img src="Image01668.gif" alt></p>
<p>（9-18）</p>
<p>这里 _L_ 是一个损失函数，用来衡量<img src="Image00541.gif" alt> 拟合 _y i _ 的好坏程度。在这个优化问题中，我们要同时求解所有的 _w j _ 和<img src="Image01660.gif" alt> 。从求解优化问题的角度讲，同时求解所有的参数难度太大，因此我们可以采用顺序求解的方法来求解这个优化问题。</p>
<p>（1）求解第1个学习器的权重<img src="Image01669.gif" alt> 和参数<img src="Image01670.gif" alt> 。</p>
<p>（2）固定权重<img src="Image01669.gif" alt> 和参数<img src="Image01670.gif" alt> ，求解第2个学习器的权重<img src="Image01671.gif" alt> 和参数<img src="Image01672.gif" alt> 。</p>
<p>（3）重复该过程，直到求解第 _m_ 个学习器的权重<img src="Image01673.gif" alt> 和参数<img src="Image01674.gif" alt> 。</p>
<p>严格地讲，在第 _j_ 步我们求解如下最优化问题：</p>
<p><img src="Image01675.gif" alt></p>
<p>（9-19）</p>
<p>作为第 _j_ 个弱学习器的权值和参数。换言之，在第 _j_ 步时，固定住已求解出的<img src="Image01676.gif" alt> 和<img src="Image01677.gif" alt> ，然后优化<img src="Image00523.gif" alt> 和<img src="Image01660.gif" alt> 。简而言之，就是由于同时求解所有的<img src="Image01678.gif" alt> 和<img src="Image01679.gif" alt> 难度太大，因此我们采用了类似贪婪算法的方法来顺次求解该优化问题。</p>
<p>接下来我们讨论指数损失函数 ，并以此函数引入AdaBoost。事实上，AdaBoost先被提出。通过分析AdaBoost算法，人们发现其对应于指数损失函数。但是在本书中为了方便读者理解AdaBoost，我们先从指数损失函数开始讨论，进而推导出AdaBoost算法。</p>
<p>指数函数适用于分类问题。在本章中我们假设类标<img src="Image00930.gif" alt> ，则指数损失函数的定义为：</p>
<p><img src="Image01680.gif" alt></p>
<p>（9-20）</p>
<p>图9-3给出了指数函数的图像，同时也给出了其他一些常用损失函数的图像。</p>
<p>下面我们解释为何指数损失函数的定义是合理的。由于我们假设<img src="Image00914.gif" alt> ，因此，当<img src="Image01681.gif" alt> 时，我们希望<img src="Image00541.gif" alt> 越大越好；当<img src="Image01682.gif" alt> 时，我们希望<img src="Image00541.gif" alt> 越小越好，最好是负数。综合起来，就是希望<img src="Image00925.gif" alt> 越大越好。与在SVM中的讨论类似，当<img src="Image01683.gif" alt> 时，分类是正确的，我们不应该引入损失或者引入极小的损失。而当<img src="Image00925.gif" alt> &lt;0时，我们应引入损失，而且<img src="Image00925.gif" alt> 的值越小，引入的损失越大。因此，这里我们引入指数函数<img src="Image01684.gif" alt> 来处理这两种不同的情况。注意，当<img src="Image01683.gif" alt> 时，我们仍然引入了损失，但是很小。指数函数的最大优点是它是连续可导的，而且导数非常容易计算，使得我们在求解相关的优化问题时很容易计算。</p>
<p><img src="Image01685.jpg" alt></p>
<p>图9-3 分类算法中的常用损失函数</p>
<h3 id="9-4-2-AdaBoost算法"><a href="#9-4-2-AdaBoost算法" class="headerlink" title="9.4.2 AdaBoost算法"></a>9.4.2 AdaBoost算法</h3><p>事实上，当使用指数损失函数时，如果我们在boosting中使用顺次求解 （forward stage-wise additive modeling）的方式来导出权重和分类器，就得到了著名的AdaBoost（Adaptive Boosting）算法。下面我们进行推导并给出AdaBoost算法的具体流程。</p>
<p>假设我们已经推导出前面的 _j_ -1个学习器的权重和参数，并记前 _j_<br>-1个学习器的加权和所得的模型<img src="Image01686.gif" alt> 为：</p>
<p><img src="Image01687.gif" alt></p>
<p>（9-21）</p>
<p>在使用指数损失函数的情况下，在第 _j_ 步我们需要求解如下问题：</p>
<p><img src="Image01688.gif" alt></p>
<p>（9-22）</p>
<p>这里我们将目标值简写为：</p>
<p><img src="Image01689.gif" alt></p>
<p>（9-23）</p>
<p>注意，<img src="Image01690.gif" alt> 在这里只依赖于<img src="Image01691.gif" alt> 和<img src="Image01692.gif" alt> ，因此在求解第 _j_ 步时为定值，可以将其视为在第 _j_ 步时对于样本<img src="Image00730.gif" alt> 的权值。我们将该权值记为：</p>
<p><img src="Image01693.gif" alt></p>
<p>（9-24）</p>
<p>注意，该权值<img src="Image01694.gif" alt> 既与样本<img src="Image00730.gif" alt> 的真实类别<img src="Image01695.gif" alt> 有关，也与已经构建的分类器<img src="Image01696.gif" alt> 对该样本的当前分类结果有关。事实上，<img src="Image01697.gif" alt> 就是我们使用<img src="Image01692.gif" alt> 作为<img src="Image00730.gif" alt> 的分类结果所引起的指数损失函数值。因此，目标函数可以简写为：</p>
<p><img src="Image01698.gif" alt></p>
<p>（9-25）</p>
<p>下面我们讨论在第 _j_ 步如何具体求解权值<img src="Image00567.gif" alt> 和函数<img src="Image01699.gif" alt> 。</p>
<p>在分类问题中，我们首先注意到<img src="Image01700.gif" alt> ，<img src="Image01701.gif" alt> ，<img src="Image01702.gif" alt> &gt;0。当<img src="Image01703.gif" alt> 时，<img src="Image01704.gif" alt> ，当<img src="Image01705.gif" alt> 时，<img src="Image01706.gif" alt> 。</p>
<p>因此，可以将目标函数表示为：</p>
<p><img src="Image01707.gif" alt></p>
<p><img src="Image01708.gif" alt></p>
<p>（9-26）</p>
<p>如果假定权重 _w_ &gt;0，则<img src="Image01709.gif" alt> &gt;0, exp (-_w_ )&gt;0。注意，此时 _F_ 中只有前一项中考虑了函数<img src="Image01710.gif" alt> 。更进一步，对于任何一个给定的 _w_ &gt;0，使得 _F_ 最小的<img src="Image01710.gif" alt> 必然使得<img src="Image01711.gif" alt> 最小。换言之，函数<img src="Image01699.gif" alt> 可通过求解如下问题得到：</p>
<p><img src="Image01712.gif" alt></p>
<p>（9-27）</p>
<p>这里函数 _I_ ( _x_ )的定义为：</p>
<p><img src="Image01713.jpg" alt></p>
<p>简单地讲，函数<img src="Image01714.gif" alt> 就是使得使用权重<img src="Image01697.gif" alt> 之后的错误率最低的分类器。</p>
<p>下面我们推导在规定函数<img src="Image01699.gif" alt> 的情况下如何得到最优的<img src="Image00523.gif" alt> 。如果记 _A_ =<img src="Image01715.gif" alt> ，那么可以将目标函数表示为：</p>
<p><img src="Image01716.gif" alt></p>
<p>（9-28）</p>
<p>我们计算 _F_ 对 _w_ 的偏导数并设为0即可得到最优的 _w_ ：</p>
<p><img src="Image01717.gif" alt></p>
<p>（9-29）</p>
<p>如果对于模型<img src="Image01714.gif" alt> ，我们引入加权的错误率<img src="Image01718.gif" alt> ：</p>
<p><img src="Image01719.gif" alt></p>
<p>（9-30）</p>
<p>则最优的<img src="Image00567.gif" alt> 可表示为：</p>
<p><img src="Image01720.gif" alt></p>
<p>（9-31）</p>
<p>那么在构建第 _j_ 个分类器后，前 _j_ 个分类器的和为：</p>
<p><img src="Image01721.gif" alt></p>
<p>（9-32）</p>
<p>注意，<img src="Image01722.gif" alt> ，那么每个样本对应的权重在构建第 _j_ 个分类器后可以利用如下公式更新：</p>
<p><img src="Image01723.gif" alt></p>
<p>（9-33）</p>
<p>在式（9-33）中的倒数第二步，我们利用了性质<img src="Image01724.gif" alt> 。验证该式子很简单，只需要考虑<img src="Image01706.gif" alt><br>（即<img src="Image01705.gif" alt> )和<img src="Image01704.gif" alt><br>（即<img src="Image01703.gif" alt><br>）两种不同的情况即可。</p>
<p>这样我们就得到了著名的AdaBoost算法 。算法9-3给出了AdaBoost算法的具体步骤。</p>
<p>算法9-3 AdaBoost算法</p>
<blockquote>
<p>1. 将训练集中每个样本的权值定为<img src="Image01725.gif" alt> &gt; &gt; 2. for _j_ =1: _m_ &gt; &gt; 2.1 构建一个分类器<img src="Image01699.gif" alt> &gt; （考虑训练集中每个样本的权值<img src="Image01726.gif" alt> ） &gt; &gt; 2.2 计算该分类器加权的错误率 &gt; &gt; <img src="Image01727.gif" alt> &gt; &gt; （9-34） &gt; &gt; 2.3 计算该分类器的权值 &gt; &gt; <img src="Image01728.gif" alt> &gt; &gt; （9-35） &gt; &gt; 2.4 更新每个样本的权值 &gt; &gt; <img src="Image01729.gif" alt> &gt; &gt; （9-36） &gt; &gt; 3．输出最终的分类器 &gt; &gt; <img src="Image01730.gif" alt> &gt; &gt; （9-37）</p>
</blockquote>
<p>在该算法中，我们顺次构建 _m_ 个分类器，且每个分类器能够处理样本的权重（如果分类器不能在训练过程中处理样本的权重，则我们不能直接应用该分类器）。在第 _j_ 步，得到该分类器<img src="Image01699.gif" alt> 后，我们计算该分类器对应的错误率<img src="Image01731.gif" alt> 并据此计算<img src="Image01699.gif" alt> 对应的权重<img src="Image00523.gif" alt> 。然后根据<img src="Image01699.gif" alt> 和<img src="Image00523.gif" alt> 更新每个样本的权重<img src="Image01726.gif" alt> 。与前面推导的公式相比，在更新<img src="Image01726.gif" alt> 时我们省略了<img src="Image01732.gif" alt> ，原因是所有的权重都要乘以<img src="Image01732.gif" alt> ，因此可以直接省略该项。在更新样本的权重<img src="Image01726.gif" alt> 时，我们可以看到：如果<img src="Image01705.gif" alt> ，则<img src="Image01726.gif" alt> 不变；如果<img src="Image01703.gif" alt> ，则<img src="Image01726.gif" alt> 要乘以权重<img src="Image01733.gif" alt> ，这样在构建下一个分类器时会更加重视这些被错分的样本。因此，在实际使用AdaBoost时，我们要为每个样本记录它们当前的权重<img src="Image01726.gif" alt> ，同时也要记录每个分类器对应的权重<img src="Image00523.gif" alt> 。</p>
<h3 id="9-4-3-AdaBoost的实际使用"><a href="#9-4-3-AdaBoost的实际使用" class="headerlink" title="9.4.3 AdaBoost的实际使用"></a>9.4.3 AdaBoost的实际使用</h3><p>在本节，我们讨论如何使用R中的<code>adabag</code> 包 ④ 来调用AdaBoost算法。<code>adabag</code> 软件包实现了适用于分类问题 的boosting和bagging程序。我们提供了 AdaBoost_classification_example.R文件来介绍如何使用<code>adabag</code> 软件包调用AdaBoost算法。</p>
<p>在<code>adabag</code> 包中，需要使用<code>boosting</code> 函数调用AdaBoost算法。该函数实现了AdaBoost算法，能够解决两类分类问题和多类分类问题。在使用<code>boosting</code> 函数时，需要指定数据，包括具体的输入数据和对应的公式（指定哪个变量是目标变量，哪些变量是自变量）。下面我们简要介绍<code>boosting</code> 函数中的几个重要参数。</p>
<ul>
<li>参数<code>boos</code> ：默认值是<code>TRUE</code> ，表示我们将利用当前每个样本的权重使用bootstrap取样来得到当前的训练集。如果是<code>FALSE</code> 的话，每个样本以及当前的权重将会被使用（而不是用bootstrap取样）。</li>
<li>参数<code>mfinal</code> ：boosting中弱分类器的总数，默认值是100。</li>
<li>参数<code>coeflearn</code> ：用来指定在boosting算法中计算权重<img src="Image01734.gif" alt> 的方法。可选的值包括<code>&quot;Breiman&quot;</code> （默认值）、<code>&quot;Freund&quot;</code> 和<code>&quot;Zhu&quot;</code> ，其中<code>&quot;Breiman&quot;</code> 对应算法9-3中<img src="Image01734.gif" alt> 的更新方法。</li>
<li>参数<code>control</code> ：用来控制每个弱分类器的参数。在<code>adabag</code> 包中，由于我们调用<code>rpart</code> 包训练得到决策树作为弱分类器，因此使用<code>rpart</code> 包中的<code>rpart.control</code> 函数来具体指定控制参数，如每棵决策树的深度等。</li>
</ul>
<p>与前面讨论的标准AdaBoost算法相比，这里的参数<code>boos</code> 指定了在构建每个分类器时利用权值的方法。<code>boosting</code> 函数返回一个<code>boosting</code> 对象（也称<code>boosting</code> ，但是这个是对象，前一个是函数），包括所涉及的决策树的数目、每棵决策树的权重，以及所得的所有决策树的具体参数。我们可以逐一检查返回的决策树。</p>
<p>与随机森林类似，一棵决策树很容易理解，但是很多决策树的聚合却不容易理解。在这种情况下，如果我们能够计算每个变量的重要程度，将会帮助我们理解所得的模型。利用返回的<code>boosting</code> 对象和<code>errorevol</code> 函数，我们可以计算每个变量的重要程度。这里变量的重要程度是利用每棵决策树中每个变量带来的基尼指数的减小，并综合考虑每棵决策树的权重所得到的。</p>
<p>下面我们通过讲解AdaBoost_classification_example.R文件中的代码来说明如何使用<code>adabag</code> 包。</p>
<p>首先我们检查<code>adabag</code> 包是否已经安装。如果没有安装，则首先安装该软件包。在第1步，我们载入<code>Sonar</code> 数据。注意，<code>mlbench</code> 包自带该数据集，而<code>adabag</code> 依赖于<code>mlbench</code> 包。当我们使用<code>library(adabag)</code> 载入<code>adabag</code> 包时，<code>mlbench</code> 包也自动载入，因此，这里可以使用<code>data(Sonar)</code> 来直接载入该数据集。该数据集的最后一列是类标，我们将其列名改为<code>&#39;classLabel&#39;</code> 。在第2步，我们将数据按照70%和30%的比例分为训练集和测试集，并分别保存在数据框<code>D_train</code> 和<code>D_test</code> 中。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"># Step 0. Check required package is installed or not. If not, install first.</span><br><span class="line"># 1. adabag</span><br><span class="line">adabag.installed &lt;-&apos;adabag&apos; %in% rownames(installed.packages())</span><br><span class="line">if (adabag.installed) &#123;</span><br><span class="line">　print(&quot;the adabag package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the adabag package first...&quot;)</span><br><span class="line">　install.packages(&apos;adabag&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(&apos;adabag&apos;)</span><br><span class="line">library(&apos;rpart&apos;)</span><br><span class="line"></span><br><span class="line"># Step 1. Load the Sonar data in the mlbench library</span><br><span class="line">data(Sonar)</span><br><span class="line">D &lt;-Sonar</span><br><span class="line">colnames(D)[ncol(D)] &lt;-&apos;classLabel&apos;</span><br><span class="line"></span><br><span class="line"># Step 2. Split the data into training and test sets</span><br><span class="line"># Randomly split the whole data set into a training and a test data set</span><br><span class="line"># After spliting, we have the training set: (X_train, y_train)</span><br><span class="line"># and the test data set: (X_test, y_test)</span><br><span class="line">train_ratio &lt;-0.7</span><br><span class="line">n_total &lt;-nrow(D)</span><br><span class="line">n_train &lt;-round(train_ratio * n_total)</span><br><span class="line">n_test &lt;-n_total -n_train</span><br><span class="line">set.seed(42)</span><br><span class="line">list_train &lt;-sample(n_total, n_train)</span><br><span class="line">D_train &lt;-D[list_train,]</span><br><span class="line">D_test &lt;-D[-list_train,]</span><br><span class="line">y_train &lt;-D_train$classLabel</span><br><span class="line">y_test &lt;-D_test$classLabel</span><br><span class="line"></span><br><span class="line"># Step 3. Benchmark: train a single decision tree using rpart</span><br><span class="line">M_rpart1 &lt;-rpart(classLabel~., data = D_train)</span><br><span class="line">print(&apos;show the summary of the trained model&apos;)</span><br><span class="line">summary(M_rpart1)</span><br><span class="line"></span><br><span class="line"># Compute the performance on the training and test data sets</span><br><span class="line">y_test_pred_rpart1 &lt;-predict(M_rpart1, D_test, type=&apos;class&apos;)</span><br><span class="line">accuracy_test_rpart1 &lt;-sum(y_test==y_test_pred_rpart1) / n_test</span><br><span class="line">msg &lt;-paste0(&apos;accuracy_test_rpart1 = &apos;, accuracy_test_rpart1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_train_pred_rpart1 &lt;-predict(M_rpart1, D_train, type=&apos;class&apos;)</span><br><span class="line">accuracy_train_rpart1 &lt;-sum(y_train==y_train_pred_rpart1) / n_train</span><br><span class="line">msg &lt;-paste0(&apos;accuracy_train_rpart1 = &apos;, accuracy_train_rpart1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line"># Step 4. Train a simple AdaBoost model</span><br><span class="line">maxdepth &lt;-4</span><br><span class="line">mfinal &lt;-60</span><br><span class="line">M_AdaBoost1 &lt;-boosting(classLabel~., data = D_train, </span><br><span class="line">　　　　　　　　　　　　boos = FALSE, mfinal = mfinal, coeflearn = &apos;Breiman&apos;,</span><br><span class="line">　　　　　　　　　　　　control=rpart.control(maxdepth=maxdepth))</span><br><span class="line"></span><br><span class="line"># Check the summary of the trained AdaBoost model</span><br><span class="line">summary(M_AdaBoost1)</span><br><span class="line"># We get all trees</span><br><span class="line">M_AdaBoost1$trees</span><br><span class="line"># print the 1st tree</span><br><span class="line">M_AdaBoost1$trees[[1]]</span><br><span class="line"># We get the weights for all trees</span><br><span class="line">M_AdaBoost1$weights</span><br><span class="line"># We get the variable importance</span><br><span class="line">M_AdaBoost1$importance</span><br><span class="line"># We get the evolution of the error</span><br><span class="line">errorevol(M_AdaBoost1, D_train)</span><br><span class="line"># Check the first tree trained in AdaBoost</span><br><span class="line">t1 &lt;-M_AdaBoost1$trees[[1]]</span><br><span class="line"># Plot the tree</span><br><span class="line">plot(t1, uniform=T, branch=0, margin=0.1, main = &apos;classification tree&apos;)</span><br><span class="line">text(t1, splits=T, all = T, fancy = T)</span><br><span class="line"></span><br><span class="line"># Compute the accuracy of AdaBoost model on the training and test data sets</span><br><span class="line">y_test_pred_AdaBoost1 &lt;-predict(M_AdaBoost1, D_test)</span><br><span class="line">accuracy_test_AdaBoost1 &lt;-sum(y_test==y_test_pred_AdaBoost1$class) / n_test</span><br><span class="line">msg &lt;-paste0(&apos;accuracy_test_AdaBoost1 = &apos;, accuracy_test_AdaBoost1)</span><br><span class="line">print(msg)</span><br><span class="line"></span><br><span class="line">y_train_pred_AdaBoost1 &lt;-predict(M_AdaBoost1, D_train)</span><br><span class="line">accuracy_train_AdaBoost1 &lt;-sum(y_train==y_train_pred_AdaBoost1$class) / n_train</span><br><span class="line">msg &lt;-paste0(&apos;accuracy_train_AdaBoost1 = &apos;, accuracy_train_AdaBoost1)</span><br><span class="line">print(msg)</span><br></pre></td></tr></table></figure>

</details>


<p>在第3步，我们使用<code>rpart</code> 包构建一个决策树的分类模型以与AdaBoost相比较。</p>
<p>在第4步，我们构建了一个AdaBoost模型。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">M_AdaBoost1 &lt;-boosting(classLabel~., data = D_train, </span><br><span class="line">　　　　　　　　　　　　boos = FALSE, mfinal = mfinal, coeflearn = &apos;Breiman&apos;,</span><br><span class="line">　　　　　　　　　　　　control=rpart.control(maxdepth=maxdepth))</span><br></pre></td></tr></table></figure>

</details>


<p>在该步骤中，指定参数<code>boos</code> 为<code>FALSE</code> ，表示我们要使用所有的样本；将参数<code>mfinal</code> 设为<code>60</code> ，表示我们将构建<code>60</code> 棵决策树。此外，我们还将<code>control</code> 设为<code>rpart.control(maxdepth=maxdepth)</code> ，表示我们使用<code>rpart</code> 包构建决策树时将<code>rpart</code> 函数中的控制参数<code>maxdepth</code> 设为4。</p>
<p>这样我们就得到了一个称为<code>M_AdaBoost1</code> 的<code>boosting</code> 对象。我们使用<code>summary</code> 函数来输出该AdaBoost模型的主要信息。此外，我们可以使用<code>M_AdaBoost1</code> 中的如下成员来查看该<code>boosting</code> 对象的相关信息。</p>
<ul>
<li><code>trees</code> ：得到构建的所有决策树。其中<code>trees[</code> [i]]表示其中的第<code>i</code> 棵决策树。</li>
<li><code>weights</code> ：保存每棵决策树对应的权重。</li>
<li><code>importance</code> ：保存每个变量的重要程度。</li>
</ul>
<p>在上面的程序中，我们使用<code>t1</code> 来保存<code>M_AdaBoost1</code> 中的第一棵决策树，并使用<code>plot</code> 和<code>text</code> 函数绘出<code>t1</code> 所对应的决策树的图像，如图9-4所示。</p>
<p><img src="Image01735.jpg" alt></p>
<p>图9-4 AdaBoost模型中所得的第一棵树</p>
<p>程序的输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;accuracy_test_rpart1 = 0.693548387096774&quot;</span><br><span class="line">[1] &quot;accuracy_train_rpart1 = 0.876712328767123&quot;</span><br><span class="line">[1] &quot;accuracy_test_AdaBoost1 = 0.854838709677419&quot;</span><br><span class="line">[1] &quot;accuracy_train_AdaBoost1 = 1&quot;</span><br><span class="line"></span><br><span class="line">&gt; summary(M_AdaBoost1)</span><br><span class="line">　　　　　 Length Class　 Mode　　 </span><br><span class="line">formula　　　3　　formula call　　 </span><br><span class="line">trees　　　 60　　-none-　list　　 </span><br><span class="line">weights　　 60　　-none-　numeric　</span><br><span class="line">votes　　　292　　-none-　numeric　</span><br><span class="line">prob　　　 292　　-none-　numeric　</span><br><span class="line">class　　　146　　-none-　character</span><br><span class="line">importance　60　　-none-　numeric　</span><br><span class="line">terms　　　　3　　terms　 call　　 </span><br><span class="line">call　　　　 7　　-none-　call　　 </span><br><span class="line">&gt; M_AdaBoost1$trees[[1]]</span><br><span class="line">n= 146 </span><br><span class="line"></span><br><span class="line">node), split, n, loss, yval, (yprob)</span><br><span class="line">　　　* denotes terminal node</span><br><span class="line"></span><br><span class="line"> 1) root 146 0.486301400 M (0.52737671 0.47262329)　</span><br><span class="line">　 2) V11&gt;=0.17885 85 0.143835600 M (0.76299475 0.23700525)　</span><br><span class="line">　　 4) V17&lt; 0.6618 61 0.047945210 M (0.89069716 0.10930284)　</span><br><span class="line">　　　 8) V6&lt; 0.17975 54 0.020547950 M (0.94725111 0.05274889) *</span><br><span class="line">　　　 9) V6&gt;=0.17975 7 0.020547950 R (0.44204322 0.55795678) *</span><br><span class="line">　　 5) V17&gt;=0.6618 24 0.068493150 R (0.43004587 0.56995413)　</span><br><span class="line">　　　10) V51&gt;=0.0131 12 0.020547950 M (0.76013514 0.23986486) *</span><br><span class="line">　　　11) V51&lt; 0.0131 12 0.006849315 R (0.08761682 0.91238318) *</span><br><span class="line">　 3) V11&lt; 0.17885 61 0.075342470 R (0.18857143 0.81142857)　</span><br><span class="line">　　 6) V5&gt;=0.0696 15 0.041095890 M (0.61307902 0.38692098) *</span><br><span class="line">　　 7) V5&lt; 0.0696 46 0.013698630 R (0.04581552 0.95418448) *</span><br><span class="line">&gt; M_AdaBoost1$weights</span><br><span class="line"> [1] 0.9808293 1.3074799 0.9044111 0.9944090 0.9292653 0.7900282 1.1807839 1.0509344</span><br><span class="line"> [9] 0.9620091 0.8103219 0.8708190 0.8113213 0.7464254 1.3198593 1.1971926 0.9290395</span><br><span class="line">[17] 1.0168932 0.9753769 0.9764947 1.1010859 1.0100822 1.0342562 1.0733267 0.7857949</span><br><span class="line">[25] 1.2606899 0.8798757 0.9931865 0.9910637 1.0070627 1.0809842 1.2443802 1.0938129</span><br><span class="line">[33] 0.8468769 1.0964881 1.2147754 1.1274905 1.0866019 1.1412484 0.9799360 1.0320253</span><br><span class="line">[41] 1.1127173 0.9088232 0.9389669 0.9261423 0.9555823 0.9837087 0.9924242 0.8690865</span><br><span class="line">[49] 0.9102623 0.9291862 0.9977138 1.1003268 0.8939846 0.9973063 0.8033537 1.1148003</span><br><span class="line">[57] 0.9812458 0.9828331 0.9007920 1.3148560</span><br><span class="line">&gt; M_AdaBoost1$importance</span><br><span class="line">　　　 V1　　　 V10　　　 V11　　　 V12　　　 V13　　　 V14　　　 V15　　　 V16</span><br><span class="line">1.4812561 1.5426775 7.8247186 5.3938873 1.5469844 1.1158881 0.4165955 0.8154501</span><br><span class="line">　　　V17　　　 V18　　　 V19　　　　V2　　　 V20　　　 V21　　　 V22　　　 V23</span><br><span class="line">2.1841351 0.0000000 0.2783136 0.3078752 0.9507150 1.3128143 1.1018555 2.7719996</span><br><span class="line">　　　V24　　　 V25　　　 V26　　　 V27　　　 V28　　　 V29　　　　V3　　　 V30</span><br><span class="line">0.1851123 0.0000000 1.0870610 4.5926235 2.0182482 0.0000000 0.2588854 0.0000000</span><br><span class="line">　　　V31　　　 V32　　　 V33　　　 V34　　　 V35　　　 V36　　　 V37　　　 V38</span><br><span class="line">0.8016263 0.4935591 0.5347451 3.7400880 1.3330086 6.4139607 1.1847926 0.2944026</span><br><span class="line">　　　V39　　　　V4　　　 V40　　　 V41　　　 V42　　　 V43　　　 V44　　　 V45</span><br><span class="line">0.4898486 2.7445508 0.0000000 0.4289211 1.9320352 3.1744688 1.3742040 5.4004112</span><br><span class="line">　　　V46　　　 V47　　　 V48　　　 V49　　　　V5　　　 V50　　　 V51　　　 V52</span><br><span class="line">3.5877499 0.3224826 3.2149998 1.7710574 0.9377694 0.2963930 2.1314830 2.6108785</span><br><span class="line">　　　V53　　　 V54　　　 V55　　　 V56　　　 V57　　　 V58　　　 V59　　　　V6</span><br><span class="line">0.7701156 1.0362824 2.5357311 0.6760309 1.2456797 1.7136457 1.6977423 1.6227034</span><br><span class="line">　　　V60　　　　V7　　　　V8　　　　V9 </span><br><span class="line">1.5132980 1.1750007 1.3802089 2.2330285</span><br></pre></td></tr></table></figure>

</details>


<p>在上面的程序中，我们比较了<code>rpart</code> 所构建的决策树模型和AdaBoost所构建的分类模型在测试集和训练集上的准确率。从上面的输出可以看出，与<code>rpart</code> 所构建的单个决策树相比，AdaBoost所构建的模型在训练集上的准确率从0.877提高到了1.000；同时，在测试集上的准确率从0.694提高到了0.855。这里我们为了显示结果的方便，在使用AdaBoost时只构建了60棵决策树。当我们把决策树的数目提高到100时，AdaBoost所得的模型在测试集上的准确率会进一步提高到0.887，当决策树数目提高到200时，测试集上的准确率会提高到0.903。</p>
<h3 id="9-4-4-讨论"><a href="#9-4-4-讨论" class="headerlink" title="9.4.4 讨论"></a>9.4.4 讨论</h3><p>与bagging相比，boosting更加激进地从训练集中提取信息来训练模型。因此，boosting更容易受到噪声的影响，更容易导致过拟合。举一个简单的例子，如果我们在训练集中将某一样本的类标标错了，那么该样本有可能成为训练集中较难分类的样本，导致我们在使用boosting的过程中增大了该样本的权重，从而使得训练所得的分类器受到了错误的影响。此外，由于在boosting中我们要顺次构建模型，在算法的实现中不如容易并行处理的bagging速度快。</p>
<h2 id="9-5-提升决策树和梯度提升算法"><a href="#9-5-提升决策树和梯度提升算法" class="headerlink" title="9.5 提升决策树和梯度提升算法"></a>9.5 提升决策树和梯度提升算法</h2><p>在本节中，我们着重介绍提升决策树 以及对应的梯度提升算法 ，并介绍R中相应的<code>gbm</code> 包。</p>
<h3 id="9-5-1-提升决策树和梯度提升算法的基本原理"><a href="#9-5-1-提升决策树和梯度提升算法的基本原理" class="headerlink" title="9.5.1 提升决策树和梯度提升算法的基本原理"></a>9.5.1 提升决策树和梯度提升算法的基本原理</h3><p>在boosting中构建每个新的分类器时，我们希望它能够弥补以前模型的“弱点”。这里的“弱点”可以以多种形式表现出来。在前面讨论的AdaBoost算法中，我们给每个样本赋予一个权值，而那些用现有模型难以分类的样本则被赋予较高的权值，从而突出前面分类器的“弱点”。在本节介绍的梯度提升算法中，我们每次根据已有的分类情况，构建新的目标值以更好地分类那些“难分类”的样本。具体来说，对于样本<img src="Image00730.gif" alt> ，在构建第 _j_ 个学习器时，我们用<img src="Image01736.gif" alt> 来表示对应的目标值（注意，开始时<img src="Image01737.gif" alt><br>）。通过不停地改变<img src="Image01736.gif" alt> ，使得聚合后的分类器能够更好地分类样本<img src="Image00730.gif" alt> 。</p>
<p>在提升决策树中，我们假设每个弱学习器都是一棵决策树。我们知道AdaBoost算法对应指数损失函数，而在提升决策树中，我们可以采用不同的损失函数。与机器学习中的很多算法类似，在提升决策树中，给定训练集<img src="Image00879.gif" alt> ，我们最小化模型<img src="Image00504.gif" alt> 对应的损失函数 _L_ ( _y_ , _f_ )来求解最优的模型：</p>
<p><img src="Image01738.gif" alt></p>
<p>（9-38）</p>
<p>在提升决策树中，我们假设<img src="Image00506.gif" alt> 表示为一系列决策树的和：</p>
<p><img src="Image01739.gif" alt></p>
<p>（9-39）</p>
<p>这里我们把第 _j_ 棵决策树对应的函数记为<img src="Image01699.gif" alt> 。将第<img src="Image01740.gif" alt> 步所得的模型<img src="Image01741.gif" alt> 记为<img src="Image01742.gif" alt> 。</p>
<p>在梯度提升算法 中，我们顺次建立模型<img src="Image01699.gif" alt> 。该算法的核心思想是在第<img src="Image01743.gif" alt> 步构建模型<img src="Image01699.gif" alt> 时，采用负梯度<img src="Image01744.gif" alt> 作为<img src="Image00730.gif" alt> 的新目标值来训练模型。从数值最优化的角度讲，这种策略近似于数值最优化中的梯度下降 算法。下面我们从平方和损失函数对应的残差和数值最优化的角度阐述使用负梯度值<img src="Image01744.gif" alt> 作为新目标值的合理性。</p>
<p>我们首先讨论平方和损失函数对应的残差。假设在第 _j_ 步，我们已得到模型<img src="Image01686.gif" alt> ，可以计算残差<br>（residual）<img src="Image01745.gif" alt> 。残差表示了真实值和当前的预测值之间的差异。如果能使得差异为0，则意味着能得到极好的模型。因此，一种朴素的想法是不妨将残差<img src="Image01745.gif" alt> 作为下一个模型中<img src="Image00730.gif" alt> 对应的新目标值。</p>
<p>下面我们从数值优化的角度来说明使用残差作为新目标值的合理性。考虑使用平方和损失函数来度量训练集上的损失：</p>
<p><img src="Image01746.gif" alt></p>
<p>（9-40）</p>
<p>我们要优化<img src="Image00504.gif" alt> 使得损失函数<img src="Image01747.gif" alt> 最小。注意，这里要优化的对象<img src="Image00504.gif" alt> 是一个函数。换言之，我们需要在函数空间中求解该优化问题。如果只考虑训练集，为了简化讨论，可以将<img src="Image00504.gif" alt> 考虑为一个 _n_ 维向量<img src="Image01748.gif" alt> 。这样的话，我们需要在 _n_ 维向量空间中寻找使得损失函数<img src="Image01749.gif" alt> 最小的向量 _<strong>f</strong> _ 。</p>
<p>这样我们就可以直接使用数值最优化中的结果。在数值优化中，假设要优化的参数是 _<strong>f</strong> _ （这里<img src="Image01750.gif" alt> ，为 _n_ 维向量），要最小化的目标函数是<img src="Image01749.gif" alt> 。在梯度下降算法的每一步，假设 _<strong>f</strong> _ 的当前估计为<img src="Image01751.gif" alt> ，我们计算其对应的梯度<img src="Image01752.gif" alt> ，然后使用如下公式来更新 _<strong>f</strong> _ ：</p>
<p><img src="Image01753.gif" alt></p>
<p>（9-41）</p>
<p>当使用平方和损失函数<img src="Image01749.gif" alt> 时，可以计算<img src="Image01749.gif" alt> 对于 _<strong>f</strong> _ 中每个分量<img src="Image01754.gif" alt> 的偏导数：</p>
<p><img src="Image01755.gif" alt></p>
<p>（9-42）</p>
<p>写成向量的形式，负梯度可以表示为：</p>
<p><img src="Image01756.gif" alt></p>
<p>（9-43）</p>
<p>在第 _j_ 步更新模型<img src="Image00504.gif" alt> 时，<img src="Image00541.gif" alt> 的当前值是<img src="Image01692.gif" alt> 。此时的负梯度<img src="Image01757.gif" alt> 值为：</p>
<p><img src="Image01758.gif" alt></p>
<p>（9-44）</p>
<p>这里我们把第 _j_ 步的负梯度记为<img src="Image01759.gif" alt> 。可以看出，在使用平方和损失函数的情况下，负梯度等于我们前面讨论的第 _j_ 步的残差，也是在第 _j_ 步训练新模型<img src="Image01699.gif" alt> 时的目标值。</p>
<p>我们可以将以上的讨论推广到一般情况。对于不同的问题，我们需要选择不同的损失函数（而不局限于平方和损失函数）。对于不同的损失函数，可以计算相应的负梯度，并使用负梯度作为新的目标值来训练新的学习器。在介绍R中的<code>gbm</code> 包时，会涉及不同的损失函数。</p>
<p>注意，在梯度提升算法中，采用负导数<img src="Image01759.gif" alt> 作为新的目标来构建模型，我们只用到了训练集中的数据来计算导数，并最小化训练集上的损失函数。但是在机器学习中，我们希望得到的模型不但要在训练集上表现优秀，更重要的是能够很好地处理训练集之外的数据（即我们希望所得模型的泛化能力要好）。因此，在实际中，我们要尽量避免在训练集上的过拟合现象。通常我们采用一类函数（在提升决策树中是回归决策树）来拟合负导数，并不要求完美拟合。</p>
<p>算法9-4 梯度提升算法</p>
<blockquote>
<p>1．得到初始函数<img src="Image01760.gif" alt> ，这里<img src="Image01761.gif" alt> &gt; &gt; 2．for _j_ =1: _m_ &gt; &gt; 2.1 计算负梯度： &gt; &gt; <img src="Image01762.gif" alt> &gt; &gt; （9-45） &gt; &gt; 2.2 以<img src="Image01763.gif" alt> &gt; 作为新的目标，在训练集上构建一个新模型<img src="Image01699.gif" alt> &gt; &gt; 2.3 求解步长<img src="Image01764.gif" alt> ： &gt; &gt; <img src="Image01765.gif" alt> &gt; &gt; （9-46） &gt; &gt; 2.4 将函数 _f_ 更新为： &gt; &gt; <img src="Image01766.gif" alt> &gt; &gt; （9-47） &gt; &gt; 3．输出最终的模型： &gt; &gt; <img src="Image01767.gif" alt> &gt; &gt; （9-48）</p>
</blockquote>
<p>算法9-4给出了梯度提升算法的具体步骤。在第一步中，我们求解一个恒量 _c_ 来最小化损失函数<img src="Image01768.gif" alt> 作为初始模型<img src="Image01769.gif" alt> 。</p>
<p>在算法9-4的2.3步，我们还进一步求解最优的步长<img src="Image01764.gif" alt> 以更快地最小化损失函数。我们可以把<img src="Image01770.gif" alt> 看成一个关于 _s_ 的函数 _F_ ( _s_ )，找出使得 _F_ ( _s_ )最小的 _s_ 。简单的实现包括找出使得导数<img src="Image01771.gif" alt> 的 _s_ 。在第8章介绍LambdaMART算法时，我们使用牛顿近似（即二阶泰勒展式）来逼近<img src="Image01772.gif" alt> ，这里<img src="Image01773.gif" alt> 和<img src="Image01774.gif" alt> 是函数<img src="Image01775.gif" alt> 关于 _s_ 的一阶导数和二阶导数。注意，<img src="Image01776.gif" alt> 是关于 _s_ 的二次函数，使得其最小的 _s_ 为<img src="Image01777.gif" alt> 。</p>
<p>注意，如果我们选择的每个模型<img src="Image01699.gif" alt> 是决策树的话，则在2.3步求解步长时，可以进一步对新决策树<img src="Image01699.gif" alt> 的每个叶结点求解最优的步长以进一步最小化损失函数。我们知道，当使用训练集来训练得到一棵决策树时，每个叶结点都对应了一组训练样本。假设我们的决策树<img src="Image01699.gif" alt> 有<img src="Image01778.gif" alt> 个叶结点，将第 _l_ 个叶结点所对应的训练样本的集合记为<img src="Image01779.gif" alt> ，则可以求出其对应的权重<img src="Image01780.gif" alt> ：</p>
<p><img src="Image01781.gif" alt></p>
<p>（9-49）</p>
<p>与之相对应的是，在2.4步更新模型时，我们要为不同的样本赋予不同的权重。具体来说，更新公式如下：</p>
<p><img src="Image01782.gif" alt></p>
<p>（9-50）</p>
<p>这里函数<img src="Image01783.gif" alt> 的定义为：</p>
<p><img src="Image01784.jpg" alt></p>
<p>（9-51）</p>
<h3 id="9-5-2-如何避免过拟合"><a href="#9-5-2-如何避免过拟合" class="headerlink" title="9.5.2 如何避免过拟合"></a>9.5.2 如何避免过拟合</h3><p>下面我们讨论实际使用梯度提升算法时的一些技巧。利用这些技巧，可以显著地提高算法的性能，同时能有效地避免过拟合。</p>
<p>从理论上讲，在梯度提升算法中，我们可以无限制地构建新的决策树以优化分类模型的性能。与机器学习中的很多算法类似，如果对模型的复杂度不加以控制的话，则很容易导致过拟合。在梯度提升算法中，主要通过如下途径避免过拟合。</p>
<p>（1）控制学习率；</p>
<p>（2）控制决策树的总数；</p>
<p>（3）控制每棵决策树的大小；</p>
<p>（4）子取样。</p>
<p>下面逐一讨论这些防止过拟合的措施。</p>
<h4 id="1．学习率和决策树的数目"><a href="#1．学习率和决策树的数目" class="headerlink" title="1．学习率和决策树的数目"></a>1．学习率和决策树的数目</h4><p>在提升决策树中，我们构建了多棵决策树。从直观上讲，最开始构造的决策树更多地描述了最后所得模型的主要框架；而后面的决策树更多地是考虑那些难分类的样本。因此，我们可以认为应该给前面的决策树更大的权重。而随着更多的决策树的建立，我们应该逐渐降低其权重。具体来说，在建立第 _j_ 个模型时，我们使用如下的公式来更新模型：</p>
<p><img src="Image01785.gif" alt></p>
<p>（9-52）</p>
<p>这里参数 _v_ 满足0&lt; _v_ &lt;1，是新的决策树<img src="Image01714.gif" alt> 的权重。如果我们把这个公式和数值最优化中的更新公式相比较，可以把 _ν_ 看成是沿着负梯度方向移动的步长，因此 _ν_ 也可以认为是控制了boosting算法的学习率 。另外，我们可以将学习率与我们前面反复讨论的模型的正则化相联系。事实上，梯度提升算法的发明者Jerome Friedman在这方面就有相关的讨论，这里我们就不深入讨论了。</p>
<p>在实际使用梯度提升算法时，决策树数目和学习率是相互依赖的。一般而言，较小的 _ν_ 意味着我们要以较慢的速度去拟合目标函数，需要构建较多的决策树；较大的 _ν_ 则意味着可以构建较少的决策树。在实际中，较小的学习率一般能够得到更好的模型。举一个简单的例子，将 _v_ 设置为0.005，在OOB样本中的表现一般来说要显著强于将 _v_ 设置为0.05时的情况。但是为了得到较好的性能，较小的 _v_ 值就会要求较多的决策树，这就要求更多的存储空间和计算时间。如果将 _v_ 从0.05降到0.005，基本上需要多近10倍的计算时间。</p>
<p>在实际使用R中的<code>gbm</code> 包时，两个最重要的参数是决策树的数目和学习率，它们在<code>gbm</code> 包的<code>gbm</code> 函数中分别对应于参数<code>n.trees</code> 和<code>shrinkage</code> 。图9-5显示了在一个回归问题中，当使用不同的<code>shrinkage</code> 值时，模型在OOB样本上的RMSE随着<code>n.trees</code> 的增长而发生的变化。从图9-5中可以看出，在保证<code>n.trees</code> 足够大的情况下，降低<code>shrinkage</code> 的值能够提高模型的性能。当<code>shrinkage</code> 降低到一定程度之后，模型性能的提升就不明显了。从图9-5中还可以看出，当我们将<code>shrinkage</code> 从0.01降到0.005时，最优的<code>n.trees</code> 的值也差不多增加了一倍，但是最优的RMSE只是稍稍减少。因此，一般的经验：在计算时间允许的条件下，设置尽量小的<code>shrinkage</code> 值，再设置合适的<code>n.trees</code> 的值。我们一般倾向于较小的 _ν_ 值（ _v_ &lt;0.1）。</p>
<p><img src="Image01786.jpg" alt></p>
<p>图9-5 <code>gbm</code> 中学习率（<code>shrinkage</code> ）和决策树数目（<code>n</code> .trees）的关系 ⑤<br>（每条曲线对应于一个学习率的值，横坐标是决策树的数目，纵坐标是OOB样本上的RMSE值）</p>
<h4 id="2．决策树的大小"><a href="#2．决策树的大小" class="headerlink" title="2．决策树的大小"></a>2．决策树的大小</h4><p>控制决策树的大小可以通过控制决策树的深度或者叶结点的数目来实现。由于在梯度提升算法中我们构建了多棵决策树，因此我们一般倾向于每棵决策树的复杂度不宜太高。例如，我们可以限制每棵决策树的叶结点的数为一恒定值，或者每棵决策树的深度为一恒定值。而这些都可以作为梯度提升算法的参数，可根据实际的数据予以调节。</p>
<h4 id="3．子取样"><a href="#3．子取样" class="headerlink" title="3．子取样"></a>3．子取样</h4><p>子取样<br>（subsampling）的思想与bootstrap取样类似。在随机森林中，通过bootstrap取样显著降低了生成的决策树之间的相关性。在梯度提升算法中，也可以采用类似的方法。</p>
<p>具体来说，在使用子取样时，我们从所有的样本中随机取出一部分（但不是重复取样）的样本。这样，每次构建新的决策树时，只通过子取样得到一部分训练样本来构建新的模型。通过使用子取样，能够在构建决策树的时候引入一些随机性，从而能够增强各个模型的多样性，进而提高聚合之后的模型的性能。在R的<code>gbm</code> 包中，子取样是通过参数<code>bag.fraction</code> 来控制的。在实践中，每次子取样时我们使用一半左右的样本，这在很多时候都被证明是一个行之有效的推荐值。当样本量大时，我们还可以进一步降低每次取样时所得样本的数目。</p>
<h4 id="4．更加完备和实用的梯度提升算法"><a href="#4．更加完备和实用的梯度提升算法" class="headerlink" title="4．更加完备和实用的梯度提升算法"></a>4．更加完备和实用的梯度提升算法</h4><p>综合考虑多种防止过拟合的措施，我们将得到更加完备和实用的梯度提升算法。算法9-5列出了在R中常用的<code>gbm</code> 包中实现的梯度提升算法。事实上，在实际使用<code>gbm</code> 包时，还有更多的输入参数，在算法9-5中我们仅列出了最主要的控制参数及具体步骤。</p>
<p>算法9-5 <code>gbm</code> 包中实现的梯度提升算法</p>
<blockquote>
<p>输入： &gt; &gt;   <em> 损失函数的形式（参数<code>distribution</code> ） &gt; &gt;   </em> 决策树的数目 _m_ （参数 <code>n.trees</code> ） &gt; &gt;   <em> 决策树的叶结点数目 _K_ （由参数<code>interaction.depth</code> 决定） &gt; &gt;   </em> 学习率 _v_ （参数<code>shrinkage</code> ） &gt; &gt;   * 抽样比例 _p_ （参数<code>bag.fraction</code> ） &gt; &gt;</p>
<blockquote>
<p>算法的具体步骤如下。 &gt; &gt; 1．得到初始函数<img src="Image01760.gif" alt> 这里<img src="Image01761.gif" alt> &gt; &gt; 2．for _j_ =1: _m_ &gt; &gt; 2.1 计算负梯度： &gt; &gt; <img src="Image01787.gif" alt> &gt; &gt; （9-53） &gt; &gt; 2.2 从原始数据集中随机选出 _pn_ 个样本（总样本数为 _n_ ） &gt; &gt; 2.3 以<img src="Image01788.gif" alt> &gt; 作为新的目标，使用2.2步中选出的训练集，训练得到一个新的决策树<img src="Image01640.gif" alt> ，且该决策树的叶结点数是 &gt; _K_ &gt; &gt; 2.4 使用当前选出的训练集，为<img src="Image01640.gif" alt> &gt; 的每个叶结点求出对应的权重<img src="Image01780.gif" alt> 。将第 _l_ &gt; 个叶结点所对应的训练样本的集合记为<img src="Image01779.gif" alt> &gt; ，则<img src="Image01780.gif" alt> 为： &gt; &gt; <img src="Image01789.gif" alt> &gt; &gt; （9-54） &gt; &gt; 2.5 将函数 _f_ 更新为： &gt; &gt; <img src="Image01790.gif" alt> &gt; &gt; （9-55） &gt; &gt; 3．输出最终的模型： &gt; &gt; <img src="Image01767.gif" alt> &gt; &gt; （9-56）</p>
</blockquote>
</blockquote>
<h3 id="9-5-3-gbm包的实际使用"><a href="#9-5-3-gbm包的实际使用" class="headerlink" title="9.5.3 gbm包的实际使用"></a>9.5.3 gbm包的实际使用</h3><p>这里我们介绍R中最流行的实现了梯度提升算法的<code>gbm</code> 包 ⑥ 。在<code>gbm</code> 包中，采用的是决策树作为基本的弱学习器。利用<code>gbm</code> 包中的<code>gbm</code> 函数，可以使用梯度提升算法来构建分类、回归和排序模型。<code>gbm</code> 函数的主要参数设置如下：</p>
<ul>
<li>损失函数的形式（<code>distribution</code> ）；</li>
<li>决策树的数目（<code>n.trees</code> ）；</li>
<li>决策树的内结点数目（<code>interaction.depth</code> ）；</li>
<li>学习率（<code>shrinkage</code> ）；</li>
<li>子取样比例（<code>bag.fraction</code> ）。</li>
</ul>
<p>下面我们具体介绍这些参数并给出一些参数设置的建议，并使用实际数据来说明<code>gbm</code> 函数的用法。</p>
<p>首先我们要根据问题的类型来决定损失函数的形式。损失函数是我们首先需要确定的参数。在上面的讨论中，我们使用了平方和损失函数来进行推导。事实上，很多其他的损失函数都可以在梯度提升算法中使用。<code>gbm</code> 包中提供的适用于分类问题的常用损失函数有：</p>
<ul>
<li><code>&#39;bernoulli&#39;</code></li>
<li><code>&#39;adaboost&#39;</code></li>
<li><code>&#39;huberized&#39;</code></li>
<li><code>&#39;multinomial&#39;</code></li>
</ul>
<p>其中<code>&#39;bernoulli&#39;</code> 使用交叉熵损失函数，<code>&#39;adaboost&#39;</code> 是前面讨论过的指数损失函数，<code>&#39;huberized&#39;</code> 是huberized Hinge损失函数（Hinge损失函数的一种变体）。前面3个都适用于两类分类问题。而<code>&#39;multinomial&#39;</code> 对应于多类分类问题的损失函数。一般而言，对于两类分类问题，我们使用<code>&#39;bernoulli&#39;</code> 和<code>&#39;adaboost&#39;</code> 就足够了。</p>
<p>对于回归问题，可以将损失函数设为：</p>
<ul>
<li><code>&#39;gaussian&#39;</code></li>
<li><code>&#39;laplace&#39;</code></li>
<li><code>&#39;quantile&#39;</code></li>
</ul>
<p>其中，<code>&#39;gaussian&#39;</code> 表示损失函数是平方和损失函数，<code>&#39;laplace&#39;</code> 表示损失函数是预测值与真实值的差的绝对值之和：</p>
<p><img src="Image01791.gif" alt></p>
<p>（9-57）</p>
<p>这里假设我们的预测值是<img src="Image00915.gif" alt> ，而真实值是<img src="Image00540.gif" alt> 。当选用<code>&#39;quantile&#39;</code> 时，我们使用分位数回归 （quantile regression）所对应的损失函数。一般而言，在回归问题中，我们采用<code>&#39;gaussian&#39;</code> 可适用于大部分问题。</p>
<p><code>gbm</code> 包还能处理生存分析 （survival analysis）。在这种情况下，需要将<code>distribution</code> 设为<code>&#39;coxph&#39;</code> 。此外，我们也可以使用<code>gbm</code> 包来解决排序问题。在排序相关章节我们已经作了详细介绍，这里就不再讨论了。</p>
<p>在默认情况下，如果不指定<code>distribution</code> 参数的值，<code>gbm</code> 会根据数据的特征来确定<code>distribution</code> 的值。如果输入数据的目标变量只有两个不同的值，则认为问题是分类问题，并将<code>distribution</code> 的值设为<code>&#39;bernoulli&#39;</code> 。如果输入数据的目标变量有多于两个不同的值，<code>gbm</code> 再检查输入数据的目标变量的类型：如果是因子类型，就将<code>distribution</code> 的值设为<code>&#39;multinomial&#39;</code> ；如果不是因子类型，<code>gbm</code> 自动检查问题是否是生存分析问题；如果都不是的话，将默认问题为回归问题并将<code>distribution</code> 的值设为<code>&#39;gaussian&#39;</code> 。</p>
<p>在决定损失函数后，接下来最重要的参数是学习率（<code>shrinkage</code> ）和决策树的数目（<code>n.trees</code><br>）。一般的经验为：在计算时间允许的条件下，设置尽量小的<code>shrinkage</code> 值，再设置合适的<code>n.trees</code> 的值。一般来讲，可以将<code>shrinkage</code> 的参数设置在0.01～0.001。对应地，可以将<code>n.trees</code> 设置在3000至10000之间。</p>
<p>在设定了<code>shrinkage</code> 的值后，如何设置决策树的数目？<code>gbm</code> 包给出了3种估计最优的决策树的数目的方法，分别为：</p>
<ul>
<li>利用训练集；</li>
<li>利用OOB中的样本；</li>
<li>利用交叉检验。</li>
</ul>
<p>核心思想都是利用某一个数据集来估计决策树的数目。具体来说，在<code>gbm</code> 包中，我们可以使用<code>gbm.perf</code> 函数来计算估计决策树的数目。</p>
<p>对于每棵决策树的大小，<code>gbm</code> 中使用参数<code>interaction.depth</code> 来控制。该参数指定了每棵决策树中内结点的数目。根据内结点的数目，可以相应地控制叶结点的数目。此外，利用<code>gbm</code> 包中的函数<code>pretty.gbm.tree</code> ，可以提取<code>gbm</code> 所构建的决策树中的某棵决策树的对应信息。</p>
<p>我们提供了文件gbm_classification_example.R，用它介绍如何使用梯度提升算法来解决分类问题。具体的代码如下。</p>
<p>首先我们检查<code>gbm</code> 包有没有安装，如果没有安装，那么首先安装它。这里我们使用<code>mlbench</code> 包中的<code>PimaIndiansDiabetes</code> 数据集，所以也要安装<code>mlbench</code> 包。</p>
<p>然后我们导入数据，并检查每列的类型。之后我们将数据按照70%和30%的比例分为训练集和测试集，并分别保存在数据框<code>D_train</code> 和<code>D_test</code> 中。</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"># Step 0. Check required package is installed or not. If not, install first.</span><br><span class="line"># 1. gbm</span><br><span class="line">gbm.installed &lt;-&apos;gbm&apos; %in% rownames(installed.packages())</span><br><span class="line">if (gbm.installed) &#123;</span><br><span class="line">　print(&quot;the gbm package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the gbm package first...&quot;)</span><br><span class="line">　install.packages(&apos;gbm&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(gbm)</span><br><span class="line"># 2. mlbench</span><br><span class="line">mlbench.installed &lt;-&apos;mlbench&apos; %in% rownames(installed.packages())</span><br><span class="line">if (mlbench.installed) &#123;</span><br><span class="line">　print(&quot;the mlbench package is already installed, let&apos;s load it...&quot;)</span><br><span class="line">&#125;else &#123;</span><br><span class="line">　print(&quot;let&apos;s install the mlbench package first...&quot;)</span><br><span class="line">　install.packages(&apos;mlbench&apos;, dependencies=T)</span><br><span class="line">&#125;</span><br><span class="line">library(mlbench)</span><br><span class="line"></span><br><span class="line"># Step 1. Load the data</span><br><span class="line">data(PimaIndiansDiabetes2,package=&apos;mlbench&apos;)</span><br><span class="line">D &lt;-PimaIndiansDiabetes2</span><br><span class="line">y &lt;-D[[ncol(D)]]</span><br><span class="line">y &lt;-as.integer(y) -1</span><br><span class="line">D[[ncol(D)]] &lt;-NULL</span><br><span class="line">D$classLabel &lt;-y</span><br><span class="line"></span><br><span class="line"># Show the type for each col</span><br><span class="line">for(i in 1:ncol(D)) &#123;</span><br><span class="line">　msg &lt;-paste(&apos;col &apos;, i, &apos; and its type is &apos;, class(D[,i]))</span><br><span class="line">　print(msg)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Step 2. Split the data into training and test sets</span><br><span class="line"># Randomly split the whole data set into a training and a test data set</span><br><span class="line"># After spliting, we have the training set: (X_train, y_train)</span><br><span class="line"># and the test data set: (X_test, y_test)</span><br><span class="line">train_ratio &lt;-0.7</span><br><span class="line">n_total &lt;-nrow(D)</span><br><span class="line">n_train &lt;-round(train_ratio * n_total)</span><br><span class="line">n_test &lt;-n_total -n_train</span><br><span class="line">set.seed(42)</span><br><span class="line">list_train &lt;-sample(n_total, n_train)</span><br><span class="line">D_train &lt;-D[list_train,]</span><br><span class="line">D_test &lt;-D[-list_train,]</span><br><span class="line"></span><br><span class="line">y_train &lt;-D_train$classLabel</span><br><span class="line">y_test &lt;-D_test$classLabel</span><br><span class="line"></span><br><span class="line"># Step 3. Train several gbm models</span><br><span class="line"># Train a simple gbm model</span><br><span class="line">M_gbm1 &lt;-gbm(classLabel~.,</span><br><span class="line">　　　　　　　data = D_train, </span><br><span class="line">　　　　　　　distribution = &apos;bernoulli&apos;, </span><br><span class="line">　　　　　　　shrinkage = 0.01, </span><br><span class="line">　　　　　　　interaction.depth = 1,</span><br><span class="line">　　　　　　　n.trees = 300, </span><br><span class="line">　　　　　　　verbose = T)</span><br><span class="line">print(&apos;the summary of M_gbm1 is&apos;)</span><br><span class="line">print(M_gbm1)</span><br><span class="line"></span><br><span class="line"># Expand the gbm model by adding more trees</span><br><span class="line">M_gbm1.1 &lt;-gbm.more(M_gbm1, n.new.trees = 100)</span><br><span class="line">print(&apos;the summary of M_gbm1.1 is&apos;)</span><br><span class="line">print(M_gbm1.1)</span><br><span class="line"></span><br><span class="line"># Train a gbm model using cross-validation</span><br><span class="line">set.seed(1)</span><br><span class="line">M_gbm2 &lt;-gbm(classLabel~.,</span><br><span class="line">　　　　　　　data = D_train, </span><br><span class="line">　　　　　　　distribution=&apos;bernoulli&apos;, </span><br><span class="line">　　　　　　　shrinkage = 0.01,</span><br><span class="line">　　　　　　　n.trees=3000,</span><br><span class="line">　　　　　　　cv.folds = 5,</span><br><span class="line">　　　　　　　verbose=F)</span><br><span class="line">print(&apos;the summary of M_gbm2 is&apos;)</span><br><span class="line">print(M_gbm2)</span><br><span class="line"></span><br><span class="line"># We use gbm.perf to get the estimate of the optimal number of trees using </span><br><span class="line"># cross-validation</span><br><span class="line">best.iter2 &lt;-gbm.perf(M_gbm2,method = &apos;cv&apos;)</span><br><span class="line">msg &lt;-paste0(&apos;the best n.tree is &apos;, best.iter2)</span><br><span class="line">print(msg)</span><br><span class="line"># Show the variable importance using the first best.iter2 trees</span><br><span class="line">varImp2 &lt;-summary(M_gbm2, best.iter2, main = &apos;variable importance of M_gbm2&apos;)</span><br><span class="line"># Show the marginal effect of the selected variables by &quot;integrating&quot; out the </span><br><span class="line"># other variables</span><br><span class="line"># Here we select the 3rd variable.</span><br><span class="line">plot.gbm(M_gbm2, 3, best.iter2)</span><br><span class="line"></span><br><span class="line"># compactly print the first and last trees for curiosity</span><br><span class="line">print(&apos;the structure of the 1st tree&apos;)</span><br><span class="line">print(pretty.gbm.tree(M_gbm2,1))</span><br><span class="line">print(&apos;the structure of the last tree&apos;)</span><br><span class="line">print(pretty.gbm.tree(M_gbm2, M_gbm2$n.trees))</span><br><span class="line"></span><br><span class="line"># predict the new data using the &quot;best&quot; number of trees</span><br><span class="line">y_test_pred_gbm2 &lt;-predict(M_gbm2, D_test, best.iter2, type = &apos;response&apos;)</span><br><span class="line">print(&apos;we are going to print the first few predictions of y_test_pred_gbm2&apos;)</span><br><span class="line">print(head(y_test_pred_gbm2))</span><br><span class="line">y_test_pred_gbm2_raw &lt;-predict(M_gbm2, D_test, best.iter2, type = &apos;link&apos;)</span><br><span class="line">print(&apos;we are going to print the first few predictions of y_test_pred_gbm2_raw&apos;)</span><br><span class="line">print(head(y_test_pred_gbm2_raw))</span><br><span class="line"></span><br><span class="line"># Train a more complicated model</span><br><span class="line">M_gbm3 &lt;-gbm(classLabel~.,</span><br><span class="line">　　　　　　　data = D_train, </span><br><span class="line">　　　　　　　distribution=&apos;bernoulli&apos;, </span><br><span class="line">　　　　　　　shrinkage = 0.005, </span><br><span class="line">　　　　　　　bag.fraction = 0.4, </span><br><span class="line">　　　　　　　cv.folds = 5,</span><br><span class="line">　　　　　　　interaction.depth = 2,</span><br><span class="line">　　　　　　　n.cores = 4,</span><br><span class="line">　　　　　　　n.trees=3000, </span><br><span class="line">　　　　　　　verbose=F)</span><br><span class="line">print(&apos;the summary of M_gbm3 is&apos;)</span><br><span class="line">print(M_gbm3)</span><br><span class="line"></span><br><span class="line"># Step 4. Use gbm to do multi-class classification</span><br><span class="line">data(iris)</span><br><span class="line">M_iris &lt;-gbm(Species ~ ., </span><br><span class="line">　　　　　　　distribution=&quot;multinomial&quot;, </span><br><span class="line">　　　　　　　data=iris,</span><br><span class="line">　　　　　　　n.trees=2000, </span><br><span class="line">　　　　　　　shrinkage=0.01, </span><br><span class="line">　　　　　　　cv.folds=5,</span><br><span class="line">　　　　　　　verbose=F, </span><br><span class="line">　　　　　　　n.cores=1)</span><br><span class="line">print(&apos;the summary of M_iris is&apos;)</span><br><span class="line">print(M_iris)</span><br><span class="line">print(&apos;the variable importance of M_iris&apos;)</span><br><span class="line">summary(M_iris, main = &apos;variable importance of M_iris&apos;)</span><br></pre></td></tr></table></figure>

</details>


<p>在第3步，我们构建了多个<code>gbm</code> 模型。由于这里使用的数据对应两类分类问题，因此将<code>distribution设为&#39;bernoulli&#39;</code> 。注意，当<code>distribution</code> 为<code>&#39;bernoulli&#39;</code> 时，我们必须保证对应于类标的列的值必须是0或者1。这也是我们在第一步对<code>y</code> 进行预处理的原因。使用如下代码我们构建了第一个<code>gbm</code> 模型<code>M_gbm1</code> ：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">M_gbm1 &lt;-gbm(classLabel~., data = D_train, distribution = &apos;bernoulli&apos;, </span><br><span class="line">shrinkage = 0.01, interaction.depth = 1, n.trees = 300, verbose = T)</span><br></pre></td></tr></table></figure>

</details>


<p>上述参数的含义我们在前面已经解释过了。我们在这里将参数<code>verbose</code> 设为真，表示R将会打印模型构建中的一些信息。然后，我们使用<code>print(M_gbm1)</code> 打印出该模型的主要信息，则对应的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">Iter　 TrainDeviance　 ValidDeviance　 StepSize　 Improve</span><br><span class="line">　　 1　　　　1.2772　　　　　　 nan　　 0.0100　　0.0025</span><br><span class="line">　　 2　　　　1.2722　　　　　　 nan　　 0.0100　　0.0024</span><br><span class="line">　　 3　　　　1.2675　　　　　　 nan　　 0.0100　　0.0022</span><br><span class="line">　　 4　　　　1.2624　　　　　　 nan　　 0.0100　　0.0022</span><br><span class="line">　　 5　　　　1.2580　　　　　　 nan　　 0.0100　　0.0021</span><br><span class="line">　　 6　　　　1.2534　　　　　　 nan　　 0.0100　　0.0020</span><br><span class="line">　　 7　　　　1.2480　　　　　　 nan　　 0.0100　　0.0020</span><br><span class="line">　　 8　　　　1.2439　　　　　　 nan　　 0.0100　　0.0021</span><br><span class="line">　　 9　　　　1.2401　　　　　　 nan　　 0.0100　　0.0020</span><br><span class="line">　　10　　　　1.2361　　　　　　 nan　　 0.0100　　0.0020</span><br><span class="line">　　20　　　　1.1983　　　　　　 nan　　 0.0100　　0.0015</span><br><span class="line">　　40　　　　1.1375　　　　　　 nan　　 0.0100　　0.0012</span><br><span class="line">　　60　　　　1.0962　　　　　　 nan　　 0.0100　　0.0008</span><br><span class="line">　　80　　　　1.0626　　　　　　 nan　　 0.0100　　0.0006</span><br><span class="line">　 100　　　　1.0338　　　　　　 nan　　 0.0100　　0.0005</span><br><span class="line">　 120　　　　1.0106　　　　　　 nan　　 0.0100　　0.0005</span><br><span class="line">　 140　　　　0.9901　　　　　　 nan　　 0.0100　　0.0003</span><br><span class="line">　 160　　　　0.9720　　　　　　 nan　　 0.0100　　0.0004</span><br><span class="line">　 180　　　　0.9552　　　　　　 nan　　 0.0100　　0.0002</span><br><span class="line">　 200　　　　0.9410　　　　　　 nan　　 0.0100　　0.0002</span><br><span class="line">　 220　　　　0.9279　　　　　　 nan　　 0.0100　　0.0000</span><br><span class="line">　 240　　　　0.9169　　　　　　 nan　　 0.0100　　0.0002</span><br><span class="line">　 260　　　　0.9069　　　　　　 nan　　 0.0100　　0.0002</span><br><span class="line">　 280　　　　0.8969　　　　　　 nan　　 0.0100　 -0.0000</span><br><span class="line">　 300　　　　0.8877　　　　　　 nan　　 0.0100　　0.0001</span><br><span class="line"></span><br><span class="line">[1] &quot;the summary of M_gbm1 is&quot;</span><br><span class="line">gbm(formula = classLabel ~ ., distribution = &quot;bernoulli&quot;, data = D_train, </span><br><span class="line">　　n.trees = 300, interaction.depth = 1, shrinkage = 0.01, verbose = T)</span><br><span class="line">A gradient boosted model with bernoulli loss function.</span><br><span class="line">300 iterations were performed.</span><br><span class="line">There were 8 predictors of which 8 had non-zero influence.</span><br><span class="line">Iter　 TrainDeviance　 ValidDeviance　 StepSize　 Improve</span><br><span class="line">　 301　　　　0.8874　　　　　　 nan　　 0.0100　 -0.0001</span><br><span class="line">　 302　　　　0.8871　　　　　　 nan　　 0.0100　 -0.0001</span><br><span class="line">　 303　　　　0.8868　　　　　　 nan　　 0.0100　 -0.0001</span><br><span class="line">　 304　　　　0.8863　　　　　　 nan　　 0.0100　　0.0000</span><br><span class="line">　 305　　　　0.8859　　　　　　 nan　　 0.0100　　0.0001</span><br><span class="line">　 306　　　　0.8855　　　　　　 nan　　 0.0100　 -0.0001</span><br><span class="line">　 307　　　　0.8853　　　　　　 nan　　 0.0100　 -0.0001</span><br><span class="line">　 308　　　　0.8848　　　　　　 nan　　 0.0100　　0.0001</span><br><span class="line">　 309　　　　0.8843　　　　　　 nan　　 0.0100　　0.0000</span><br><span class="line">　 310　　　　0.8838　　　　　　 nan　　 0.0100　　0.0000</span><br><span class="line">　 320　　　　0.8799　　　　　　 nan　　 0.0100　　0.0001</span><br><span class="line">　 340　　　　0.8722　　　　　　 nan　　 0.0100　 -0.0000</span><br><span class="line">　 360　　　　0.8658　　　　　　 nan　　 0.0100　 -0.0000</span><br><span class="line">　 380　　　　0.8588　　　　　　 nan　　 0.0100　　0.0001</span><br><span class="line">　 400　　　　0.8528　　　　　　 nan　　 0.0100　　0.0000</span><br></pre></td></tr></table></figure>

</details>


<p>在实际中，如果当前的<code>gbm</code> 模型不理想，那么可能要进一步增加决策树的数目。在<code>gbm</code> 模型中，我们不必从头开始重新构建<code>gbm</code> 模型。我们可以使用<code>gbm.more</code> 函数增加当前<code>gbm</code> 模型中决策树的数目。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_gbm1.1 &lt;-gbm.more(M_gbm1, n.new.trees = 100)</span><br></pre></td></tr></table></figure>
<p>这里我们又增加了100棵决策树，而且将新的决策树模型记为<code>M_gbm1.1</code> 。同样地，我们也可以使用<code>print</code> 函数打印新<code>M_gbm1.1</code> 的主要信息。其输出如下：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;the summary of M_gbm1.1 is&quot;</span><br><span class="line">gbm.more(object = M_gbm1, n.new.trees = 100)</span><br><span class="line">A gradient boosted model with bernoulli loss function.</span><br><span class="line">400 iterations were performed.</span><br><span class="line">There were 8 predictors of which 8 had non-zero influence.</span><br></pre></td></tr></table></figure>

</details>


<p>接下来我们介绍如何使用<code>gbm.perf</code> 函数来估计最优的决策树的数目。首先我们使用交叉检验得到模型<code>M_gbm2</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">M_gbm2 &lt;-gbm(classLabel~., data = D_train, distribution=&apos;bernoulli&apos;, shrinkage = 0.01, n.trees=3000, cv.folds = 5, verbose=F)</span><br></pre></td></tr></table></figure>
<p>与前面的不同是，我们将<code>cv.folds</code> 设为5，表示在训练中采用5重交叉检验。接下来我们调用<code>gbm.perf</code> 就可以得到利用交叉检验得到的最优决策树的数目。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best.iter2 &lt;-gbm.perf(M_gbm2,method = &apos;cv&apos;)</span><br></pre></td></tr></table></figure>
<p>这段代码的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;the summary of M_gbm2 is&quot;</span><br><span class="line">gbm(formula = classLabel ~ ., distribution = &quot;bernoulli&quot;, data = D_train, </span><br><span class="line">　　n.trees = 3000, shrinkage = 0.01, cv.folds = 5, verbose = F)</span><br><span class="line">A gradient boosted model with bernoulli loss function.</span><br><span class="line">3000 iterations were performed.</span><br><span class="line">The best cross-validation iteration was 774.</span><br><span class="line">There were 8 predictors of which 8 had non-zero influence.</span><br><span class="line">[1] &quot;the best n.tree is 774&quot;</span><br></pre></td></tr></table></figure>

</details>


<p>可以看出，最优的决策树的数目是774。注意，<code>gbm.perf</code> 函数同时也绘出了损失函数在训练集和检验集上随着决策树增加而引起的变化，如图9-6 所示。从图9-6中可以看出，当增加决策树的数目时，训练集对应的损失函数一直在单调降低，而检验集上对应的损失函数先降低再上升。</p>
<p>接下来我们使用<code>summary</code> 函数得到<code>gbm</code> 模型对应的参数的重要程度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">varImp2 &lt;-summary(M_gbm2, best.iter2, main = &apos;variable importance of M_gbm2&apos;)</span><br></pre></td></tr></table></figure>
<p>该函数除了返回一个数据框外，还绘图显示了各个变量的重要程度，如图9-7所示。</p>
<p><img src="Image01792.jpg" alt></p>
<p>图9-6 <code>gbm</code> 中所使用的损失函数随着决策树数目的增加而引起的变化（下面的曲线对应于训练集，上面的曲线对应于检验集，竖直的虚线代表我们选取的最优的决策树的数目的位置）</p>
<p><img src="Image01793.jpg" alt></p>
<p>图9-7 <code>M_gbm2</code> 模型中变量的重要程度</p>
<p>接下来我们使用<code>pretty.gbm.tree(M_gbm2, i)</code> 来取得<code>gbm</code> 模型<code>M_gbm2</code> 中第<code>i</code> 棵树中变量重要性的信息。下面是第一棵和最后一棵树对应的变量重要信息：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;the structure of the 1st tree&quot;</span><br><span class="line">　SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight</span><br><span class="line">0　　　　1　1.390000e+02　　　　1　　　　 2　　　　　 3　　　 15.45783　　   269</span><br><span class="line">1　　　 -1 -6.290815e-03　　　 -1　　　　-1　　　　　-1　　　　0.00000　　  191</span><br><span class="line">2　　　 -1　1.780270e-02　　　 -1　　　　-1　　　　　-1　　　　0.00000　　   73</span><br><span class="line">3　　　 -1　3.714074e-04　　　 -1　　　　-1　　　　　-1　　　　0.00000　　  　5</span><br><span class="line">　　 Prediction</span><br><span class="line">0　0.0003714074</span><br><span class="line">1 -0.0062908155</span><br><span class="line">2　0.0178027030</span><br><span class="line">3　0.0003714074</span><br><span class="line">[1] &quot;the structure of the last tree&quot;</span><br><span class="line">　SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight</span><br><span class="line">0　　　　2 87.0000000000　　　　1　　　　 2　　　　　 3　　　0.3832303　　  269</span><br><span class="line">1　　　 -1　0.0004975395　　　 -1　　　　-1　　　　　-1　　　0.0000000　　  229</span><br><span class="line">2　　　 -1 -0.0072377243　　　 -1　　　　-1　　　　　-1　　　0.0000000　　 27</span><br><span class="line">3　　　 -1　0.0019612510　　　 -1　　　　-1　　　　　-1　　　0.0000000　　  13</span><br><span class="line">　　 Prediction</span><br><span class="line">0 -0.0002081254</span><br><span class="line">1　0.0004975395</span><br><span class="line">2 -0.0072377243</span><br><span class="line">3　0.0019612510</span><br></pre></td></tr></table></figure>

</details>


<p>函数<code>pretty.gbm.tree</code> 返回一个数据框以显示<code>gbm</code> 生成的某一棵树的具体结构。在返回的数据框中，每行对应生成的决策树中的一个结点，且结点的编号从0开始。在上面显示的结果中，每行的开头显示了结点的编号。返回的数据框中主要包括8列。</p>
<ul>
<li><code>SplitVar</code> ：表示结点对应的变量编号，<code>-1</code> 表示是决策树的叶结点。</li>
<li><code>SplitCodePred</code> ：表示进一步划分该结点时对应变量的划分值。如果该结点对应的变量是连续的数值变量的话，<code>SplitCodePred</code> 就是划分点的值。如果变量是分类变量的话，<code>SplitCodePred</code> 是对应的<code>gbm</code> 对象中<code>c.splits</code> 列表存储的分类变量划分条件中的索引。</li>
<li><code>LeftNode</code> ：该结点左子结点的编号；如果没有左子结点，则为<code>-1</code> 。</li>
<li><code>RightNode</code> ：该结点右子结点的编号；如果没有右子结点，则为<code>-1</code> 。</li>
<li><code>MissingNode</code> ：该结点缺失的子结点的编号。在使用决策树时，如果新数据中该结点对应的变量为缺失值，则我们需要使用缺失结点来处理这些数据，因此，在构建决策树时，我们需要构建缺失子结点。</li>
<li><code>ErrorReduction</code> ：由于该结点划分为其对应的子结点所导致的损失函数下降的量；如果该结点是叶结点，则<code>ErrorReduction</code> 为0。</li>
<li><code>Weight</code> ：该结点对应的所有样本点的总权重。如果每个样本的权重都是1，则<code>Weight</code> 为该结点对应的样本的数目。</li>
<li><code>Prediction</code> ：表示在该结点没有被进一步划分之前，我们应该赋给该结点所有样本点的最优目标值。</li>
</ul>
<p>在上面第一个棵树的输出中，我们可以看出有4个结点，标号为0～3，其中第一个结点是根结点（其左子结点为结点1，右子结点为结点2），后面3个结点都是叶结点（<code>SplitVar</code> 为−1）。</p>
<p>这里我们要特别说明，在使用<code>predict</code> 函数（实质上我们调用的是<code>predict.gbm</code> 函数）处理<code>gbm</code> 对象时，需要指定以下参数<code>type</code> 的值。</p>
<ul>
<li><code>&#39;reponse&#39;</code> ：在两类分类问题中，是一个介于0～1之间的概率值。</li>
<li><code>&#39;link&#39;</code> ：原始值。</li>
</ul>
<p>R程序中关于这段的输出为：</p>
<details>
  <summary>代码详情</summary>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[1] &quot;we are going to print the first few predictions of y_test_pred_gbm2&quot;</span><br><span class="line">[1] 0.03375870 0.66036940 0.05968122 0.26782885 0.50472148 0.49354606</span><br><span class="line">[1] &quot;we are going to print the first few predictions of y_test_pred_gbm2_raw&quot;</span><br><span class="line">[1] -3.35417558　0.66494083 -2.75720155 -1.00566613　0.01888648 -0.02581718</span><br></pre></td></tr></table></figure>

</details>


<p>最后，对于这个两类分类问题，我们构建了一个比较复杂的模型<code>M_gbm3</code> 。</p>
<p>在第4步中，我们使用<code>gbm</code> 在<code>iris</code> 数据集上为多类分类问题构建了一个模型，主要区别在于将参数<code>distribution</code> 设为了<code>&quot;multinomial&quot;</code> 。我们也使用<code>summary</code> 函数打印变量的重要程度信息，结果如图9-8所示。</p>
<p><img src="Image01794.jpg" alt></p>
<p>图9-8 <code>M_iris</code> 模型中变量的重要程度</p>
<h3 id="9-5-4-讨论"><a href="#9-5-4-讨论" class="headerlink" title="9.5.4 讨论"></a>9.5.4 讨论</h3><p>与AdaBoost相比，在梯度提升算法中，我们通过使用负梯度为新的目标值来构建后继的模型从而处理那些难以分类的样本；而在AdaBoost中，我们直接给每个样本赋予了一个权值，而权值的大小直接反映了下面构建的模型对每个点的重视程度。一般而言，对于一般的数据集，在随机森林中，我们都要构建数百棵决策树。而对于boosting，要取得同样的性能，所需的决策树的数目更多。但是如果仔细选择参数，一般而言，当达到boosting所需的决策树数目后，boosting的性能能够优于随机森林。我们的经验为：对于随机森林的使用，简单的参数设置就能得到较好的性能；而对于boosting，对于很多数据集，我们都需要仔细调节参数才能得到较好的性能。我们推荐读者使用R中的<code>randomForest</code> 包和<code>gbm</code> 包来学习如何使用随机森林和梯度提升算法，从而获得第一手的使用经验。</p>
<h2 id="9-6-学习器的聚合及stacking"><a href="#9-6-学习器的聚合及stacking" class="headerlink" title="9.6 学习器的聚合及stacking"></a>9.6 学习器的聚合及stacking</h2><p>在本节中，我们讨论如何将多个学习器的结果聚合以得到更好的结果。在前面的讨论中，我们假设每个基学习器为弱学习器 ，如在随机森林和提升决策树中的基学习器都是决策树。在本节的讨论中，则不限于弱学习器。特别是在一些注重算法性能的场合，通常我们会构建多个不同类型的学习器。例如，对于一个给定的分类问题，可以构建多个模型，如随机森林、SVM和逻辑回归；然后再将这3个模型的结果聚合以得到最终的模型。在本节中，我们根据方法的复杂程度，从易到难地介绍简单平均、加权平均，并着重讨论stacking。与简单平均和加权平均相比，stacking构建了新的模型来聚合多个模型。在实践中，stacking是一种行之有效的聚合多个模型的方法，如在很多数据挖掘的竞赛中，最终获胜的方案几乎都是使用stacking聚合多个模型得到的。</p>
<h3 id="9-6-1-简单平均"><a href="#9-6-1-简单平均" class="headerlink" title="9.6.1 简单平均"></a>9.6.1 简单平均</h3><p>简单平均是一种看似简单但非常有效的方法。假设我们有 _m_ 个模型<img src="Image01795.gif" alt> 。在回归中，我们可以计算 _m_ 个模型的平均作为最后的输出：</p>
<p><img src="Image01615.gif" alt></p>
<p>（9-58）</p>
<p>在分类问题，如果类标为±1，则聚合 _m_ 个模型之后最后的输出可以表示为：</p>
<p><img src="Image01796.gif" alt></p>
<p>（9-59）</p>
<p>这里sign是符号函数。在前面的讨论中，随机森林就是使用简单平均来聚合多个弱学习器的。</p>
<p>该方法简单易行。在实际中，很多场合通过使用简单平均就能够显著提高性能。因此，简单平均是最常用的综合多种算法的方法。在很多实际应用中，简单平均是聚合多个模型的第一选择，也是其他复杂聚合方法的第一比较对象。</p>
<p>当然，也存在一些极端情况使得简单平均不能取得较好的性能。例如，如果各个模型<img src="Image01613.gif" alt> 相互关联且关联度很高，则简单平均不易取得较好的性能。一个简单的极端例子：</p>
<p><img src="Image01797.gif" alt></p>
<p>（9-60）</p>
<p>在这种情况下，使用简单平均后所得的模型与原来的 _m_ 个模型相比，性能没有任何提高。</p>
<h3 id="9-6-2-加权平均"><a href="#9-6-2-加权平均" class="headerlink" title="9.6.2 加权平均"></a>9.6.2 加权平均</h3><p>加权平均是简单平均的进一步扩展。在加权平均中，我们使用如下公式来聚合多个分类器：</p>
<p><img src="Image01798.gif" alt></p>
<p>（9-61）</p>
<p>这里<img src="Image00523.gif" alt> 是第 _j_ 个模型的权重。事实上，简单平均是加权平均的特例，只需要将权重设为：</p>
<p><img src="Image01799.gif" alt></p>
<p>（9-62）</p>
<p>在一些应用中，我们还可以对<img src="Image01800.gif" alt> 添加新的约束条件。例如，可以要求<img src="Image01801.gif" alt> 满足如下约束条件：</p>
<p><img src="Image01802.gif" alt></p>
<p>（9-63）</p>
<p>在前面的讨论中，在boosting中一般使用加权平均。例如，在AdaBoost中，我们根据每个学习器的错误率来决定其相应的权重。在梯度提升算法中，权重则是由学习率决定的。</p>
<p>加权平均的表达能力更强，但是更容易导致过拟合。很多实验结果都证实加权平均并不一定优于简单平均。一般而言，对于相似的模型或者性能（如准确率）相近的模型，我们倾向于简单平均；如果模型间的差异较大（.包括性能差异较大的情况），我们更倾向于使用加权平均。</p>
<h3 id="9-6-3-stacking的基本思想及应用"><a href="#9-6-3-stacking的基本思想及应用" class="headerlink" title="9.6.3 stacking的基本思想及应用"></a>9.6.3 stacking的基本思想及应用</h3><p>与前面讨论的简单平均和加权平均相比，stacking是更加通用的聚合多个学习器的方法。</p>
<p>与前面介绍的简单平均和加权平均类似，在stacking中，我们也采用两层的结构来将多个模型聚合。其具体结构如图9-9所示。在stacking的第一层中，通常构建多个学习器，并将这一层的学习器称为第一层学习器<br>（first-level learner）。在得到第一层学习器后，我们将这一层的输出作为新的“特征”，重新训练一个学习器，称为第二层学习器 （second-level learner）。简而言之，stacking就是将已有的学习器的输出作为新的特征，再训练一个学习器以进一步优化性能。在stacking的第二层中，我们可以选择多种不同的算法，如一些非线性算法（如人工神经网络等）。但在stacking的实践中，在第二层采用复杂的模型很容易引起过拟合。因此，很多时候，在第二层中我们一般倾向于比较简单的模型，如线性回归。</p>
<p>在这里，我们要着重指出，在stacking中第一层的学习器并不局限于弱学习器。事实上，在采用stacking时，第一层学习器一般都是性能较好的模型，如随机森林等。为了尽量提高第一层学习器的性能，我们要求第一层学习器之间存在多样性。</p>
<p><img src="Image01803.jpg" alt></p>
<p>图9-9 stacking中的第一层学习器和第二层学习器</p>
<p>在使用stacking时，特别需要注意的是，在训练第二层学习器的时候不要使用第一层学习器已经使用过的训练数据。如图9-10所示，假设我们使用训练集 _S_ 1 来训练第一层学习器。在得到第一层的模型后，我们计算这些模型在新的训练集 _S_ 2 上的输出，并将这些输出作为新的特征。利用新的训练集 _S_ 2 ，我们训练第二层学习器。得到所有的第一层学习器和第二层学习器后，我们可以得到最后的模型在测试集 _T_ 上的输出，并作为最终输出。注意，如果 _S_ 1 和 _S_ 2 是同一数据集的话，那么在实际中很难避免过拟合。</p>
<p><img src="Image01804.jpg" alt></p>
<p>图9-10 训练数据和测试数据的划分</p>
<p>为了充分利用训练集中的信息，我们还可以在stacking中使用交叉检验。具体来说，我们可将整个训练集 _S_ 等分成 _k_ 个子集： _S_ 1 ， _S_ 2 ，…， _S k _ ，并记<img src="Image01805.gif" alt> 为<img src="Image01806.gif" alt> 的补，如图9-11所示。</p>
<p><img src="Image01807.jpg" alt></p>
<p>图9-11 使用交叉检验来应用stacking时训练数据和测试数据的划分</p>
<p>这里我们考虑建立 _m_ 个不同的模型<img src="Image01795.gif" alt> 。对于每个模型<img src="Image01613.gif" alt> ，我们使用如下流程来得到<img src="Image01613.gif" alt> 在整个训练集上的输出。对于<img src="Image01808.gif" alt> ，我们使用<img src="Image01809.gif" alt> 作为训练集，使用 <img src="Image01806.gif" alt> 作为测试集。我们将这样训练得到的模型记为 <img src="Image01810.gif" alt> 。这样，对于测试集 <img src="Image01806.gif" alt> 中的每个样本<img src="Image00730.gif" alt> ，我们可以计算 <img src="Image01810.gif" alt> 对应的输出。具体来说，对于<img src="Image01811.gif" alt> ，我们记<img src="Image01812.gif" alt> 。这样，当 _p_ 值从1增长到 _k_ 、 _j_ 从1增长到 _m_ 后，交叉检验结束，我们将得到 _m_ 个模型<img src="Image01795.gif" alt> 在整个训练集上的输出，记为<img src="Image01813.gif" alt> ：</p>
<p><img src="Image01814.gif" alt></p>
<p>（9-64）</p>
<p>这里<img src="Image00543.gif" alt> 是样本<img src="Image00730.gif" alt> 对应的类标或者目标值。</p>
<p>利用这个新的训练集，我们可以构建第二层学习器 _h_ 。当得到第二层学习器后，我们重新回到第一层学习器：利用整个训练集 _S_ 重新训练所有的第一层学习器<img src="Image01795.gif" alt> ，然后利用刚才得到的第二层学习器 _h_ 得到最终的结果。</p>
<p>在回归问题中，我们可以直接将第一层模型<img src="Image01815.gif" alt> 的输出作为特征。在分类问题中，我们既可以将第一层模型的输出作为特征，也可以将分类对应的概率作为特征（如果第一层模型可以输出概率的话，如逻辑回归）。</p>
<p>为何stacking能够提高第一层学习器的性能？与前面的bagging类似，stacking也利用不同学习器之间存在的多样性。这样第一层学习器之间就能够取长补短，有效提高学习器的综合性能。因此，我们一般在第一层构建不同类型的模型。或者，在构建每个第一层学习器时，我们每次都是用不同的数据集，如使用不同样本的子集，或者不同特征的子集。</p>
<p>但是与bagging和boosting相比，在实际使用中stacking更容易导致过拟合。当stacking使用不当时，其性能甚至低于简单平均。在大多数情况下，在stacking中我们一般不推荐使用特别复杂的第二层模型。事实上，线性回归在很多时候就能够有效提高第一层学习器的性能.。</p>
<h2 id="9-7-小结"><a href="#9-7-小结" class="headerlink" title="9.7 小结"></a>9.7 小结</h2><p>在实际使用集成学习时，bagging方法最终得到的模型几乎总是比单个模型效果要好。一般来讲，boosting比bagging对噪声更敏感，更易于过拟合。因此，在实际使用boosting时，要仔细调节参数以避免过拟合。通过选择合适的参数，boosting的效果可以明显好于bagging和单个模型。另外，bagging和boosting主要是聚合弱学习器，而stacking主要是聚合一些不那么“弱”的学习器。一般来讲，stacking适合在建模的最后阶段聚合已有的多个模型。</p>
<p>在集成学习中，一个很重要的概念是随机性。在随机森林中，我们通过bootstrap取样和在特征空间随机选择特征子集以引入随机性。在梯度上升算法中，我们也通过子取样来引入随机性。事实上，在stacking中，也可以采用类似的方法来引入随机性。为什么随机性重要呢？原因是在很多机器学习算法中，我们都是通过贪心学习<br>（greedy learning）的方法从数据中学习模型的参数。贪心学习的好处是可以很快地收敛到局部最优点，坏处就是容易导致过拟合。因此，在集成学习中，通过引入随机性，我们希望能够消灭或者降低贪心学习带来的过拟合风险。</p>
<hr>
<p>① <a href="https://studio.azureml.net/" target="_blank" rel="noopener">https://studio.azureml.net/</a></p>
<p>② 该模块是Permutation Feature Importance模块。</p>
<p>③ <a href="https://cran.r-project.org/web/packages/randomForest/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/randomForest/index.html</a></p>
<p>④ <a href="https://cran.r-project.org/web/packages/adabag/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/adabag/index.html</a></p>
<p>⑤ 本图来源于：Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package。</p>
<p>⑥ <a href="https://cran.r-project.org/web/packages/gbm/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/gbm/index.html</a></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] MITCHELL T M. Machine learning [M]. New York: McGraw-Hill, 1997.</p>
<p>[2] HASTIE T, TIBSHIRANI R, FRIEDMAN J. The elements of statistical learning: data mining, inference, and prediction [M]. 2nd ed. New York: Springer, 2009.</p>
<p>[3] JAMES G, WITTEN D, HASTIE T, TIBSHIRANI R. An introduction to statistical learning: with applications in R [M]. New York: Springer, 2013.</p>
<p>[4] BISHOP C M. Pattern recognition and machine learning [M]. New York: Springer, 2006.</p>
<p>[5] MURPH K P. Machine learning: a probabilistic perspective [M]. Cambridge: MIT Press, 2012.</p>
<p>[6] KUHN M, JOHNSON K. Applied predictive modeling [M]. New York: Springer, 2013.</p>
<p>[7] LANTZ B. Machine learning with R [M]. 2nd ed. Birmingham: Packt Publishing, 2015.</p>
<p>[8] DUDA R O, HART P E, STORK D G. Pattern classification [M]. New York: John Wiley &amp; Sons, 2000.</p>
<p>[9] TAN P, STEINBACH M, KUMAR V. Introduction to data mining [M]. New York: Pearson Education, 2006.</p>
<p>[10] HAN J, Pei J, KAMBER M. Data mining: concepts and techniques [M]. 3rd ed. Burlington: Morgan Kaufmann, 2012.</p>
<p>[11] WITTEN I, FRANK E, HALL M. Data mining: practical machine learning tools and techniques [M]. 3rd ed. Burlington: Morgan Kaufmann, 2011.</p>
<p>[12] LESKOVEC J, RAJARAMAN A, ULLMAN J. Mining of massive datasets [M]. Cambridge: Cambridge University Press, 2014.</p>
<p>[13] BECKER R A, CHAMBERS J M. Design of the S system for data analysis [J]. AT&amp;T Technical Journal, 1985, 64(9): 2131-2151.</p>
<p>[14] BECKER R A, CHAMBERS J M, WILKS A R. The new S language: a programming environment for data analysis and graphics [J]. Economic Journal, 1988,<br>-1(401): 85-89.</p>
<p>[15] Data Analysis Products Division. S-plus programmer’s guide (version 4.5) [M]. MathSoft Inc, 1998.</p>
<p>[16] SUSSMAN G J, STEELE G L. An interpreter for extended lambda calculus [M].Cambridge: MIT Press, 1975.</p>
<p>[17] IHAKA R, GENTLEMAN R. R: a language for data analysis and graphics [J]. Journal of Computational &amp; Graphical Statistics, 1996, 5(3): 299-314.</p>
<p>[18] VENABLES W N, SMITH D M, R Core Team. An introduction to R [G/OL]. [2016-10-31]. <a href="https://cran.rstudio.com/doc/manuals/r-release/R-intro.html" target="_blank" rel="noopener">https://cran.rstudio.com/doc/manuals/r-release/R-intro.html</a> .</p>
<p>[19] R Core Team. R language definition [G/OL]. [2016-10-31]. <a href="https://cran.rstudio.com/doc/manuals/r-release/R-lang.html" target="_blank" rel="noopener">https://cran.rstudio.com/doc/ manuals/r-release/R-lang.html</a> .</p>
<p>[20] R Core Team. Writing R extensions [G/OL]. [2016-10-31]. <a href="https://cran.rstudio.com/doc/manuals/r-release/R-lang.html" target="_blank" rel="noopener">https://cran.rstudio.com/ doc/manuals/r-release/R-lang.html</a> .</p>
<p>[21] GOLUB G H, VAN LOAN C F. Matrix computations [M]. 4th ed. Baltimore: Johns Hopkins University Press, 2013.</p>
<p>[22] PETERSEN K B, PEDERSEN M S. The matrix cookbook [M]. Denmark: Technical University of Denmark, 2012.</p>
<p>[23] WICKHAM H. ggplot2: Elegant graphics for data analysis [M]. 2nd ed. New York: Springer, 2016.</p>
<p>[24] GOLUB G H, VAN LOAN C F. Matrix computations [M]. 3rd ed. Baltimore: Johns Hopkins Press, 1996.</p>
<p>[25] TIBSHIRANI R. Regression shrinkage and selection via the Lasso [J]. Journal of the Royal Statistical Society, Series B, 1996, 58(1): 267-288.</p>
<p>[26] VAPNIK V. The nature of statistical learning theory [M]. New York: Springer, 2000.</p>
<p>[27] BOYD S, VANDENBERGHE L. Convex optimization [M]. New York: Cambridge University Press, 2004.</p>
<p>[28] LESKOVEC J, RAJARAMAN A, ULLMAN J. Mining of massive datasets [M]. Cambridge: Cambridge University Press, 2011.</p>
<p>[29] SHASHUA A, LEVIN, A. Ranking with large margin principles: two approaches [C]. //Advances in Neural Information Processing Systems. 2002: 937–944.</p>
<p>[30] BURGES C, SHAKED T, RENSHAW E, et al. Learning to rank using gradient descent [C]. //International Conference on Machine Learning. ACM, 2005: 89-96.</p>
<p>[31] BURGES C, RAGNO R, LE Q V. Learning to rank with non-smooth cost functions [C]. //Advances in Neural Information Processing Systems, 2006: 193-200.</p>
<p>[32] WU Q, BURGES C, SVORE K, GAO J. Adapting boosting for information retrieval measures [J]. Information Retrieval Journal, 2010, 13(3): 254–270.</p>
<p>[33] TSOCHANTARIDIS I, HOFMANN T, JOACHIMS T, ALTUN Y. Large margin methods for structured and interdependent output variables [J]. Journal of Machine Learning Research, 2005: 6(Sep): 1453–1484.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      <div>    
       
       
      <ul class="post-copyright">
        <li class="post-copyright-author">
            <strong>本文作者：</strong>hac_lang
        </li>
        <li class="post-copyright-link">
          <strong>本文链接：</strong>
          <a href="/2017/07/30/book-《实用机器学习》/" title="book_《实用机器学习》">2017/07/30/book-《实用机器学习》/</a>
        </li>
        <li class="post-copyright-license">
          <strong>版权声明： </strong>
          许可协议，请勿用于商业，转载注明出处！
        </li>
      </ul>
      
      </div>
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/编程/" rel="tag"># 编程</a>
          
            <a href="/tags/计算机/" rel="tag"># 计算机</a>
          
            <a href="/tags/自评/" rel="tag"># 自评</a>
          
            <a href="/tags/books/" rel="tag"># books</a>
          
            <a href="/tags/更毕/" rel="tag"># 更毕</a>
          
            <a href="/tags/豆瓣7/" rel="tag"># 豆瓣7</a>
          
            <a href="/tags/数据分析/" rel="tag"># 数据分析</a>
          
            <a href="/tags/MachineLearning/" rel="tag"># MachineLearning</a>
          
            <a href="/tags/算法策略/" rel="tag"># 算法策略</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/25/book-《用户网络行为画像》/" rel="next" title="book_《用户网络行为画像》">
                <i class="fa fa-chevron-left"></i> book_《用户网络行为画像》
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/02/book-《面向机器智能的TensorFlow实践》/" rel="prev" title="book_《面向机器智能的TensorFlow实践》">
                book_《面向机器智能的TensorFlow实践》 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
    <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="hac_lang">
  
  <p class="site-author-name" itemprop="name">hac_lang</p>
  <div class="site-description motion-element" itemprop="description">小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理</div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    

    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>







  <div class="links-of-author motion-element">
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://github.com/HACLANG" title="GitHub &rarr; https://github.com/HACLANG" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://stackoverflow.com/yourname" title="StackOverflow &rarr; https://stackoverflow.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://gitter.im" title="Gitter &rarr; https://gitter.im" rel="noopener" target="_blank"><i class="fa fa-fw fa-github-alt"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.jianshu.com/u/442ddccf3f32" title="简书 &rarr; https://www.jianshu.com/u/442ddccf3f32" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="Quora &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://plus.google.com/yourname" title="Google &rarr; https://plus.google.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="mailto:haclang.org@gmail.com" title="E-Mail &rarr; mailto:haclang.org@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="skype:haclang?call|chat" title="Skype &rarr; skype:haclang?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://twitter.com/haclang2" title="Twitter &rarr; https://twitter.com/haclang2" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="FaceBook &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
    
  </div>








          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#内容提要"><span class="nav-text">内容提要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#序一"><span class="nav-text">序一</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#序二"><span class="nav-text">序二</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-引论"><span class="nav-text">第1章 引论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-什么是机器学习"><span class="nav-text">1.1 什么是机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-机器学习算法的分类"><span class="nav-text">1.2 机器学习算法的分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-实际应用"><span class="nav-text">1.3 实际应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-病人住院时间预测"><span class="nav-text">1.3.1 病人住院时间预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-信用分数估计"><span class="nav-text">1.3.2 信用分数估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-Netflix上的影片推荐"><span class="nav-text">1.3.3 Netflix上的影片推荐</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-酒店推荐"><span class="nav-text">1.3.4 酒店推荐</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-5-讨论"><span class="nav-text">1.3.5 讨论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-本书概述"><span class="nav-text">1.4 本书概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-1-本书结构"><span class="nav-text">1.4.1 本书结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-2-阅读材料及其他资源"><span class="nav-text">1.4.2 阅读材料及其他资源</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第2章-R语言"><span class="nav-text">第2章 R语言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-R的简单介绍"><span class="nav-text">2.1 R的简单介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-R的初步体验"><span class="nav-text">2.2 R的初步体验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-基本语法"><span class="nav-text">2.3 基本语法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-语句"><span class="nav-text">2.3.1 语句</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．注释语句"><span class="nav-text">1．注释语句</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．表达式语句"><span class="nav-text">2．表达式语句</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．函数调用语句"><span class="nav-text">3．函数调用语句</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4．控制语句"><span class="nav-text">4．控制语句</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-函数"><span class="nav-text">2.3.2 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．基本函数"><span class="nav-text">1．基本函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．自定义函数"><span class="nav-text">2．自定义函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-常用数据结构"><span class="nav-text">2.4 常用数据结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-向量"><span class="nav-text">2.4.1 向量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．向量和基本数据类型"><span class="nav-text">1．向量和基本数据类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．向量的运算"><span class="nav-text">2．向量的运算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-因子"><span class="nav-text">2.4.2 因子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-3-矩阵"><span class="nav-text">2.4.3 矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．矩阵的定义"><span class="nav-text">1．矩阵的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．矩阵的运算"><span class="nav-text">2．矩阵的运算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-4-数据框"><span class="nav-text">2.4.4 数据框</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．数据框的定义"><span class="nav-text">1．数据框的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．数据框的操作"><span class="nav-text">2．数据框的操作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-5-列表"><span class="nav-text">2.4.5 列表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-6-下标系统"><span class="nav-text">2.4.6 下标系统</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-公式对象和apply函数"><span class="nav-text">2.5 公式对象和apply函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-R软件包"><span class="nav-text">2.6 R软件包</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-1-软件包的安装"><span class="nav-text">2.6.1 软件包的安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-2-软件包的使用"><span class="nav-text">2.6.2 软件包的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-3-软件包的开发"><span class="nav-text">2.6.3 软件包的开发</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-网络资源"><span class="nav-text">2.7 网络资源</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第3章-数学基础"><span class="nav-text">第3章 数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-概率"><span class="nav-text">3.1 概率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-基本概念"><span class="nav-text">3.1.1 基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-基本公式"><span class="nav-text">3.1.2 基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-常用分布"><span class="nav-text">3.1.3 常用分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．常用离散分布"><span class="nav-text">1．常用离散分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．常用连续分布"><span class="nav-text">2．常用连续分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-随机向量及其分布"><span class="nav-text">3.1.4 随机向量及其分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．随机向量与分布函数"><span class="nav-text">1．随机向量与分布函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．随机向量的常用分布"><span class="nav-text">2．随机向量的常用分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-5-随机变量的数字特征"><span class="nav-text">3.1.5 随机变量的数字特征</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．数学期望"><span class="nav-text">1．数学期望</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．方差"><span class="nav-text">2．方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．协方差与相关系数"><span class="nav-text">3．协方差与相关系数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4．偏度"><span class="nav-text">4．偏度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-6-随机向量的数字特征"><span class="nav-text">3.1.6 随机向量的数字特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-统计"><span class="nav-text">3.2 统计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-常用数据特征"><span class="nav-text">3.2.1 常用数据特征</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．一元数据常用数字特征"><span class="nav-text">1．一元数据常用数字特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．多元数据常用数字特征"><span class="nav-text">2．多元数据常用数字特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-参数估计"><span class="nav-text">3.2.2 参数估计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-矩阵"><span class="nav-text">3.3 矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-基本概念"><span class="nav-text">3.3.1 基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．矩阵的定义-1"><span class="nav-text">1．矩阵的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．特殊矩阵"><span class="nav-text">2．特殊矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．矩阵的行列式"><span class="nav-text">3．矩阵的行列式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4．秩"><span class="nav-text">4．秩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5．向量的范数"><span class="nav-text">5．向量的范数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-基本运算"><span class="nav-text">3.3.2 基本运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．和-差运算"><span class="nav-text">1．和/差运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．数乘运算"><span class="nav-text">2．数乘运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．乘法运算"><span class="nav-text">3．乘法运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4．幂运算"><span class="nav-text">4．幂运算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-特征值与特征向量"><span class="nav-text">3.3.3 特征值与特征向量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．方程法求解"><span class="nav-text">1．方程法求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．幂方法求解"><span class="nav-text">2．幂方法求解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-4-矩阵分解"><span class="nav-text">3.3.4 矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．基于特征值与特征向量的分解"><span class="nav-text">1．基于特征值与特征向量的分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．奇异值分解"><span class="nav-text">2．奇异值分解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-5-主成分分析"><span class="nav-text">3.3.5 主成分分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．拉格朗日函数法"><span class="nav-text">1．拉格朗日函数法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．特征值分解法"><span class="nav-text">2．特征值分解法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．贡献率"><span class="nav-text">3．贡献率</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-6-R中矩阵的计算"><span class="nav-text">3.3.6 R中矩阵的计算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第4章-数据探索和预处理"><span class="nav-text">第4章 数据探索和预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-数据类型"><span class="nav-text">4.1 数据类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-数据探索"><span class="nav-text">4.2 数据探索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-常用统计量"><span class="nav-text">4.2.1 常用统计量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-使用R实际探索数据"><span class="nav-text">4.2.2 使用R实际探索数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-数据预处理"><span class="nav-text">4.3 数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-1-缺失值的处理"><span class="nav-text">4.3.1 缺失值的处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-2-数据的标准化"><span class="nav-text">4.3.2 数据的标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-3-删除已有变量"><span class="nav-text">4.3.3 删除已有变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-4-数据的变换"><span class="nav-text">4.3.4 数据的变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-5-构建新的变量：哑变量"><span class="nav-text">4.3.5 构建新的变量：哑变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-6-离群数据的处理"><span class="nav-text">4.3.6 离群数据的处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-数据可视化"><span class="nav-text">4.4 数据可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-1-直方图"><span class="nav-text">4.4.1 直方图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-2-柱状图"><span class="nav-text">4.4.2 柱状图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-3-茎叶图"><span class="nav-text">4.4.3 茎叶图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-4-箱线图"><span class="nav-text">4.4.4 箱线图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-5-散点图"><span class="nav-text">4.4.5 散点图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第5章-回归分析"><span class="nav-text">第5章 回归分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-回归分析的基本思想"><span class="nav-text">5.1 回归分析的基本思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-线性回归和最小二乘法"><span class="nav-text">5.2 线性回归和最小二乘法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-1-最小二乘法的几何解释"><span class="nav-text">5.2.1 最小二乘法的几何解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-线性回归和极大似然估计"><span class="nav-text">5.2.2 线性回归和极大似然估计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-岭回归和Lasso"><span class="nav-text">5.3 岭回归和Lasso</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-1-岭回归"><span class="nav-text">5.3.1 岭回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-2-Lasso与稀疏解"><span class="nav-text">5.3.2 Lasso与稀疏解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-3-Elastic-Net"><span class="nav-text">5.3.3 Elastic Net</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-回归算法的评价和选取"><span class="nav-text">5.4 回归算法的评价和选取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-1-均方差和均方根误差"><span class="nav-text">5.4.1 均方差和均方根误差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-2-可决系数"><span class="nav-text">5.4.2 可决系数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-3-偏差-方差权衡"><span class="nav-text">5.4.3 偏差-方差权衡</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-案例分析"><span class="nav-text">5.5 案例分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-1-数据导入和探索"><span class="nav-text">5.5.1 数据导入和探索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-2-数据预处理"><span class="nav-text">5.5.2 数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-3-将数据集分成训练集和测试集"><span class="nav-text">5.5.3 将数据集分成训练集和测试集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-4-建立一个简单的线性回归模型"><span class="nav-text">5.5.4 建立一个简单的线性回归模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-5-建立岭回归和Lasso模型"><span class="nav-text">5.5.5 建立岭回归和Lasso模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-6-选取合适的模型"><span class="nav-text">5.5.6 选取合适的模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-7-构造新的变量"><span class="nav-text">5.5.7 构造新的变量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-6-小结"><span class="nav-text">5.6 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第6章-分类算法"><span class="nav-text">第6章 分类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-分类的基本思想"><span class="nav-text">6.1 分类的基本思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-决策树"><span class="nav-text">6.2 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-1-基本原理"><span class="nav-text">6.2.1 基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-2-决策树学习"><span class="nav-text">6.2.2 决策树学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．熵与基尼指数"><span class="nav-text">1．熵与基尼指数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．信息增益"><span class="nav-text">2．信息增益</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-3-过拟合和剪枝"><span class="nav-text">6.2.3 过拟合和剪枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-4-实际使用"><span class="nav-text">6.2.4 实际使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-5-讨论"><span class="nav-text">6.2.5 讨论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-逻辑回归"><span class="nav-text">6.3 逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-1-sigmoid函数的性质"><span class="nav-text">6.3.1 sigmoid函数的性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-2-通过极大似然估计来估计参数"><span class="nav-text">6.3.2 通过极大似然估计来估计参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-3-牛顿法"><span class="nav-text">6.3.3 牛顿法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-4-正则化项的引入"><span class="nav-text">6.3.4 正则化项的引入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-5-实际使用"><span class="nav-text">6.3.5 实际使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-支持向量机"><span class="nav-text">6.4 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-1-基本思想：最大化分类间隔"><span class="nav-text">6.4.1 基本思想：最大化分类间隔</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-2-最大分类间隔的数学表示"><span class="nav-text">6.4.2 最大分类间隔的数学表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-3-如何处理线性不可分的数据"><span class="nav-text">6.4.3 如何处理线性不可分的数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-4-Hinge损失函数"><span class="nav-text">6.4.4 Hinge损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-5-对偶问题"><span class="nav-text">6.4.5 对偶问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-6-非线性支持向量机和核技巧"><span class="nav-text">6.4.6 非线性支持向量机和核技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-7-实际使用"><span class="nav-text">6.4.7 实际使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-5-损失函数和不同的分类算法"><span class="nav-text">6.5 损失函数和不同的分类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-1-损失函数"><span class="nav-text">6.5.1 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-2-正则化项"><span class="nav-text">6.5.2 正则化项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-6-交叉检验和caret包"><span class="nav-text">6.6 交叉检验和caret包</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-6-1-模型选择和交叉检验"><span class="nav-text">6.6.1 模型选择和交叉检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-6-2-在R中实现交叉检验以及caret包"><span class="nav-text">6.6.2 在R中实现交叉检验以及caret包</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-7-分类算法的评价和比较"><span class="nav-text">6.7 分类算法的评价和比较</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-7-1-准确率"><span class="nav-text">6.7.1 准确率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-7-2-混淆矩阵"><span class="nav-text">6.7.2 混淆矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-7-3-精确率、召回率和F1度量"><span class="nav-text">6.7.3 精确率、召回率和F1度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-7-4-ROC曲线和AUC"><span class="nav-text">6.7.4 ROC曲线和AUC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-7-5-R中评价标准的计算"><span class="nav-text">6.7.5 R中评价标准的计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-8-不平衡分类问题"><span class="nav-text">6.8 不平衡分类问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-8-1-使用不同的算法评价标准"><span class="nav-text">6.8.1 使用不同的算法评价标准</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-8-2-样本权值"><span class="nav-text">6.8.2 样本权值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-8-3-取样方法"><span class="nav-text">6.8.3 取样方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-8-4-代价敏感学习"><span class="nav-text">6.8.4 代价敏感学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第7章-推荐算法"><span class="nav-text">第7章 推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-推荐系统基础"><span class="nav-text">7.1 推荐系统基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-1-常用符号"><span class="nav-text">7.1.1 常用符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-2-推荐算法的评价标准"><span class="nav-text">7.1.2 推荐算法的评价标准</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-基于内容的推荐算法"><span class="nav-text">7.2 基于内容的推荐算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-基于矩阵分解的算法"><span class="nav-text">7.3 基于矩阵分解的算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-1-无矩阵分解的基准方法"><span class="nav-text">7.3.1 无矩阵分解的基准方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-2-基于奇异值分解的推荐算法"><span class="nav-text">7.3.2 基于奇异值分解的推荐算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-3-基于SVD推荐算法的变体"><span class="nav-text">7.3.3 基于SVD推荐算法的变体</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．AFM模型"><span class="nav-text">1．AFM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．翻转的AFM模型"><span class="nav-text">2．翻转的AFM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．ASVD模型"><span class="nav-text">3．ASVD模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4．翻转的ASVD模型"><span class="nav-text">4．翻转的ASVD模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5．引入时间信息的模型"><span class="nav-text">5．引入时间信息的模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-4-基于邻域的推荐算法"><span class="nav-text">7.4 基于邻域的推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-1-基于用户的邻域推荐算法"><span class="nav-text">7.4.1 基于用户的邻域推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#评价的标准化"><span class="nav-text">评价的标准化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-2-基于商品的邻域推荐算法"><span class="nav-text">7.4.2 基于商品的邻域推荐算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-3-混合算法"><span class="nav-text">7.4.3 混合算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-4-相似度的计算"><span class="nav-text">7.4.4 相似度的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．余弦相似度"><span class="nav-text">1．余弦相似度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．Jaccard相似度"><span class="nav-text">2．Jaccard相似度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．Pearson相关系数"><span class="nav-text">3．Pearson相关系数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4．Spearman秩相关系数"><span class="nav-text">4．Spearman秩相关系数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5．相似度的显著性"><span class="nav-text">5．相似度的显著性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6．相似度计算的实例"><span class="nav-text">6．相似度计算的实例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-5-R中recommenderlab的实际使用"><span class="nav-text">7.5 R中recommenderlab的实际使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-6-推荐算法的评价和选取"><span class="nav-text">7.6 推荐算法的评价和选取</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第8章-排序学习"><span class="nav-text">第8章 排序学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1-排序学习简介"><span class="nav-text">8.1 排序学习简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-1-解决排序问题的基本思路"><span class="nav-text">8.1.1 解决排序问题的基本思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-2-构造特征"><span class="nav-text">8.1.2 构造特征</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．BM25模型"><span class="nav-text">1．BM25模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．其他特征"><span class="nav-text">2．其他特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-3-获取相关度分数"><span class="nav-text">8.1.3 获取相关度分数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-4-数学符号"><span class="nav-text">8.1.4 数学符号</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-排序算法的评价"><span class="nav-text">8.2 排序算法的评价</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-1-MAP"><span class="nav-text">8.2.1 MAP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-2-DCG"><span class="nav-text">8.2.2 DCG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-3-NDCG"><span class="nav-text">8.2.3 NDCG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-4-讨论"><span class="nav-text">8.2.4 讨论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-3-逐点方法"><span class="nav-text">8.3 逐点方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-1-基于SVM的逐点排序方法"><span class="nav-text">8.3.1 基于SVM的逐点排序方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-2-逐点方法讨论"><span class="nav-text">8.3.2 逐点方法讨论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-4-逐对方法"><span class="nav-text">8.4 逐对方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-1-Ranking-SVM算法"><span class="nav-text">8.4.1 Ranking SVM算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-2-IR-SVM算法"><span class="nav-text">8.4.2 IR-SVM算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-3-RankNet算法"><span class="nav-text">8.4.3 RankNet算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RankNet算法的Mini-batch实现"><span class="nav-text">RankNet算法的Mini-batch实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-4-LambdaRank算法"><span class="nav-text">8.4.4 LambdaRank算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-5-LambdaMART算法"><span class="nav-text">8.4.5 LambdaMART算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gbm包和LambdaMART算法"><span class="nav-text">gbm包和LambdaMART算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-5-逐列方法"><span class="nav-text">8.5 逐列方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-1-SVMmap-算法"><span class="nav-text">8.5.1 SVMmap 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-2-讨论"><span class="nav-text">8.5.2 讨论</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第9章-集成学习"><span class="nav-text">第9章 集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-1-集成学习简介"><span class="nav-text">9.1 集成学习简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2-bagging简介"><span class="nav-text">9.2 bagging简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-3-随机森林"><span class="nav-text">9.3 随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-1-训练随机森林的基本流程"><span class="nav-text">9.3.1 训练随机森林的基本流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-2-利用随机森林估计变量的重要性"><span class="nav-text">9.3.2 利用随机森林估计变量的重要性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-3-随机森林的实际使用"><span class="nav-text">9.3.3 随机森林的实际使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#R程序以及介绍"><span class="nav-text">R程序以及介绍</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-4-boosting简介"><span class="nav-text">9.4 boosting简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-1-boosting和指数损失函数"><span class="nav-text">9.4.1 boosting和指数损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-2-AdaBoost算法"><span class="nav-text">9.4.2 AdaBoost算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-3-AdaBoost的实际使用"><span class="nav-text">9.4.3 AdaBoost的实际使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-4-讨论"><span class="nav-text">9.4.4 讨论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-5-提升决策树和梯度提升算法"><span class="nav-text">9.5 提升决策树和梯度提升算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-5-1-提升决策树和梯度提升算法的基本原理"><span class="nav-text">9.5.1 提升决策树和梯度提升算法的基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-5-2-如何避免过拟合"><span class="nav-text">9.5.2 如何避免过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1．学习率和决策树的数目"><span class="nav-text">1．学习率和决策树的数目</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2．决策树的大小"><span class="nav-text">2．决策树的大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3．子取样"><span class="nav-text">3．子取样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4．更加完备和实用的梯度提升算法"><span class="nav-text">4．更加完备和实用的梯度提升算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-5-3-gbm包的实际使用"><span class="nav-text">9.5.3 gbm包的实际使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-5-4-讨论"><span class="nav-text">9.5.4 讨论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-6-学习器的聚合及stacking"><span class="nav-text">9.6 学习器的聚合及stacking</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-1-简单平均"><span class="nav-text">9.6.1 简单平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-2-加权平均"><span class="nav-text">9.6.2 加权平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-3-stacking的基本思想及应用"><span class="nav-text">9.6.3 stacking的基本思想及应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-7-小结"><span class="nav-text">9.7 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hac_lang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  





  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="true"></script>









  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  
  

  
  

  


  

  

  

  

  


  


  




  




  




  



<script>
// GET RESPONSIVE HEIGHT PASSED FROM IFRAME

window.addEventListener("message", function(e) {
  var data = e.data;
  if ((typeof data === 'string') && (data.indexOf('ciu_embed') > -1)) {
    var featureID = data.split(':')[1];
    var height = data.split(':')[2];
    $(`iframe[data-feature=${featureID}]`).height(parseInt(height) + 30);
  }
}, false);
</script>


  

  

  


  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":250,"height":500},"mobile":{"show":false,"scale":0.5},"react":{"opacity":0.7},"log":false,"tagMode":false});</script></body>
</html>
