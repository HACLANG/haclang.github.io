<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">






<link rel="stylesheet" href="/css/main.css?v=7.2.0">






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="作者: 冯超出版社: 电子工业出版社出品方: 博文视点副标题: 核心算法与TensorFlow实现出版年: 2018-6ISBN: 9787121340000 内容简介 《强化学习精要：核心算法与TensorFlow实现》用通俗幽默的语言深入浅出地介绍了强化学习的基本算法与代码实现，为读者构建了一个完整的强化学习知识体系，同时介绍了这些算法的具体实现方式。从基本的马尔可夫决策过程，到各种复杂的强化">
<meta name="keywords" content="计算机,人工智能,自评,books,续更,算法,豆瓣6,tensorflow,强化学习,工程">
<meta property="og:type" content="article">
<meta property="og:title" content="book_《强化学习精要 核心算法与TensorFlow实现》_冯超">
<meta property="og:url" content="https://haclang.github.io/2018/07/22/book-《强化学习精要-核心算法与TensorFlow实现》-冯超/index.html">
<meta property="og:site_name" content="Hac_lang">
<meta property="og:description" content="作者: 冯超出版社: 电子工业出版社出品方: 博文视点副标题: 核心算法与TensorFlow实现出版年: 2018-6ISBN: 9787121340000 内容简介 《强化学习精要：核心算法与TensorFlow实现》用通俗幽默的语言深入浅出地介绍了强化学习的基本算法与代码实现，为读者构建了一个完整的强化学习知识体系，同时介绍了这些算法的具体实现方式。从基本的马尔可夫决策过程，到各种复杂的强化">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-08-15T09:20:05.313Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="book_《强化学习精要 核心算法与TensorFlow实现》_冯超">
<meta name="twitter:description" content="作者: 冯超出版社: 电子工业出版社出品方: 博文视点副标题: 核心算法与TensorFlow实现出版年: 2018-6ISBN: 9787121340000 内容简介 《强化学习精要：核心算法与TensorFlow实现》用通俗幽默的语言深入浅出地介绍了强化学习的基本算法与代码实现，为读者构建了一个完整的强化学习知识体系，同时介绍了这些算法的具体实现方式。从基本的马尔可夫决策过程，到各种复杂的强化">





  
  
  <link rel="canonical" href="https://haclang.github.io/2018/07/22/book-《强化学习精要-核心算法与TensorFlow实现》-冯超/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>book_《强化学习精要 核心算法与TensorFlow实现》_冯超 | Hac_lang</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hac_lang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">小白hac_lang的笔记，涉及内容包含但不限于：人工智能   基因工程    信息安全   软件工程   嵌入式   天文物理</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-news">

    
    
      
    

    

    <a href="/news/" rel="section"><i class="menu-item-icon fa fa-fw fa-rss"></i> <br>news</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    
  
  

  

  <a href="https://github.com/HACLANG" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://haclang.github.io/2018/07/22/book-《强化学习精要-核心算法与TensorFlow实现》-冯超/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hac_lang">
      <meta itemprop="description" content="小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hac_lang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">book_《强化学习精要 核心算法与TensorFlow实现》_冯超

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-07-22 19:35:19" itemprop="dateCreated datePublished" datetime="2018-07-22T19:35:19+08:00">2018-07-22</time>
            </span>
          

          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>作者: 冯超<br>出版社: 电子工业出版社<br>出品方: 博文视点<br>副标题: 核心算法与TensorFlow实现<br>出版年: 2018-6<br>ISBN: 9787121340000</p>
<p><strong>内容简介</strong></p>
<p>《强化学习精要：核心算法与TensorFlow实现》用通俗幽默的语言深入浅出地介绍了强化学习的基本算法与代码实现，为读者构建了一个完整的强化学习知识体系，同时介绍了这些算法的具体实现方式。从基本的马尔可夫决策过程，到各种复杂的强化学习算法，读者都可以从本书中学习到。本书除了介绍这些算法的原理，还深入分析了算法之间的内在联系，可以帮助读者举一反三，掌握算法精髓。书中介绍的代码可以帮助读者快速将算法应用到实践中。</p>
<p>《强化学习精要：核心算法与TensorFlow实现》内容翔实，语言简洁易懂，既适合零基础的人员入门学习，也适合相关科研人员研究参考。  </p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>从你拿起本书的那一刻起，我们就一起踏上了这段有关强化学习的冒险之旅。本书中有简单直白的叙述，也有复杂冗长的证明；有诙谐幽默，也有深刻的思考。所有的一切，都是为了帮助你更轻松地对强化学习有更多了解、更多感悟。</p>
<p>一年之前，我在拙作《深度学习轻松学：核心算法与视觉实践》中，同样以这段话作为开篇，唯一不同的是这一次书的主题换成了强化学习。近年来，强化学习领域有了很大的发展，基于强化学习开发的智能体也被人们熟知。于是越来越多的人开始关注强化学习，了解这个领域，并使用强化学习的方法解决问题。随着深度学习的发展，强化学习也逐渐发挥出了自己的实力，凭借更灵活的问题定义方式，解决了很多其他方法难以解决的问题。</p>
<p>虽然强化学习这个领域已经有了多年的积累与发展，但是与深度学习相比，国内外与此相关的书籍与课程偏少。同时，由于强化学习融合了很多领域的知识，本身的技术难度较大，学习起来比较困难。两者叠加，使得强化学习仍然是一个令很多人感到困惑的概念。</p>
<p>为了学习强化学习中的各种知识，作为本书的作者，我也花费了很多的精力。在阅读了很多经典的书籍、论文和博客之后，我开始对强化学习有了自己的理解和体会。同时也积累了不少关于强化学习的“学习笔记”，其中的一部分已经在一些公开场合，例如我的知乎专栏《无痛的机器学习》（https：//zhuanlan.zhihu.com/hsmyy）中与读者分享过。在交流的过程中，我能感受到读者对强化学习的浓厚兴趣，同时自己也在分享中得到了提高。</p>
<p>经过长时间的积累，这些心得与体会最终凝结成了本书。在这个过程中我重新回顾了自己所学的知识，也进一步思考了强化学习各部分知识点之间的联系。我希望可以给大家呈现一个尽可能完整而丰富的强化学习世界，与大家共同探讨强化学习中的无限可能。</p>
<p>为了使更多的人能够从本书中得到收获，我选择强化学习中比较经典的算法进行了介绍，这些都是强化学习中十分重要的内容，对不了解强化学习的读者来说，可以帮助你快速了解强化学习的知识体系，对这个领域有一个全面的认识；对正在学习强化学习的读者来说，本书中的知识同样可以帮助你梳理强化学习的核心要点，加深对这些基本问题的理解。</p>
<h1 id="第一部分-强化学习入门与基础知识"><a href="#第一部分-强化学习入门与基础知识" class="headerlink" title="第一部分 强化学习入门与基础知识"></a>第一部分 强化学习入门与基础知识</h1><p>作为本书的第一部分，我们首先介绍强化学习的基本概念和基本框架，同时将与强化学习有关的所有基础知识做详细介绍。虽然这一部分并没有讲述太多强化学习的内容，但它仍然十分重要，只有了解了这些内容，才能更好地学习后面的知识。如果读者已经对数学基础和机器学习的基本概念有一定的了解，可以通过阅读本部分复习；如果读者对上述内容不太了解，则要认真学习这一部分的知识。</p>
<p>第一部分共包含六章：</p>
<p>· 第1章介绍了强化学习的定义、目标等基础知识，并介绍了全书的结构。</p>
<p>· 第2章介绍了数学基础知识。</p>
<p>· 第3章介绍了本书使用的优化算法相关的知识。</p>
<p>· 第4章介绍了本书代码使用的框架——TensorFlow的基本知识。</p>
<p>· 第5章介绍了强化学习相关的项目——Gym和Baselines的基本知识。</p>
<p>· 第6章介绍了强化学习的理论基础——马尔可夫决策过程及基于动态规划的求解方法。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><h3 id="1-1-强化学习的概念"><a href="#1-1-强化学习的概念" class="headerlink" title="1.1 强化学习的概念"></a>1.1 强化学习的概念</h3><p>作为开篇，最重要的一件事是介绍强化学习的入门知识和相关应用。我们首先要解决一个问题：什么是强化学习？这个词并不那么容易理解，“强化”在这里是一个动词还是一个名词，又或者是一个人名？还有人把它称为“增强学习”，其实名字所表达的含义差不多，但总感觉这个名字并没有很清楚地表达它想表达的含义，即使一些对强化学习有一定经验的人也可能感到疑惑。</p>
<p>不得不承认，当今很多知识和概念都是从西方传来的，强化学习也是一个“外来品”。它的英文是Reinforcement<br>Learning，在英文辞典里查询reinforcement这个词，我们找到了下面的解释（来自https：//www.merriam-<br>webster.com/dictionary/reinforcement）：</p>
<p>something that strengthens or encourages something,such as a response to some-<br>one’s behavior that is intended to make that person more likely to behave that<br>way again.</p>
<p>将英文翻译后的大概意思是：“reinforcement”是一种“套路”，或者是某种方法，也可能是某种形式。它会强化或者鼓励某个人或者某个事物以更高的可能性产生同样的行为。当然，这里的行为不只是让某个人或事物去做某些事情，也可能是不去做某些事情，具体产生什么样的行为取决于使用什么样的“reinforcement”。</p>
<h4 id="1-1-1-巴浦洛夫的狗"><a href="#1-1-1-巴浦洛夫的狗" class="headerlink" title="1.1.1 巴浦洛夫的狗"></a>1.1.1 巴浦洛夫的狗</h4><p>简单地说，强化学习的完整含义是利用这种“套路”完成学习。这个抽象的概念还有一个心理学上的解释，不禁让笔者想起了一个经典的心理学实验：巴浦洛夫的狗。在这个经典的实验中，每次实验者都对着狗摇铃铛，并给狗一点食物。久而久之，铃铛和食物的组合影响了狗的行为，此后每次对着狗摇铃铛，狗就会不由自主地流口水，并期待食物的到来，这样实验者就让狗“学会”了铃铛和食物的关系，这也可以算作强化学习的一个简单的例子。</p>
<p>了解了这个套路的一个具体例子，我们现在思考的是：能不能将这个套路的活动过程抽象成一个框架，使我们能更方便地将这个框架应用到更多的实例上呢？经过前辈们的苦心研究，最终我们得到了一个具有很高抽象度的强化学习框架。在“巴浦洛夫的狗”实验中，我们发现了下面几个关键要素。</p>
<p>· 狗：实验的主角。</p>
<p>· 实验者：负责操控和运转实验。</p>
<p>· 铃铛：给狗的一个刺激。</p>
<p>· 口水：狗对刺激的反应。</p>
<p>· 食物：给狗的奖励，也是改变狗行为的关键。</p>
<p>接下来，我们要给上面的每个要素赋予一个抽象的名字。</p>
<p>·<br>实验的主角：Agent。这个词不好翻译，一般翻译为智能体，但是这个翻译更像意译，和英文单词本身的含义有点距离。因此，本书将主要使用英文单词Agent；在一些游戏问题中，为了更贴切行文环境，会以“玩家”来表示。</p>
<p>· 实验的操控者：System Environment，一般翻译为系统环境。</p>
<p>· 给Agent的刺激（铃铛）：Observation，一般翻译为观察值，但是在很多强化学习文献中，会将其等价为状态（State）。</p>
<p>· Agent的反应（口水）：Action，一般翻译为行动。</p>
<p>· Agent的奖励（食物）：Reward，一般翻译为回报或者反馈。</p>
<p>在经典的强化学习中，智能体要和环境完成一系列的交互。</p>
<p>（1）在每一个时刻，环境都将处于一种状态。</p>
<p>（2）智能体将设法得到环境当前状态的观测值。</p>
<p>（3）智能体根据观测值，结合自己历史的行为准则（一般称为策略，Policy）做出行动。</p>
<p>（4）这个行动会影响环境的状态，使环境发生一定的改变。Agent将从改变后的环境中得到两部分信息：新的环境观测值和行为给出的回报。这个回报可以是正向的，也可以是负向的。这样Agent就可以根据新的观测值做出新的行动，这个过程如图1-1所示。</p>
<p>图1-1 强化学习的过程表示</p>
<p>可以想象，在实验的早期，当实验者对着狗摇铃铛时，狗并不会有任何准备进食的反应；随着实验的进行，铃铛和食物这两个观测内容不断地刺激狗，使狗最终提高了准备进食这个行动的可能性。实际上，这样的交互模式在很多场景下都会出现。</p>
<h4 id="1-1-2-俄罗斯方块"><a href="#1-1-2-俄罗斯方块" class="headerlink" title="1.1.2 俄罗斯方块"></a>1.1.2 俄罗斯方块</h4><p>我们以一个经典的电子游戏俄罗斯方块为例，回顾强化学习的整个过程。常见的游戏界面如图1-2所示。</p>
<p>图1-2 游戏俄罗斯方块的界面</p>
<p>以与巴浦洛夫的狗相同的角度看待俄罗斯方块游戏，我们发现此时的实验者就是游戏制作团队，而被实验的对象就是玩家。实验者通过构建一个俄罗斯方块游戏的环境，强化玩家一些行为的可能性。控制玩家行为的关键在于游戏的得分机制，由于我们明确了玩家的目标是尽可能地获得更高的分数，那么游戏团队对什么样的行为强化，玩家就倾向于产生什么样的行为。</p>
<p>从上面的描述中我们发现，如果游戏团队对消除方块的行动给予强化，那么玩家将更倾向于产生消除方块的行为；如果游戏团队对不消除方块的行动给予强化，玩家会更倾向于产生不消除方块的行为，当然这样会更快地结束游戏。如果游戏团队只对某种特定的消除方块行为（例如，用“竖条”一次性消除多行方块）给予较大的强化，那么玩家会尽可能地产生这种行为，其他消除方块技巧的使用频率会降低。实际上，俄罗斯方块这款游戏就是采用了这样的设计方案。</p>
<p>我们通过两个例子阐述了强化学习的核心思想：通过某种手段影响被实验者的行为。为了实现这个目标，实验者需要构建一个完整的实验环境，通过给予被实验者一定的观测和回报，让其产生实验者想要的结果。理解了这个思想，就算迈进了强化学习的大门。本书后面的内容就是剖析如何施展这些手段。</p>
<p>虽然本书从心理学的角度切入介绍了强化学习的概念，但毕竟是一本介绍人工智能技术的书籍，所以本书的“被实验者”全部是计算机模型或者仿真机器人等非有机生物。实际上，上面的实验框架也可以应用在人类或者动物身上，此时我们就转变成了研究心理学、社会学、管理学等人文学科的学者。因此从本质上看，文科和理科的界线并不是非常明确。如果你是一名对上述人文学科感兴趣的读者，不妨使用本节介绍的思想套用到这些学科，相信你会有意想不到的收获。</p>
<h3 id="1-2-站在被实验者的角度看问题"><a href="#1-2-站在被实验者的角度看问题" class="headerlink" title="1.2 站在被实验者的角度看问题"></a>1.2 站在被实验者的角度看问题</h3><p>看到这里，相信读者对强化学习有了一定的了解，前面我们提到的例子都是站在局外人的角度分析的，读者并没有参与到实验本身；而强化学习的定义是站在实验者的角度给出的，被实验者更像是一只被操纵的“木偶”。虽然这种控制他物的感觉十分奇妙，但是现实往往会将我们打回原形。实际上在日常生活中，我们往往会扮演“被实验者”这个角色。以俄罗斯方块为例，我们往往不是在扮演一个游戏的开发者，站在制高点控制玩家的行为，而是扮演一个玩家，思考如何才能玩好这个游戏。就算是游戏的开发者，也不能单纯地通过游戏影响别人的行为，他其实身处另外一个游戏，在这个游戏中，他需要通过制作优秀的游戏让更多人爱上自己的游戏，从而获得更多的收入和更大的影响力。从这个角度观察人类的行为，有时不免觉得悲哀。</p>
<p>让我们停止这样的思考，回到一个被实验者应有的视角。站在俄罗斯方块玩家的角度，我们的目标自然是尽可能地多得分。从游戏界面中，我们只能知道当前方块累积的效果和下一个到来的方块，而且所有的操作要在有限的时间内完成。除此之外，还有一些隐含的信息需要靠我们持续地观察才能得到，例如游戏中方块自然下落的速度。游戏的制作者可以从引擎中找到对应的参数，而作为玩家，我们只能通过画面变换的速度进行判断。</p>
<p>这个差异可以由强化学习中的两个抽象实体表示。在1.1.1节的介绍中我们提到了两个概念：状态和观测值。其中状态表示Agent所处环境的所有信息，对俄罗斯方块这个游戏来说，等同于与游戏相关的所有信息；而观测值只是作为玩家能看到的一部分信息，两者实际上存在很大的差异。当然，对俄罗斯方块来说，由于游戏设计的区域相对有限，而且作为玩家我们可以知道下一个方块的信息，实际上信息量还算足够。</p>
<p>但是在另外一些问题中，状态和观测值存在很大的差异，这个差异会影响玩家的操作。以一些角色扮演游戏（Role-Playing<br>Game，简称RPG）的迷宫为例，从玩家的角度只能看到所处位置附近的信息，而这只占完整迷宫的一小部分，这时状态和观测的差异会对决策产生很大影响，由于每一时刻只能观测到部分信息，玩家需要记住其他区域的信息，才能更好地做出全局最优的决策。笔者以游戏《仙剑奇侠传3》中的经典迷宫——蜀山锁妖塔第4层为例，展示两者的不同。迷宫的抽象版地图如图1-3所示。</p>
<p>图1-3 玩家制作的本层迷宫通关指南</p>
<p>地图中标示了几个主要的物体，其中以三角形+数字组合标示的物体为浮台，关卡开始时它在游戏空间下方，当玩家触发了圆圈+数字组合标示的机关，浮台就会升起。虚线框+字母组合标示的物体是传送台，相同字母的一对传送台可以将玩家传送到对应的位置。</p>
<p>想要通过这个迷宫，玩家必须按照顺序触动机关将1～5这几个浮台升起，并站在6号浮台上同它一起升起，然后从游戏空间上方通过进入下一关。要想把这些浮台升起，玩家需要在游戏空间下方穿梭于几个浮岛，触动每一个浮台的机关。如果读者多花一点时间分析这张图，就可以看出玩家需要按一定的顺序触发机关，如果触发顺序错误，将无法把所有的浮台升起。例如，如果玩家先把5号浮台升起，就无法到达图中左上角的那个浮岛，也就无法触发3号浮台。</p>
<p>相信玩家可以感受到这个迷宫的复杂性，即使以这样抽象的形式表示出来，我们仍然需要经过一定的分析才能过关。在真实的游戏中，玩家根本不会看到这样简洁的迷宫地图，玩家的游戏界面如图1-4所示。</p>
<p>图1-4 迷宫游戏截图</p>
<p>游戏的主界面下玩家只能看到一个浮岛的情况，而且传送台上并未标明它前往的目标。虽然在画面的左下角我们可以看到一个小地图，但是这个地图没有展现迷宫所有的内容，玩家想通过迷宫就变得非常困难。</p>
<p>通过上面的例子，相信读者已经了解了环境状态和观测的区别，也了解了很多问题的难度。在很多强化学习问题中，我们并不刻意区分这两个名词，而是将这两个名词混合使用。实际上，观测值也可以理解为Agent眼中环境的状态，希望读者可以注意这一点。</p>
<p>既然我们现在扮演了Agent的角色，我们的目标实际上发生了一定的变化，前面我们站在实验者的角度，目标是通过设计好的实验影响实验者的行为，而现在我们站在被实验者的角度，目标就变成了怎样又快又好地达到实验者的目标——改变行为，本书介绍的算法也是要达到这样的目标。</p>
<p>“又快又好地达到实验者的目标”这句话实在有些抽象，而且还是站在实验者的角度看待问题，我们需要一个更具体的目标。从前面的介绍中我们知道，实验者会通过一些手段强化Agent的某些行动，这个手段就是回报。如果Agent执行了实验者想要的行动，实验者就会给Agent更多的回报。反过来说，如果Agent配合实验者尽可能地将行为改变成实验者想要的效果，那么Agent就能获得最多的回报。所以站在Agent的角度，我们的目标是最大化所获得的回报，这个回报包含了Agent与环境交互的所有回报。这样，我们从观察内容和目标两个方面重新定义了被实验者眼中的强化学习。</p>
<h3 id="1-3-强化学习效果的评估"><a href="#1-3-强化学习效果的评估" class="headerlink" title="1.3 强化学习效果的评估"></a>1.3 强化学习效果的评估</h3><p>1.2节我们转换了强化学习的关注角度，并明确了学习的目标：最大化所获得的回报。但是这个目标还是有些抽象，我们需要把这个目标变得更容易量化。首先，我们介绍强化学习的两个显著特点，正是这两个特点使得它与其他学习方法存在不同。</p>
<h4 id="1-3-1-不断试错"><a href="#1-3-1-不断试错" class="headerlink" title="1.3.1 不断试错"></a>1.3.1 不断试错</h4><p>前面提到强化学习通过一些手段影响Agent的行动，这实际上是站在环境或实验者的角度来看的。如果站在Agent的角度，就是另外一幅景象：根据环境状态给出行动的Agent有时会收到较多回报，有时回报较少，回报可以以数值的形式标示，那么Agent还可能收到负的回报。究竟怎样才能获得最多的回报呢？Agent自己并不知道，“实验者”也不会告诉Agent。所以Agent需要根据回报的多少不断地调整自己的策略，从而尽可能多地获得回报。这个过程中Agent需要不断尝试，尝试应对状态的各种可能的行动，并收集对应的回报，只有收集到各种反馈信息，才能更好地完成学习任务。因此这是一个不断试错（Trial<br>and Error）的过程，只有经过尝试、遇到失败，才能获得最终的成功。</p>
<h4 id="1-3-2-看重长期回报"><a href="#1-3-2-看重长期回报" class="headerlink" title="1.3.2 看重长期回报"></a>1.3.2 看重长期回报</h4><p>强化学习的任务通常需要长时间的交互，比如上面提到的俄罗斯方块游戏，玩家与游戏交互的周期可以是一局游戏。在这样的时间跨度下，眼前一步或两步操作获得的回报就变得没那么重要了。前面也提到，俄罗斯方块对同时消除不同层数方块的奖励不同，分4次消除一行方块得到的分数远远小于一次消除4行方块得到的分数。消除一行方块比较简单，只需要几个方块就可以做到；而消除4行方块就比较困难，需要前期方块的累积和准确地摆放一根竖棍。更难的方块摆放也使玩家更快地积累分数，从而更快赢得胜利，这和游戏设计者的初衷一致。因此，从更长远的角度看，玩家要学习如何一次消除多行，而不是追逐短期的得分。当然，追求长期分数需要多探索、多尝试，也可能遇到更多的失败（例如没等到竖棍出现），所以看重长期回报和不断试错存在一定的一致性。</p>
<p>正因为这两个特点，我们在评价强化学习算法的优劣上与其他算法不同。除了一些常见的衡量指标（算法的效果、计算时间、稳定性和泛化性等），我们还要重点考虑一个指标：学习时间。由于学习和尝试相关，所以这个指标一般也看作尝试和探索的次数。如果一个算法需要尝试的次数比较多，我们一般认为算法要花费的时间比较长；如果一个算法需要尝试的次数比较少，那么相对来说花费的时间比较短。</p>
<p>站在机器学习的角度，我们可以认为尝试的样本本身会影响学习的时间，例如样本的代表性、重合度等。对强化学习来说，由于学习本身的特点，我们需要考虑训练样本的使用率（Sample<br>Efficiency）。不同算法对样本的重复使用次数不同，有的算法对于尝试的样本只能使用一次，而有的算法可以反复使用同样的样本。这些算法的特点我们会在后面的章节介绍。</p>
<p>训练样本的使用率会直接影响学习时间。前面提到Agent的学习样本要通过自身与环境的交互得到，而这个过程是要花费时间的。需要的样本量少，学习时间就可以缩短；反之学习时间会比较长。对计算机模拟的学习问题来说，样本量并不算个大问题，因为计算机可以在短时间内快速模拟出大量的样本；但是对于在真实场景进行训练的问题来说，产生样本意味着要在真实世界的时间尺度下进行交互，花费的时间会很长。为了一点效果的提升花费大量的时间，对我们来说有点得不偿失。因此，很多研究人员都在思考如何提高真实世界学习的速度，这就涉及提高样本利用率、迁移学习等内容。</p>
<p>这样我们就了解了强化学习关注的两个目标：学习效果和学习时间。学习时间也成了算法十分看重的一个目标。</p>
<h3 id="1-4-强化学习与监督学习"><a href="#1-4-强化学习与监督学习" class="headerlink" title="1.4 强化学习与监督学习"></a>1.4 强化学习与监督学习</h3><p>有些读者会有疑问：强化学习和其他常见的方法有什么相同与不同呢？有些读者应该知道监督学习，有些读者可能不太了解，监督学习是一种很经典的机器学习方法，它的目标是训练一个模型，使模型能够根据确定的输入得到对应的输出。为了实现这个目标，我们需要准备好一定数量的训练样本，这些样本包含了一对对的输入和输出数据，我们使用输入和输出数据计算得到模型的参数，从而完成模型的学习。</p>
<h4 id="1-4-1-强化学习与监督学习的本质"><a href="#1-4-1-强化学习与监督学习的本质" class="headerlink" title="1.4.1 强化学习与监督学习的本质"></a>1.4.1 强化学习与监督学习的本质</h4><p>从学习的目标来看，两个学习方法十分相近。监督学习希望模型根据指定的输入得到对应的输出，强化学习希望Agent（也可以想象成模型）根据指定的状态得到使回报最大化的行动。看上去二者都是完成从一个事物到另一个事物的映射，但是只要做进一步的分析，我们就可以梳理出它们的不同。</p>
<p>1.二者在目标的明确性上有所不同</p>
<p>监督学习的目标更明确，输入对应的是确定的输出，而且理论上一个输入只对应一个输出；而强化学习的目标没有这么明确，使当前状态获得最大回报的行动可能有很多。</p>
<p>以俄罗斯方块为例。如果采用监督学习进行训练，那么模型以每一帧游戏画面或游戏状态为输入，对应的输出是确定的，要么移动方块，要么翻转方块。这样的限定实际上有些死板，某个操作序列能够达到想要的目标（例如得更多的分），但是得更多的分数的方法不止一种。强化学习直接设定某个目标，并根据这个目标设定反馈，这样的限定相对宽松，定义问题的难度也更低。</p>
<p>2.从时间维度上看，二者输出的意义不同</p>
<p>监督学习主要看重输入和输出的匹配程度，如果输入和输出匹配，学习的效果就是好的，即使存在序列到序列的映射，我们也希望每一个时刻的输出都能和输入对应上；而对强化学习而言，我们的目标是让回报最大化，但是在与环境交互的过程中，并不是每一个行动都会获得回报（本书中也会涉及一些这样的任务）。当我们完成了一次完整的交互后，会得到一个行动序列，这个行动序列中哪些行动为回报产生了正向的贡献，哪些产生了负向的贡献，有时很难界定。</p>
<p>以棋类游戏为例，游戏的目标是战胜对方（获得胜利），在得到最终结果之前，Agent可能不会得到任何回报。单一的行动是优是劣无从判断，我们只能把所有的行动考虑成一个整体，给整体一个回报。为了最终的回报，游戏中的某些行动可能会走一些看似不好的招法（例如被对方吃掉棋子），这是为达成最终目标做出的牺牲。从这个角度来看，强化学习和监督学习存在一定的不同，这个不同也将在后面的章节展开介绍。</p>
<p>总的来说，强化学习相比监督学习有两个优点。</p>
<p>（1）定义模型需要的约束更少，影响行动的反馈虽然不及监督学习直接，却降低了定义问题的难度。</p>
<p>（2）更看重行动序列带来的整体回报，而不是单步行动的一致性。</p>
<h4 id="1-4-2-模仿学习"><a href="#1-4-2-模仿学习" class="headerlink" title="1.4.2 模仿学习"></a>1.4.2 模仿学习</h4><p>实际上有一种学习方法的目标和强化学习一致，都是最大化长期回报，但是学习方法和监督学习类似，收集大量的单步决策样本，并让模型学习这些单步决策的逻辑。这种方法被称为“模仿学习”（Imitation<br>Learning），也被称为“行为克隆”（Behavior Cloning），它是模仿学习的一种。它的执行流程为：</p>
<p>（1）寻找一些“专家”（Expert）代替Agent与环境交互，得到一系列的交互序列。</p>
<p>（2）假设这些交互序列为对应状态下的“标准答案”，我们就可以使用监督学习的方法让模型学习这些数据，从而完成将状态和专家行动对应的工作。</p>
<p>这种方法在一些问题上可以获得比较好的效果，但也存在如下问题。</p>
<p>（1）如何收集满足目标的样本？我们必须在某一个领域存在一个专家，所有的样本通过专家和环境的交互产生。</p>
<p>（2）如何收集大量的样本？如果样本数量不够多，我们很难学习出好的策略模型。</p>
<p>（3）如何确保学习的模型拥有足够的泛化性？模仿学习常常会遇到“分布漂移”（Distributional<br>Shift）的问题，这与模型的泛化性有一定的关系。我们的样本通常无法覆盖问题的所有情况，在实际使用过程中必然会出现一些训练样本中没有出现的观测值，导致泛化能力不强的Agent出现重大失误。为了解决这个问题，我们需要从模型和训练样本两方面入手。</p>
<p>以上三个问题都不容易解决，因此模仿学习的难度并不小，这才使得大家把目光集中在强化学习上，希望它能够解决模仿学习无法解决的问题。</p>
<h3 id="1-5-强化学习的实验环境"><a href="#1-5-强化学习的实验环境" class="headerlink" title="1.5 强化学习的实验环境"></a>1.5 强化学习的实验环境</h3><p>前面我们介绍了强化学习的概念，可以看出强化学习相比监督学习有更多的优势，可以应用到更广的范围中。现在已知的强化学习问题有很多，例如，解决一些长期决策问题、博弈问题，使机器人模拟真实人类完成一些特定动作等。</p>
<p>由于强化学习需要不断地在环境中尝试，因此一些问题的学习效率成了挑战。例如，仿真机器人模拟人类行为这样的学习，让仿真机器人一遍又一遍尝试人类的动作是比较慢的，可能要花很长时间才能让机器人学到期望的行动，但如果将学习的内容转移到计算机上，通过模拟的方式进行学习，速度将快许多。因此，目前大部分的强化学习是在计算机上进行的。</p>
<p>为了让更多人快速地应用强化学习，也为了大家能够针对同样的问题讨论交流，业界开放了一批可以公开访问搭建的实验环境，这里有我们常见的计算机游戏环境，也有机器人仿真的环境，本节介绍一些广为人知的环境。</p>
<h4 id="1-5-1-Arcade-Learning-Environment"><a href="#1-5-1-Arcade-Learning-Environment" class="headerlink" title="1.5.1 Arcade Learning Environment"></a>1.5.1 Arcade Learning Environment</h4><p>Arcade Learning Environment（简称ALE）环境封装了很多基于Atari<br>2600游戏机且曾经知名的游戏，例如乒乓球游戏（Pong），游戏界面如图1-5所示。</p>
<p>游戏的双方控制一个板子，通过反弹的方式将乒乓球弹到对方那边，如果哪一方没有接到球，对方就可以得1分。在一局对弈中，如果一方率先得到21分，这一方就获得了比赛的胜利。从游戏界面可以看出，这个游戏只需要玩家操纵“上”和“下”两个按键，游戏的规则和操作十分简单，最终的决策结果也是离散的形式。</p>
<p>当年的游戏是在一个很小的屏幕上展示的，屏幕的宽为160像素，高为210像素，每个像素有128个颜色。游戏机共有18种操作方式，所有的操作都是通过图1-6所示的简单的游戏杆和按钮完成的。</p>
<p>图1-5 Pong游戏的界面</p>
<p>图1-6 Atari 2600游戏机</p>
<p>ALE是基于另一个开源的Atari<br>2600模拟器构建完成的。这个模拟器实现了简单的操作接口，我们可以给模拟器发送指定的被离散化后的操作，模拟器会将模拟得到的最近的屏幕信息返回。在实时状态下，模拟器可以以每秒60帧的速度生成游戏画面，还可以在更高的帧率生成画面<br>[1]<br>。ALE还提供保存当前游戏模拟器状态的功能，这个功能就像存档一样，把当时游戏的状态保存下来，这样即使游戏继续进行，也可以回到当时的场景重新进行游戏。这个功能可以帮助研究环境已知（Model-<br>based）方法的研究人员，让他们在长期规划（Long Time Planning）方面所有突破。</p>
<h4 id="1-5-2-Box2D"><a href="#1-5-2-Box2D" class="headerlink" title="1.5.2 Box2D"></a>1.5.2 Box2D</h4><p>前面提到的游戏环境更像在一个虚幻的世界里构建了一些物理法则，而对接下来要介绍的Box2D环境来说，它更多地遵从了真实世界的物理规则。Box2D是一个模拟刚体运动的环境库，使用它，我们可以在游戏中创造出类似真实世界的物体运动效果，如图1-7所示。所谓刚体，是指这个物体在经历任何运动后都不会发生形变（官方的定义为：物体上任意两点的位置永远保持固定）。这个环境中还包含摩擦力、重力、碰撞等物体运动的效果。</p>
<p>Gym中也有一些基于Box2D的游戏，例如月球登陆者（Lunar<br>Lander），如图1-8所示。这个游戏通过控制登陆器的平衡使登陆器稳定地降落在指定的位置。这其中涉及很多物理环境的构建。</p>
<p>图1-7 使用Box2D实现的模拟场景</p>
<p>图1-8 Gym中展示的Box2D游戏</p>
<h4 id="1-5-3-MuJoCo"><a href="#1-5-3-MuJoCo" class="headerlink" title="1.5.3 MuJoCo"></a>1.5.3 MuJoCo</h4><p>MuJoCo[2]<br>是一个3D环境下的物理仿真引擎，它很好地模拟了三维环境下的一些物理性质。我们可以使用这个引擎设计一些仿真环境，训练模型实现一些近似现实生活中的任务。它和Box2D的功能类似。图1-9展示了利用MuJoCo实现的物理环境，我们可以通过操纵图1-10所示的人体结构的关节和骨骼，实现一些仿人的任务。如果模型在仿真环境上得到了验证，就可以应用到现实世界中的机器人上，让机器人进行进一步适应性训练也会容易些。</p>
<p>图1-9 MuJoCo的简单环境实现</p>
<p>图1-10 3D场景下的仿人类行动实验</p>
<h4 id="1-5-4-Gym"><a href="#1-5-4-Gym" class="headerlink" title="1.5.4 Gym"></a>1.5.4 Gym</h4><p>Gym是一个集成了众多强化学习实验环境的平台，拥有这个平台，我们可以很方便地搭建起强化学习需要的环境，从而集中精力完成策略部分的工作。在Gym诞生之前，已经有一些优秀的强化学习平台存在，但这些平台相对比较分散。Gym将这些平台的模拟环境做了整合，使环境配置变得简单。</p>
<p>Gym的设计目标是为研究人员提供一个易于操作的环境，例如，Atari里的经典游戏Pong，仅需运行下面的代码：</p>
<p>就可以搭建游戏环境。至于模型的策略该如何执行，则没有包含在其中，不过他们在另外一个项目中提供了对基本策略的实现，这个项目叫作Baselines。</p>
<p>同时，为了方便强化学习研究人员的沟通交流，Gym的官方网站还提供了一个“打擂”平台，通过官方提供的一些开发库，每个人都可以记录自己算法的一些细节（例如，每一轮学习花费的时间、为了学习进行的交互数量）。同时，每个人都可以将自己实现的代码和结果上传到对应的强化学习问题页面上，这样既可以互相学习，又促进了对问题的深入研究，如图1-11所示。</p>
<p>图1-11 Gym的效果展示平台</p>
<p>从页面上可以看出，Gym不仅关注模型的最终效果（即最终得到的回报总值），还关注学习模型所使用的样本数量，而样本数量和学习时间相对应，这也和我们提到的强化学习的目标相呼应。关于Gym和Baselines的内容将在本书的第5章详细介绍。</p>
<h3 id="1-6-本书的主要内容"><a href="#1-6-本书的主要内容" class="headerlink" title="1.6 本书的主要内容"></a>1.6 本书的主要内容</h3><p>强化学习在机器学习中的难度不低，它需要很多方面的知识辅助，同时自身也已经形成了一个庞大的体系。本书并不能帮助读者完全掌握强化学习的所有知识点，所能做的只是展示其中部分基础内容。本书不是一本科普读物，想要阅读本书需要具备一定的基础知识，如微积分、线性代数等。部分章节也会梳理这些基础知识，以确保读者掌握这些知识的核心思想。本书各章节的核心内容如下。</p>
<p>第一部分主要介绍与强化学习有关的基础知识，例如数学基础、对应的程序开发基础、强化学习的基本计算方法等。</p>
<p>第1章已经介绍了强化学习的基本概念，相信读者对强化学习的目标、特点已经有了一定的了解。</p>
<p>第2章介绍相关的数学知识，如线性代数、概率论、重要性采样、信息论等，帮助读者快速回顾即将用到的数学知识。</p>
<p>第3章介绍强化学习中会用到的优化知识，主要介绍常见的梯度下降法（Gradient Descent）、共轭梯度法和自然梯度法，这三种方法将出现在后面的算法中。</p>
<p>第4章介绍书中代码使用的计算框架TensorFlow。TensorFlow是一款使用十分广泛的框架，很多强化学习的算法选择使用它进行实现，因此我们有必要学习它。本章将简单介绍它的使用方法和一些基本原理，熟悉TensorFlow的读者可以跳过本章。</p>
<p>第5章介绍本书使用的另一个框架Gym及在此框架上实现的算法集合Baselines。Gym集成了大量的强化学习仿真环境，Baselines则基于TensorFlow和Gym实现了一些经典的算法。本章将简单介绍这两个框架的基础知识。</p>
<p>第6章介绍强化学习的基础知识。例如马尔可夫决策过程（Markov Decision Pro-<br>cess），以及在简单问题上的两种经典动态规划求解法：策略迭代法和价值迭代法。这些方法是强化学习算法的基石，绝大多数强化学习方法都是根据这些知识演变来的。</p>
<p>第二部分介绍以最优值函数为思想的一系列算法，其中的代表算法为Deep Q Net-work和Rainbow。</p>
<p>第7章介绍蒙特卡罗（Monte-Carlo）和时序差分（Temporal-Difference）两种求解Model-free问题的方法，并介绍Deep Q<br>Network算法的细节。</p>
<p>第8章介绍Deep Q-Learning的一些改进算法，如Priority Replay Buffer、Duel Net-<br>work等，并介绍改进的集成算法Rainbow。</p>
<p>第三部分介绍以策略梯度（Policy Gradient）为思想的一系列算法，其中的代表算法为Actor-Critic。</p>
<p>第9章介绍策略梯度法和Actor-Critic算法的原理，同时介绍A2C算法的实现细节。</p>
<p>第10章介绍使策略单调提升的算法，其中的代表为Trust Region Policy Optimization （TRPO）算法。</p>
<p>第11章介绍高样本使用率的策略梯度算法，其中的代表算法为ACER算法和确定策略梯度法（Deterministic Policy Gradient）。</p>
<p>第四部分介绍强化学习其他方面的内容。</p>
<p>第12章介绍回报稀疏情况下的一些求解方法，其中包括基于层次的强化学习和基于课程学习（Curriculum Learning）思想的方法。</p>
<p>第13章介绍模型已知的一些算法，如基于蒙特卡罗树搜索（Monte Carlo Tree Search，MCTS）的强化学习算法和iLQR算法的原理。</p>
<p>第五部分介绍反向强化学习的基础知识。</p>
<p>第14章介绍反向强化学习的基础，以及基本的求解方法。</p>
<p>第15章介绍最大熵反向强化学习（Max Entropy Inverse Reinforcement<br>Learning）和生成对抗模仿学习（Generative Adversarial Imitation Learning）算法的内容。</p>
<p>虽然本书介绍了很多强化学习的内容，但实际上这只是强化学习中的一小部分，不过这些内容可以作为读者入门强化学习的基础，希望在下面的学习中读者能够有所收获，并以此为基础了解更多有关强化学习的内容。</p>
<h3 id="1-7-参考资料"><a href="#1-7-参考资料" class="headerlink" title="1.7 参考资料"></a>1.7 参考资料</h3><p>[1] Naddaf Y,Naddaf Y,Veness J,et al.The arcade learning environment:an<br>evalua-tion platform for general agents[J].Journal of Artificial Intelligence<br>Research,2012,47(1):253-279.</p>
<p>[2] Todorov E,Erez T,Tassa Y.MuJoCo:A physics engine for model-based<br>control[C]//Ieee/rsj International Conference on Intelligent Robots and<br>Systems.IEEE,2012:5026-5033.</p>
<p>[3] Brockman G,Cheung V,Pettersson L,et al.OpenAI Gym[J].2016.</p>
<p>[4] Box2D v2.3.0 User Manual <a href="http://box2d.org/manual.pdf" target="_blank" rel="noopener">http://box2d.org/manual.pdf</a></p>
<h2 id="2-数学与机器学习基础"><a href="#2-数学与机器学习基础" class="headerlink" title="2 数学与机器学习基础"></a>2 数学与机器学习基础</h2><p>在正式开始强化学习之旅前，我们还是要对数学和机器学习的基础知识做一些介绍。不同于第1章生动的文字描述，从这一刻开始书中将出现令人讨厌的数学公式。物理学界泰斗霍金曾说过：“每多写一个公式就会吓跑一半读者”，但愿这句话不要成为现实。下面的一些内容总体上比较基础，有这方面基础的读者可以跳过本章，想要温习基础知识的读者可以仔细阅读。总而言之，看完本章，我们在机器学习基础方面将取得很多共识，阅读本书后面的章节时不会被突然出现的名词搞晕。</p>
<h3 id="2-1-线性代数基础"><a href="#2-1-线性代数基础" class="headerlink" title="2.1 线性代数基础"></a>2.1 线性代数基础</h3><p>线性代数是一门与机器学习紧密相关的数学课程，其中的很多定理、性质和方法论在机器学习中起到了关键性的作用。本节旨在简单回顾线性代数中的一些基本内容，为后面的内容做铺垫。实际上，这些内容都被涵盖在大学的线性代数课本中，只是大学课本没有专门围绕着机器学习内容进行讲述，下面的内容将围绕着线性变换的核心概念展开。</p>
<p>在线性代数中，大家都接触过“矩阵”这个概念，对于计算机专业的读者来说，矩阵Am×n<br>可以看作一个二维数组A[m][n]，其中第一维表示行，第二维表示列。矩阵也拥有自己的运算体系，相同维度的矩阵可以相加，矩阵可以和标量相乘，在维度匹配的情况下，矩阵与矩阵之间也可以相乘。矩阵运算满足加法交换律、乘法结合律，不满足乘法交换律。</p>
<p>矩阵运算中经常会遇到一些特殊操作，例如转置。矩阵Am×n<br>经过转置后就变成了矩阵，相当于矩阵沿着主对角线进行翻转得到了新的矩阵。如果有一个矩阵，它的转置和它本身相同，那么这个矩阵就被称为对称矩阵。这类矩阵将成为本书中的一大主角。</p>
<p>矩阵的另一类运算是除法运算。并不是每一个矩阵都可以被除，在线性代数中，如果一个矩阵可以被除，那么就称这个矩阵是可逆的。关于矩阵可逆的判断条件还有很多，这里就不一一介绍了。</p>
<p>线性代数中最基本也是最经典的一类运算就是矩阵和向量的乘法。如果有矩阵Am×n 和向量xn×1 ，那么这两者是可以相乘的，有</p>
<p>这个公式实际上和机器学习中的一些线性模型相似。如果把矩阵A想象成一个运算符，那么这个运算符相当于对一个 _n_ 维的向量x做运算，得到一个 _m_<br>维的向量b。如果考虑x存在于一个 _n_ 维的实数向量空间，b存在于一个 _m_<br>维的实数向量空间，那么A的作用相当于一个映射，将不同维度的向量关联起来，而且它们之间是线性计算的关系。因此，这种计算也可以被认为是线性变换。</p>
<p>线性变换可以从两个角度理解：首先从常规的运算方法来看，b中的每一个元素都是由A中的一个行向量和x相乘得到的，它们的关系如图2-1所示。</p>
<p>图2-1 线性变换的一种运算形式</p>
<p>在这种运算形式中，A的每一行可以想象成x中每一个元素的权重，它们计算的结果汇总成一个数，相当于对x的元素做加权求和，得到的结果代表了对x数据的汇总。所以计算中将<br>_n_ 个数据做了 _m_<br>个汇总。如果再深入一点分析，这些汇总有什么含义呢？可以看出，这里的计算是两个向量的内积运算，内积运算具有自己的运算语意，假设有两个向量a和b，那么它们的内积公式可以写作</p>
<p>其中 _θ_<br>表示a和b在向量空间中的夹角。如果两个向量的长度（norm）为1，那么两个向量的内积表示了两个向量的某种相关度。如果两个向量共线且同向，那么内积的结果为1；如果两个向量相互正交，那么内积为0；如果两个向量方向相反，那么内积为-1。</p>
<p>所以从上面的分析中可以看出，线性变换相当于对x完成了 _m_<br>次内积运算，每一次内积运算的内容是在考察x与A的行向量的相关程度，所以b就是A与x的相关程度的汇总。因此作为一个运算符，A的内容就显得十分关键了——它直接决定了“心目中”理想的向量：像A则结果会比较大，不像结果就会比较小。</p>
<p>另一种理解方式并不是运算时常用的方式，但却是理解线性代数很重要的一种方式。在这种方式中，运算过程将被重新组合。</p>
<p>（1）A中的每一个列向量和x中的每一个元素依次相乘。</p>
<p>（2）相乘后的向量再加和在一起，得到结果b。</p>
<p>这种计算过程如图2-2所示。</p>
<p>图2-2 线性变换的另一种解释</p>
<p>在讨论向量的时候，我们在讨论什么？讨论向量在向量空间内的运算。向量空间有以下几种经典的运算方法：</p>
<p>（1）向量相加。</p>
<p>（2）向量与标量数字相乘。</p>
<p>（3）向量间求内积。</p>
<p>前面的第一种解释方法主要使用了第3种运算方法，后面的这种解释方法则主要使用了前面两种运算方法。</p>
<p>在这种解释下，x的每一个元素变成了A列向量的权重，实际上这个线性变换变成了向量级别的加权平均。这种解释似乎更为直观。A的每一个列向量表示了结果b所在空间的一种向量表达，那么x的作用就是平衡这些向量表达并将这些表达揉合成一个新的向量。</p>
<p>从这一种线性变换的角度来分析，我们就要引出另外一个概念——线性独立。从图2-2中读者已经了解了向量的加权平均，那么有没有可能经过加权求和得到其中的某一个向量呢？当然可能。可以让x中与这个向量相对应的元素为1，其他的元素为0，那么结果就是这个向量。但是如果这里要求这个对应的元素为0，只能通过其他向量的线性组合（也就是加权求和）来得到这个向量呢？这就不一定了，有些情况下这件事情是可以做到的，有些情况下则无法做到。以如下矩阵为例：</p>
<p>在这个矩阵里，任意一个列向量都不能被其他列向量经过加权求和得到。这样线性独立的概念就比较明确了，如果说一组向量是线性独立的，那么其中的任意一个向量都不能被其他向量通过加权相加得到。</p>
<p>这个性质有什么用处呢？实际上在对线性代数的介绍中很多概念已经被省略，例如矩阵的秩。本节不去考虑那些概念，只是用刚才的运算做进一步的分析。对于Ax=b这样的运算，如果A中的列向量组不是线性独立的，那么为了求出结果b，我们可以使用更小维度的A和x。比如说现在有如下的运算：</p>
<p>如果想要得到同样的结果，我们可以缩小A的维度：</p>
<p>这说明完成同一个运算时，矩阵A可以变得更小。上面第一段代码中，A是一个3×3的矩阵，里面包含着3个三维的列向量，而第二段代码中，A是一个2×3的矩阵，里面包含着3个二维的列向量，这说明上面一段代码中的矩阵A看似处于三维空间中，但实际上它的列向量只存在于其中的一个二维子空间中。</p>
<p>线性代数中有一种特殊的线性无关向量组，这个向量组就是基。基在英文中称为“basis”，这里也可以理解成“基础”的含义。最常见的基的形式就是欧式坐标轴体系中，那些坐标轴方向上的向量组成的向量组。基具有以下特点：</p>
<p>· 它们的组成部分——这些向量是线性无关的。</p>
<p>· 通过这些向量的线性组合，可以表示所在空间内的任意一个向量。以上面提到的二维空间为例，对于任意一个实数向量，都可以利用这组基通过线性变换表示出来：（<br>_x，y_ ）= _x_ · （1 _，_ 0）+ _y_ · （0 _，_ 1）。</p>
<p>除了线性相关与无关这样的关系，线性代数中还有一种常见的关系，被称为正交。如果一个矩阵和一个向量相乘的结果为0向量，而矩阵和向量都不全为0，那么从线性变换的角度来看，可能有两种情况：</p>
<p>· 向量和矩阵的每一个行向量点乘为0。</p>
<p>· 矩阵列向量经过被乘向量的线性组合后，相互抵消。</p>
<h3 id="2-2-对称矩阵的性质"><a href="#2-2-对称矩阵的性质" class="headerlink" title="2.2 对称矩阵的性质"></a>2.2 对称矩阵的性质</h3><p>对称矩阵是线性代数中十分重要的一种矩阵类型，因此这里将它单独列出作为一节进行分析。</p>
<h4 id="2-2-1-特征值与特征向量"><a href="#2-2-1-特征值与特征向量" class="headerlink" title="2.2.1 特征值与特征向量"></a>2.2.1 特征值与特征向量</h4><p>前面提到了线性变换的运算，那么我们就要深入分析线性变换中的一类特殊的变换。让我们再回到上面的公式</p>
<p>此时这个公式对其中的两个向量还没有太多的约束，如果我们对公式增加如下两个约束。</p>
<p>（1）x和b的维度相同，同为 _n_ 维，那么A就是一个 _n_ × _n_ 的方阵，是 _n_ 维空间内的一个算子。</p>
<p>（2）x和b共线，也就是说b= _λ_ x。</p>
<p>这里再将x换为这个公式里常用的符号——e，那么上面的公式就可以变为</p>
<p>此时我们发现，经过A的线性变换后，向量e仅仅做了尺度的缩放，而没有做其他的变换，这时我们认为矩阵A和向量e存在某种特殊的关系，于是人们将向量e称为特征向量，尺度缩放的系数<br>_λ_ 被称为特征值。</p>
<h4 id="2-2-2-对称矩阵的特征值和特征向量"><a href="#2-2-2-对称矩阵的特征值和特征向量" class="headerlink" title="2.2.2 对称矩阵的特征值和特征向量"></a>2.2.2 对称矩阵的特征值和特征向量</h4><p>对称矩阵的特征值和特征向量都有自己的特点。首先，由实数组成的对称矩阵的特征值全部为实数，即特征值的共轭数等于自身，以下就来证明。</p>
<p>对于某个实数对称矩阵A，它的共轭矩阵～A=A，对于某个特征值 _λ_ 和某个非0的特征向量e，我们有</p>
<p>所以等式两边有</p>
<p>因为e～T e不为0，所以特征值和它的共轭相等，所以对称矩阵的特征值全为实数。其次，对称矩阵的特征向量相互正交，证明如下。</p>
<p>对于对称矩阵中的两对不同的特征向量、特征值对e1 _，λ_ 1 和e2 _，λ_ 2 ，可以推导出所以有 _λ_ 1 e1 e2 = _λ_ 2 e1 e2<br>。</p>
<p>因为 _λ_ 1 /= _λ_ 2 ，所以e1 e2 =0。同理，我们可以推导出所有的特征向量对之间都正交，从而完成证明。</p>
<p>如果将对称矩阵所有特征向量的长度限制为1（也就是进行标准化），那么这些特征向量组成的矩阵就成了一个标准正交矩阵。这个标准正交矩阵也拥有一个性质，那就是它的逆矩阵等于它的转置矩阵，也就是它自身，由此可得</p>
<h4 id="2-2-3-对称矩阵的对角化"><a href="#2-2-3-对称矩阵的对角化" class="headerlink" title="2.2.3 对称矩阵的对角化"></a>2.2.3 对称矩阵的对角化</h4><p>讲完了上面的两个性质，下面介绍对称矩阵的另一个性质。前面提到了线性变换操作，如果这一次执行线性变换的是一个对称矩阵，那么会发生什么情况呢？</p>
<p>首先，根据特征值与特征向量的定义，可知</p>
<p>将所有的特征值和特征向量融合到一起，并用Λ表示对角线为特征值，其余位置为0的对角阵，就有</p>
<p>2.2.2节提到了对阵矩阵的特征向量组成了标准正交矩阵，于是有</p>
<p>这个过程被称为对称矩阵的对角化，那么这个过程有什么作用呢？如果要求矩阵A的2次幂，对角化的结构可以帮助简化这个过程：</p>
<p>于是对于任意次阶乘，都有</p>
<p>由于阶乘只发生在对角阵上，于是计算变得简单了许多。</p>
<h3 id="2-3-概率论"><a href="#2-3-概率论" class="headerlink" title="2.3 概率论"></a>2.3 概率论</h3><p>概率论也是机器学习中十分重要的一部分知识，可以说，当今机器学习的理论基础就是统计和概率论。如果要展开这部分的知识，恐怕一本书也讲不完，这里同样介绍最简单的概念。</p>
<h4 id="2-3-1-概率与分布"><a href="#2-3-1-概率与分布" class="headerlink" title="2.3.1 概率与分布"></a>2.3.1 概率与分布</h4><p>阿甘的妈妈曾经说过：“Life is like a box of chocolates，you never know what you’re going<br>to<br>get.”（电影《阿甘正传》台词）。生活中充满了不确定性，这些随机给我们带来了快乐，也带来了烦恼。概率论中的很大一部分工作就是描述这些不确定性。最先引出来的概念是随机事件，一个随机事件就是一件充满了不确定性的事情，比如说明天下雨这件事。明天可能下雨，也可能不下雨，那么到底下不下雨呢？这就要靠概率来描述了。概率一般用<br>_P_ （ _X_ ）表示，括号里的内容 _X_ 就是计算概率的实体。为了度量的方便，概率值的范围被限定在[0 _，_<br>1]。概率值为0表示事件完全不会发生，概率值为1表示事件一定会发生。那么随机事件的概率值就可以写作</p>
<p>实际上上面的写法还是有些不方便，于是前辈又发明了随机变量这样的概念，把随机事件抽象成随机变量的一种取值。于是我们用一个随机变量 _X_ 表示明天是否下雨，<br>_X_ =0表示不下雨， _X_ =1表示下雨。于是概率又被重新定义为</p>
<p>实际上通过这样的定义，概率的表示变成了一种映射，它将随机变量的取值和概率值一一对应，成为这个空间中的测度。由于这个测度空间具有很多与欧氏空间相同的良好性质，很多数学分析相关的知识都可以用在这类空间上。如果继续细分，上面这个例子中的随机变量被称为离散随机变量。还有一种变量被称为连续随机变量，比如说明天下雨的降雨量这个随机变量，它取值的范围是[0<br>_，_ +∞]。这时很多概率相关的运算就需要用上微积分的知识了。</p>
<p>随机变量毕竟是随机变量，我们无法预知它的结果，但是如果已经知道了它的概率分布，猜测它可能的结果还是有可能的，这就引出了猜测它结果的一些描述量，例如期望<br>_E_ 和方差Var。</p>
<p>离散随机变量的期望</p>
<p>连续随机变量的期望</p>
<p>离散随机变量的方差</p>
<p>连续随机变量的方差</p>
<p>在很多场景下，这两个描述量很好地刻画了随机变量的性质，因此它们也被广泛使用。</p>
<p>聊过了单一随机变量的一些定义，下面将进入多随机变量的概率度量刻画问题。对于两个随机变量 _X，Y_ ，它们同时取值的概率被称为联合概率 _P_ （<br>_X，Y_ ）。当然在实际运算过程中，并不需要两个随机变量在真实世界的同一时刻发生才能知道结果，只需要计算它们同时出现的可能性即可，比如说 _X_<br>表示天要不要下雨， _Y_ 表示要不要带伞。那么 _P_ （ _X_ =1 _，Y_ =1）就表示天下雨且带了伞的概率。</p>
<p>之所以强调两个随机变量同时取值，是因为有时两个随机变量的联合概率并不等于两件事情单独发生概率的乘积，即</p>
<p>原因在于随机变量之间存在着某种联系。比如说天下了雨，那么人就容易产生带伞的行为，所以两件事情同时发生实际上会包含这层含义。但是把两件事情的概率单独考虑就没有这层意思了，所以对概率的度量就会产生偏差。为了解决这个问题，条件概率应运而生。<br>_P_ （ _Y_ | _X_ ）表示当 _X_ 取值之后 _Y_ 取值的概率，所以联合概率正确的展开方式是</p>
<p>上面这个公式又可以推导出机器学习界最重要的概率公式（没有之一）——贝叶斯定理：</p>
<p>这个重要的公式相信读者都有所了解。这个公式中的每一项还有自己的名字， _Y_ 被称为隐含变量， _X_ 被称为观察变量， _P_ （ _Y_<br>）被称为先验（Prior），它表示我们对一个随机变量概率最初的认识； _P_ （ _X_ | _Y_<br>）被称作似然（Likelihood），它表示在承认先验的条件下另一个与之相关的随机变量的表现； _P_ （ _Y_ | _X_<br>）被称作后验（Posterior），表示当拥有 _X_ 这个条件后 _Y_ 的概率，由于有 _X_ 这个条件，后验概率可能与先验概率不同； _P_ （<br>_X_ ）是一个标准化常量（Normalized Constant），由于公式中一般认为 _X_ 已知，所以它一般被当成一个常量看待。</p>
<p>除了计算两个随机变量取值的概率，有些场景还需要计算两个随机变量的相关关系，这个关系可以用协方差表示，它的公式为</p>
<p>如果 _X_ 和 _Y_ 的期望为0，那么协方差就等于第一项的内容。</p>
<p>上面介绍了随机变量的基本表达形式和基本运算方式，但是对于如何用函数具体表达一个或多个随机变量，这个问题还是充满了挑战。为了让大家方便地分析这些随机变量，找出其中的规律，前辈们发明了很多经典的概率分布，这些概率分布可以当作对同一类问题的模板，满足类似性质的随机变量都可以用这些模板帮助分析。比如说对于离散随机变量，有经典的伯努利分布：</p>
<p>对于连续随机变量，经典的分布有高斯分布：</p>
<p>相信大家对这些分布都不陌生。一般来说，如果一个随机变量的表现和某个概率分布的具体形式非常接近（几乎处处相等），就可以认为这个随机变量服从这个概率分布。由于这些概率分布只是一个模板，具体的参数并不固定，因此在说明分布时还要明确分布的参数。</p>
<h4 id="2-3-2-最大似然估计"><a href="#2-3-2-最大似然估计" class="headerlink" title="2.3.2 最大似然估计"></a>2.3.2 最大似然估计</h4><p>这些分布虽然规定了它们的形式，但是其中仍然包含一些参数，如果读者已经观察到某个随机变量采样出来的样本服从某种分布，就可以根据这些样本对分布的参数进行估计。参数估计的方法在很多机器学习算法中也都有用到，其中一类十分经典的算法被称为最大似然法。由于每一个样本是否出现都对应着一定的概率，而且一般来说这些样本的出现都不那么偶然，因此我们希望这个概率分布的参数能够以最高的概率产生这些样本。如果观察到的数据为<br>_D_ 1 _，D_ 2 _，D_ 3 _，_ ··· _，D_ N ，那么极大似然的目标如下：</p>
<p>计算联合概率总归不是一件很容易的事，如果样本数量非常大，那么对联合概率的计算会让人崩溃。所以这里一般会引入一个假设，也就是常说的独立同分布，inde-<br>pendent and identically distributed<br>（i.i.d.）。如果每一个样本既属于我们现在要求解的分布，同时它们彼此之间又相互独立，彼此出现的概率互不影响，那么根据条件独立的原则，目标公式就可以变为</p>
<p>看上去计算复杂度降低了许多，对于优化问题，最容易想到的方法就是求导数取极值。如果目标函数是一个凸函数，那么它的导数为0的点就是极值点。但是现在的公式是连乘式，求导十分麻烦，这时对数函数就派上了用场。将函数取对数，函数的极值点不会改变，于是公式就变为</p>
<p>公式变成了这个样子，求导也变得简单了许多，下面的计算也就方便了。</p>
<p>下面将举两个最大似然法计算的例子。首先介绍伯努利分布下随机变量的最大似然计算方法，假设 _P_ （ _X_ =1）= _p_ ， _P_ （ _X_<br>=0）=1 _-p_ ，综合起来就有</p>
<p>如果有一组数据 _D_ ，这一组数据是从这个随机变量中采样得来的，那么就有</p>
<p>对这个式子求导，就有</p>
<p>令导数为0，就有</p>
<p>这就是伯努利分布下最大似然法求出的结果，结果相当于所有采样值的平均值。</p>
<p>其次是基于高斯分布的最大似然法计算，推导过程也比较类似，有</p>
<p>那么用同样的方式计算，有</p>
<p>首先对 _µ_ 求导，有</p>
<p>令导数为0，就有</p>
<p>其次对 _σ_ 2 求导，有</p>
<p>令导数为0，有</p>
<p>从伯努利分布和高斯分布的最大似然法结果来看，它们最终求得的参数结果和期望方差的计算方式完全一致。</p>
<p>有关概率论的内容就介绍这些，更多内容将随着后面章节的内容介绍。</p>
<h3 id="2-4-重要性采样"><a href="#2-4-重要性采样" class="headerlink" title="2.4 重要性采样"></a>2.4 重要性采样</h3><p>重要性采样（Importance<br>Sampling）是统计中的一种采样方法。在强化学习中经常用到这种采样方法，它主要用在一些难以直接采样的数据分布上。采样是统计中常见的一种手法，有时我们需要从某个分布中采样一些数据，并利用这些数据完成更多的运算。例如，假设有一个很复杂的概率密度函数<br>_p_ （ _x_ ），求解随机变量基于此概率下的某个函数期望，形式化的表示就是</p>
<p>求解它至少有两种方法。其中一种就是采用解析的方法，将上面的公式展开变成积分，然后将积分求解出来：</p>
<p>对于简单的分布形式，我们当然可以用这种方法，通过使用学到的微积分的知识直接得到结果，但如果函数 _f_ （ _x_<br>）比较复杂，直接求解变得不那么直接，这种方法就不太可行了。</p>
<p>另一种方法采用蒙特卡罗法，这算是一种“曲线救国”的方法。我们曾经学习过大数定理，当采样数量足够大时，采样的样本就可以无限近似地表示原分布，基于这样的思想，我们的公式就变为</p>
<p>像 numpy 这样的函数库也为我们提供了一些基本的采样函数，比如<br>np.random.normal（），它可以从正态分布中采样数据。采样得到的数据可以用于计算函数值，这样问题同样可以得到比较好的解决。最终结果的精确度取决于采样的数量，采样数量越多，结果的近似效果就越好，这也符合大数定理的解释。</p>
<p>当我们想从这些较常见的分布进行采样时，上面的函数就可以帮助我们。但是这个世界是多样的，我们总会遇到一些奇怪的概率分布，那时我们该如何进行采样呢？如果概率密度函数没有具体的公式形式呢？比如说概率函数由一个深度神经网络构成，我们只能通过输入给定的样本得到它的概率，但显然很难写出分布的具体表达式，更别说直接使用它进行采样了。</p>
<p>这时重要性采样就派上了用场。我们虽然无法从这个分布函数采样，但我们还有其他常见的、可以采样的分布，我们能不能对上面的公式进行一些变换，使用常见的分布采样呢？我们令待采样的分布为<br>_p_ （ _x_ ），另一个简单可采样且定义域与 _p_ （ _x_ ）相同的概率密度函数为 _p_ ～（ _x_ ），我们可以得到</p>
<p>此时我们发现，公式变成了类似上一个方法的形式，而且我们只需要从这个简单分布 _p_ ～（ _x_<br>）中采样，然后分别计算样本在两个分布中的概率和函数值，最后将三者组合起来就可以得到结果。</p>
<p>当然，我们也不能太乐观，毕竟天下没有免费的午餐，为了检验这个方法的效果，我们将用两个常见的分布进行实验。我们假设要采样的数据来自均值为1，标准差为1的高斯分布，希望用另一个高斯分布近似这个分布。这里选取了三个分布：均值都为1，但标准差分别为1、0.5和2的高斯分布。为了更好地看出重要性采样的效果，这里的函数将选择一个比较简单的形式：<br>_f_ （ _x_ ）= _x_ 。</p>
<p>接下来，我们分别使用三个分布近似原始分布，共完成三组实验。每一组实验中取出一个分布进行重要性采样的计算，共进行十轮计算。每一轮计算进行100000次采样，并将所有的采样值直接加和。最终将十轮计算结果聚合统计，得到它们的均值和标准差，然后对比三组实验的结果。对应的代码如下所示：</p>
<p>在揭晓结果前，我们先来看三个分布的图像，如图2-3所示。</p>
<p>图2-3 三个高斯分布的图像</p>
<p>可以看出，为了实现 _σ_ 等于1.0的分布的采样，我们更应该使用 _σ_<br>等于1.0和2.0的分布，因为它们在取值空间上的概率分布更均匀，概率较大的区域可以覆盖原始分布概率较大的区域，不会出现因为某些区域概率较小而采样不到的情况，计算结果也相对更精准；而<br>_σ_ 等于0.5时更有可能出现采样不均匀的情况，结果也会差一些。</p>
<p>基于这个分析，最终结果如下：</p>
<p>可以看出，虽然三者在结果上存在一定差距， _σ_ 等于0.5的分布在均值上离理想值稍远，另外两个分布更接近；更重要的是， _σ_<br>等于0.5的分布标准在标准差上和另外两个分布存在明显差异，这也说明选择一个合适的分布对重要性采样的重要性：要选择与原始分布尽可能接近的近似分布进行采样。如果选择不当，最终结果不会很好。后面我们还会接触一些利用重要性采样解决问题的场景，请读者记住决定重要性采样效果的关键因素，在后面的应用中细细体会。</p>
<h3 id="2-5-信息论基础"><a href="#2-5-信息论基础" class="headerlink" title="2.5 信息论基础"></a>2.5 信息论基础</h3><p>信息论也是一门和机器学习紧密相关的学科，这里面的一些基本概念，例如熵，读者应该多少有些了解。下面笔者就来介绍熵的那些事。</p>
<p>大家都知道这个世界上有很多不确定的事情，比如说明天的天气，足球世界杯哪支队伍摘得桂冠。如果有人能将这些不确定的事情确定下来，或者提前告知人们它未来的状态，那么很显然这个人的能力是非常强大的。这里可以把这种描述不确定性事物的断言称为一种信息，事实上，描述任何一种不确定的事物都可能产生信息，那么这些信息的价值有没有什么差别呢？很多谍战片都在讲述地下特工如何冒着生命危险将一个机密情报传递给后方的军队，可见这个情报是非常有价值的，而敌方军队是否会不战而降呢？这个问题似乎不用考虑，大家都能给出结论。</p>
<p>可见不同的信息拥有不同的价值，那么它的价值该如何体现呢？从信息论的角度来说，如果一个信息的随机性越大，那么把它确定下来的价值也就越大。例如，我们有一枚正常的硬币，一面是字，另一面是花，将硬币抛在空中然后落下，朝上的那一面会是哪一面呢？由于这里提到的是一枚正常的硬币，于是可以认为每一面朝上的概率是相同的，都是50%；如果另一枚硬币两面都是字，那么字的一面朝上的概率就是100%。所以猜中第二枚硬币朝上的那面比第一枚要容易得多。</p>
<p>虽然定性地猜测“哪枚硬币朝上一面的信息量大”这件事并不难，但是这个世界上有许多类似的事情，如何从信息论的角度比较任意两个随机变量的信息价值呢？这就需要一种定量的分析方法。于是前辈发明了“熵”这个概念。</p>
<p>首先来看看离散随机变量的“熵”。有些著作中将“熵”形容为了解真相的“惊喜度”。如果一个随机事件发生的概率为100%，那么它的发生对我们来说毫无惊喜可言——因为它肯定会发生；而如果它发生的概率为百万分之一，那么它的发生一定会让人惊喜不已。可以想象百万分之一概率发生的都是什么事件——比如说彩票中奖，这样的事件让人惊喜的可能性当然非常大。</p>
<p>这样就把一个离散随机变量拆解成有限个或者无限个随机事件，分别把这些事件的“惊喜度”计算出来，然后把惊喜度结合起来，就是这个随机变量的惊喜度了。看上去这里需要对“惊喜度”做更具体的定义。同其他公理性质的概念一样，“惊喜度”的计算公式不会被直接给出，给出的是“惊喜度”公式应该具备的一些性质，这一点和勒贝格测度的定义方法有些类似。令这个“惊喜度”的计算公式为<br>_f_ （ _p_ （ _x_ ）），其中的输入 _p_ （ _x_ ）为某个随机事件发生的概率， _p_ （ _x_ ）∈[0 _，_<br>1]。它需要满足下面的性质：</p>
<p>（1） _f_ （1）=0。一定发生的事情没有“惊喜”。</p>
<p>（2）如果 _p_ （ _x_ ） _＜p_ （ _y_ ），那么 _f_ （ _p_ （ _x_ ）） _＞f_ （ _p_ （ _y_<br>））。换句话说，一个事情的概率越小，它的“惊喜度”越大。</p>
<p>（3）函数 _f_ 在输入空间是连续的。</p>
<p>（4）两个事件同时发生的“惊喜度”等于各自事件发生的“惊喜度”的和，由于两个事件是互不相交的，所以这条定理和测度的次可加性性质很像。</p>
<p>（5） _f_ （0 _._ 5）=1，这个性质主要是做归一化用的，就像外测度对测度定义的限制一样。</p>
<p>那么，什么样的函数能同时满足这些条件呢？科研人员最终找到了一个函数 _f_ （ _x_ ）=-log2 _x_<br>。它完美地满足所有的性质。最后把随机事件发生的概率当作这个事件的权重，把所有事件的“惊喜度”进行加权平均，就得到了随机变量整体的“惊喜度”</p>
<p>同样地，熵的定义也可以推广到连续变量上：</p>
<p>这个公式虽然满足上面的那些性质，但是看上去总是不够直观，回到刚才那个硬币的例子，假设手里有一枚硬币，硬币有字一面朝上的概率为 _p_<br>，所以花一面朝上的概率为1 _-p_ 。知道这些就可以写出求这个问题熵的公式</p>
<p>这个公式的图像可以通过下面的代码生成：</p>
<p>结果如图2-4所示。</p>
<p>图2-4 硬币问题的概率与熵的关系图</p>
<p>当两个面朝上的概率相同时，它的熵最大。这也和我们的直觉相符，正是因为两个事件的概率相同，我们才难以判断最终结果，这时知道结果的“总体惊喜度”才会最大。</p>
<h3 id="2-6-KL散度"><a href="#2-6-KL散度" class="headerlink" title="2.6 KL散度"></a>2.6 KL散度</h3><p>KL散度是概率论和信息论中十分重要的一个概念。它是描述两个概率分布 _P_ 和 _Q_ 差异的一种方法。</p>
<p>离散概率分布的KL散度计算公式为</p>
<p>连续概率分布的KL散度计算公式为</p>
<p>KL散度可以很好地测量两个概率分布之间的距离。两个分布越接近，KL散度越小；两个分布越远，KL散度就越大。</p>
<p>KL散度的结果是非负的，这里可以简单证明得出</p>
<p>由于对数函数是一个上凸函数，所以有</p>
<p>由此得证。</p>
<p>看完了KL散度的基本性质，下面来看一个实际的案例：假设有两个随机变量 _x_ 1 和 _x_ 2<br>，各自服从一个高斯分布，那么这两个分布的KL散度该怎么计算呢？</p>
<p>在前面的2.3节中，读者知道了高斯分布的概率密度函数：</p>
<p>那么KL（ _p_ 1‖ _p_ 2）就等于</p>
<p>到这一步，公式的最后一项可以化简，这个积分符号里面的东西是不是看着很熟悉？没错，它就是连续变量方差的计算公式，于是经过约分，这一项就变成了。继续推导，有</p>
<p>经过大段让人发懵的公式推导，最终的结论已经得出。假设 _N_ 2 是一个正态分布，也就是说，那么 _N_ 1 长成什么样子能够让KL散度尽可能地小呢？</p>
<p>将 _N_ 1 中的变量代入，公式变成</p>
<p>读者简单分析一下就能猜测到，当 _µ_ 1 =0 _，σ_ 1 =1时，KL散度最小。从公式中可以看出，如果 _µ_ 1<br>偏离了0，那么KL散度一定会变大；而 _σ_ 1 的变化对KL散度的影响则不那么明显：</p>
<p>· 当 _σ_ 1 大于1时，将越变越大，而-log _σ_ 1 越变越小。</p>
<p>· 当 _σ_ 1 小于1时，将越变越小，而-log _σ_ 1 越变越大。</p>
<p>那么，哪边的力量更强大呢？由于公式比较简单，这里就将它的图像画出来进行比较，如图2-5所示。</p>
<p>图2-5 两个高斯分布的KL散度图</p>
<p>从图2-5中可以看出二次项的威力更大，函数一直保持为非负，这和前面推导的结论（KL散度非负的结论）是完全一致的。</p>
<h3 id="2-7-凸函数及其性质"><a href="#2-7-凸函数及其性质" class="headerlink" title="2.7 凸函数及其性质"></a>2.7 凸函数及其性质</h3><p>凸函数是机器学习中经常见到的一种形式。它拥有非常好的性质，在计算上拥有更多的便利。这种感觉就好像看到一个复杂的7次函数 _f_ （ _x_ ）= _ax_<br>7 + _bx_ + _c_ ，但实际上它的7次项系数 _a_<br>=0，这个披着复杂函数外皮的家伙其实很好分析。虽然后面介绍的深度学习网络的优化曲面并不具备凸函数的性质，但是凸函数还是作为一个十分经典的模型被大家不断研究学习，更何况它也是众多优化算法的基础，因此下面开始介绍这类函数及其相关特点。</p>
<p>要介绍凸函数，首先要介绍凸集。如果一个集合 _C_ 被称为凸集，那么这个集合中的任意两点间的线段仍然包含在集合中，如果用形式化的方法描述，那么对于任意两个点<br>_x_ 1 _，x_ 2 ∈ _C_ ，和任意一个处于[0 _，_ 1]的实数 _θ_ ，都有</p>
<p>下面给出凸函数和非凸函数的对比图像，如图2-6所示。可以看出，对于左边的凸集区域，任意两点间的线段都在集合中，而对于右边的非凸集区域，我们可以找到两点间的一条线段，使得线段上的点在集合外。</p>
<p>图2-6 凸集和非凸集</p>
<p>凸函数的定义域就是一个凸集，除此之外，它还具备另外一个性质：给定函数中任意两点 _x，y_ ，和任意一个处于[0 _，_ 1]的实数 _θ_ ，有</p>
<p>这里给出一个凸函数的例子： _f_ （ _x_ ）= _x_ 2<br>，然后看看这个性质在函数上的表现，它的图像如图2-7所示，图中的横截线代表不等式右边的内容，横截线下方的曲线代表不等式左边的内容，从图中确实可以看出不等式所表达的含义，如果一个凸函数像<br>_x_ 2 这样是一个严格的凸函数，那么实际上除非 _θ_ 等于0或者1，否则等号不会成立。</p>
<p>图2-7 凸函数性质展示</p>
<p>凸函数满足的这个不等式也被称为Jensen不等式，要判定一个函数是不是凸函数，除了用Jensen不等式来判定，还可以用下面两种方式判定。首先是一阶导数条件。令<br>_x，y_ 是凸函数 _f_ 的任意两个点，那么下式成立：</p>
<p>其次是二阶导数条件：令 _x_ 是凸函数 _f_ 上的任意一点，那么下式成立：</p>
<p>这两个条件是凸函数成立的充要条件，同时也揭示了凸函数的重要性质。从直观的角度分析，对于前面提到的凸函数 _f_ （ _x_ ）= _x_ 2<br>，这两个性质显然成立：</p>
<p>凸函数究竟有什么良好的性质，值得读者花大力研究呢？</p>
<p>如果一个点是凸函数的局部最优值，那么这个点就是函数的全局最优值。更进一步，如果这个函数是强凸函数，那么这个点是函数唯一的全局最优值。这个问题可以通过一阶条件证明，当一个点<br>_x_ 是凸函数的局部最优值时，它的导数为0，那么对于任意的点 _y_ ，都有</p>
<p>当函数为强凸函数时，大于等于号将变为大于号。</p>
<p>有了这个性质，我们就可以放心大胆地做函数的优化了。只要找到一个导数为0的局部最优值，我们就找到了全局最优值。凸函数相关的理论还有很多，本节就介绍到这里。</p>
<h3 id="2-8-机器学习的基本概念"><a href="#2-8-机器学习的基本概念" class="headerlink" title="2.8 机器学习的基本概念"></a>2.8 机器学习的基本概念</h3><p>在经典的机器学习入门书籍中，读者一般会学到很多经典的机器学习模型，由于本书介绍的问题主要是监督学习，所以主要介绍一些监督学习相关的内容。通过前面的介绍，读者已经对模型这个概念有了一定的了解，也了解了模型和函数的关系。在很多场景下，模型都是由函数构成的，函数的形式有很多种，有的比较复杂，有的比较简单。下面假设有为模型准备好作为输入的特征数据<br>_X_ 和作为输出的结果数据 _Y_ ，针对不同的任务，监督学习又可以分成几个具体的类别。</p>
<p>首先是输出 _Y_ 的形式，如果结果 _Y_ 是一个连续的变量，那么这个问题通常被称为回归问题。这里可以用Python的scikit-<br>learn工具包随机生成一个可以可视化的回归问题数据集。</p>
<p>这个例子相对来说十分简单，它的结果也十分直观，输入数据和输出数据都是1维的，它们的关系如图2-8所示。</p>
<p>如果输出 _Y_ 的形式是离散的变量，例如上面提到的手写数字分类问题，那么这个问题就是分类问题。这里也给出一个简单的分类问题的数据集。</p>
<p>这个例子对应的图像也十分简单，如图2-9所示。</p>
<p>图2-8 一个回归问题的数据展示</p>
<p>图2-9 分类问题数据示例</p>
<p>分类问题的目标就是让模型把点号和加号的点区分开。</p>
<p>上面看过了问题的形式，下面来看看问题的解决方法。这里以回归问题为例，为了解决问题，我们可以建立一个模型，让模型充当输入和输出的桥梁。例如，对于上面的回归问题，可以建立线性模型</p>
<p>其中 _x_ 和 _y_ 分别是模型的输入和输出， _w_ 和 _b_ 是模型的两个参数，这两个参数在模型学习训练前是不固定的，经过训练后，它才会确定下来。</p>
<p>第一步明确了模型，下面就来看看模型的目标函数。这里采用平方损失函数，如果数据的真实输出为 _t_ ，模型的输出为 _y_ ，那么损失函数定义为</p>
<p>平方损失函数很好地度量了真实输出和模型输出之间的距离。如果模型输出和真实输出差距较大，那么模型损失函数的结果也较大，说明了模型还有很大的改进空间。从损失函数的定义来看，损失函数的最小值为0，这就是模型要努力的方向。所以，最终函数的目标是最小化损失函数：</p>
<p>完成了模型和目标函数的定义，下面就来看看模型如何学习。对于这个简单的问题，直接求出它的闭式解是完全可行的。但在大多数真实场景中，闭式求解模型最优值是不现实的，因为那将花费非常长的时间。通常，我们用基于梯度的方式采用优化的方法求解，因此这个过程也被称作优化。优化的基本流程分成3个步骤：</p>
<p>（1）为参数设定一些初始值。</p>
<p>（2）利用梯度信息计算参数的更新量。</p>
<p>（3）判断参数优化是否完成，如果完成，则停止优化，否则回到第2步。</p>
<p>这3个步骤中，第3步的内容相对简单，一般有以下几种判断方法。</p>
<p>（1）判断参数所在位置的梯度是否靠近0（依梯度收敛）。</p>
<p>（2）判断参数的更新量是否靠近0（柯西列收敛）。</p>
<p>（3）判断预设的迭代轮数是否已经完成。</p>
<p>由于机器学习中的模型一般是定义良好的函数，因此这一部分受到的关注比较小。此外，对于凸函数优化来说，第1步的内容也比较简单，优化的重头戏全部在第2步中。</p>
<p>对深度学习来说，由于种种原因，越来越多的人发现了第1步的重要性。本节只涉及机器学习中的这几个概念，更多的概念将在后面章节中慢慢介绍。通过本节，希望读者至少对模型、目标函数和优化这3个概念有一些了解。</p>
<h3 id="2-9-机器学习的目标函数"><a href="#2-9-机器学习的目标函数" class="headerlink" title="2.9 机器学习的目标函数"></a>2.9 机器学习的目标函数</h3><p>前面介绍了机器学习中常见的组成部分，下面要花一点时间介绍损失函数的内容。常见的损失函数有平方损失函数和交叉熵损失函数，这两种损失函数有什么区别和联系呢？</p>
<p>平方损失函数是一种比较容易理解的损失函数，它直接衡量了模型输出和标准答案在数值上的差距。定义模型输出的向量为y，训练数据的标签向量为t，那么平方损失函数的定义为</p>
<p>交叉熵的定义就没那么直接了，它需要用到2.5节中信息论的知识。这里简单回顾一下，熵这个概念衡量了一个随机变量带来的“惊喜度”，它通过计算每一个取值的惊喜度汇总得到。实际上，这个公式的计算涉及两个部分——惊喜度+汇总。如果惊喜度的计算使用一个概率分布，而汇总使用另一个概率分布，那么结果会变成什么样子呢？这就是交叉熵要表达的内容。</p>
<p>举一个简单的例子，有两个服从伯努利分布的随机变量 _P_ 和 _Q_ ，它们的交叉熵 _H_ （ _P，Q_ ）为</p>
<p>由于这个公式只有2个参数，于是可以将它的图像画出来：</p>
<p>生成的图像如图2-10所示。</p>
<p>图2-10 伯努利分布下两个随机变量的交叉熵</p>
<p>当两个分布的取值完全相同时，交叉熵的取值最小。对于损失函数来说，当然是希望模型输出和数据标签相同，因此损失函数的目标就是让交叉熵尽可能地变小。在这一点上函数和目标是一致的。</p>
<p>看上去两个损失函数的目标是差不多的，那么该如何应用它们呢？一般来说，如果最终输出的结果是回归问题的一个连续型变量，使用平方损失函数更合适；如果最终输出是分类问题的一个离散Ont-<br>Hot向量，那么交叉熵损失函数更合适。</p>
<p>但是，看上去平方损失函数既可以做回归问题的损失函数，又可以做分类问题的损失函数，因为它们都可以用距离来衡量，那么平方损失函数在做分类问题的损失函数时有什么弱点呢？</p>
<p>这个问题可以从两个方面回答。一方面，直观上看，平方损失函数对每一个输出结果都十分看重，而交叉熵损失函数只看重正确分类的结果。假设我们遇到了一个三分类问题，模型的输出自然是一个三维的实数向量（<br>_a，b，c_ ），向量的每一个元素都表示了对这个类别的预测概率，假设数据的真实结果为（1，0，0），那么两个损失函数的公式为</p>
<p>可以看出，平方损失函数考虑的内容实际上比交叉熵损失函数多。也就是说，在模型优化的过程中，交叉熵损失的梯度只和正确分类的预测结果有关，而平方损失的梯度还和错误分类有关。除了让正确分类尽可能变大，平方损失函数还会让错误分类都变得更平均，但实际中后面这个调整是不必要的，所以平方损失实际上完成了额外的工作，因此它的实用性就显得没那么强了。但是话说回来，这个考虑在回归问题上显得非常重要，因此在回归问题上交叉熵损失显然就不合适了。</p>
<p>另一方面，从理论角度分析，两个损失函数的源头不一样。平方损失函数假设最终结果都服从高斯分布，而高斯分布实际上是一个连续变量，并不是一个离散变量。如果假设结果变量服从均值为<br>_t_ ，方差为 _σ_ 的高斯分布，那么利用最大似然法就可以优化它的负对数似然，公式最终变为</p>
<p>除去与 _y_ 无关的项目，最终剩下来的公式就是平方损失函数的形式。</p>
<h3 id="2-10-总结"><a href="#2-10-总结" class="headerlink" title="2.10 总结"></a>2.10 总结</h3><p>本章和读者一同回顾了机器学习中常用的数学基础知识。</p>
<p>（1）线性代数基础：矩阵、线性变换、线性相关、特征值与特征向量。</p>
<p>（2）线性代数中的对阵矩阵。</p>
<p>（3）概率论基础：概率、贝叶斯公式、最大似然法估计和重要性采样。</p>
<p>（4）信息论基础：熵和KL散度。</p>
<p>（5）凸函数：凸函数的性质。</p>
<p>（6）机器学习的基本概念：分类回归模型、目标函数和优化。</p>
<h2 id="3-优化算法"><a href="#3-优化算法" class="headerlink" title="3 优化算法"></a>3 优化算法</h2><p>作为机器学习中十分重要的一环，优化算法受到了极大的重视。好的优化算法意味着可以更好、更快地找到目标模型，所以优化算法相关的知识向来充满了吸引力。深层模型的发展又让它备受瞩目：深层模型的目标函数不再是凸函数，优化曲面变得更复杂，别说寻找一个优秀的优化算法，完成正常的优化都变得困难。本章将介绍三个被学术界和工业界广泛研究和使用的优化算法：梯度下降法、共轭梯度法和自然梯度法。了解这三个优化算法会对后面的学习有很大帮助。</p>
<h3 id="3-1-梯度下降法"><a href="#3-1-梯度下降法" class="headerlink" title="3.1 梯度下降法"></a>3.1 梯度下降法</h3><h4 id="3-1-1-什么是梯度下降法"><a href="#3-1-1-什么是梯度下降法" class="headerlink" title="3.1.1 什么是梯度下降法"></a>3.1.1 什么是梯度下降法</h4><p>作为大众耳熟能详的优化算法，梯度下降法（Gradient<br>Descent）受到了太多的关注。梯度下降法极易理解，但凡学过数学的读者都知道，函数的梯度方向表示了函数值增长速度最快的方向，那么和它相反的方向就可以看作函数值减少速度最快的方向。就机器学习模型优化的问题而言，当目标设定为求解目标函数最小值时，只要朝着梯度下降的方向前进，就能不断逼近最优值。</p>
<p>那么梯度下降算法怎么实现呢？这里给出一种最简单的梯度下降算法——固定学习率的方法，这种梯度下降算法由两个函数和三个变量组成。</p>
<p>· 函数1：待优化的函数 _f_ （ _x_ ），它可以根据给定的输入返回函数值。</p>
<p>· 函数2：待优化函数的导数 _g_ （ _x_ ），它可以根据给定的输入返回函数的导数值。</p>
<p>· 变量 _x_ ：保存当前优化过程中的参数值，优化开始时该变量将被初始化成某个数值，优化过程中这个变量会不断变化，直到它找到最小值。</p>
<p>· 变量grad：保存变量 _x_ 点处的梯度值。</p>
<p>· 变量step：表示沿着梯度下降方向行进的步长，也被称为学习率。它就是本节的主角，在优化中它将固定不变。</p>
<p>包含上面5个元素的梯度下降算法如下所示。</p>
<p>除了上面介绍的内容，代码中还加入了优化的终止条件。优化的目标是寻找梯度为0的极值点，代码在每一轮迭代结束后衡量变量 _x_<br>所在的梯度值，因此当梯度值足够小时，就认为 _x_ 已经进入最优值附近一个极小的邻域， _x_ 和最优值之间的差别不再明显。这时就可以停止优化。</p>
<h4 id="3-1-2-优雅的步长"><a href="#3-1-2-优雅的步长" class="headerlink" title="3.1.2 优雅的步长"></a>3.1.2 优雅的步长</h4><p>明确了算法，下面就通过一个例子展示梯度下降法的优化效果。这里用一个十分简单的函数作为例子。</p>
<p>这个函数 _f_ （ _x_ ）就是读者在中学常见的二次函数 _f_ （ _x_ ）= _x_ 2 -2 _x_ +1，读者可以很轻松地看出，最小值是<br>_x_ =1，此时函数值为0。为了让读者对这个函数有更直观的认识，这里将图画出来。</p>
<p>这个函数如图3-1所示。</p>
<p>图3-1 梯度下降法的案例函数</p>
<p>可以清楚地看出这个函数的图像就是一个很简单的抛物线。 _x_ =1是函数的最小值。下面就用梯度下降法计算它的极值：</p>
<p>运行后得到下面的输出（结果有省略）：</p>
<p>可以看到，初始值 _x_ 从5出发，梯度值在不断下降，经过20轮迭代， _x_ 虽然没有完全等于1，但是在迭代中它正不断地逼近最优值 _x_ =1。</p>
<p>以上就是梯度下降法的展示。这个例子留下了一点遗憾：最后 _x_<br>离最优值的距离并不算近。是不是因为每一轮迭代时的步长设置得太小，导致优化值没有更快地收敛到最优值呢？如果是这样，那么是不是增大步长就可以在20轮迭代内看到优化结束呢？抱着这样的想法，让我们重新进行一次优化，这一次的优化步长被设置为之前的1000倍（有点夸张）。</p>
<p>如下代码就是步长加大后的结果：</p>
<p>可以看到，参数的梯度不但没有收敛，反而越来越大。优化的本意是让目标值朝着梯度下降的方向前进，结果它却走向了另外一个方向。为什么会出现这样的情况呢？为什么梯度会不降反升呢？是我们的算法本身有问题，还是梯度的设置有问题？解释这个问题还要回到梯度这个概念本身。</p>
<p>实际上，函数在某一点的梯度指的是它在当前变量处的梯度，对于这一点来说，它的梯度方向指向了函数上升的方向，可以利用泰勒公式证明在一定范围内，沿着负梯度方向前进，函数值是会下降的。但是，公式只能保证在一定范围内是成立的，从函数的实际图像中也可以看出，如果优化的步长太大，就有可能跳出函数值下降的范围，那么函数值是否下降就不好说了。当然有可能越变越大，造成优化的悲剧。</p>
<p>如何避免这种悲剧发生呢？简单的方法就是将步长减少，像前面的实验那样设得小一些。当然还有一些线搜索（Line-<br>Search）的方法可以通过其他限定条件避免这样的事情发生，本节就不做介绍了。</p>
<p>既然理论没错，那么看起来只能通过修改步长来完成这个问题了。既然小步长会使目标值的梯度下降，大步长会使梯度发散，那么有没有一个步长会让优化问题原地打转呢？在这个问题中，这样的步长是存在且容易找到的。</p>
<p>由于梯度优化总会造成数值的改变，所以每一步优化都让目标值原地打转是不太现实的，这里可以假设目标值从 _A_ 变为 _B_ ，再从 _B_ 变为 _A_<br>，如此循环更新。假设：从 _x_ =5出发，经过一轮迭代， _x_ 被更新到了另一个值 _x_ ′ ，再用 _x_ ′ 继续迭代， _x_ ′<br>又更新到了5。上述过程可以写成如下方程。</p>
<p>_x_ =5， _g_ （ _x_ ）=8，新的值 _x_ ′ =5-8×step</p>
<p>_g_ （ _x_ ′ ）=2×（5-8×step）-2，回到过去： _x_ ′ _-g_ （ _x_ ′ ）×step= _x_ =5</p>
<p>合并公式求解得，step=1。</p>
<p>也就是说step=1时，求解会原地打转，那么接下来就来试一下。</p>
<p>它的结果如下所示：</p>
<p>和预想的一样，目标值在函数中打转，梯度下降法失效了。</p>
<p>通过上面的实验可以发现，对于初始值为5这个点，当步长大于1时，梯度下降法会出现求解目标值发散的现象；而步长小于1时，则不会发散，参数会逐渐收敛。所以1就是步长的临界点。那么问题又来了，对于别的初始值，这个规律还适用吗？接下来就把初始值换成4，再进行一次实验。</p>
<p>它的结果如下所示：</p>
<p>实验发现，步长等于1时，初始值设置成（最优值除外）任何数字，参数都不会收敛到最优值。这个实验揭示了一个道理：对于这个二次函数，如果采用固定步长的梯度下降法进行优化，步长要小于1，否则不论初始值等于多少，问题都会发散或者原地打转！</p>
<p>如果再换一个函数：，它的安全步长是多少呢，还是1吗？当然不是。这一次安全步长的值是0.25，相应的实验如下：</p>
<p>它的结果如下所示：</p>
<p>实验结果和猜测完全一致。至此，相信读者对步长的设置有了更深的认识。这组实验说明了梯度下降法简单中的不简单，虽然固定步长的方法看上去简单直观，但是步长的选择要慎重。一个一维的二次函数都存在优化问题，对CNN网络优化来说，优化曲面要复杂得多。采用固定步长优化的方法实际上是“步步惊心”，实战中需要一定的技巧并且多尝试才能找到最合适的步长。</p>
<h3 id="3-2-动量算法"><a href="#3-2-动量算法" class="headerlink" title="3.2 动量算法"></a>3.2 动量算法</h3><p>本节将介绍动量（Momentum）算法 [1]<br>。动量是物理课上学习过的一个概念，正如它的中文名一样，在优化求解的过程中，动量代表了前面优化过程积累的“能量”，它将在后面的优化过程中持续发威，推动目标值前进。拥有了动量，一个已经结束的更新量不会立刻消失，只会以一定的形式衰减，剩下的能量将继续在优化中发挥作用。</p>
<p>上面介绍的概念比较抽象，以下就是基于动量的梯度下降的代码（基于前面的梯度下降法做了一些修改）。</p>
<p>代码中多出了一个新变量pre_grad。这个变量就是用于存储历史积累的动量，每一轮迭代动量都会乘以一个打折量（discount）做能量衰减，但是它依然会被用于更新参数。</p>
<p>每个事物的存在必然有它的道理，动量算法和前面介绍的梯度下降法相比有什么优点呢？用形象的话来说，它可以帮助目标值穿越“狭窄山谷”形状的优化曲面，从而到达最终的最优点。那么，什么是“山谷”，怎么理解“穿越山谷”这个词语呢？下面用一个例子来解释。这次的问题复杂些，是一个二元二次函数：<br>_z_ = _x_ 2 +50 _y_ 2 。</p>
<p>函数在等高线图上的样子如图3-2所示。</p>
<p>图3-2 二次目标函数的等高线图</p>
<p>其中中心的点表示最优值。把等高线上的图像想象成地形图，从等高线的疏密程度可以看出，这个函数在 _y_ 轴方向十分陡峭，在 _x_<br>轴方向则相对平缓。也就是说，函数在 _y_ 轴的方向导数比较大，在 _x_<br>轴的方向导数比较小。图3-2所示的区域可以看作一个“山谷”。如果用3.1节介绍的固定步长的梯度下降法尝试优化，则有：</p>
<p>将50轮迭代过程中参数的优化过程图画出来，如图3-3所示。</p>
<p>图3-3 梯度下降法的优化曲线</p>
<p>可以看出目标值从某个点出发，整体趋势向着最优点前进，它和最优值的距离不断靠近，说明优化过程在收敛，步长设置是没有问题的，但是前进的速度似乎有点乏力，50轮迭代并没有到达最优值，有可能是步长设置得偏小。有了3.1节的经验，这次设置步长时很谨慎。我们只将步长稍变大一点，结果如图3-4所示。</p>
<p>图3-4 增大步长的梯度下降法优化曲线</p>
<p>虽然优化效果有了一定的提高，但成效依然不明显，而且优化的过程图中出现了参数值左右抖动的现象，这是怎么回事呢？看上去参数在优化过程中产生了某种“打转儿”的现象，做了很多无用功。看到这个曲线路径，读者可能会想到一个比较熟悉的极限运动，如图3-5所示。</p>
<p>图3-5 极限运动中的U形赛道</p>
<p>实际上，算法眼中优化曲面和这张图很像，算法没有让大家“失望”，一直在选择梯度下降最快的方向前进，但梯度下降最快的方向不一定能为优化提供太多的帮助，因此沿着这条道路进行优化就十分艰难——就像图中做极限运动的滑板少年一样，从一边的高台滑下，然后滑到另一边，不断反复。这个过程很像“打转”现象，朝正确优化方法前进的步伐比较小，而无效重复的前进步伐比较大。虽然优化的效率很低，但这就是梯度下降法。它的眼中只有梯度，并且只相信梯度。</p>
<p>如果继续增加步长，优化曲线会变成什么样子呢？“滑板少年”还能不能再快点前进呢？实验结果如图3-6所示。</p>
<p>图3-6 极限步长下梯度下降法的优化曲线</p>
<p>从结果来看，这是滑板少年能尽的最大力了······<br>这个步长已经是能设置的最大步长，如果步长再设大些，滑板少年就要从U形赛道飞出去，优化参数的梯度也将发散出去。在这个问题中，由于两个坐标轴方向的函数的陡峭性质不同，两个方向对最大步长的限制不同，显然<br>_y_ 方向对步长的限制更严格。但满足 _y_ 方向的更新， _x_ 方向就无法获得充分的更新，这样梯度下降法将无法获得很好的效果。</p>
<p>下面我们将目光转向动量，看看动量算法如何帮助这位滑板少年。</p>
<p>在开始动量算法计算前，首先要对滑板少年的行动方向做一个总结，读者会发现滑板少年每一次的行动只会在以下三个方向进行。</p>
<p>· 沿 _-x_ 方向滑行。</p>
<p>· 沿+ _y_ 方向滑行。</p>
<p>· 沿 _-y_ 方向滑行。</p>
<p>这样看来，要是少年能把行动的力量集中在往 _-x_ 方向走而不是沿 _y_ 方向打转就好了。</p>
<p>这个想法可以被动量算法实现。我们可以想象，使用了动量后，历史的更新量会以衰减的形式不断作用在这些方向上，那么沿 _-y_ 和+ _y_<br>两个方向的动量就可以相互抵消，而 _-x_ 方向的力则会一直加强，这样滑板少年虽然还会沿 _y_ 方向打转，但是他在 _-x_<br>方向的速度会因为之前的累积变得越来越快。</p>
<p>基于上面的分析，下面来看看加了动量技能的滑板少年的优化表现，如图3-7所示。优化曲线果然没有令人失望，尽管滑板少年还是有打转现象，但是在50轮迭代后，他进入了最优点的邻域，完成了优化任务，和前面的梯度下降法相比有了很大的进步。</p>
<p>图3-7 动量算法的优化曲线</p>
<p>当然，还是暴露了动量优化存在的一点问题，前面几轮迭代过程中目标值在 _y_<br>轴上的震荡比过去还要大。在发明动量算法后，又有科研人员发明了基于动量算法的改进算法，解决了动量算法没有解决的问题——更强烈的抖动，干脆让滑板少年停止玩耍，专心赶路。这就是Nesterov算法<br>[2] 。这里给出代码和刚才问题的优化过程，如图3-8所示。</p>
<p>图3-8 Nesterov算法的优化曲线</p>
<p>滑板少年不再贪玩，放弃在U形赛道上摩擦。那么，Nesterov算法和动量算法相比有什么区别呢？动量算法计算的梯度是在当前的目标点的；而Nesterov算法计算的梯度是在动量更新后的优化点的。这其中关键的区别在于计算梯度的点。</p>
<p>读者可以想象一个场景，当优化点已经积累了某个抖动方向的梯度后，对动量算法来说，虽然当前点的梯度指向积累梯度的相反方向，但是量不够大，所以最终的优化方向还会在积累的方向上前进一段，这就是图3-8所示的效果。对Nesterov方法来说，如果按照积累方法再向前多走一段，则梯度中指向积累梯度相反方向的量会变大许多，最终两个方向的梯度抵消，反而使抖动方向的量迅速减少。Nesterov的衰减速度确实比动量方法的快不少。</p>
<p>经过上面的介绍，相信读者能够理解动量算法在“穿越山谷”上的卓越表现。本节的最后来讲述动量在数值上的事情。科研人员已经给出了动量打折率的建议配置——0.9（刚才的例子全部是0.7），那么0.9的动量打折率能使历史更新量发挥多大作用呢？</p>
<p>如果用G表示每一轮迭代的动量，g表示当前一轮迭代的更新量（方向×步长）， _t_ 表示迭代轮数， _γ_ 表示动量的打折率，那么对于时刻 _t_<br>的梯度更新量就有公式</p>
<p>这样可以计算出对于第一轮迭代的更新g0 来说，从G0 到GT ，它的总贡献量为</p>
<p>相信读者已经发现它的贡献和是一个等比数列的和，比值为 _γ_ 。如果 _γ_ =0 _._<br>9，那么根据公比小于1的等比数列的极限公式，可以知道更新量在极限状态下的贡献值</p>
<p>那么当 _γ_ =0 _._ 9时，它一共贡献了相当于自身10倍的能量。如果 _γ_ =0 _._<br>99呢？那就是100倍能量。在实际应用中，打折率的设置需要分析具体任务中更新量需要多“持久”的动力。</p>
<h3 id="3-3-共轭梯度法"><a href="#3-3-共轭梯度法" class="headerlink" title="3.3 共轭梯度法"></a>3.3 共轭梯度法</h3><p>本章的前面部分介绍了有关一阶梯度下降法的优化方法。本节介绍另一种一阶梯度方法：共轭梯度法。它也是活跃在优化世界的一个经典算法。它的计算速度还算不错，方法也相对简单，虽然比梯度下降法复杂不少，但是和二阶方法相比，也不算特别困难。</p>
<p>在前面的问题中，我们并没有给出待优化问题的具体形式，本节中为了使问题更具体，我们将待优化的问题形式定义为</p>
<p>其中X为待优化的向量，A为半正定的矩阵，b为已知向量，对公式进行求导，并令导数为0，就可以得到</p>
<p>其中X∗ 为最优值。从公式的形式来看，虽然可以通过高斯消元法直接解方程，但计算量比较大，计算复杂度比较高，所以我们希望通过优化的方式找到最优解。</p>
<h4 id="3-3-1-精妙的约束"><a href="#3-3-1-精妙的约束" class="headerlink" title="3.3.1 精妙的约束"></a>3.3.1 精妙的约束</h4><p>我们发现虽然梯度下降法的每一步都朝着局部最优的方向前进，但它在不同的迭代轮数会选择非常相近的方向，这说明当某一次选择了一个更新方向和步长后，这个方向并没有被更新完，未来还会存在这个方向的残差。如果把参数更新的轨迹显示出来，我们可以看到有时轨迹会走成zig-<br>zag的形状，如3.2节的图3-4、图3-6和图3-7所示。于是，我们对优化提出了更高的要求——能不能在选择优化方向和步长上更智能，每朝一个方向走，就把这个方向走到“极致”？所谓极致，可以理解为在优化过程中再也不需要朝这个方向更新了。于是我们引出了一个变量——误差，假设最优的参数为x∗<br>，当前第 _t_ 轮的参数为xt ，误差可以定义为</p>
<p>这个误差表示了参数的最优点和当前点之间的距离。那么目标就更明确了，我们希望每一步优化后，当前的误差和刚才优化的方向正交。请读者注意，这里我们没有使用梯度这个词，而是使用优化方向，因为从现在开始，我们的优化方向不一定是梯度。现在令rt<br>表示第 _t_ 轮的更新方向，就可以得到下面的公式</p>
<p>这真的是一个很漂亮的公式，假设每一轮的优化量都和误差正交，那么如果我们的优化空间有 _d_ 维，理论上最多只需要迭代 _d_<br>轮就可以求解出来，这样在优化时间上就有了保证。如果我们直接使用这个公式，就会发现一个问题：公式中需要知道误差。换句话说，我们需要知道最优点在哪儿。如果我们知道最优点在哪儿，就不用优化了。可是不知道最优点儿，这个方法又无法直接使用，于是我们仿佛掉入了“先有鸡还是先有蛋”的陷阱。</p>
<p>不过庆幸的是，我们还有数学武器在手，可以用线性代数的知识偷天换日，解决这个问题，让这个美好的约束发挥它的作用。</p>
<h4 id="3-3-2-共轭"><a href="#3-3-2-共轭" class="headerlink" title="3.3.2 共轭"></a>3.3.2 共轭</h4><p>为了解决3.3.1节提到的问题，我们要使用共轭这个工具。相信读者看到共轭这个词的时候会想，这词是什么意思？共轭中的轭实际上是一个工具，如图3-9所示。</p>
<p>轭就是绑在两头牛身上的木头，它让两个本来独立的个体变成了一个整体，属于牵线搭桥的关键道具。我们再回到刚才的问题，前面我们提到优化方向和误差正交，如果使用了共轭这个工具，现在两者的关系将变为共轭正交，也就是存在一个矩阵A（A就是轭），使得优化方向和误差这两头牛“牵手成功”，满足正交的性质：</p>
<p>图3-9 轭的示意图</p>
<p>看到这里可能有的读者依然很迷惑，为什么要加入这个矩阵，它的加入不就使原本正交的两项变得不正交了吗？别着急，其实如果我们在上面的原始公式中间加一个特殊的矩阵，例如单位阵，看上去就舒服很多了：</p>
<p>加入单位阵并不会改变结果，所以原本正交的二者依然正交。所以换个角度理解，我们以前简单的正交都是在单位阵这个简单且性质优良的“轭”的牵引下实现的。就好比图3-9中，两头老黄牛的“身高”十分接近，因此牵引它们的轭是一个水平的轭，现在来了一大一小两头黄牛，就需要一个不那么水平的轭把它们绑定在一起。经过轭的作用，原本不那么般配的一对牛也可以协同工作。所以回到正交中，如果使用单位阵作为轭，那么绑定在一起的就是常规意义上正交的一对向量；如果使用其他矩阵，那么共轭正交的向量肯定也会满足其他的性质。</p>
<p>从线性代数的角度解释，我们可以认为这个矩阵要完成线性变换的作用，将一个向量从一个线性空间转换到另一个空间。在第2章我们已经介绍了矩阵的作用，矩阵和向量相乘可以想象成是矩阵列向量的加权求和。在上面的公式中，我们通过计算Aet+1<br>将误差向量映射到矩阵A组成的列空间中，只要优化方向与这个新的向量正交，就可以满足上面的例子。在优化过程中，如果A固定不变，就可以认为它们只是从一个空间的正交关系变成另一个空间的正交关系，其他的并没有什么不同。</p>
<p>那么有没有更实际的例子呢？例如我们有两个向量：</p>
<p>很显然它们不是正交的，但如果多了一个矩阵A：</p>
<p>就会发现，b经过A转换后就与a正交了。</p>
<p>如果将上面两个向量的计算过程在坐标图上画出来，就可以得到这样的过程，首先如图3-10和图3-11所示，两幅图分别展示了在单位阵I<br>和矩阵A共轭作用下的坐标轴形式。图3-10展示了以单位阵为轭的坐标形式，大家都比较熟悉，就是我们常见的坐标轴形式。图3-11展示了以矩阵A为轭的坐标形式，我们可以看出它在<br>_x_ 轴没有发生改变，但是在 _y_ 轴数值扩大一倍，并指向了相反方向。我们分别在两个坐标系下观察这两个向量，发现它们都不会产生正交。</p>
<p>图3-10 单位阵为轭的坐标图</p>
<p>图3-11 矩阵A为轭的坐标图</p>
<p>如图3-12所示，如果我们把单位阵坐标系中的a和矩阵A坐标系中的b放在一起，会发现两者产生了正交。所以我们也可以理解为原本不是a和b正交，而是a和另外一个向量b′<br>正交，但是b′ 经过从矩阵A的还原得到了b。</p>
<p>图3-12 两个向量结合的效果</p>
<p>所以这个新的正交条件实际上只是多绕了一个弯，和前面的正交条件差距并不大。一旦接受了这样的设定，下面的内容就很好理解了。</p>
<h4 id="3-3-3-优化步长的确定"><a href="#3-3-3-优化步长的确定" class="headerlink" title="3.3.3 优化步长的确定"></a>3.3.3 优化步长的确定</h4><p>明确了共轭梯度法的目标和特点，我们就要开始推导算法公式了。共轭梯度法属于线搜索的一种，因此和梯度下降法类似，我们的总体思路不变，优化过程分如下两步：</p>
<p>（1）确定优化方向。</p>
<p>（2）确定优化步长。</p>
<p>在共轭梯度法中，确定优化方向比确定优化步长麻烦，因此我们先来介绍第二步（优化步长）的计算方法。假设当前的参数为Xt ，我们已经得到了优化方向rt<br>，下面要确定的就是步长 _α_ t ，根据前面提过的共轭正交公式</p>
<p>我们可以开始推导：</p>
<p>于是可以整理得到</p>
<p>我们知道AX∗ =b，第 _t_ 轮的梯度gt =AXt -b，于是公式最终变为</p>
<p>到这里，我们利用矩阵A成功地把公式中的误差e抵消，于是步长变得可解。</p>
<p>完成了步长的求解，接下来就要回到第一步看看优化方向的计算方法。我们要解决的主要问题是如何让优化方向和误差正交。由于每一次的优化后，剩下的误差和本次的优化正交（共轭正交），所以可以看出每一个优化方向彼此间都是正交的。那么，如何构建这些彼此正交的方向呢？</p>
<h4 id="3-3-4-Gram-Schmidt方法"><a href="#3-3-4-Gram-Schmidt方法" class="headerlink" title="3.3.4 Gram-Schmidt方法"></a>3.3.4 Gram-Schmidt方法</h4><p>在线性代数课上，我们曾经学过一个向量正交化的方法——Gram-Schmidt方法。这个算法的输入是 _N_ 维空间中 _N_<br>个线性无关的向量，由于向量间线性无关，任何一个向量都无法通过其他向量表达出来。算法的输出是 _N_ 个相互正交的向量，也就是我们最终想要的向量组合。</p>
<p>它的具体算法如下，令输入向量为u1 _，_ u2 _，_ ··· _，_ uN ，输出向量为d1 _，_ d2 _，_ ··· _，_ dN ，那么有：</p>
<p>（1）对于第一个向量，我们保持它不变：u1 =d1 。</p>
<p>（2）对于第二个向量，我们去掉其中和第一个向量共线的部分，令去掉的比例为 _β_ i ，所以第二个向量d2 等于u2 + _β_ 1 d1 。</p>
<p>（3）对于第三个向量，我们去掉其中和第一、第二个向量共线的部分：d3 =u3 +。</p>
<p>（4）对于第 _N_ 个向量，我们去掉其中和第一、第二、第 _N_ -1个向量共线的部分：。</p>
<p>那么我们怎么求这些比例项 _β_ 呢？我们利用前面提到的性质，向量之间正交（这里还是共轭正交），于是有</p>
<p>进一步展开，可以得到</p>
<p>利用正交的性质，可以将公式化简为</p>
<p>所以最终我们得到</p>
<p>求得了这个变量，就可以依此求出当前向量与其他向量的共线部分，从而确保所有向量相互正交的性质。当然，我们也看到，为了实现正交的效果，算法的复杂度为 _O_ （<br>_N_ 2 ），在实现速度上不是很快。</p>
<h4 id="3-3-5-共轭梯度"><a href="#3-3-5-共轭梯度" class="headerlink" title="3.3.5 共轭梯度"></a>3.3.5 共轭梯度</h4><p>虽然Gram-Schmidt方法并不快，但它的确是一个可行的方案。到这里，我们还有如下两个问题没有解决。</p>
<p>（1）我们要用什么向量构建这些正交向量？</p>
<p>（2）随着向量数量的增加，我们需要计算的参数越来越多，我们一共要计算 _O_ （ _N_ 2 ）数量级的 _β_<br>，这个计算的数目还是有点多，能不能再减少呢？</p>
<p>上面的问题当然能够解决，方法也比较简单——用每一步的梯度构建这些正交向量，同时利用梯度的特点减少计算量。这就是算法名字的来源——共轭思想+梯度计算。</p>
<p>那么如何用梯度解决第二个问题呢？我们要依次证明下面三个推论。</p>
<p>（1）第 _t_ 步计算的梯度gt 和前 _t_ -1步的更新正交。</p>
<p>（2）第 _t_ 步计算的梯度gt 和前 _t_ -1步的梯度正交。</p>
<p>（3）第 _t_ 步计算的梯度gt 和前 _t_ -2步的更新共轭正交。</p>
<p>先来证明第一个推论。前面提到共轭梯度法的基本思想：每一轮把某一方向优化彻底，保证后面的优化不再对这个方向做任何操作。假设算法一共进行了 _T_<br>轮迭代，我们就可以用这 _T_ 轮求出的优化方向组合成最终的误差。</p>
<p>假设我们的初始参数值为x1 ，那么它到最优点x∗ 的距离为e1 ，这也是当前的误差。第 _t_ 步求出的优化方向为dt ，每一轮的优化步长为 _γ_ i<br>，根据上面的定义，可以得到如下公式</p>
<p>这样我们就用更新方向表示了误差。对于不同迭代轮数的误差，我们也可以用类似的公式表示。上面的公式利用所有的更新组合成误差，我们反过来思考也可以理解，通过每一步的更新，算法最终收敛，误差变为0。所以，这里的<br>_γ_ 和前面提到的步长 _α_ 之间是等价的关系。</p>
<p>下面就来证明：</p>
<p>当 _i＜j_ 时，</p>
<p>证明过程如下</p>
<p>因为前面已经说明任意两个优化方向是相互正交的（Gram-<br>Schmidt方法），所以这个式子的结果为0。利用共轭正交的性质，我们得到了更好的正交性质，这就是共轭梯度法最精髓的地方。</p>
<p>下面证明第二个推论。我们直接给出推导过程。</p>
<p>此时我们发现，由于每一步的梯度与此前的梯度正交，因此我们可以使用它作为线性无关的向量组，从而组成共轭正交的向量组。</p>
<p>第三个推论将以第二个推论为基础，也就是</p>
<p>将公式进行变换，可以得到</p>
<p>对公式进行变换，可以得到</p>
<p>此时我们发现公式的左边就是求解 _β_ 的分子，那么公式右边等于什么呢？我们令 _j ＜i_ ，于是有：</p>
<p>· 当 _j_ = _i_ -1时，等式右边为，可以看出当gi 不等于0时，这部分公式不等于0。</p>
<p>· 当 _j ＜i_ -1时，等式右边为，由于 _j_ +1 _＜i_ ， _j ＜i_ ，根据前面的推断，这部分公式为0。</p>
<p>所以可以得到，使用Gram-Schmidt方法可以保证更新方向共轭正交。对于第 _t_ 轮优化，我们只计算 _β_ t-1 即可，其他的 _β_<br>值均为0，不用计算，这样计算量得到了极大的降低。</p>
<p>到这里我们完成了共轭梯度法的理论推导，由于算法比较复杂，我们需要对算法进行回顾。</p>
<p>（1）最初，我们希望我们的更新方向与误差正交，从而得出了。</p>
<p>（2）由于误差无法提前知道，直接计算正交比较困难，所以我们将正交改为共轭正交，于是公式变为。</p>
<p>（3）我们希望使用函数的梯度作为优化方向的基础，每一次求出当前的梯度，就用梯度减去之前更新方向的成分，以保证每一次的优化都保持共轭正交。</p>
<p>（4）我们采用Gram-Schmidt方法确保更新方向能够维持共轭正交的性质，根据证明，我们发现在使用梯度时，每一轮只需要计算一个 _β_<br>值就能实现共轭正交的效果，计算量大大降低。 _β_ 的求解方法为</p>
<p>.</p>
<p>（5）完成了更新方向的计算，我们根据前面的推论，计算每一步的步长： _α_ t =。</p>
<p>至此不免有些感慨，向前辈们的想象力致敬，能够利用一些看上去抽象又无关紧要的定理帮助我们解决这些大问题，这里面存在的不仅是数学的美丽，甚至是哲学的精彩……</p>
<p>下面我们来简单介绍这个算法的实现，在第5章将介绍的Baselines项目中，base-<br>lines/common/cg.py文件中给出了共轭梯度法的实现，代码如下所示。</p>
<p>乍一看这份代码，相信读者也有些疑惑，看上去代码的实现和前面的理论并不相同，这其中有什么区别呢？</p>
<p>我们的目标是求出AX=b的解。代码中的残差为r，真正的优化方向为p，优化步长为 _v_ 。算法默认的X 初始值为0，所以初始的残差r1<br>=b，由于第1轮我们没有一个已经执行的优化方向，因此优化方向p1 =r1 ，代码的第3～5行实现了上述功能。由于rT<br>r将在后面反复使用，于是在代码第6行我们计算了rdotr的值。</p>
<p>在后面的循环中，我们首先要计算步长，前面给出的计算步长的公式为，对其进行变换，可以得到</p>
<p>于是第8行和第9行代码实现了计算Apt 和 _α_ t 的功能。其中第8行的函数由用户提供，用于实现 Apt 。接下来就是更新 X，于是第10行实现了对 X<br>的更新操作：Xt+1 =Xt + _α_ t pt 。</p>
<p>得到了新的X，我们就可以计算下一轮的梯度，也就是残差rt+1 。正常计算残差的方法为rt+1 =b-AXt+1 ，公式经过变换，就得到了代码中第11行的公式</p>
<p>得到残差后，下一步就是计算真正的优化方向，前面推导的公式为 pt+1 =rt+1 -，经过变换，我们就能得到代码中第12～14行的公式</p>
<p>根据残差和更新方向的正交关系，可以得到</p>
<p>这样就和代码一致了，我们可以复用这样的中间计算结果。</p>
<p>第15～17行代码用于检查当前的梯度值，当梯度小于一定值时，我们可以认为算法达到了一定的精度，于是停止优化，算法结束；如果梯度仍比较大，就继续进行下一轮优化，此时我们已经知道了rt+1<br>和pt+1 ，所以接下来就是计算 _α_ t+1 ，也就是从第8行开始的又一次循环。</p>
<p>以上就是共轭梯度法的介绍，实际上共轭梯度法不止这些内容，而其他复杂的知识与本书的内容关系不大，这里不再赘述。第10章将介绍置信区间策略优化算法，那时还会涉及共轭梯度法的知识。</p>
<h3 id="3-4-自然梯度法"><a href="#3-4-自然梯度法" class="headerlink" title="3.4 自然梯度法"></a>3.4 自然梯度法</h3><h4 id="3-4-1-基本概念"><a href="#3-4-1-基本概念" class="headerlink" title="3.4.1 基本概念"></a>3.4.1 基本概念</h4><p>梯度下降法是一种十分直观的优化方法，它的计算比较简单，实现也比较简单，但是它存在一些问题。我们从3.2节的例子中看到，当优化问题的两个坐标轴的尺度差异比较大时，使用一个统一的学习率会产生问题：某一个坐标轴可能会发散。那么产生这个问题的根源是什么呢？</p>
<p>客观地说，优化问题中参数尺度不同是很难避免的。因此，在不同的优化曲面上，虽然每一轮迭代对参数的更新量相差不多，但是它们对模型的影响完全不同，有时会使模型有较大的改进，有时对模型产生的改进较小，有时甚至会使模型倒退，这些效果都可以在loss曲线上体现出来。</p>
<p>这样我们就发现了一个很关键的问题：梯度下降法针对的目标是参数，我们更新的目标也是参数，而优化的真正目标是找到最优的模型。在每一轮迭代中，对参数的改进和对模型的改进是不同的。对不同参数进行同样数值大小的更新，不一定带来同样的模型改变，反过来也是如此。那么有没有一种方法，不简单地使用学习率对参数更新进行量化，而是对模型效果进行量化呢？这就是自然梯度下降法（Natural<br>Gradient Descent）的思想来源。</p>
<p>一般来说，我们会假设模型的参数来自高维的实数空间Rd ，在这样的空间中，欧氏距离是空间的度量方法，即对空间内的两个点 _x_ 和 _y_ ，它们的距离（<br>_d_ ）为：</p>
<p>如果我们的模型有 _N_ 个参数，那么模型的所有参数可以组成一个长度为 _N_ 的向量。我们假设存在两组参数 _θ_ 1 和 _θ_ 2 ，对应的模型为<br>_f_ （ _x_ ； _θ_ 1 ）和 _f_ （ _x_ ； _θ_ 2<br>），两组参数的距离可以通过欧式距离体现，那么这两个模型的距离（也就是表现效果）该如何体现呢？</p>
<p>在2.6节我们曾介绍过KL散度，它可以比较两个概率分布之间的差异。如果我们认为模型最终的输出是一个概率分布，就可以使用它来表示：</p>
<p>很显然，参数和模型对距离的衡量方法是不同的。梯度下降法只考虑了在梯度方向上对参数进行更新，并没有考虑模型层面的更新，因此就会出现模型更新不均匀的现象。</p>
<p>对一些简单的优化问题来说，梯度下降法已经可以做得很好，但对一些复杂的问题来说，梯度下降法可能会难以优化，甚至造成发散，所以找到一个稳定优化的方法显得十分重要。</p>
<p>按照梯度下降法的思想，我们定义每一轮的迭代优化都要解决这样一个子问题：</p>
<p>将原函数进行一阶泰勒展开，问题就变为</p>
<p>对问题求导，并对更新量做一定的限制，就可以得到梯度下降法。如果我们改为对模型的距离进行约束，就可以得到另外一个形式的公式</p>
<p>这就是自然梯度法的优化形式。可以看出，有了模型层面的约束，每一轮迭代无论参数发生多大的变化，模型的变化都会限制在一定的范围内，因此不论我们使用什么样的模型，这个约束都会起到相同的效果，因此这个约束是具有普适性的，在任何模型上都能发挥同样稳定的效果。</p>
<p>自然梯度法的优点已经显而易见，所以让我们来看看它的缺点：这个KL散度该怎么计算、怎么优化呢？看上去这并不是一个容易处理的式子。</p>
<h4 id="3-4-2-Fisher信息矩阵"><a href="#3-4-2-Fisher信息矩阵" class="headerlink" title="3.4.2 Fisher信息矩阵"></a>3.4.2 Fisher信息矩阵</h4><p>以往的经验一次又一次地告诉我们，当我们遇到一个难以计算的复杂公式时，最好先思考能不能化简它。通过计算，KL散度可以变成熵和交叉熵的运算，具体的推导过程为</p>
<p>其中 _E_ 表示为依 _f_ （ _w_ ）概率计算得到的期望。对等式右边的第二项进行二阶泰勒展开，可以得到</p>
<p>由于我们定义的函数 _f_ （ _w_ ）一般都是一个连续、可导、有界、性质优良的函数，所以这里第一项的积分和微分可以互换，同时我们将简写的 _f_ （<br>_w_ ）用完整的形式写出： _f_ （ _x_ ； _w_ ），于是上式的第一项就变为</p>
<p>最终得到</p>
<p>这里包含一个二阶导的期望值，虽然看上去比KL散度直观，但它仍然比较复杂。我们需要用Fisher信息矩阵（Fisher Information<br>Matrix）来表示它。Fisher信息是信息几何中的一个概念，它也被应用到机器学习中。前面提到 _f_ （ _w_<br>）表示某个概率分布，我们首先定义Score函数（Score Function）为对数似然函数的一阶导数</p>
<p>通过计算可以发现，score函数的期望值为0，公式推导如下：</p>
<p>Fisher信息矩阵可以通过score函数定义：</p>
<p>Fisher信息矩阵有什么用处呢？接下来我们要证明一个重要的结论：Fisher信息矩阵和KL散度的负二阶导数相等。一旦这个定理证明成功，就可以用更简单的方法计算KL散度。首先将二阶导数进行变换：</p>
<p>这样就得到了二阶导数和score函数的关系，进一步推导，可以得到</p>
<p>我们假设表示概率分布的函数的二阶导存在并满足良好的性质，于是积分和求导符号可以进行互换，得到</p>
<p>这样我们就得出在一定条件下（概率分布函数要具备良好的性质），Fisher信息矩阵和KL散度二阶导的相反数相等。虽然这样的推导十分严谨，但对读者来说，这些抽象的概念很难让我们建立起直观的感觉。那么Fisher信息矩阵究竟是什么样的，它的近似效果如何呢？我们以最简单的伯努利分布为例，看看它的Fisher信息是什么。</p>
<p>伯努利分布可以写成</p>
<p>那么Fisher信息矩阵为</p>
<p>展开公式，得到</p>
<p>利用KL散度的二阶导，同样可以得到</p>
<p>因为它只有一个参数，因此这个Fisher信息矩阵只能算一个标量。我们假设存在一个分布为 _θ_ =0 _._<br>5的伯努利分布，希望求解其他伯努利分布和这个分布的距离。这里分别使用KL散度和Fisher信息矩阵近似两种方式计算另一个分布之间的距离。代码如下所示：</p>
<p>最终的结果如图3-13所示。</p>
<p>图3-13 KL散度与Fisher信息近似的对比图（ _θ_ =0 _._ 5的伯努利分布与另一个伯努利分布的比较）</p>
<p>由于我们选取的参照点为 _θ_ =0 _._ 5的分布位置，可以看出在0 _._<br>5附近，KL散度和Fisher信息矩阵近似的KL散度十分接近。因此我们确信在局部范围内两者是可以相互替换的。这就是Fisher信息矩阵的实际意义。</p>
<h4 id="3-4-3-自然梯度法目标公式"><a href="#3-4-3-自然梯度法目标公式" class="headerlink" title="3.4.3 自然梯度法目标公式"></a>3.4.3 自然梯度法目标公式</h4><p>通过前面的推演，我们的目标函数变为</p>
<p>这个有约束的问题可以通过拉格朗日乘子法表示为</p>
<p>对公式进行求导，并求解对应的极限点，可以得到</p>
<p>公式中的可以当作梯度下降法的学习率类似的分量，那么自然梯度法的优化方向就可以看作，与梯度下降法不同，它需要额外求解Fisher信息矩阵的逆。</p>
<p>熟悉优化算法的读者在看到这个公式形式后，一定会联想到牛顿法。牛顿法是一个二阶梯度算法，它求解优化方向的公式为</p>
<p>我们很容易看出两个公式的区别与联系。根据Fisher信息矩阵的求解方法，自然梯度法可以变成一个一阶优化问题，也可以变成一个二阶优化问题。如果把它看作一个一阶优化问题，那么需要对优化步长做更多的考量；如果把它看作一个二阶优化问题，那么牛顿法中可能遇到的一些问题同样会在自然梯度法中出现。</p>
<p>实际上，自然梯度法是一个十分复杂的方法，本节只介绍基本原理，在第10章中，我们将继续介绍基于此方法进行优化的细节。</p>
<h3 id="3-5-总结"><a href="#3-5-总结" class="headerlink" title="3.5 总结"></a>3.5 总结</h3><p>本节我们主要介绍了4种优化算法，让我们在此做一个总结。</p>
<p>（1）梯度下降法：算法简单直接，但是在设计优化步长时需要认真考量。</p>
<p>（2）动量法：更好地穿越一些平坦的优化区域。</p>
<p>（3）共轭梯度法：强调每一步优化迭代的质量。</p>
<p>（4）自然梯度法：将每一轮迭代中对参数的更新转变为对模型效果的更新。</p>
<h2 id="4-TensorFlow入门"><a href="#4-TensorFlow入门" class="headerlink" title="4 TensorFlow入门"></a>4 TensorFlow入门</h2><p>古语讲：“工欲善其事，必先利其器”，拥有好的工具才能完成更大的挑战。为了更快地实现机器学习相关的工作，我们需要有一个强大的工具帮我们管理数据、构建模型、完成训练与预测的工作。本章介绍一款优秀的工具——TensorFlow。TensorFlow是一款由Google开发的使用数据流图进行数值计算的开源软件库<br>[1]<br>。由于它具有使用便捷、功力强大、扩展性好等特点，现在已经被很多公司机构使用，用于实验或者生产部署环境。本书将使用TensorFlow介绍强化学习的算法。为了帮助读者更好、更快地了解TensorFlow及其相关内容，本章介绍TensorFlow的基本操作及其背后涉及的部分原理知识，其中包括TensorFlow的基本使用方法、TensorFlow框架中的一些实现细节，以及分布式的训练框架。</p>
<h3 id="4-1-TensorFlow的基本使用方法"><a href="#4-1-TensorFlow的基本使用方法" class="headerlink" title="4.1 TensorFlow的基本使用方法"></a>4.1 TensorFlow的基本使用方法</h3><p>虽然TensorFlow是一款流传度很广的框架，但笔者还是要用一定篇幅介绍它的基本使用方法。本章的示例代码使用TensorFlow<br>1.3及以上版本，TensorFlow的安装比较简单，这里不再赘述，我们直接进入功能操作的介绍。以一个两层的神经网络模拟一个异或运算为例，介绍使用TensorFlow进行机器学习的基本步骤，其中涉及计算图准备、计算安排和执行计算三部分。</p>
<p>首先介绍的是问题背景。对异或运算来说，输入是两个逻辑变量，输出是一个逻辑变量。当两个输入都是0时，输出为1；其他情况下均输出0。假设我们并不清楚它的逻辑运算方法，同时假设变量是一个浮点数，这样我们的问题就有4条训练数据，它的输入是：</p>
<p>输出为：</p>
<p>下面我们开始计算图准备的部分。首先定义模型的输入，这里将用到placeholder这个结构。它会定义一些占位变量，这些变量在定义计算图时并不会被明确指定，它的数值将在执行计算图时才被赋值，所以这个结构一般被用于传入网络的数据。我们可以从上面的输入数据中看到数据的维度为2，所以可以定义输入变量为：</p>
<p>从代码中可以很明显地看出每个参数的含义，其中shape=[None，2]表示在定义占位符时只知道输入的第二维是2，第一维的大小并不确定，我们可以根据实际需要灵活地传入不同大小的数据。</p>
<p>接下来我们定义网络结构。我们要定义一个两层的神经网络，它的结构如图4-1所示。</p>
<p>图4-1 两层的神经网络结构</p>
<p>从图4-1中可以看出网络参数和运算细节。网络第一层的输出维度为4，第二层的输出维度为1。在定义神经网络的运算前，我们要先定义网络的参数变量。参数可以用Variable这个变量表示，它和前面提到的placeholder不同，在计算图执行前，这些变量需要被事先赋值，同时这个变量可以在计算过程中被更新。一般来说，我们有两种创建变量的方法，我们用第一种方法定义第一层的变量w1和b1：</p>
<p>可以看出这种方法直接使用了Variable变量进行构造。我们再用另一种方法定义第二层的变量w2和b2：</p>
<p>两种定义变量方法的区别将在4.2节介绍。定义好变量后，再来定义这些变量的运算。这里将用到一些定义好的运算函数，具体过程如下：</p>
<p>从名字可以看出matmul和sigmoid方法分别代表了矩阵乘法和非线性计算两个操作，实际上每一行代码实现了一层网络的计算过程。至此，我们完成了模型部分的定义，接下来定义目标函数，我们需要再定义另一个placeholder，用于传入目标值：</p>
<p>对于这个问题，我们直接比较输出值和目标值的差距，这个差距可以用l2_loss函数表示：</p>
<p>完成目标函数的设定后，接下来定义优化器和优化策略：</p>
<p>我们定义了一个最基本的梯度下降法，它的学习率是0.01。调用它的minimize方法就可以完成运算的定义，它的返回值train_op就是开启优化流程的钥匙，我们只要给两个placeholder赋值，并请求计算train_op，就可以完成一轮优化。</p>
<p>到这里我们就完成了计算图的定义，回顾刚才的过程可以看出，代码的全过程完成了机器学习的三个核心部分的定义：定义模型、定义目标函数和定义优化方法。下面就要对计算过程进行安排。每一个计算过程都要放置在特定的计算资源上，具体来说就是CPU和GPU上，现在的计算机大多是多CPU核和多GPU的，有些情况下需要把计算安置在特定的计算资源上，这就需要我们对计算做指定。当然，即使我们不对计算过程做明确的指定，TensorFlow也会对每一个计算安排一个计算资源。安排资源的方法为：</p>
<p>我们在引号内填入具体的设备编号，同时将前面提到的定义放置在这句代码之内，就完成了计算的安排。一般来说，TensorFlow拥有一套策略用于判断怎样的安排方式可以尽可能地保证运算速度，所以我们并不需要对运算做太多的安排，这部分可操作的内容也相对较少。</p>
<p>最后一步就是完成真正的计算。计算前首先要创建一个变量Session，它一般被翻译为会话，这里也可以认为是开启一次与计算图交互的过程，这个Session中会保存计算图中的变量信息，用以完成真正的计算。我们可以把前面的工作想象成“写代码”，而开启一个Session就是执行刚才写好的代码。开启Session的方法为：</p>
<p>后面的代码将在这句代码之内实现。下面我们要做的第一步是完成对所有变量的初始化。有的读者会问，在构建计算图时，我们已经对变量的初始值做了定义，为什么还要进行一次初始化呢？实际上，前面的初始化只是声明了变量对应的初始值，并没有真正执行赋值。这里我们要做的就是执行赋值：</p>
<p>完成了这一步，我们就可以进行真正的计算，也就是训练模型，进行多轮的梯度下降法迭代。每一轮迭代时，除了执行训练运算，还要计算并输出loss值，这样可以更好地监控模型的训练进展情况：</p>
<p>虽然代码中只提到了执行train_op这个操作，但实际上TensorFlow会执行所有与它有关的操作，包括神经网络的前后向计算和参数更新，所以在完成计算后，我们就完成了模型参数的训练。如果读者对被执行的操作感兴趣，可以通过访问计算图寻找这些操作。虽然我们没有具体声明某个计算图，但这个计算图还是在我们定义过程中悄悄构建好了，只要访问TensorFlow为我们保存的默认计算图：</p>
<p>就可以看到TensorFlow为我们构建的一张计算图。除此之外，我们还可以使用Tensor-<br>Flow生态中的TensorBoard将这个计算图以图画的形式显示出来，这部分内容将在后面章节中介绍。完成了模型的训练，将模型的参数保存下来，就完成了这个训练的过程。</p>
<p>接下来我们将介绍更多关于TensorFlow的复杂应用，想要对TensorFlow有更深刻的了解，还需读者不断地练习和体会。</p>
<h3 id="4-2-TensorFlow原理介绍"><a href="#4-2-TensorFlow原理介绍" class="headerlink" title="4.2 TensorFlow原理介绍"></a>4.2 TensorFlow原理介绍</h3><p>4.1节介绍了TensorFlow的基本使用方法，本节将介绍TensorFlow的内部原理，同时对编程过程提出一些建议。不同于其他书籍简单地介绍每一个函数的功能和直接使用方式，笔者将从代码片段入手，通过分析代码的原理理解TensorFlow中的一些特性。</p>
<p>读者也许会问，为什么要把精力放在研究这种鸡毛碎皮的小事上，而不是直接介绍使用TensorFlow解决问题的案例？笔者认为，这种深入代码内部的工作确实不是必需的，但它却是对框架深入理解的基础。当我们想打开TensorBoard看Graph可视化图时，当程序抛出某个变量有问题（维度不对、未参与梯度计算等）时，理解命名规则等细节能帮你快速定位错误，修炼好内功可以让你走得更远！</p>
<p>前面我们已经介绍过，TensorFlow是一个基于计算图的运算框架，它的核心操作分为三步。</p>
<p>（1）构建计算图。</p>
<p>（2）分发计算任务。</p>
<p>（3）执行计算任务。</p>
<p>对大多数的任务来说，第2步和第3步操作对于用户来说相对容易，于是我们的主要精力都花在第1步——也就是构建计算图上。</p>
<h4 id="4-2-1-创建变量的scope"><a href="#4-2-1-创建变量的scope" class="headerlink" title="4.2.1 创建变量的scope"></a>4.2.1 创建变量的scope</h4><p>4.1节介绍了创建变量的方法，这个过程中一个十分重要的环节就是为每一个变量命名。除此之外，我们还可以给每一个操作创建名字。为了更好地组织变量和操作的名字，需要使用层次结构定义名字，此时就需要scope这个概念。如果一个模型中包含多个全连接层和卷积层的计算，我们不能把它们的名字都定义为“fc”或者“conv”，也不应该将所有的操作按出现顺序依次起名，如将第18个出现的卷积操作称为“conv18”，这样做不便于定位对应的操作。就像我们在使用计算机时会用文件夹管理存储的文件一样，模型有时也需要类似的组织形式，比如名字为“vf/block2/conv2”可以让我们大概猜测出这个操作来自于值函数模型（value<br>function<br>model）中第2个运算组中的第2个卷积操作。这样当这个操作出现问题时，我们可以快速完成定位。定义上面提到的层次型名字就需要scope操作。</p>
<p>scope对分离变量的命名空间有十分重要的作用，为了更好地理解scope的用法，我们将通过几段代码展示它的用法。首先是第一段代码：</p>
<p>请读者思考，代码中的两句输出分别是什么？如果一个人号称精通TensorFlow，那么这个问题应该可以答上来。当然，像这类问题即使猜也可以猜得八九不离十。执行代码可以得到：</p>
<p>从结果可以看出，对直接构建的Variable变量来说，每一个scope都会对名字产生影响；而对使用函数get_variable创建的变量来说，并不是所有的scope都会被它考虑在内。只有variable_scope会对它的名字产生影响，而name_scope不会。这也可以看出两种创建变量方法对scope的反应。</p>
<p>下面我们增加问题的难度，如下代码（后文称第二段代码）会输出什么样的结果呢？</p>
<p>这段代码比较复杂，前面两个输出语句与第一段代码相同，后面两句该输出什么呢？执行这段代码可以得到：</p>
<p>从结果可以看出，在同一个scope内，同样名字的name_scope被声明第二次时，scope的名字并不会直接被复用出现，而是通过改名的形式创建一个全新的scope。</p>
<p>看到这里读者一定会想，name_scope重复出现时，系统会自动改名，那么variable_scope呢？于是我们有了第三段代码：</p>
<p>当同一个scope下出现两遍variable_scope时，scope的名字会不会出现名称修改的情况呢？这个问题留给读者，请读者亲自尝试。看到这里，相信读者对这两种scope有了更深的理解，下面就来看看这两个scope创建的实现细节。</p>
<p>1.name_scope</p>
<p>为了探究原理，我们需要去源代码中寻找答案。在TensorFlow源代码tensorflow/ten-<br>sorflow/python/framework/ops.py中，我们可以找到name_scope函数的定义，抛开一些与核心功能无关的代码，这个函数首先要找到框架中的默认计算图（Default<br>Graph），再调用这个默认计算图的name_scope方法生成scope。</p>
<p>进一步跟进代码，我们会来到Graph这个类的name_scope方法中，这个方法是一个基于contextlib的方法。如果传入的scope<br>name在结尾包含‘/’，把这个符号删除，就可以作为命名空间的名字了。如果不是这样，就要进入一个叫作unique_name的方法中，和自己的父级别的命名空间的名字拼起来，在名字库中比对。如果有重名，就采用“原名_%d”的方式进行修改并尝试是否重名，直到不再重名为止。在上面的示例代码中我们看到“456_1”这样的名字，就是这个原因。有关这部分的代码如下：</p>
<p>unique_name方法的核心代码如下所示：</p>
<p>看完上面的源码，我们实际上还发现了一些更奇怪的逻辑，于是可以给出一段十分诡异的代码：</p>
<p>这里输出的内容又是什么样的呢？读者可以根据上面的实现代码猜猜看。</p>
<p>2.variable_scope</p>
<p>实际上，variable_scope和name_scope是两个有点相互独立的命名体系，name_scope只对通过Variable创建的变量有效，而variable_scope还可以为get_variable这个可以复用的变量的方法服务。那么variable_scope是怎么做到的呢？</p>
<p>我们从 tensorflow/tensorflow/python/ops/variable_scope.py<br>文件中找到对应的方法，文件中最核心的一段代码是这样的：</p>
<p>可以看出，它首先调用了name_scope方法，然后在name_scope的context中调用了另外一个生成variable_scope的方法。将这个方法简化，它完成的主要工作如代码和注释所示：</p>
<p>如注释所示，我们将整体过程分成五步，第1步的主要方法为get_variable_scope，它的代码如下：</p>
<p>它的功能其实就是创建一个全局的var_scope，并确保collection中有内容。实际上，这里只有一个元素，因为collection设计成存储列表的形式，所以访问元素需要访问0号下标。下一句代码中，这个确定创建的default_varscope就被取出来了。</p>
<p>确保了var_scope的创建，第2步就是访问_get_default_variable_store方法，它的代码如下：</p>
<p>可以看到，这个函数创建了一个var_store对象。它将完成名字回收的工作。</p>
<p>在第3步中，代码要设定新varscope的名字，new_name和当前var_scope的名字有关，但和我们前面看到的name_scope并无关系。第4步，代码将按照这个新名字定义scope的名字，并创建一个VariableScope对象。第5步则是在scope使用完成后回收scope信息。</p>
<p>当完成了context内的操作并退出varscope时，这种栈结构的命名空间模式会将默认的varscope替换为进入方法时原本的varscope。这样，基本的代码过程就结束了。</p>
<p>总体来看，两种scope存在一些相同和不同，让我们来总结一下。</p>
<p>（1）两种scope形成的命名空间将在Variable创建的变量上产生影响。</p>
<p>（2）variable_scope创建的命名空间将对get_variable的变量名产生影响。</p>
<p>（3）创建named_scope时，遇到同名的scope，系统会自动改名创建一个新的scope。</p>
<p>（4）创建named_scope时，可以通过将名字设置为None抹掉前面设定的所有命名空间的名字。</p>
<p>希望读者能够了解scope为命名带来的便利和复杂。</p>
<h4 id="4-2-2-创建一个Variable背后的故事"><a href="#4-2-2-创建一个Variable背后的故事" class="headerlink" title="4.2.2 创建一个Variable背后的故事"></a>4.2.2 创建一个Variable背后的故事</h4><p>4.2.1节介绍了scope的细节，本节介绍创建变量需要关注的内容。我们已经知道了两种创建变量的方法，它们究竟创造了些什么呢？我们同样从几段代码中寻找线索。</p>
<p>1.tf.Variable()</p>
<p>先来看第一段代码：</p>
<p>在这段代码中，我们用Variable方法创建了一个变量，并获取了系统的默认计算图，将计算图中的操作输出，它会输出什么呢？看上去我们只创建了一个变量，在默认的计算图上应该只有一个变量的相关操作吧？实际上并不是这样，输出结果如下所示：</p>
<p>我们发现系统创建了四个操作——除了一个和变量名相同但类型为“VariableV2”的操作外，还有三个操作：Const、Assign和Indentiy。这些操作的输入和输出又是什么呢？</p>
<p>我们将四个操作（Operation，以下简称Op）的定义输出，可以看到更多细节：</p>
<p>对照着输出的结果，我们再来看tensorflow/python/ops/variables.py中创建变量的缩减版代码和注释就很清楚了：</p>
<p>如代码中的注释所示，创建操作的工作就是在这里完成的，整个过程分为四步。</p>
<p>（1）将初始值变为一个Tensor。</p>
<p>（2）创建一个变量。</p>
<p>（3）将初始值赋值给变量。</p>
<p>（4）将变量的值读取出来，变成可以被后续操作使用的对象。</p>
<p>当我们想进一步追踪这些方法背后的调用时，会发现这些方法基本被指向了一些C++的代码，由于C++里面的内容会更复杂，这里我们暂时不去触碰这些代码。通过这段代码分析我们发现，创建一个变量的工作是由好几个操作完成的，这能够帮助我们更好地理解Python代码和真实计算图之间的关系。</p>
<p>2.tf.get_variable()</p>
<p>看完了第一种创建变量的方法，我们再来看另一种创建变量的方法，对应的代码为：</p>
<p>它会不会和前面一样产生四个Op呢？经过执行发现，实际上它得到了更多的Op：</p>
<p>从名字上看，方法生成的Op总体上和上一个方法类似，只不过它不是直接用数值初始化，而是使用一个初始化的方法，因此我们需要设定这个初始化方法的参数。下面需要关注的是变量的正则项：</p>
<p>从这部分内容可以看出，如果在创建变量时设置了正则项，那么正则项将被加入collection中，我们可以根据需要将其取出并加入loss中。</p>
<p>经过分析我们发现，虽然我们在创建变量时只写了一句代码，背后却产生了很多更细粒度的Op，所以在计算图执行时，我们要明白这些Op的来源。</p>
<h4 id="4-2-3-运算操作"><a href="#4-2-3-运算操作" class="headerlink" title="4.2.3 运算操作"></a>4.2.3 运算操作</h4><p>从上面的代码中我们已经看出，在创建变量时，我们的默认Graph实际上创建了许多Op，这些Op组成了整个计算图。执行计算图就是执行其中的Op。那么每一个Op又包含什么属性呢？在探索更复杂的内容之前，我们先来看看组成Graph的基石——Op更多的细节。</p>
<p>我们来看一段简单的代码：</p>
<p>代码对应的输出为：</p>
<p>当我们完成了一个加法运算的声明，图中就多出了这个“add”Op。这个Op拥有自己的op_def方法，显示出来是这样的：</p>
<p>从这里可以看到有关Op的所有信息。我们还可以获得有关它的输入和输出的定义：</p>
<p>可以看出，虽然在代码中相加之后赋值的变量为c，但是在计算图中，它把相加输出的值叫作“add：0”。如果读者不了解这个原理，那么看到这个名字也许会发懵。这也说明在平时编程时，为每一个Op起名的重要性。如果将中间的代码写成：</p>
<p>那么它的输出就会变成：</p>
<p>虽然它不叫作c，但至少与c有关，我们可以比较方便地查找定位它。如果我们想从计算图中找到这个代表输出的Tensor，可以采用下面的方法：</p>
<p>通过上面的分析，我们发现代码和计算图存在一定的不同，为了编程的方便，一些繁杂的设定被系统以默认值替代，但有时会为我们带来麻烦，如果前面的加法出现错误（例如参数运算的Tensor维度不一致），那么’add：0’一定不如’c：0’容易定位，所以要尽可能地为每一个Op起好名字。</p>
<h4 id="4-2-4-tf-gradients"><a href="#4-2-4-tf-gradients" class="headerlink" title="4.2.4 tf.gradients"></a>4.2.4 tf.gradients</h4><p>前面我们介绍了与变量、操作相关的一些知识，下面我们介绍关于反向计算的一些细节。在介绍神经网络时，我们一直提到“前向计算”和“反向计算”这两个词，这容易给读者留下一个印象：前向计算和后向计算的计算是在同一批计算节点上完成的，只不过计算的方向不同。但实际上真的如此吗？如果不是这样，那么反向计算的结构是什么样的呢？我们来看看下面的代码：</p>
<p>这段代码的输出结果为：</p>
<p>经过梯度计算，我们增加了许多新的Op。这些Op是用来做什么的？它们之间是什么关系？因为涉及的节点比较少，所以可以用TensorBoard将计算图画出来。执行下面的代码：</p>
<p>这样计算图就会绘制在我们指定的目录logs中。再利用TensorBoard显示绘制的信息：</p>
<p>就可以从显示的页面中看到如图4-2所示的图像。</p>
<p>图4-2 代码对应的计算图</p>
<p>我们可以看到图4-2由两部分组成，右边是一个很小的图，这部分是前向计算的图，左边是一个很大的图，外层的名字为gradients，内层的名字为add_grad，add_grad中的内容就是右边add操作的反向计算部分。那么，这么庞大的内容是如何生成的呢？与这部分内容有关的代码在/tensorflow/tensorflow/python/ops/gradients_impl.py中，下面就来看看它的基本流程，代码和注释如下所示：</p>
<p>函数的输入分别为代求导的输出值 _ys_ 和输入值 _xs_ ，对于上面的这段代码，我们可以将整个构建梯度计算图的过程分解成下面9个步骤。</p>
<p>第1步（Step 1）：为每个求导的 _y_<br>赋值一个默认导数，这段代码将在Graph中加入’gradients/Fill’这个Op，也就是默认的梯度值1；同时，将反向计算求导的起点和终点的Op保存下来，其中to_ops保存了起点，也就是<br>_ys_ 中的Op，from_ops保存了终点，也就是 _xs_ 。反向计算就是从to_ops出发计算梯度，直到我们将所有from_ops的梯度计算完成。</p>
<p>第2步（Step<br>2）：根据计算图的结构，从to_ops方向向from_ops计算pending_count数组。这个数组记录了Graph中每一个节点在计算梯度时依赖的节点数目，一个Op有多少个输出的Op，就有多少个依赖。当pending_count中节点对应的值为0时，说明它依赖的前置梯度已经计算完成，可以计算它的梯度了。</p>
<p>第3步（Step 3）：将现在的梯度保存在一个用于存储Op与对应梯度Op关系的map中，这个map被称为grads。</p>
<p>第4步（Step 4）：遍历to_ops，将可以进行梯度计算的Op（pending_count=0）放入队列（queue）中。</p>
<p>第5步（Step<br>5）：从queue中取出Op，从TensorFlow的系统中找到对应的反向Op。TensorFlow会事先记录好一个Op和对应的反向Op的关系，使用时只需要按名字查找即可。</p>
<p>第6步（Step 6）：如果找到了反向Op，那么执行编译工作将生成反向Op；如果没有，则返回None。</p>
<p>第7步（Step 7）：将第6步新生成的Op保存到grads中。</p>
<p>第8步（Step 8）：更新pending_count，将更多满足求导条件的Op放入queue中。</p>
<p>第9步（Step 9）：将待求的Tensor的梯度Op从grads中查找出来，作为返回。</p>
<p>在这些步骤中，第2步的过程相对复杂，也比较重要，它基于计算图构建了后面几步工作的基础：每一个Op计算梯度时的依赖数——pending_count。这一步需要对计算图进行多次遍历，对应的简化版代码如下所示：</p>
<p>上面的代码看得有些复杂，我们来举一个例子看看它的计算方法。假设有图4-3所示的计算图，其中包含每个节点被访问的情况，待求的是to_ops和from_ops。</p>
<p>第2.1步，我们将to_ops中的节点标记为reached，也就是说它已经被访问了，如图4-4所示。</p>
<p>第2.2步，从from_op出发，将正向的节点标记为reached，如图4-5所示。</p>
<p>第2.3步，从to_ops出发，反向遍历，将节点由reached标记改为between标记，如图4-6所示。</p>
<p>图4-3 计算图的基本结构</p>
<p>图4-4 第2.1步：计算后的结果</p>
<p>图4-5 第2.2步：计算后的结果</p>
<p>图4-6 第2.3步：计算后的结果</p>
<p>第2.4步，遍历由between标记的节点，记录pending_count，对于前面所举的例子来说，它的数值为：</p>
<p>了解了这部分的功能，我们来看最终得到的grads变量的内容为：</p>
<p>我们将这里的Op和图4-2中的名字进行对应，就可以更深入地了解反向计算图的构造。</p>
<p>通过对这部分源码的分析，相信大家对TensorFlow构建反向计算有了一定的了解，可以看出反向计算比前向计算更复杂，也容易出现问题，因此了解一点细节对读者大有裨益。</p>
<h4 id="4-2-5-Optimizer"><a href="#4-2-5-Optimizer" class="headerlink" title="4.2.5 Optimizer"></a>4.2.5 Optimizer</h4><p>4.2.4节我们了解了反向计算的细节，本节介绍优化部分计算图的构建过程。我们来看一个简单的例子：</p>
<p>上面代码的三句输出分别输出了下面的内容：</p>
<p>（这里省略相同的内容……）</p>
<p>（这里省略相同的内容……）</p>
<p>最终生成的计算图如图4-7所示。</p>
<p>图4-7 代码对应的计算图</p>
<p>从图4-7中可以看出，除了前面已经知道的用于反向计算的部分外，Optimizer还为计算图增加了一部分。这部分增加的内容由apply_gradients函数生成，它的主要结构如下：</p>
<p>这部分代码其实比较简单，如代码中的注释一样，可以分为如下3个步骤。</p>
<p>（1）为每一个变量找到对应的processor。实际上变量有很多种类型，它们的处理方法有所不同，因此我们需要用不同的方法对不同类型的变量进行更新。TensorFlow为每一类变量实现了一个processor，将一些必要的功能封装在其中，这使得代码更清晰。</p>
<p>（2）创建slots。在一些优化算法中，每个变量都有一些额外的信息需要保存，例如基于动量的方法，我们要记录每一个变量的累积梯度。其他的优化算法也同样如此，在这里我们可以为它们创建了一些专为优化私用的变量，这些变量被称为slots。这些slots和普通的变量不同，它们不需要计算梯度，而且更新的操作由优化方法自行定义。</p>
<p>（3）获得processor后，我们就可以用processor创建更新参数的计算Op。</p>
<p>这里值得一提的是第2步：创建slots。我们以Adam算法为例介绍如何实现它。这个算法需要保存一些参数变量，如 _β_ 1 和 _β_ 2<br>，还需要保存和动量功能类似的变量，如 _m_ 和 _t_ 。它的主要优化流程为</p>
<p>可以看出，为了算法可以执行，优化器中需要维护这些积累变量，而它们就要保存在slots中。Adam的代码在tensorflow/python/training/adam.py中，其中就包含了创建slots的代码。</p>
<p>可以看到，代码中一共创建了4个变量，其中两个是beta1_power和beta2_power，它们保存了和这两个变量的值，另外两个是 _m_ 和 _v_<br>，它们被保存为slots值。</p>
<p>介绍完slots的创建过程，我们可以通过访问计算图中的变量集合找到优化器为我们添加的变量。</p>
<p>从结果可以看出，虽然我们只创建了一个变量，但为了优化，系统又为我们创建了4个变量，其中最后两个变量分别是 _m_ 和 _v_<br>，但实际上，可以训练的变量只有我们创建的 _a_ 。</p>
<h4 id="4-2-6-TensorFlow的反向传播技巧"><a href="#4-2-6-TensorFlow的反向传播技巧" class="headerlink" title="4.2.6 TensorFlow的反向传播技巧"></a>4.2.6 TensorFlow的反向传播技巧</h4><p>TensorFlow 的反向传播可以根据前向运算自动计算得到，这也是 TensorFlow<br>的一个特点，也是很多计算图引擎的特点。但是这部分完全透明的内容并不像我们想象的那样简单，接下来笔者就介绍由论文 _Shake-Shake<br>regularization of 3-branch residual networks_<br>的实现引出的一个问题：如何使用TensorFlow在反向传播的过程中修改某个变量的数值，从而修改最终的梯度？</p>
<p>我们将其转变成一个简单的问题，令函数 _f_ （ _x_ ）= _ax_ ，其中 _a_ 是一个0～1的随机数，当 _a_ 固定不变且 _x_<br>=1时，的值是相等的。现在我们希望计算图在分别进行前向和后向计算时， _a_<br>能够分别进行一次随机采样，得到两个不同的数，并使得的最终结果不一样。也就是说，计算图执行的顺序是：</p>
<p>（1）随机采样得到 _a_ 。</p>
<p>（2）前向计算得到 _f_ （ _x_ ）。</p>
<p>（3）随机采样得到新的 _a_ 。</p>
<p>（4）反向计算得到。</p>
<p>那么这个过程该如何实现呢？我们先完成如下代码：</p>
<p>直接执行这段代码，我们发现 _f_ 和 _g_<br>的输出结果是一样的，这并没有达到我们想要的效果。那么，有没有别的方法可以实现呢？我们知道TensorFlow中有tf.control_dependencies方法，我们能不能用它来控制求导的顺序，在反向计算前强制执行一次随机赋值呢？于是上面代码的核心部分变为：</p>
<p>修改后的效果如何呢？感兴趣的读者可以尝试，实际效果是：两个数字仍然相同。</p>
<p>这是为什么呢？实际上在TensorFlow求导时，除了函数输出部分的导数值可以通过传入参数修改（也就是方法中的grad_ys参数，默认为1），其他部分Tensor和Variable的数值已经被确定并保存，对它们的修改不会影响最终梯度的计算，它还是会以前向计算时的数值进行计算。这使得我们无法以常规手段对变量进行修改。</p>
<p>对于这样的问题，我们有什么好的解决方法呢？接下来介绍两个方法。</p>
<p>方法1：stop_gradient技巧</p>
<p>tf.stop_gradient方法用于告诉计算图，方法中的变量将不参与梯度计算。在反向计算时，它的梯度将直接设为0。这个特性可以帮助我们实现上面的工作。具体的方法是：我们创建两个随机数<br>_a_ 1 和 _a_ 2 ，让前向计算时的数值等于 _a_ 1 _x_ ，让反向计算时的数值等于 _a_ 2 _x_<br>，两者的差距则放入stop_gradient方法中。具体方法如下：</p>
<p>我们来分析这关键的一句，在前向计算时，我们不需要考虑tf.stop_gradient的效果，因此前向计算的结果为 _f_ = _f_ 2+ _f_ 1<br>_-f_ 2= _f_ 1；而在反向计算时，tf.stop_gradient中的内容没有梯度，所以相当于只对 _f_<br>2进行求导，这样我们就实现了想要的效果，但是并没有改变前后向计算的机制。读者可以尝试一下，此时 _f_ 和 _g_ 输出的结果将变得不同。</p>
<p>方法2：注册梯度函数</p>
<p>与stop_gradient方法不同，笔者将要介绍的方法可能显得更优雅，当然它也会带来一定的额外工作。在这个方法中，我们并不使用TensorFlow中的原生梯度函数，而是使用一个自己定制的梯度函数。前面例子中的运算实际上就是一个标量乘法，我们来写一个替换标量运算的梯度函数：</p>
<p>这个函数实现了乘法的梯度运算，它的输入分别为待求梯度的Op和Op输出的梯度值。对于函数 _f_ （ _x_ ）= _ax_<br>，我们的函数应该返回两个偏导数：，但是因为第2个参数需要进行一次随机，因此我们在方法中实现了这个随机。</p>
<p>然后就可以进行下面的运算：</p>
<p>在代码的第4行，我们要调用计算图的方法，将Op原本的梯度计算覆盖，换成我们想要的计算方法，这样就可以达到我们想要的效果。读者可以自行运行，查看结果是否符合预期。</p>
<p>通过对比可以看出，第1种方法实现更简单，但是逻辑上有些绕，需要读者厘清思路，同时它会增加一些额外的计算量；第2种方法从逻辑上更清晰，但是需要重写反向传播函数，这里存在着一定的风险，例如函数存在Bug。当读者有这样的需求时，可以自行选择其中任意一种方法解决这个问题。虽然TensorFlow留下了解决这个问题的方法，但两种方法都不够直观，这也算是TensorFlow设计上的一点缺陷。</p>
<h4 id="4-2-7-arg-scope的使用"><a href="#4-2-7-arg-scope的使用" class="headerlink" title="4.2.7 arg_scope的使用"></a>4.2.7 arg_scope的使用</h4><p>TensorFlow大而全的设计使我们能够实现更多的模型，但与此同时它也无法像一些框架那样保持小而美的优雅代码。我们一定很厌烦使用TensorFlow原生的卷积层方法，因为它的参数实在有些长：</p>
<p>我们更厌烦的是在一个模型里把这样的语句反复写多遍：</p>
<p>于是为了减少重复劳动，封装模型和默认参数就成了我们必须要做的事情。为了解决这个问题，很多优秀的第三方接口库随之诞生，arg_scope就是其中之一，它被包含在TensorFlow的contrib文件夹中。使用这个功能，我们只需要定义一个关于参数的上下文空间，并在其中定义默认参数的赋值。对于空间内的语句，我们不需要对那些拥有相同赋值的参数再次赋值，参数空间会为我们自动完成这个工作。上面的代码用arg_scope的方式进行重写，就可以得到：</p>
<p>从代码中可以看出，相同赋值的参数被定义在arg_scope的定义中，这里所有语句的参数将被自动赋值。通过这个改变，代码变得清爽了。因为那些不变的参数只需要定义一次，所以我们只需关注那些经常变化的参数即可。</p>
<p>其实实现这样的功能并不复杂，这种参数共享的实现用到了contextlib库和装饰器这样的设计模式，我们可以看看这个框架的实现方式。从思路上看，整个框架需要完成以下两个主要工作。</p>
<p>（1）在定义时传入一些设定好的共享参数，并将其进行保存，必要时还可以实现参数的嵌套覆盖等机制，这部分工作主要由contextlib完成。</p>
<p>（2）在函数被调用时，将设定的默认参数传入函数中。这部分工作主要由装饰器完成。</p>
<p>接下来，我们就来介绍这两个工作。这部分的代码在tensorflow/contrib/framework/python/ops/arg_scope.py中。</p>
<p>1.contextlib部分</p>
<p>contextlib是Python提供的帮助用户管理环境信息的库，我们可以通过它来实现一些控制部分代码块的功能。假设我们想实现一个基于代码块的计时器，可以这样定义：</p>
<p>代码的第7行是一个yield（）函数，执行这个函数时，程序将暂时离开这个context函数，继续执行context内我们定义的代码，等到context内的代码执行完毕，再回到这个函数，继续执行下面的第8行和第9行代码。定义完这个计时器后，我们只要在对应的代码块上调用它即可：</p>
<p>通过这种方法，我们以相对简洁的方式在代码块内插入了一段代码，同时不会造成代码混乱。利用这个方法，我们可以创建一个上下文对象（Context<br>Object），用于存储要共享的参数。这部分参数将被保存在文件的_ARGSTACK变量中，这是一个全局变量。考虑到嵌套arg_scope的需要，变量采用栈结构实现，内层的arg_scope的优先级将高于外层的arg_scope。</p>
<p>每当一个arg_scope被声明出来，最近保存的共享参数集合就会被取出来，新的参数将会和旧的参数融合。如果有相同的参数，那么新的参数将覆盖旧的参数。这样一份新的共享参数集合就诞生了，我们将其保存到_ARGSTACK的栈顶，如果这个context结束，就把这份参数集合从栈中弹出删除。</p>
<p>这部分的核心代码如下：</p>
<p>通过使用contextlib，我们已经完成了默认参数的存储工作。虽然参数已经保存好，但是我们仍然希望函数在调用时能够利用这些参数，为此我们需要另一部分功能的帮助。</p>
<p>2.装饰器部分</p>
<p>装饰器也是Python中的一个功能，每个函数通过装饰器的封装，可以在不改变函数代码的情况下为它增加功能。例如，同样是实现一个拥有计时功能的装饰器：</p>
<p>定义完成后，只要在函数前面加上与上面定义的函数名同名的注解（Annotation）即可，代码如下：</p>
<p>这样在执行时，就可以看到程序被执行的效果，并保证代码的整洁和可读性。</p>
<p>看到这里，读者可能会想，上面介绍的两个功能都是为既有的代码注入一些新功能的代码，这两者有什么区别呢？contextlib是为一段代码的环境注入代码，而装饰器是为一个函数注入代码，它们注入的目标不同。contextlib的功能是保存默认参数，所以装饰器的功能就是把注入的参数传入制定的函数中。</p>
<p>所以每一个被arg_scope包围的函数，如果想实现自动传入默认参数的功能，都需要一个装饰器辅助，装饰器可以确保函数在执行前先执行装饰器的代码。装饰器会在调用函数前从_ARGSTACK中把参数取出来，和函数本身的参数进行融合，再一同放入调用的函数中，这样就完成了参数共享的工作。对应的代码如下所示：</p>
<p>在使用时，需要对TensorFlow原生的函数做一定的封装，这样就可以直接在arg_scope中使用它了。我们可以使用一个小例子展示它的效果。具体的代码如下所示：</p>
<p>上面这段代码的输出结果为：</p>
<p>可以看到arg_scope中设定的参数被传入了函数中。以上就是arg_scope的原理及应用，相信这个功能能够帮助读者节省不少写代码的时间。</p>
<p>至此，我们完成了对 TensorFlow<br>部分实现原理的介绍，实际上本章介绍的只是TensorFlow的冰山一角，真正复杂的部分并未涉及，对原理强烈好奇的读者可以深入学习。</p>
<h3 id="4-3-TensorFlow的分布式训练"><a href="#4-3-TensorFlow的分布式训练" class="headerlink" title="4.3 TensorFlow的分布式训练"></a>4.3 TensorFlow的分布式训练</h3><p>本节介绍使用TensorFlow进行分布式训练的方法。虽然GPU及其他专用处理器的出现使计算效率有了惊人的提升，但我们对模型的野心是不断膨胀的。面对不断增长的数据，越来越复杂的模型，单机单GPU训练已经变得力不从心，单机多GPU训练也显得有些乏力，于是我们开始追求更高的目标：分布式训练。分布式训练更有利于运算资源扩展，通过若干台机器的协作，计算效率可以达到接近线性效果的增长，这极大地减少了运算时间。</p>
<p>图4-8所示为分布式同步训练的整体流程。首先，分布在不同机器上的进程（pro-<br>cess）完成了必要的通信，彼此确认了对方的存在；然后，所有机器完成参数初始化的同步，所有进程拥有相同的参数；此后在每一轮训练过程中，每一个进程分别完成自己的前向计算（forward）和后向计算（backward），并得到每一个参数的更新量；接着所有的进程进行同步（All-<br>Reduce），对所有的参数更新量求平均，得到基于所有进程训练数据的平均参数更新量；最后，每个进程将获得的更新量应用到自己的参数上，此时每一个进程的参数享受所有进程的计算，同时保持了参数的一致，然后进行下一轮的训练计算。</p>
<p>图4-8 分布式训练的示意图</p>
<p>从计算流程来看，分布式计算和单机多GPU计算没有太大的区别，主要的区别在于参与同步的数据没有在同一台机器上。原本我们在一台机器上只有4个或者8个GPU，现在通过通信设施，可以将更多GPU组织在一起。</p>
<p>在论文 _Accurate，Large Minibatch SGD：Training ImageNet in 1 Hour_<br>中，作者介绍了团队是如何利用分布式集群完成模型的快速学习的。除了介绍很多训练过程中应该注意的技巧外，作者还介绍了自己使用的分布式架构。文章中包含很多有实用价值的技巧，因这部分知识和本书的关系并不密切，故不展开，对这些内容感兴趣的读者可以自行阅读。</p>
<p>接下来介绍如何实现分布式训练模型。在此之前，笔者先介绍分布式同步训练的关键技术之一：MPI协议及其操作。</p>
<h4 id="4-3-1-基于MPI的数据并行模型"><a href="#4-3-1-基于MPI的数据并行模型" class="headerlink" title="4.3.1 基于MPI的数据并行模型"></a>4.3.1 基于MPI的数据并行模型</h4><p>MPI的全称是Message Passing<br>Interface，它是一套十分经典的解决多进程通信问题的框架，框架中定义了一些最基本的多进程通信协议，并定义了这些协议的详细参数和调用方式，使我们在一些问题上可以直接使用这套框架解决问题。</p>
<p>前面我们提到，分布式同步训练需要不同进程间相互通信，这里涉及很多通信的细节：要用什么样的方式进行通信？TCP、InfiniBand，还是其他？采用包含中心节点的架构进行通信，还是采用无中心的方法？节点间的关系如何表达？进程出现错误该怎么办？这些琐碎的问题包含了很多技术细节，这些细节很可能不是一个研究机器学习的人擅长的。于是有前辈提出了一套方案：用统一的接口定义进程间通信的行为，至于通信的细节则由专业人士开发完成，使用者无须精通。MPI就是这样的一套接口。</p>
<p>在使用MPI的接口时，我们需要遵循一定的基本定义。使用MPI框架时，所有进程将运行相同的程序。程序开始时，所有进程将通过通信组成一个Group，Group内的进程彼此知晓对方，能够协同工作。每一个进程可以得到两个用于协同的数值。</p>
<p>（1）size：表示Group内总的进程数。</p>
<p>（2）rank：表示自己在Group中的序号。MPI的实现会为Group中的每一个进程提供唯一的rank值。</p>
<p>作为应用的开发者，我们可以通过rank为每一个进程安排不同的工作，从而完成通信与协作。MPI最基础的操作就是send和receive。</p>
<p>（1）send：把数据从一个进程发送到另一个进程。</p>
<p>（2）receive：等待接收其他进程发来的数据。</p>
<p>基于上面提到的两个基本操作，我们实现了MPI中的一些经典操作，下面就来看看这些操作。</p>
<p>1.Scatter</p>
<p>Scatter操作用于将一个进程的数据发送到同一环境的所有进程中。数据发送的过程并不是将一份数据同时复制到多台机器中，而是将数据切成 _N_<br>份，每一个进程收到一份数据。</p>
<p>我们假设有4个进程： _A_ 、 _B_ 、 _C_ 和 _D_ 。数据发起者为进程 _A_ ，接收者为 _A_ 、 _B_ 、 _C_ 和 _D_<br>。启动前，所有的数据都在进程 _A_ ，因此待传输的数据被分成1、2、3、4共4份，保存在进程 _A_<br>中，操作完成后，4份数据将按顺序分别传输到4个进程中。操作的具体流程如图4-9所示。</p>
<p>2.Gather</p>
<p>Gather操作用于将多个进程的数据聚集到单一进程中。Gather操作可以理解为Scat-ter操作的逆操作。假设有4个进程： _A_ 、 _B_ 、<br>_C_ 和 _D_ 。数据发起者为进程 _A_ 、 _B_ 、 _C_ 和 _D_ ，接收者为进程 _A_<br>。启动前每个进程拥有1份数据，操作过程中每个进程都将自己的数据发送到进程 _A_ ，操作完成后，4份数据将按照进程的顺序放置到进程 _A_<br>中。操作的具体流程如图4-10所示。</p>
<p>图4-9 Scatter操作的示意图</p>
<p>图4-10 Gather操作的示意图</p>
<p>3.Allgather</p>
<p>Allgather操作将gather应用到所有的进程，gather操作中只有一个进程收到所有进程发来的数据，而对于Allgather操作，所有的进程都会收到这些数据，操作流程如图4-11所示。</p>
<p>4.MPI_Bcast</p>
<p>这个操作名是广播（Broadcast）的缩写，用于将一个进程的数据发送给所有的进程。与Scatter不同的是，Bcast并不将数据分成 _N_<br>份，而是将所有的数据发出，操作流程如图4-12所示。</p>
<p>图4-11 Allgather操作的示意图</p>
<p>图4-12 Bcast操作的示意图</p>
<p>5.MPI_Reduce</p>
<p>这个操作类似Gather，将所有进程的数据汇聚到单一进程中。与Gather不同的是，Reduce操作将对这些数据进行某种操作，例如求和、求最大值等，操作流程如图4-13所示。</p>
<p>6.MPI_Allreduce</p>
<p>这个操作从名字来看显然和Reduce有关。就像Gather和Allgather的关系，Reduce和All-<br>Reduce也只是在接收数据的数量上有所区别。这个操作是分布式同步优化最关键的操作，操作流程如图4-14所示。</p>
<p>图4-13 Reduce操作的示意图</p>
<p>图4-14 All-Reduce操作的示意图</p>
<p>看完了上面对这些经典的通信模型的介绍，读者一定会想，这些操作能用到什么地方？它们是如何实现的呢？笔者先来回答第一个问题。在分布式同步训练中，我们主要用到两个操作：MPI_Bcast和MPI_Allreduce。MPI_Bcast用于同步所有进程的初始化参数，由于每一个进程都可以对参数进行初始化，它们对参数的初始化效果是可以等价的，所以可以选择其中一个进程作为发送方，将其参数值发送到其他进程上，这样参数的初始化同步就完成了。MPI_Allreduce用在参数更新量的求和上，我们将所有参数聚集起来，并对其进行求和，最后将这些结果发送到每一个进程上。</p>
<p>第二个问题涉及对MPI算法细节的了解。笔者主要介绍在训练中出现频率最高的All-<br>Reduce方法。从它的工作原理中我们可以看出整个流程涉及数据的聚集和分发，其中聚集部分一般被称为Scatter-<br>Reduce，也就是说每一个进程会完成部分数据的聚集和计算；而分发部分就是All-<br>Gather操作，将每一个小部分的计算结果发送到所有的进程中，详细过程如图4-15所示。</p>
<p>从图4-15可以看出，在计算前每一个进程会根据Group的数量将待通信数据划分成相同数量的部分，如果Group内有4个进程，那么每个进程都要把数据切成4份。由于计算时每一个位置数据的计算是独立的，因此切分后的数据可以独立计算。为什么要采用这样的计算流程，把计算分散到每一个进程，而不是把所有的数据聚集到一个进程中呢？把所有的数据聚集到一个进程中当然是可以的，但会导致每个进程的计算量不同，有些机器（例如聚集的机器）耗时较长，有些机器则处于闲置。为此我们需要把工作量尽可能地分摊到所有的机器上。</p>
<p>图4-15 All-Reduce的计算流程</p>
<p>上面只介绍了两个步骤执行操作后的结果，并没有介绍计算过程，下面将重点介绍Scatter-Reduce部分的计算方法，All-Gather的计算与此类似。</p>
<p>第一种方法被称为Ring Method。之所以叫这样的名字，是因为方法将所有进程的通信的拓扑结构想象成环状，如果Group中有4个进程，分别被称为 _A_<br>、 _B_ 、 _C_ 和 _D_<br>，每个进程的数据将分成4份，分别为Segment1、Segment2、Segment3和Segment4。它们的组织结构如图4-16所示。</p>
<p>图4-16 Ring Method的进程拓扑结构</p>
<p>从图4-16中可以看出，基于这样的拓扑结构，对于进程 _i_ 来说，它只能和进程 _i_ +1和进程 _i_ -1进行通信，一般来说它将数据发送到进程<br>_i_ +1，并接收来自进程 _i_ -1的数据。从数据片段的角度看，每个片段需要穿越这条环线将数据聚集，因此整个过程将持续 _N_<br>-1个循环。对有4个进程进行通信的场景，在第一轮循环中，每个数据片段的传输流程如下。</p>
<p>在第二轮循环中，每个数据片段的传输流程如下。</p>
<p>最后一轮循环中，每个片段的数据再遵循类似的方式传输数据，就可以完成通信的全过程，以上过程如图4-17所示。</p>
<p>Ring Method的时间和进程数量有关。进程越多，时间越长，这是Ring<br>Method方法的短板，但它的优点是对Group的组织形式没有特别要求，数据传输时对带宽压力也相对较小。</p>
<p>第二种方法被称为Recursive Halving&amp;Doubling Method。这个算法在循环轮数上比Ring Method少，但是要求进程的总数为2的<br>_N_ 次方，这样才能获得最好的效果。在这个方法中，进程的通信拓扑结构也有所不同，我们要求每一个进程和 _N_<br>个进程进行通信，同时对进程的通信量需求也更大。对应的拓扑结构如图4-18所示。</p>
<p>这个方法共执行 _N_<br>轮循环，每一轮循环的数据通信方式都不相同。我们仍以四个进程A、B、C、D为例，看看它的通信方式。在第一轮循环中，相邻的两个进程互相交换数据，进程0和进程1互换，进程2和进程3互换，每一个进程都将自己一半的数据发送给对方，同时收集对方一半的数据；在第二轮循环中，距离为2的两个进程相互交换数据，进程0和进程2互换，进程1和进程3互换，每一个进程都将自己四分之一的数据发送给对方，同时收集对方一半的数据，整个流程如图4-19所示。</p>
<p>在Recursive Halving&amp;Doubling<br>Method中，由于前面几轮数据传输量比较大，系统的整体带宽存在很大的压力。同时，它对系统内的进程数量有要求，不满足指定数量的线程时，需要对算法进行一定的改变才能使用。虽然这个方法减少了循环次数，但是在实际执行中与不同的进程通信和传输大量数据也会造成一定的时间损耗，因此理论上Recursive<br>Halving&amp;Doubling Method会更快一些，但是实际上有时Ring Method执行得更快。读者需要根据自己的硬件环境情况选择合适的通信方法。</p>
<p>图4-17 Ring Method的执行流程</p>
<p>图4-18 Recursive Halving&amp;Doubling Method的拓扑结构</p>
<p>图4-19 Recursive Halving&amp;Doubling Method的执行流程</p>
<h4 id="4-3-2-MPI的实现：mpi-adam"><a href="#4-3-2-MPI的实现：mpi-adam" class="headerlink" title="4.3.2 MPI的实现：mpi_adam"></a>4.3.2 MPI的实现：mpi_adam</h4><p>下面我们来看MPI在实际代码中的实现，我们从在第5章将介绍的Baselines代码中可以找到它的实现。代码在common/mpi_adam.py中，它的核心代码如下：</p>
<p>从代码中可以看出，在更新参数时，每隔一段时间，系统会检查所有进程的参数是否一致，再进行All-<br>Reduce操作，最后除以进程数量，把更新量赋给参数。这和我们前面提到的流程完全一致。在执行时我们可以使用这样的命令：</p>
<p>在这个命令中，我们使用MPI执行程序，这个Group中有4个进程，其中ip1的机器中有2个进程，ip2的机器中有2个进程。它们的通信方式可以通过其他参数进行设定。</p>
<p>当然，这个方法也存在一定的缺陷，我们需要把本地所有参数的梯度计算出来，再进行All-<br>Reduce操作。这个过程不会造成结果的偏差，但会造成时间的浪费。因为计算完梯度，就相当于进行了一次同步操作，这个计算过程如图4-20所示。</p>
<p>实际上，中间一段梯度的同步工作可以省略，任意一个参数的梯度计算完成后，就可以进行这个参数的同步，所以更好的更新流程如图4-21所示。</p>
<p>至于这种一段式的更新流程，请读者自行思考实现完成，在此不再赘述。</p>
<p>图4-20 两段式梯度更新流程</p>
<p>图4-21 一段式梯度更新流程</p>
<h3 id="4-4-基于TensorFlow实现经典网络结构"><a href="#4-4-基于TensorFlow实现经典网络结构" class="headerlink" title="4.4 基于TensorFlow实现经典网络结构"></a>4.4 基于TensorFlow实现经典网络结构</h3><p>为了让读者更快地掌握TensorFlow的使用方法，本节将展示基于TensorFlow的一些经典网络实现，这些网络包括多层感知器（Multi Layer<br>Perceptron，MLP）、卷积神经网络（Convolutional Neural Network，CNN）和循环神经网络（Recurrent<br>Neural Network，RNN）。</p>
<h4 id="4-4-1-多层感知器"><a href="#4-4-1-多层感知器" class="headerlink" title="4.4.1 多层感知器"></a>4.4.1 多层感知器</h4><p>多层感知器主要由全连接层和非线性激活层组成，全连接层一般由矩阵乘法表示，输入的向量x和参数矩阵W<br>相乘后，得到的结果和向量b相加，就得到了结果z，计算过程如下所示：</p>
<p>由于输出中的每一个值要经过所有输入值的计算，因此这个结构被称为全连接层。非线性激活层一般由线性整流函数（Rectified Linear<br>Unit，ReLU）表示，它可以将输入的负数部分变为0，这样模型便满足了一定的非线性效果。对于一个3层网络组成的感知器，它的结构如图4-22所示。</p>
<p>图4-22 3层感知器的结构图</p>
<p>对应的TensorFlow代码如下所示：</p>
<h4 id="4-4-2-卷积神经网络"><a href="#4-4-2-卷积神经网络" class="headerlink" title="4.4.2 卷积神经网络"></a>4.4.2 卷积神经网络</h4><p>随着近年来的发展，卷积神经网络已经成功应用于很多视觉、语音、自然语言处理的任务上。卷积神经网络的卷积层可以在提取局部信息的同时尽可能地减少计算量。VGG<br>Net是卷积神经网络曾经的代表作，它在网络结构设计上拥有自己的特点，其中一些设计思路也被很多网络沿用至今，它的网络结构如图4-23所示。</p>
<p>图4-23 卷积神经网络结构图</p>
<p>从图4-23中可以看出，整个模型分为6个部分，前5个部分由卷积层组成，最后一个部分由全连接层组成。在每一个卷积部分内，图像的尺度并没有发生变化，只有在进入下一个部分时，图像的长宽依次减半，但与此同时，它的通道数也扩大了一倍，这就相当于图像的信息在不断地集中变少，而对图像分析的方式在不断增多。VGG<br>Net的很多设计也被后来的模型使用。</p>
<p>对应的实现代码如下：</p>
<p>卷积神经网络的世界还有很多优秀的模型结构，例如残差网络等，希望读者能够自行学习了解。</p>
<h4 id="4-4-3-循环神经网络"><a href="#4-4-3-循环神经网络" class="headerlink" title="4.4.3 循环神经网络"></a>4.4.3 循环神经网络</h4><p>在前面介绍的模型中，我们可以通过传入某个固定的输入得到固定的输出，模型并不能对输入数据产生记忆。当我们遇到一些序列预测问题时，这种基于时间或位置关系的问题又需要考虑时间或空间上的依赖关系，因此前面提到的模型不能满足这样的需求，于是我们就需要使用循环神经网络。循环神经网络实现了对序列数据的建模和拟合，当我们使用模型计算出某一输入数据的结构后，部分信息就被记录到模型中，这些信息会在下一个输入计算时产生作用，这些记录的信息被称为隐含层的状态信息。模型的整体结构如图4-24所示，其中x表示模型的输入，h表示模型的隐含状态，o表示每一时刻的输出，而U、V<br>和W 表示模型的参数。图4-24中左边展示的是聚合版本的模型，经过展开后可以得到右边的样子，这样可以清晰地看出时刻之间的依赖关系。</p>
<p>图4-24 循环神经网络原理图</p>
<p>经典的RNN模型在训练时存在梯度消失的问题，为了解决这个问题，前人发明了很多改进版的模型，而LSTM（Long Short Term<br>Memory）是其中最常见的模型，它能很好地解决因为时间跨度长而导致的梯度消失问题，它的基本结构如图4-25所示。</p>
<p>图4-25 LSTM的结构图</p>
<p>图4-25中的变量分成以下几类。</p>
<p>· 输入：x。</p>
<p>· 模型内部的状态：c表示cell的状态值，h表示隐层的状态值。</p>
<p>· 模型内部的三道门及对应的参数变量：输入门i、W （i） 和U （i） ；遗忘门f、W （f） 和U （f） ；输出门o、W （o） 和U （o） 。</p>
<p>这里的计算过程分为三步，第一步计算三个门的值，三个门用于控制各部分信息参与运算的量，对控制记忆和保证梯度传递起到重要的作用：</p>
<p>第二步计算cell的值：</p>
<p>第三步计算隐层的状态值，也是模型的输出值：</p>
<p>由于这部分的计算比较复杂，TensorFlow对其进行了封装，我们只需要简单地定义每一步使用的RNN结构和模型的长度等基本信息，就可以构建出完整的计算结构。对应的代码如下所示：</p>
<p>这部分代码实现了一个根据序列进行单一结果预测的模型，我们只需要最后一个时刻的输出信息，并将其传入全连接层进行处理，得到最终结果，模型对应的原理图如图4-26所示。</p>
<p>读者了解了上面这些网络的实现方式，就能较轻松地完成网络构建相关的操作。</p>
<p>图4-26 示例代码对应的模型图</p>
<h3 id="4-5-总结"><a href="#4-5-总结" class="headerlink" title="4.5 总结"></a>4.5 总结</h3><p>本章介绍了TensorFlow的基本使用方法和一些常见的与训练相关的内容，让我们回顾一下。</p>
<p>（1）TensorFlow的使用主要包括：计算图定义、操作运算地点安排和计算图执行三部分。</p>
<p>（2）TensorFlow内部的一些运行机理很值得读者做一定的研究。</p>
<p>（3）使用MPI的通信方式可以使TensorFlow完成分布式训练。</p>
<p>（4）TensorFlow可以实现常见的各种经典模型。</p>
<h3 id="4-6-参考资料"><a href="#4-6-参考资料" class="headerlink" title="4.6 参考资料"></a>4.6 参考资料</h3><p>[1] <a href="https://www.tensorflow.org/?hl=zh-cn" target="_blank" rel="noopener">https://www.tensorflow.org/?hl=zh-cn</a></p>
<h2 id="5-Gym与Baselines"><a href="#5-Gym与Baselines" class="headerlink" title="5 Gym与Baselines"></a>5 Gym与Baselines</h2><p>第4章介绍了TensorFlow这个十分强大的工具，它可以帮助我们很好地完成机器学习相关的工作。但是对本书主要介绍的强化学习来说，它还欠缺很多。为此，我们还需要寻找其他工具辅助完成TensorFlow没有完成的工作，于是我们找到了Gym和Baselines两个优秀的项目。Gym主要用于生成常见的强化学习模拟环境，有了它我们就可以直接进行交互学习，不必考虑环境中的种种复杂逻辑。Baselines则是一个包含了很多经典强化学习算法的项目，我们可以通过直接使用项目中的代码来感受算法的效果，也可以通过阅读代码了解算法的原理。本书也将介绍项目中大部分的算法。</p>
<h3 id="5-1-Gym"><a href="#5-1-Gym" class="headerlink" title="5.1 Gym"></a>5.1 Gym</h3><p>在本章开始时我们已经介绍了Gym的作用：生成常见的强化学习模拟环境。为了快速地完成学习，一个简单方便的环境模拟器非常重要。在Gym之前，已经有很多优秀的仿真平台被创建，它们也具备不错的易用性，但是Gym具备了自己的特点：使用方便，同时集成了大量的仿真环境。本节就来介绍Gym的基本使用方法。</p>
<h4 id="5-1-1-Gym的安装"><a href="#5-1-1-Gym的安装" class="headerlink" title="5.1.1 Gym的安装"></a>5.1.1 Gym的安装</h4><p>Gym的安装过程并不复杂，执行以下命令就可以完成安装：</p>
<p>或者执行：</p>
<p>完成这一步后，我们只完成了最基础的Gym库安装。如果想要将对应的游戏环境安装好，还需要在Gym的代码目录下执行以下命令：</p>
<p>上面的命令每执行一句，系统就会安装一类游戏。以Atari为例，里面有我们熟悉的空间入侵者（SpaceInvaders）、吃豆人（MsPacman）等经典游戏，也有考验模型探索能力的蒙特祖玛的复仇（Montezuma’s<br>Revenge），如图5-1～图5-3所示。</p>
<p>图5-1 空间入侵者游戏</p>
<p>图5-2 吃豆人游戏</p>
<p>图5-3 蒙特祖玛的复仇</p>
<h4 id="5-1-2-Gym的基本使用方法"><a href="#5-1-2-Gym的基本使用方法" class="headerlink" title="5.1.2 Gym的基本使用方法"></a>5.1.2 Gym的基本使用方法</h4><p>Gym官方给出了它的基本使用方法，还提供了一个最简单但十分“通用”的Agent。在此，笔者将对Gym的基本使用方法和它的架构进行更详细的介绍。</p>
<p>Gym中接触的第一个对象就是环境（简称Env）。它和强化学习概念中的环境类似，模拟了我们交互的环境，同时将一些经常用到的功能封装起来。我们只需要知道游戏的名字就可以创建对应的环境：</p>
<p>env有如下三个核心方法。</p>
<p>（1）reset：重新开始任务，将环境设置成任务初始的状态s0 。</p>
<p>（2）step（a）：这个方法要传入一个参数 _a_<br>，这个参数代表Agent在此时刻将要完成的行动。环境会依照行动模拟生成一系列的信息：下一时刻的观测状态、获得的奖励、此次任务是否已经结束，以及其他信息。</p>
<p>（3）render：输出当前的状态。5.1.1节中的图片都是通过这个函数生成的。</p>
<p>加上Agent的操作，就可以将其组成一个完整的强化学习的交互流程，如图5-4所示。</p>
<p>图5-4 基于Gym的强化学习交互流程</p>
<p>了解了这三个方法，就可以使用它们实现一个最简单的Agent：“佛系”Agent。这个Agent的行动完全随机，它不考虑每一步行动可能带来的影响。基本代码如下所示：</p>
<p>代码的第8行就是这个Agent执行的关键：它从env中获得了问题的行动空间，然后从中进行采样，得到一个随机行动。实际上，本书的主要任务就是将这行代码换成一个更强大的实现，这需要将随机操作换成更强大的模型和训练方法。</p>
<p>1.Gym对空间的定义</p>
<p>实际上，Gym并没有实现太多环境细节，只是将很多的环境集成在自己的框架下。为了统一每一个平台的环境接口，Gym实现了一套关于MDP的基本框架，并将其他平台的接口适配到这套框架下。对每一个任务，我们都需要描述状态所处的空间和行动所处的空间。定义好这两个空间，模型接口的数据传输维度就基本确定了。为了简化操作，Gym中实现了一些常见的空间供用户使用。</p>
<p>Box：Box用于实现连续数据构成的空间，其中包含两组参数：空间内数据的范围（上限和下限），以及空间维度的大小，我们可以这样定义空间的形式：</p>
<p>这样就定义了一个数值在-1～1之间，维度为3×4的空间。这个类型的空间主要使用在Atari游戏和棋盘类游戏状态中。Atari的状态主要是二维的游戏画面，棋盘也是一个二维的数据，这两个空间都很适合用Box表示。</p>
<p>Discrete：这个空间描述了一个Categorical分布的空间，空间由 _N_<br>个离散状态构成，每一个状态之间相互排斥，我们只能从中选择一个状态。它一般被用在一些分类问题上，且主要用于描述行动所在的空间。它的定义形式为：</p>
<p>这其实就是一个Bernoulli分布所在的空间。</p>
<p>2.Wrapper</p>
<p>前面介绍Gym提供了环境最基本的功能，如显示基本的环境，根据行动进入下一个状态等。在现实中，我们有时需要对环境做一些改变，扩展新的功能，这就需要对环境代码做改变。为了更方便地扩展，Gym中定义了Wrapper这个类，它可以在既有环境的基础上添加更多的功能。这个类在创建时需要传入一个env对象，它是一个已经创建好的环境对象，这个对象可能是env本身，也可能是已经被封装过的env类。在执行环境的功能时，Wrapper有两个选择，如果实现了以下画线开始的同名方法，就会调用这个函数，否则就会调用env中对应的函数。以前面提到的“step”功能为例，如果Wrapper内实现了名为“_step”的函数，那么在调用Wrapper的step函数时就会执行“_step”方法，如果没有则执行env的step方法。</p>
<p>我们来看一个官方给出的例子：ObservationWrapper。这个类可以对观测值做额外的处理，例如为任务增加难度、去掉部分的观测值，或者为观测值增加随机噪声等，这个类的核心方法如下所示：</p>
<p>通过实现_observation方法中的内容，我们就可以操纵游戏的观测值，从而对游戏增加难度。后面还会介绍一些与Wrapper相关的实现。</p>
<h4 id="5-1-3-利用Gym框架实现一个经典的棋类游戏：蛇棋"><a href="#5-1-3-利用Gym框架实现一个经典的棋类游戏：蛇棋" class="headerlink" title="5.1.3 利用Gym框架实现一个经典的棋类游戏：蛇棋"></a>5.1.3 利用Gym框架实现一个经典的棋类游戏：蛇棋</h4><p>下面以一个实际问题为例，剖析如何用Gym框架实现一个全新的任务。笔者是一名“85后”，年幼时曾接触过一种被称为蛇棋的棋类游戏。这个游戏是一个棋盘游戏，棋盘上有许多小格子，每个格子代表一个位置，除此之外，游戏还需要几个棋子和一个骰子。棋盘的样子如图5-5所示。</p>
<p>图5-5 蛇棋的棋盘</p>
<p>蛇棋的玩法如下。</p>
<p>（1）玩家每人拥有一个棋子，出发点在图中标为“1”的格子处。</p>
<p>（2）依次掷骰子，根据骰子的点数将自己的棋子向前行进相应的步数。假设笔者的棋子在“1”处，并且投掷出“4”，则笔者的棋子就可以到达“5”的位置。</p>
<p>（3）棋盘上有一些梯子，它的两边与棋盘上的两个格子相连。如果棋子落在其中一个格子上，就会自动走到梯子对应的另一个格子中。以图5-5所示的棋盘为例，如果笔者的棋子在“1”处，并且投掷出“2”，那么棋子将到达“3”处，由于此处有梯子，棋子将直接前进到梯子的另一段——“20”的位置。</p>
<p>（4）最终的目标是到达“100”处，如果在到达时投掷的数字加上当前的位置超过了100，那么棋子将首先到达100，剩余的步数将反向前进。例如，笔者的棋子在“99”处，如果投掷出“3”，则先前进1步到达100，再后退2步，到达“98”处。“98”处恰好还有一个梯子，于是棋子将顺着梯子到达“81”处。</p>
<p>这个游戏是强化学习的一个小案例。蛇棋中的棋盘和游戏规则是既定的，胜利规则也是既定的，游戏中唯一存在变化的就是掷骰子。虽然我们通常认为掷骰子是一个很简单而且随机性很大的操作，但实际上，每个人掷骰子时的状态都不相同，有些人掷骰子时要气沉丹田，有些要大喝一声，有些小心翼翼……因此我们可以认为掷骰子的方式不同，骰子各个面出现的概率就不同。</p>
<p>基于这样的想法，我们认为玩家可以通过使用不同掷骰子的方法来控制游戏。假设玩家的手法只包含两种：一种可以均匀投掷出1～6这6个数字，另一种可以均匀投掷出1～3这3个数字。这样玩家相当于在每一个状态下有两个选择。</p>
<p>接下来看看游戏的最终获胜条件。一般来说，游戏是多人一起玩的，那么谁先到达“100”处，谁就获得了胜利。换句话说，用最少的投掷次数到达“100”处是每一个玩家的目标。为了达到这个目标，玩家最好能更多地“乘坐”梯子上升，以便快速到达终点。一般来说，游戏喜欢用一些数字或得分的形式记录给予玩家回报，这里做一个约定，玩家投掷一次骰子，如果没有到达终点，将获得-1分；到达终点后，将一次性得到100分。这样先到达终点的玩家一定会得到最高分，也就相当于这个人获得了胜利。</p>
<p>相信读者已经清楚游戏规则，下面介绍游戏的具体实现。我们给出下面这段代码：</p>
<p>在这段代码中，构造函数需要传入两个参数：梯子数量和不同投掷骰子方法的最大值。我们用一个dict存储梯子相连的两个格子的关系，用一个list保存可能的骰子的可投掷最大值；reset方法将pos设置为1，也就是游戏开始的位置；step完成一次投掷，参数a表示玩家将采用何种手法。完成位置的更新后，函数将返回玩家的新位置、得分和其他信息。</p>
<p>我们可以利用这个环境进行游戏，设定游戏中共有10个梯子，两种投掷骰子的方法分别可以投掷[1 _，_ 3]和[1 _，_<br>6]的整数值，玩家将完全使用第一种策略进行游戏，那么代码如下：</p>
<p>对应的输出为：</p>
<p>最终玩家完成了一次游戏，在这次游戏中，玩家共进行了294次投掷才到达终点，最终得分-193分。从上面的梯子情况可以看出，有些梯子真的给玩家制造了不小的麻烦：</p>
<p>这些梯子既会让玩家迅速靠近终点，又会让玩家迅速远离终点，一旦玩家多次掉入这种陷阱，游戏时间就会变长。好胜心强的读者肯定在想，刚才的游戏中玩家只使用了一种手法，要是两种手法并用，甚至更多的手法并用，一定能更快地完成游戏。事实是这样吗？第6章和第7章我们将使用强化学习中的基本算法在这个例子上继续尝试。</p>
<h3 id="5-2-Baselines"><a href="#5-2-Baselines" class="headerlink" title="5.2 Baselines"></a>5.2 Baselines</h3><p>Baselines是OpenAI主导的一个开源项目，主要的编程语言为Python<br>3，它实现了当前强化学习界一些经典的算法，并将这些算法和自家的Gym环境结合，形成一个非常好的生态环境。如果说Gym实现了环境相关的功能，那么Baselines就实现了Agent相关的功能。这使得研究人员和学生能够在非常短的时间内体验经典强化学习算法的效果，并了解这些算法的内容。项目的代码结构如图5-6所示，其中包含了子项目对应的算法名称和本书对算法介绍的章节。</p>
<p>由于Baselines中的算法将在本书后面的章节中逐一介绍，因此这里就先略去不谈。本节重点介绍这个项目中的一些辅助功能，其中包括它为Gym和TensorFlow开发的一些辅助功能，也就是通用类代码common中的一部分内容。实际上，前面的章节已经介绍了其中部分内容，在3.3节中，我们介绍了共轭梯度法在代码中的实现，对应的代码在common/cg.py中；在4.3节中，我们介绍了基于MPI协议的优化器，对应的代码在common/mpi_adam.py中。本节将介绍其他比较值得关注的部分。</p>
<p>图5-6 Baselines的代码结构</p>
<h4 id="5-2-1-Baselines中的Python-3新特性"><a href="#5-2-1-Baselines中的Python-3新特性" class="headerlink" title="5.2.1 Baselines中的Python 3新特性"></a>5.2.1 Baselines中的Python 3新特性</h4><p>这个项目比较好地完成了针对Gym环境平台的功能扩展和经典算法的实现。当然，除了为用户带来便利，它对开发环境也做了一定的限制。项目基于Python<br>3开发完成，同时深度集成了TensorFlow的一些特性，这需要读者对这两部分知识有一定的了解。TensorFlow的内容在前面已经介绍过，下面笔者简单介绍这个项目中用到的两个Python<br>3的特性，使用Python 2的读者可以通过下面的内容快速填补对新特性了解的空白。</p>
<p>第一个特性就是在函数参数中加入星号（*），在Python<br>3中，星号有了许多新的作用，它的出现使我们的代码更简洁易读。我们常常会定义带有很多参数的函数，代码如下：</p>
<p>使用这样的函数需要每次填入三个参数，有时是比较麻烦的，于是我们会给其中一些参数赋予默认值，然后代码就变成了这样：</p>
<p>虽然不用填写复杂的参数，但是某一天在阅读代码时，读者可能会突然发现这样的代码：</p>
<p>时间一长，就会记不清楚25是干什么用的。为了解决这个问题，Python 3中要求某些参数必须附加参数名称，增强代码的可读性：</p>
<p>从代码中可以看出两个hello函数的区别，在参数定义时，Python<br>3可以在指定的位置加入一个星号，这样星号后面的参数在传入时必须附加参数的名字，否则就会报错，如果我们采用下面的调用方式，就会报错：</p>
<p>第二个特性同样和星号有关。有时我们会遇到返回值比较多的函数，代码如下所示：def func（）：</p>
<p>我们凭直觉看出返回值分为3类：x、featX和y。如果使用Python 2，则需要先用一个变量接收返回值，然后将返回值拆开：</p>
<p>而在Python 3中，我们只需要写成：</p>
<p>除第一个和最后一个返回值外，中间的所有返回值都会保存到feat变量中，代码和之前相比显得更整洁。</p>
<p>除这两个语法外，Baselines中还使用了其他新语法，但它们都比较直观，容易理解，因此本节不再赘述。</p>
<h4 id="5-2-2-tf-util"><a href="#5-2-2-tf-util" class="headerlink" title="5.2.2 tf_util"></a>5.2.2 tf_util</h4><p>Baselines项目对TensorFlow的一些基本功能做了封装，为了在后面的章节详细介绍其中提供的代码，在此对项目中常见的TensorFlow封装内容做一个介绍。这部分的代码在common/tf_util.py中。</p>
<p>tf_util也对Session管理部分进行了实现。由于强化学习的算法中可能会同时出现多个模型，而且模型的优化和预测过程相互依赖，因此Session无法被某一个模型独占。代码采用全局单例Session的设计方法，将Session对象设计成一个全局对象，统一通过一个方法调用获取：</p>
<p>除此之外，它还实现了初始化、取值等常见的操作，让这个普通的Session操作更简便。</p>
<p>代码中另一个比较有特点的部分是Theano-<br>like函数。TensorFlow的计算图在操作时粒度比较小，在面对比较复杂的模型时，太多细粒度的Op会让代码变得很乱。例如，读者在后续章节中会接触到，一个Off-<br>Policy的Actor Critic的算法可能包含Actor、Critic、Target Actor和Target<br>Critic四个模型，这些模型都包含预测和更新的操作，这些操作交织在一起就会显得比较杂乱。为此，tf_util中实现了这样一个功能，不论模型的定义多么复杂，我们可以将一个模型的输入、输出和更新集合成为一个整体，这个整体被称为函数，未来的交互只要通过这个函数的接口即可。</p>
<p>函数的主要属性如下。</p>
<p>（1）inputs：函数的输入Tensor。</p>
<p>（2）outputs：函数的输出Tensor。</p>
<p>（3）updates：函数的更新Op。</p>
<p>（4）givens：函数输入的默认值。</p>
<p>所以函数的主要执行过程就是：</p>
<p>其中还有一些错误检查的代码，在此略过。我们可以使用它的Function功能完成一个小的示例：可以将4.1节中示例的模型修改成一个函数。在4.1节中实现了基于神经网络完成异或判断的模型，我们可以定义输入为模型的输入，输出为模型的输出和loss值，更新为一轮训练后的参数更新，于是代码可以写作：</p>
<p>只要像调用函数一样调用这个类，就可以执行输出和更新两个工作。以上就是tf_util中的一些方便开发的函数，读者也可以像这样根据自己的需要开发一个常用的工具库，提高自己的工作效率。</p>
<h4 id="5-2-3-对Gym平台的扩展"><a href="#5-2-3-对Gym平台的扩展" class="headerlink" title="5.2.3 对Gym平台的扩展"></a>5.2.3 对Gym平台的扩展</h4><p>Gym平台在集成了众多环境后，并未对每一个环境提供更多深度定制的功能，玩家需要根据自己的情况进行开发。例如对Atari游戏来说，很多论文的实验章节都明确写道，在实验时，为了确保一定的随机性，游戏开始的前30帧将采取随机操作。由于Gym中没有提供这样的特性，开发者只能靠自己完成。但是Baselines开发了很多与Atari游戏有关的环境封装，这些功能在文件common/atari_wrappers.py中。创建Atari游戏环境时，可以调用文件中的这个函数：</p>
<p>从代码中可以看出，除调用Gym的接口创建游戏环境外，它还运用了Gym中的Wrapper功能，为当前的环境类增添新的功能，使得它在原本的功能上可配置地增添新的功能。</p>
<p>封装的第一个类NoopResetEnv就实现了上面提到的“前30帧采取随机操作”的功能。它的核心代码（经过修改）如下所示：</p>
<p>从代码中可以看出，在用户对它执行reset操作时，它首先让自己类内的env对象执行reset方法，然后执行30轮noop操作，完成这些之后才将观测到的状态返回。有了这样的工具类，我们就不需要为这些琐碎的内容分心了。</p>
<p>除了这一层封装，环境还经过MaxAndSkipEnv的封装。这个类又实现了怎样的功能呢？它的核心代码如下所示：</p>
<p>从代码中可以看出，每一次Agent产生一个行动，并将行动传递给环境，而环境并非只对内部的环境类调用一次step方法，而是调用self._skip次，每一次都使用相同的行动进行操作。除此之外，它还将保存最后两个时刻的观测值，并取出两个时刻的最大值进行返回。Atari游戏每秒的刷新帧数为60次，我们固然可以让Agent在每一帧完成不同的行动，但这样的操作频率远超人类，不但给算法的实时性带来负担，而且这样高频率的操作也没有必要。因此，将机器的操作频率尽可能地靠近人类显得更有意义。</p>
<p>有了这些功能，我们可以将所有的精力放在Agent的训练和执行上，节省了不少关注环境的精力。</p>
<h3 id="5-3-总结"><a href="#5-3-总结" class="headerlink" title="5.3 总结"></a>5.3 总结</h3><p>本章介绍了与强化学习有关的开发环境，总结如下。</p>
<p>（1）Gym集成了众多强化学习环境，使用户可以方便地使用各种环境训练模型。</p>
<p>（2）Baselines实现了一些方便开发的辅助函数和经典的强化学习算法，用户可以直接尝试这些算法的效果。</p>
<h2 id="6-强化学习基本算法"><a href="#6-强化学习基本算法" class="headerlink" title="6 强化学习基本算法"></a>6 强化学习基本算法</h2><p>从本章开始，我们正式介绍强化学习，接下来的章节将充满公式与定义，相信对读者来说是一个挑战。在第1章，我们已经介绍了强化学习的基本概念，在5.1节介绍了基于Gym的强化学习的基本流程。本章进一步形式化这个过程，站在数学的角度分析这个问题的解决方案。对强化学习来说，一般以马尔可夫决策过程（Markov<br>Decision<br>Process，MDP）这样一个模型作为形式化的手段。本章将介绍MDP的原理，同时介绍基于MDP的策略学习算法：策略迭代、价值迭代和泛化迭代。</p>
<h3 id="6-1-马尔可夫决策过程"><a href="#6-1-马尔可夫决策过程" class="headerlink" title="6.1 马尔可夫决策过程"></a>6.1 马尔可夫决策过程</h3><p>MDP是当前强化学习理论推导的基石，通过这套框架，强化学习的交互流程可以很好地以概率论的形式表示出来，解决强化学习问题的关键定理也可以依此表示出来。从此我们就可以把目光集中在这些抽象的数学符号上，并使用数学工具推演性质帮助强化学习的性质。本节的重点就是介绍MDP和相关算法。</p>
<h4 id="6-1-1-MDP：策略与环境模型"><a href="#6-1-1-MDP：策略与环境模型" class="headerlink" title="6.1.1 MDP：策略与环境模型"></a>6.1.1 MDP：策略与环境模型</h4><p>5.1节中提到了蛇棋这个游戏，我们先来关注蛇棋的一个关键问题：哪些因素决定了蛇棋最终获得分数的多少？其实只有两个因素：选择什么样的手法投掷和投掷出的数目。第一个因素是玩家可以决定的，也是玩家唯一可以决定的；第二个因素是玩家无法确定的，它只受环境的随机性控制。</p>
<p>如果再进一步看，有哪些结果/状态决定了最终的得分呢？那就是每一步走到位置和每一步选择的投掷手法了。还以蛇棋为例，我们用st 表示 _t_<br>时刻游戏状态的观测值，也就是行走到的位置，用 at 表示 _t_ 时刻选择的手法，那么游戏过程就可以用一条状态-行动链条表示：</p>
<p>这个链条包含了两种状态转换，一种是从状态到行动的转换，另一种是从行动到状态的转换，这两种转换分别对应了前面提到的两个决定因素。第一种转换是由Agent的策略决定的，第二种转换是由环境决定的。下面就来看看这两种转换的具体形式。</p>
<p>作为第一种转换——策略，指的是Agent根据当前的状态选择“自认为”最好的行动方式。如果用严谨的方式表述，策略是一种映射，它将环境的状态值st<br>映射到一个行动集合的概率分布或概率密度函数上。对于蛇棋这个问题，我们可以认为Agent会在内心对每一个行动进行衡量，并最终选择评价最高的行动；如果我们认为对于每一个行动，Agent都有一定的概率去执行，且行动的评价越高，行动产生的概率越大，那么玩家就会选择概率最高的一种行动，形式化来说就是在时间<br>_t_ 的时刻，选择公式</p>
<p>的结果作为行动。可以看到，这个条件概率还是比较复杂的，行动依赖的信息比较多，需要对其做一定的化简才能方便计算。对蛇棋来说，可以使用蛇棋中状态之间无依赖的性质进行化简，即当前时刻选择什么行动只和当前的状态有关，和前面的状态与行动无关。这一点比较容易理解，如果Agent在“55”这个位置，那么它只需要考虑处于位置“55”之后的行动，至于如何从“1”走到“55”，已经不重要了，Agent的目标十分明确，就是尽快从“55”移动到“100”，于是上面的公式就变为</p>
<p>这样问题就得到了简化，实际上这步化简用到了序列的马尔可夫性，也就是说下一时刻的行动只和当前时刻的状态有关，和当前时刻之前的状态无关。以蛇棋为例，这个公式还可以进一步化简，由于行动的集合是离散有限的，可以把选择行动的问题变成一个多分类问题，这样就把策略选择问题变成了一个熟悉的问题。</p>
<p>第二种转换是环境的状态转换。当Agent完成行动后，环境会受到影响并完成状态的转换。这里也非常明显，下一步的状态只受前一步状态影响，不受更前面的状态影响，于是这里的状态转换就能以概率的形式表现为</p>
<p>假设Agent在“55”这个位置，如果它选定使用投掷“1～3”数字的手法，假设骰子投掷结果是“均匀”的，那么接下来它将等概率地出现在“56”、“57”和“58”这三个格子中。但是由于梯子的存在，落在这三个格子中的棋子可能会到达其他位置，例如通过梯子，最终到达了“56”、“40”和“80”处，那么这三个格子的概率分别为三分之一。所以实际中需要考虑棋盘中每一个格子的概率，即使绝大多数格子的概率为0。这部分的信息实际上属于环境内部的信息，在蛇棋这个问题中是公开的，而在一些问题中却是不可见的。在本章我们认为状态转换都是已知的，以便我们做接下来的推导。</p>
<p>了解了这两个过程，就可以重新研究MDP这个词了。马尔可夫决策过程包含以下三层含义。</p>
<p>（1）“马尔可夫”表示了状态间的依赖性。当前状态的取值只和前一个状态产生依赖，不和更早的状态产生联系。虽然这个条件在有些问题上有些理想，但是由于它极大地简化了问题，所以人们通常会选择使用它。</p>
<p>（2）“决策”表示了其中的策略部分将由Agent决定。Agent可以通过自己的行动改变状态序列，和环境中存在的随机性共同决定未来的状态。</p>
<p>（3）“过程”表示了时间的属性。如果把Agent和环境的交互按时间维度展开，那么Agent行动后，环境的状态将发生改变，同时时间向前推进，新的状态产生，Agent将获得观测值，于是新的行动产生，然后状态再更新……</p>
<h4 id="6-1-2-值函数与Bellman公式"><a href="#6-1-2-值函数与Bellman公式" class="headerlink" title="6.1.2 值函数与Bellman公式"></a>6.1.2 值函数与Bellman公式</h4><p>前面介绍了MDP的基本形式，我们发现游戏的关键在于策略，也就是如何做出决策与执行行动。在理想状态下，每一个行动都要为最终的目标——最大化长期回报努力，那么理论上只要能够找到一种方法，量化每一个行动对实现最终目标贡献的价值，Agent就可以根据这些量化指标做出明智的判断。所以接下来一个重要的工作就是量化这些价值。</p>
<p>幸运的是，环境已经帮我们完成了最重要的一步——它提供了可以量化的某一时刻的回报值r。虽然这只是局部的回报，和我们最终的目标有些不同，但我们可以利用它，将其扩展，使之成为我们的目标。前面已经提到关于蛇棋的回报规则，当我们落在非终点的位置上时，r=-1；当我们落在终点时，r=100。前面提到我们的策略等同于最大化下面这个公式</p>
<p>这个公式并不容易计算，它的困难反映在两方面，其中一个在计算的时间上。如果游戏的时间维度是有限的，也就是说，我们可以在有限步内完成游戏，那么这个公式虽然复杂，但至少是可以计算的；若游戏可以无限进行下去呢？例如在“94”～“99”这6个位置全架设向小数字位置移动的梯子，这样Agent将永远无法正好到达“100”这个位置，虽然问题变得不再有意义，但我们也不能直接用上面的公式了。</p>
<p>为了解决这个问题，使这个无穷数列的和收敛，我们要降低未来回报对当期的影响，也就是对未来的回报乘以一个打折率，使长期回报变得更有意义。所以修正后的公式变为</p>
<p>为什么要做这样的修改呢？这个公式的灵感实际上来源于生活，最直接的对应就是商业银行的储蓄了。我们知道把钱存入银行，银行是可以支付利息的（极端情况下还会出现负利息）。假设银行的年利率是10%（现实中恐怕很难见到），于是我们存入100元，一年后它就变成了110元。如果我们希望一年后它变成100元，那么现在需要存入多少钱呢？运用初等数学，令存入的钱数为<br>_X_ ，可以得到</p>
<p>得到的结果大概是90.9元，这个数字比100元小。也就是说，考虑到货币的时间效果，今天的90.9元和1年后的100元是等价的，如果将这90.9元存入银行两年，会使它变得更值钱。利用这个思想，既然现在的资产会随着时间不断提升价值，那么把未来的回报换算到当下的确是要打折的。这个打折率一般来说小于1，这样一来长期回报的这个数列的和就变得有界了，也就可以计算出具体的值了。</p>
<p>于是我们正式定义另外一个变量——长期回报。所谓长期回报，是将当前状态之后所有的回报取出，分别乘以对应的打折率，然后加起来得到一个汇总的值</p>
<p>解决了长期回报的可表示问题，另一个困难也出现了，用求和的形式表示长期回报显得有些复杂，而且在行动执行时我们并不知道未来的行动情况，例如投掷骰子的具体数值，那么能不能用其他方法表示对这个公式的估计呢？这时我们就需要另一个定义：策略的价值。前面提到了基于MDP形成的交互序列，其中从状态到行动的转换可以通过策略确定，而由于环境的原因，从行动到下一个状态的转换有时并不能确定。因此在衡量价值时，我们需要考虑每一种状态转换的影响，这就需要基于状态转换求解长期回报的期望。令τ<br>为根据策略和状态转换采样得到的序列，那么价值的公式定义就可以写成</p>
<p>根据MDP的模型形式，价值函数（一般称为“值函数”）可以分为两种类型。</p>
<p>（1）状态值函数 _v_ π （s）：也就是已知当前状态s，按照某种策略行动产生的长期回报期望。</p>
<p>（2）状态-行动值函数 _q_ π （s _，_ a）：也就是已知当前状态s和行动a，按照某种策略行动产生的长期回报期望。</p>
<p>实际上，即使采用这样的表达方式，计算价值仍然是个很困难的事情，如果要计算从某个状态出发的值函数，相当于依从某个策略把所有从这个状态出发的可能路径走一遍，将这些路径的长期回报依概率求期望：</p>
<p>其中τ 表示从状态st 出发的某条路径。由于我们将强化学习的过程表示成马尔可夫决策过程，于是路径部分可以展开为</p>
<p>这样的公式形式显然不够优雅，于是我们使用高中时使用的代换消元法，就可以得到</p>
<p>将下一时刻的状态价值展开，可以得到</p>
<p>两式融合，再经过一些变换，可以得到</p>
<p>通过这样的计算，我们发现状态值函数可以以递归的形式表示。假设值函数已经稳定，任意一个状态的价值可以由其他状态的价值得到，这个公式就被称为贝尔曼公式（Bellman<br>Equation，以下简称Bellman公式），是后面章节进行策略求解的基础公式之一。之所以称为“之一”，是因为状态-行动值函数同样有一个类似的公式</p>
<p>这个公式的证明方式与上面的公式类似，这里不再赘述。这两个公式可以说是整个强化学习理论的基石，请读者务必对其深入理解。接下来，我们以图的形式对两个公式进行阐述。</p>
<p>图6-1以树的形式展示了MDP的扩展过程。问题的开始节点为s0 ，当策略 _π_<br>固定时，对应的行动将根据策略以一定的概率产生，对应的状态转移也会以一定的概率产生。</p>
<p>图6-1 MDP的扩展形式与状态价值</p>
<p>图6-1中的变量名称比较复杂，其中s0 表示 _t_ =0 时刻的状态，表示 _t_ =0时刻的0号行动，而表示 _t_ =1<br>时刻从上一时刻得到的第2个状态。在状态转移过程中，我们有可能在不同时刻遇到同一个状态，那么这两个状态的价值相等吗？为了证明这一点，我们要给出一个上面证明中没有明确提到的假设：时间无关性。时间无关性是指MDP经过不断演化，最终会使不同时刻下相同状态的价值相等。举例来说，<br>_t_ =1 时刻的 _v_ （s） 和 _t_ =3 时刻的 _v_ （s）<br>相等。为了达到这个假设，我们认为MDP的状态转移过程持续时间足够长，最终每个状态、行动的转移进入了稳定的状态，从图6-1中可以看出，从下一时刻的状态s1<br>开始的价值可以用 _v_ （s1 ） 表示，于是根据它们的转移概率，同样可以得到Bellman公式的形式。图6-2则是状态-<br>行动值函数的表示形式，两者在假设和推演上是类似的。</p>
<p>图6-2 MDP的扩展形式与状态—行动价值</p>
<p>看到这里，读者一定会想，既然Bellman公式具有递归定义的特点，那么能不能利用这个特点求解出值函数呢？问题开始时，我们并不知道值函数，而求解值函数需要知道其他时刻的值函数，这似乎为我们求解价值制造了障碍，那么该如何解决问题呢？从本章的6.2节开始我们会给出答案。</p>
<h4 id="6-1-3-“表格式”Agent"><a href="#6-1-3-“表格式”Agent" class="headerlink" title="6.1.3 “表格式”Agent"></a>6.1.3 “表格式”Agent</h4><p>前面介绍了太多公式推导，我们并没有一个对MDP具体实现的概念，那么现在就来看看这些内容对应的代码实现，本节要实现“表格式”Agent需要的基本数据结构。什么是“表格式”？以蛇棋问题为例，棋盘上有100个位置，所以问题一共有100个离散的状态；投掷骰子的手法也是有限的，因此也可以通过离散的方式表示出来。所以对上面提到的所有实体（状态、行动、策略、状态转移概率），都可以以<br>_N_ 维张量的形式表示。例如，策略 _π_<br>（a|s）是一个条件概率分布，如果以“表格”的形式表示，则这个条件概率可以由一个|S|×|A|的矩阵表示，每一个数值都处于0到1的范围内，每一行的数值相加之和为1。而对于状态转移<br>_p_ （st+1 |st _，_ at ）来说，如果以“表格”的形式表示，那么这个条件概率分布就可以由一个|S|×|S|×|A|的张量表示。</p>
<p>除了“表格式”这个约束，还需要另外一个前面提到的约定：环境的状态转移概率需要对Agent公开，这样Agent就能利用这些信息做出更好的决策。对蛇棋来说，如果知道骰子的每一面朝上的概率是均匀的，以及棋盘上的每一个梯子都是可见的，就可以计算出状态转移概率。</p>
<p>所以我们现在实现一段简单的代码，它会根据环境的信息生成问题中所有实体的“表格式”数据结构，Agent代码的基本结构如下所示：</p>
<p>可以看出，这个Agent中保存了以下信息。</p>
<p>· 蛇棋的状态、行动数目：|S|、|A|。</p>
<p>· 蛇棋中的回报记录r：一个一维数组。</p>
<p>· 蛇棋的策略π：一个二维数组（|S| _，_ |A|），存储 _π_ （a|s）的结构。</p>
<p>· 蛇棋中的状态转移情况 _p_ ：这是一个三维的数组（|A| _，_ |S| _，_ |S|），存储 _p_ （s′ |s _，_ a）的结构。</p>
<p>· 状态值函数 _v_ （s）。</p>
<p>· 状态-行动值函数 _q_ （s _，_ a）。</p>
<p>· 打折率 _γ_ 。</p>
<p>到此为止，初始化结束。这些结构将在6.2节派上用场。</p>
<h3 id="6-2-策略迭代"><a href="#6-2-策略迭代" class="headerlink" title="6.2 策略迭代"></a>6.2 策略迭代</h3><p>6.1节我们使用MDP的框架定义了一系列的概念，其中包括十分重要的两种值函数。如果使用值函数重新定义强化学习的目标，我们可以得到：强化学习就是找到最优的策略，使每一个状态的价值最大化，这相当于求解</p>
<p>而对于每一个状态对应的行动，我们希望找到使其价值最大化的行动</p>
<p>可以看出为了求出最终的结果，我们需要同时更新交织在一起的策略与价值，本节将详细介绍解决这个问题的算法之一：策略迭代法（Policy Iteration）。</p>
<h4 id="6-2-1-策略迭代法"><a href="#6-2-1-策略迭代法" class="headerlink" title="6.2.1 策略迭代法"></a>6.2.1 策略迭代法</h4><p>在本节开篇提到的计算思路中，我们发现如果想知道最优的策略，就需要能够准确估计值函数。然而想准确估计值函数，又需要知道最优策略，数字才能够估计准确。所以实际上这是一个“鸡生蛋还是蛋生鸡”的问题。碰上这样无解的问题，往往需要采取一些“曲线救国”的方法。那么，能不能把这个问题考虑成一个迭代优化的问题，通过一轮一轮的计算逐渐接近最优的结果呢？答案是可以的。</p>
<p>于是我们提出如下计算思路。</p>
<p>（1）以某种策略π开始，计算当前策略下的值函数 _V_ π （s）。</p>
<p>（2）利用这个值函数，找到更好的策略π∗ 。</p>
<p>（3）用这个策略继续前行，更新值函数，得到这样经过若干轮计算，如果一切顺利，我们的策略会收敛到最优的策略，如果能够判断出策略已经收敛，问题也就得到了解答。当然，目前我们还没有理论证明，因此这个思路也可能是错的，但我们还是先试着实践这个思路。</p>
<p>为了验证结果的正确性，我们需要将蛇棋的难度降低。这里将梯子的数量变为0，同时玩家只能用两种投掷骰子的手法：投掷1～3的投掷手法和投掷1～6的投掷手法。我们可以直接推理出最优的方案：</p>
<p>（1）在前进至97、98、99三个位置之前，全部使用投掷1～6的手法显然可以获得最优的前进步数。</p>
<p>（2）到了97、98、99这三个位置时，最好使用投掷1～3的手法，因为这样有更大的概率一次性到达终点。</p>
<p>由于这个策略比较简单，我们可以用一个数组表示，我们限定Agent只有1～3和1～6两种策略，于是可以构建以下三种策略。如代码所示：</p>
<p>代码中实现了三种策略：最优策略、全部执行第一个行动的策略和全部执行第二个行动的策略。为了简化代码，我们直接用一个数组表示Agent要执行的行动。完成了策略的构建，就可以评估策略的效果了，这其实就是让Agent在真实的环境上进行交互，得到回报总和。代码如下所示：</p>
<p>，然后不断迭代……</p>
<p>在代码中，我们使用每一种策略进行1万局游戏，并显示每一种策略的平均得分。由于棋盘上没有梯子，所以棋局的环境不用发生变化。游戏最终的平均得分如下：</p>
<p>可以看出，经过设计的策略获得了最高的平均得分。下面我们就使用前面提到的思路，实现优化算法。这个方法分两步，首先是计算当前策略的值函数估计。</p>
<p>想要求解值函数，就必须用到6.1节提到的Bellman公式：</p>
<p>在6.1节我们曾提到，对于这个公式，我们利用了时间不变性这样的假设，于是等号左右两边的值函数值应该是相等的，所以这个问题可以采用解方程的方法求解。上面的公式是采用连加的形式描述的，我们现在将其变成矩阵运算的形式，于是等式变成了图6-3所示的形式。</p>
<p>图6-3 Bellman公式的矩阵运算形式</p>
<p>如果用V 表示状态值函数的向量，Π表示策略矩阵，P 表示状态转移矩阵，R表示回报向量，那么矩阵版的Bellman公式为</p>
<p>通过求解可以得到</p>
<p>只要（1 _-γ_ ΠP）-1<br>的逆矩阵存在，状态价值就可以解出来。实际中由于矩阵数值的限制，逆矩阵是存在的，所以这种方法可以求解。但是对于一些状态和行动比较多的问题，采用这种方法求解的复杂度偏高，因此实际中大家不用这种方法，而是采用迭代的方法进行计算。</p>
<p>那么迭代的方法如何计算呢？在其他数值计算算法中，我们会利用旧参数迭代更新新参数的方法，例如梯度下降法，这里也将采用类似的方法。我们将上面的Bellman公式左右两边的值函数赋予不同的值，右边是当前值，左边是新迭代的值，假设迭代轮数为{1<br>_，_ 2 _，_ 3 _，…，T_ }，那么迭代更新的公式就变为</p>
<p>由于有 _γ_<br>的存在，每个状态的价值最终将得到收敛，由于采用了表格式的表示方法，每个状态的策略实际上是确定的，因此不必进行概率计算，直接根据行动进行转换即可。我们将迭代的版本实现出来，如下所示：</p>
<p>完成了这一步，下一步就是根据前面的状态值函数计算状态-行动值函数：</p>
<p>完成计算后，就可以根据同一状态下的行动价值更新策略：</p>
<p>这样就完成了状态的更新。代码如下所示：</p>
<p>将上面的代码联起来，整个算法的执行如下所示：</p>
<p>那么这个算法最终执行的结果如何呢？</p>
<p>结果为：</p>
<p>可以看出，算法求出的策略结果和我们设想的结果一样，这说明算法在这个例子上是没问题的，而这个算法就被称为策略迭代法。可以看出，每一轮迭代结束，策略都进行了一次更新，当策略没有更新时，迭代结束。前面提到算法的两个部分也分别被称为：策略评估部分（Policy<br>Evaluation）和策略提升部分（Policy Improvement）。</p>
<p>从上面的分析中我们发现，策略评估部分在逻辑上没有问题，值函数会随着迭代逐渐收敛，主要的问题出现在策略提升上，我们能不能证明这种改进方案一定得到最优结果呢？当然可以。如果把每一轮迭代生成的策略形成一个策略组，并以迭代轮数进行编号，那么可以得到一个策略列{<br>_π_ t }，我们不但可以证明随着 _t_ 的增大，这个策略列依值函数最终收敛到最优的策略 _π_ ∗ ，也就是说</p>
<p>而且可以证明这个策略列依值函数一致收敛到最优策略，即对于任意的数 _∈_ ，我们都存在一个 _k_ ，使得当 _t＞k_ 时：</p>
<p>换句话说，随着迭代的进行，策略不断趋近于最优，每一轮迭代的策略都不会比前一轮迭代的策略差。6.2.2节就来证明这一点。</p>
<h4 id="6-2-2-策略提升的证明"><a href="#6-2-2-策略提升的证明" class="headerlink" title="6.2.2 策略提升的证明"></a>6.2.2 策略提升的证明</h4><p>我们利用Bellman公式证明。假设现在有一个策略 _π_ ，经过计算得到了对应的值函数 _v_ （ _π_ ），并以此求出对应的 _q_ （ _π_<br>）。这时我们发现存在一个状态s^和^a，使得</p>
<p>那么我们就可以对这部分策略进行更新，得到一个新的策略 _π_ +<br>，由于采用“表格式”的表示形式，这个新策略除了在某个状态s^的决策与原策略不同，其他完全一致，它的形式可以写作：</p>
<p>那么对于任意一个状态s，有</p>
<p>所以可以证明，每一次策略提升都不会使当前策略的价值下降，同理可以证明，如果策略 _π_ 1 下状态的价值不高于策略 _π_ 2 下状态的价值（ _v_ π1<br>（s） ≤ _v_ π2 （s），∀s），且 _π_ 2 下状态的价值不高于 _π_ 3 （ _v_ π2 （s） ≤ _v_ π3 （s），∀s），那么<br>_π_ 1 下状态的价值也不高于 _π_ 3 （ _v_ π1 （s）≤ _v_ π3<br>（s），∀s），基于这种传递性，就可以得到策略迭代不断趋近最优的性质。</p>
<h4 id="6-2-3-策略迭代的效果展示"><a href="#6-2-3-策略迭代的效果展示" class="headerlink" title="6.2.3 策略迭代的效果展示"></a>6.2.3 策略迭代的效果展示</h4><p>前面证明了策略迭代的收敛性质，接下来介绍前面例子中的一些细节。假设一开始所有的策略都采用1～3的投掷手法，于是在第一轮策略评估中，共进行了94轮迭代，过程中的状态的迭代值在不断变化，我们以“50”这个位置为例，图6-4所示为94轮迭代下价值的变化值。</p>
<p>横轴为迭代轮数，纵轴为价值，可以看出随着迭代轮数的增加，价值总体趋于平稳。完成第一轮的策略提升后，实际上策略已经被更新为最优策略，于是在第二轮策略评估中，再经过94轮迭代，“50”位置的价值又经历了如下变化，如图6-5所示。</p>
<p>看完上面那个简单的例子后，我们回到复杂的例子中，对一个拥有10个梯子的问题，策略迭代会给我们什么答案呢？</p>
<p>图6-4 第一轮策略评估时位置“50”的价值变化图</p>
<p>图6-5 第二轮策略评估时位置“50”的价值变化图</p>
<p>结果如下：</p>
<p>可以看出，策略迭代的方法优于前面的三种方法。可以看出它的策略中已经将两种手法混合使用了。笔者猜想它的策略思想如下：</p>
<p>（1）在靠近上升梯子附近使用1～3的投掷手法。</p>
<p>（2）在靠近下降梯子或者无梯子时使用1～6的投掷手法。</p>
<p>（3）最后几步使用1～3的投掷手法。</p>
<p>所以我们可以从最终的策略中猜出棋盘的样子及梯子所在的位置。以上就是策略迭代的算法，除了这种算法外，在接下来的章节我们还会介绍其他算法。</p>
<h3 id="6-3-价值迭代"><a href="#6-3-价值迭代" class="headerlink" title="6.3 价值迭代"></a>6.3 价值迭代</h3><h4 id="6-3-1-N-轮策略迭代"><a href="#6-3-1-N-轮策略迭代" class="headerlink" title="6.3.1 _N_ 轮策略迭代"></a>6.3.1 _N_ 轮策略迭代</h4><p>6.2节介绍了策略迭代的算法，它可以解决蛇棋的策略规划问题，帮助我们更好地完成游戏。从算法中可以看出，算法的主要时间都花费在策略评估上，对一个简单的问题来说，在策略评估上花费的时间不算长；但对复杂的问题来说，这个步骤的时间实在有些长。一个最直接的想法就是——我们能不能缩短在策略评估上花的时间呢？例如，我们大概估计出当前策略下每个状态的值函数就好，虽然不够精确，但这样也许已经足以帮助我们找出最优的策略了，再做更精细的评估实际上并不必要。这就是本节的主角——价值迭代法的思想来源。</p>
<p>回到前面的代码中，为了测量算法的用时，我们将在代码中注入计时的功能：</p>
<p>对于有10个梯子的问题，我们固定随机数，代码将变为：</p>
<p>可以得到以下的时间消耗记录：</p>
<p>从时间记录可以看出，PolicyEval的时间远大于PolicyImprove，所以策略评估确实是最花时间的。那么我们就从这里进行改进，首先限制策略评估的迭代轮数，在上面的测试中，策略迭代进行了94轮，我们把它降至50轮可以吗？</p>
<p>将策略评估限制在50轮，得到如下输出（有省略）：</p>
<p>继续激进一点，把迭代轮数改到10：</p>
<p>那么剩下的就是最激进的，把迭代轮数改到1：</p>
<p>我们发现，随着策略评估的迭代轮数不断降低，算法的总迭代数在升高。当策略评估的迭代轮数为94时，算法总迭代数为3；而策略评估的迭代轮数为1时，算法总迭代数变为7，但是这样的改变并不会影响最终的结果，反而使它成了这些实验中最快的算法。这个算法就是评估迭代轮数为1的策略评估算法。</p>
<p>实际上，我们可以认为评估迭代轮数为1的策略迭代法很像基于价值的迭代法。为什么呢？在策略迭代算法中，每一轮迭代过后，状态价值得到了收敛，但是当前策略不一定是最优的，直到策略不再发生变化，训练才会结束。策略迭代的过程如图6-6所示，其中横轴表示值函数的收敛效果，数值到达∞时完成收敛，纵轴表示策略的优异度，数值到达∞时策略到达最优。</p>
<p>而对于1轮迭代来说，在策略达到最优前，值函数都不会收敛。如果值函数收敛，则说明值函数没有发生改变，策略也就随之收敛了。对应的迭代过程图如图6-7所示。</p>
<p>从图6-7所示的更新路线可以看出，与其说这是更新策略，不如说是更新价值。当价值迭代完成后，策略迭代也就随之完成，所以我们可以将迭代的重点放在价值上。6.3.2节将介绍真正的价值迭代法。</p>
<p>图6-6 策略迭代算法的更新路线</p>
<p>图6-7 价值迭代算法的更新路线</p>
<h4 id="6-3-2-从动态规划的角度谈价值迭代"><a href="#6-3-2-从动态规划的角度谈价值迭代" class="headerlink" title="6.3.2 从动态规划的角度谈价值迭代"></a>6.3.2 从动态规划的角度谈价值迭代</h4><p>从前面的介绍可以看出，策略迭代算法相对容易理解，因为它的思路很清晰，而价值迭代算法看上去似乎是一个“贪心”版的策略迭代算法，它的收敛性质能否保证呢？回答这个问题需要从“贪心”的对立面——动态规划解释。</p>
<p>前面我们一直站在策略的角度分析，并且把价值迭代法定义为一轮迭代的策略迭代法，那么对于一轮迭代的算法，它的形式能否做一个改变呢？我们知道策略评估这一步完成了下面的计算：</p>
<p>接下来的策略改进完成了策略改进的计算：</p>
<p>能不能将这三个步骤结合，成为一个关于值函数的更新步骤呢？首先将后两个公式合并，得到</p>
<p>由于最优策略的存在，实际上策略最终的选择是单一的。也就是说，对于每一个状态，最优策略会采取某一种行动，这种行动不会比其他行动差，所以我们可以得到状态值函数的更新方法：</p>
<p>经过这样的转换，我们终于将视角转换到值函数上。可以看到公式的左右两边都出现了值函数，其中等号左边的值函数是迭代更新后的值函数，等号右边的值函数是上一轮的值函数。这时，这个公式就展示出动态规划的“气质”。</p>
<p>相信很多读者在学习算法时都接触过动态规划相关的问题。算法的一大核心思想是分解问题，将大问题划分成一个个小问题，然后用小规模下的解决方案解决小规模的问题。动态规划的思路也基本相同，它有两个核心特点：最优子结构和重复子结构。</p>
<p>所谓最优子结构，是指一个子问题的最优解是可以得到的。对应蛇棋的问题，可以理解为是“从某个位置出发行走一步能够获得的最大奖励”的问题，由于只走一步，这个问题很容易计算。</p>
<p>所谓重复子结构，是指一个更大的问题是由一些小问题组成的，而求解不同的大问题时可能会用上同一个子问题，子问题被重复利用，计算量也就减少了。对应蛇棋的问题，可以理解为是“从某个位置出发行走两步能够获得的最大奖励”的大问题，利用前面已经得到的子问题，这个大问题可以用伪代码表示：</p>
<p>“某个位置走两步的最大奖励”=max（[这一步的奖励+从到达位置出发走一步获得的最大奖励 for 走法 in 可能的走法]）</p>
<p>可以看出，这个公式用到了最优子结构的性质。由于打折率的存在，值函数的绝对值存在上界，值函数在这样的更新中最终会得到收敛，所以这个方法是可行的。</p>
<p>想要理解价值迭代和动态规划的关系，除了使用公式表示外，还可以使用图像。这里需要对问题做一个假设：假设所有的问题都存在一个“终点时刻”，对有明确终止条件的问题来说，这样的终点当然存在；但对没有终点状态的问题来说，可以假设某一个时刻为终点，例如时间的终点、外物灭亡时刻的终点……那么我们可以记录这一时刻为<br>_T_ ，所有状态的价值为0，由于没有后面的行动，此刻价值设为0也是正常的。 _T_ 时刻的状态与价值情况如图6-8所示。</p>
<p>知道了时刻 _T_ 的情况，我们就能得到 _T_<br>-1时刻从每个状态出发能获得的最优价值。对于每一个状态，我们遍历所有的行动，通过Bellman公式，可以得到这一时刻的最优长期回报，如图6-9所示。</p>
<p>再向前一步，来到 _T_ -2时刻，我们同样可以利用 _T_ -1时刻的结果计算出 _T_<br>-2时刻的最优价值。依此类推，我们可以向前推进无数步，直到价值收敛，此时的收敛值就是从当前位置到时间终结点的最优价值，也就是价值迭代法要求出的结果。</p>
<p>图6-8 _T_ 时刻的状态与价值情况</p>
<p>图6-9 _T_ -1时刻的状态与价值情况</p>
<h4 id="6-3-3-价值迭代的实现"><a href="#6-3-3-价值迭代的实现" class="headerlink" title="6.3.3 价值迭代的实现"></a>6.3.3 价值迭代的实现</h4><p>根据前面的介绍，我们可以给出价值迭代的代码，代码如下所示：</p>
<p>使用和6.2节策略迭代同样的实验，也能得到同样的结果，那么它的用时是多少呢？</p>
<p>可以看出，价值迭代和策略迭代得到了相同的结果，但它比策略迭代快一些。这个结果和两个算法在其他问题上的效果一致。读者可以看出，价值迭代的速度没有本节中的一轮策略评估迭代速度快，一轮策略评估迭代的时间为0.023s。这说明两个算法仍然存在一定的区别，所以笔者提到两者的思想相近，但并不相同。那么它们的区别是什么呢？我们还能不能让训练速度更快呢？</p>
<h3 id="6-4-泛化迭代"><a href="#6-4-泛化迭代" class="headerlink" title="6.4 泛化迭代"></a>6.4 泛化迭代</h3><h4 id="6-4-1-两个极端"><a href="#6-4-1-两个极端" class="headerlink" title="6.4.1 两个极端"></a>6.4.1 两个极端</h4><p>在6.3节我们留了一个悬念：为什么价值迭代法的运算速度不如一轮策略迭代法的速度快呢？本节将解答这个问题。在解答问题前，我们先对比策略迭代法和价值迭代法的特点。</p>
<p>策略迭代法的中心是策略函数，它通过反复执行“策略评估+策略提升”两个步骤使策略变得越来越好；价值迭代法的中心是值函数，它通过利用动态规划的方法迭代更新值函数，并最终求出策略函数。由此可以看出两者有如下特点。</p>
<p>（1）两个方法最终都求出策略函数和值函数。</p>
<p>（2）最优的策略函数都是由收敛的值函数得到的。</p>
<p>（3）值函数通过Bellman公式收敛。</p>
<p>由此发现一个关键：两者都需要训练和更新策略函数和值函数，只是侧重点不同。策略迭代的核心是策略，为了提升策略，值函数可以求解得准确，也可以求解得不那么准确；价值迭代的核心是价值，算法的核心部分根本没有出现与策略有关的内容，直到算法最后通过值函数求出策略。</p>
<p>两种方法都十分看重自己关心的那部分，可以选择忽略另一部分，因此可以看出两个方法都比较极端。既然找到了两个极端的方法，可不可以找到两种方法的中间地带呢？当然可以，这就是本节要介绍的广义策略迭代法（Generalized<br>Policy Iteration）。</p>
<h4 id="6-4-2-广义策略迭代法"><a href="#6-4-2-广义策略迭代法" class="headerlink" title="6.4.2 广义策略迭代法"></a>6.4.2 广义策略迭代法</h4><p>在6.3节中，我们给出了策略迭代法和一轮策略评估迭代法的优化路径过程图。从图6-6和图6-7中可以看出，优化的起点在坐标轴的左下角，终点在右上角，于是读者可以想象，从左下角到右上角，一定有很多种走法，前面介绍的两种是其中的走法之二。我们可以再设计其他的更新算法，例如，我们将策略迭代和价值迭代融合起来，在优化的前几轮进行策略迭代，然后进行价值迭代，这样同样可以获得正确的结果，对应的优化过程如图6-10所示。</p>
<p>所谓的广义策略迭代法，就是定义一个迭代算法族，其中的算法都是由策略迭代和价值迭代算法组合而成的。组合的方法有很多，可以形成的方法也有很多，而前面提到的两种算法是广义策略迭代法的一种特例。由于其中的算法很多，这些算法中很有可能存在一些比前面两种算法速度更快的方法，例如前面多次提到的一轮策略评估迭代法。</p>
<p>了解了广义策略迭代法，再回到6.3节的结尾，分析为什么价值迭代的方法不是最快的。由于蛇棋问题的状态是离散的，对应的行动也是离散的，因此策略所在的空间也是离散的，这个问题的策略价值坐标轴实际上应该如图6-11所示。</p>
<p>由于策略空间是离散的，而值函数空间是连续的，同样的策略可能对应着一定范围内的值函数，因此最优策略也一定对应着一定范围的值函数。所以在使用价值迭代法时，需要将值函数更新到最优，所以为了使值函数收敛，它花费了一定的时间；而如果我们不去追求值函数的最优，当值函数接近最优时，使用它进行策略改进，也许就可以得到最优策略了，这样实际上可以节省一部分时间。</p>
<p>这样我们就可以根据蛇棋问题的特点，构造出对应的方法，使它的计算速度变得更快。</p>
<p>图6-10 某种优化过程图</p>
<p>图6-11 问题的真实空间图</p>
<h4 id="6-4-3-泛化迭代的实现"><a href="#6-4-3-泛化迭代的实现" class="headerlink" title="6.4.3 泛化迭代的实现"></a>6.4.3 泛化迭代的实现</h4><p>泛化迭代是一个算法框架，想要实现好需要对框架做一定的设计。笔者简单地实现了其中的一个“计算较快的”算法，它直接将策略迭代和价值迭代结合。实现方法如下：</p>
<p>这个函数先执行10轮价值迭代的优化，然后进行策略迭代，同时策略迭代中的策略评估只进行一个循环的更新。经过这样的设计，我们来看看它的用时：</p>
<p>从代码中可以看出，同时进行了3组实验，分别是策略迭代、价值迭代和我们给出的一种广义策略迭代法，它们的结果如下：</p>
<p>虽然实验比较粗糙，但还是可以明显看出几个算法在时间上的差距。通过一定的设计，泛化迭代确实可以做到更快的计算和收敛。</p>
<h3 id="6-5-总结"><a href="#6-5-总结" class="headerlink" title="6.5 总结"></a>6.5 总结</h3><p>本章介绍了MDP、策略迭代法、价值迭代法和广义策略迭代法相关的知识，让我们一起回顾其中的要点。</p>
<p>（1）MDP包含了状态与行动交替的序列，我们可以将很多问题转化成这样的交互形式。</p>
<p>（2）值函数表示了长期回报的期望值。常见的有状态值函数 _V_ （s）和状态-行动值函数 _q_ （s _，_ a）。</p>
<p>（3）策略迭代法通过交替执行策略评估和策略提升两个步骤完成最优策略的学习。</p>
<p>（4）价值迭代法通过迭代更新值函数使值函数达到最优，然后更新策略函数。</p>
<p>（5）泛化迭代融合了两种迭代方式。</p>
<h1 id="第二部分-最优价值算法"><a href="#第二部分-最优价值算法" class="headerlink" title="第二部分 最优价值算法"></a>第二部分 最优价值算法</h1><p>第二部分将介绍基于最优价值思想的算法，这类算法将重点放在值函数上，通过交互序列的信息学习价值模型，并通过价值模型更新策略，其中的思想和价值迭代法十分相似。随着强化学习和深度学习的共同发展，基于Q-<br>Learning的算法获得了很大的突破，甚至达到了专家的水平，其中的代表为Deep Q Network。笔者还会介绍很多基于Deep Q<br>Network的改进算法，这些算法进一步提高了模型的能力。</p>
<p>第二部分共包含两章：</p>
<p>· 第7章将介绍Q-Learning算法的基础知识，并介绍Deep Q Network算法。</p>
<p>· 第8章将介绍基于Deep Q Network的改进算法，并介绍集改进算法于一身的Rain-bow算法。</p>
<h2 id="7-Q-Learning基础"><a href="#7-Q-Learning基础" class="headerlink" title="7 Q-Learning基础"></a>7 Q-Learning基础</h2><p>第6章介绍了强化学习的基础知识，学习了基于马尔可夫决策过程的强化学习框架，并学习了策略迭代、价值迭代和泛化迭代算法。我们曾提到一个很关键的前提条件：Agent知道环境的状态转移概率。本章要把这个条件去掉，并介绍新的算法：Q-Learning和基于深层模型的Deep<br>Q Network。</p>
<h3 id="7-1-状态转移概率：从掌握到放弃"><a href="#7-1-状态转移概率：从掌握到放弃" class="headerlink" title="7.1 状态转移概率：从掌握到放弃"></a>7.1 状态转移概率：从掌握到放弃</h3><p>从本章起，我们开始解决更贴近实际的问题。前面提到我们接触过的问题有一个特点，即我们可以知道环境运转的细节，具体说就是知道状态转移概率。对蛇棋来说，我们可以看到蛇棋的棋盘，也就可以了解到整个游戏的全貌，当我们发现前方有一个梯子时，我们可以根据前方是上升梯子还是下降梯子来决定使用哪个骰子。这时我们相当于站在上帝视角，能够看清一切情况。于是，动态规划这样的算法成了我们的选择。</p>
<p>但一个可悲的事实是，在很多实际问题中，我们无法得到游戏的全貌，也就是说，状态转移的信息 _P_ （St+1 |St _，_ at<br>）无法获得。一般来说，我们将知晓状态转移概率的问题称为“基于模型”的问题（Model-based<br>Problem），将不知晓的称为“无模型”问题（Model-free<br>Problem）。以下面的进阶版蛇棋为例，在这个新版蛇棋中，我们将不再对玩家显示棋盘信息和骰子可能的投掷数目，每次当玩家选择完所使用的手法后，玩家将直接得到棋子的下一个位置，而不会知道其他任何信息。</p>
<p>如果知道状态转移概率，我们就可以使用第6章介绍的学习算法，迭代执行策略评估和策略改进两个步骤得到最优策略。而对于“盲棋”，我们不知道状态转移概率，就无法直接执行策略评估，即不能使用Bellman公式进行值函数的更新。为了解决这个问题，我们需要使用其他方法完成策略评估，这就要追溯回强化学习自身的特点：不断试错，也就是通过尝试与环境交互来解决策略评估的问题。实际上，这个不断尝试的思路和人类的一些学习方式比较接近，我们可以列出新算法的大体思路。</p>
<p>（1）确定一个初始策略（这和前面的算法一致）。</p>
<p>（2）用这个策略进行游戏，得到一些游戏序列（Episode）：{s1 _，_ a1 _，_ s2 _，_ a2 _，…，_ sn _，_ an }。</p>
<p>（3）一旦游戏的轮数达到一定数目，就可以认为这些游戏序列代表了当前策略与环境交互的表现，就可以将这些序列聚合起来，得到状态对应的值函数。</p>
<p>（4）得到了值函数，就相当于完成了策略评估的过程，这样就可以继续按照策略迭代的方法进行策略改进的操作，得到更新后的策略。如果策略更新完成，则过程结束；否则回到步骤2。</p>
<p>上面的流程十分清晰地介绍了学习的过程，此时学习的关键就落在了下面两个问题上：</p>
<p>（1）如何得到这些游戏序列。</p>
<p>（2）如何使用序列进行评估。</p>
<p>本章将重点解决这两个问题。接下来介绍两个算法：蒙特卡罗法（Monte Carlo Method）和时序差分法（Temporal Difference<br>Method）。</p>
<h3 id="7-2-蒙特卡罗方法"><a href="#7-2-蒙特卡罗方法" class="headerlink" title="7.2 蒙特卡罗方法"></a>7.2 蒙特卡罗方法</h3><p>本节我们介绍蒙特卡罗法。在前面的章节里，我们曾介绍当环境信息，也就是状态转移概率已知时，可以使用Bellman公式，通过不断迭代得到状态-行动值函数：</p>
<p>然后通过值函数进行策略改进。而在无模型问题中，状态转移概率将无法知晓。于是我们需要把公式转变为</p>
<p>看到了等号右边的期望，我们很自然地联想到了蒙特卡罗法，它是一种通过随机采样估计期望值的方法，假设我们通过一些方法，从状态s和行动a开始不断地与环境交互，得到了大量的样本序列</p>
<p>对应的回报序列为</p>
<p>我们可以通过这些样本序列逼近真实的期望值，那么公式就可以近似等于：</p>
<p>如果这个序列是有限的，我们也可以直接使用序列产生的回报和作为价值：</p>
<p>这样就可以得出策略评估的结果，具体算法其实比较简单。</p>
<p>以上就是蒙特卡罗法的全部内容，我们可以将它的算法全过程总结如下。</p>
<p>（1）让Agent和环境交互后得到交互序列。</p>
<p>（2）通过序列计算出每一时刻的价值。</p>
<p>（3）将这些价值累积到值函数中进行更新。</p>
<p>（4）根据更新的值函数更新策略。</p>
<p>接下来让我们实现蒙特卡罗法的代码。同时，我们还要将这个算法的结果和策略迭代法进行比较，看看在不知道环境模型的情况下，蒙特卡罗法能否做到和策略迭代一样的效果。</p>
<p>完整的算法流程如下所示：</p>
<p>其中包含了两个子算法。其中的policy_improve和前面的算法类似，都是完成下面公式的操作：</p>
<p>所以这里不再赘述，下面介绍monte_carlo_eval方法，这个方法又分成几个部分，首先要用当前的策略玩游戏，产生一系列的序列，并将这些序列保存起来：</p>
<p>产生序列之后，我们可以沿时间的反方向计算每一个时刻的状态行动和对应的长期回报值：</p>
<p>最后，我们将每一个状态-行动对应的长期回报值交给值函数进行更新：</p>
<p>这里涉及一个小的改变，因为我们要计算期望价值，要将所有观测到的长期回报求平均值，那么令状态行动价值为q，当前的时间为 _t_ ，积累的数量为 _N_<br>，我们要求的值为，当前已知的值为和 _N_ ，每一个时刻的价值为，于是可以得到</p>
<p>这样每一时刻我们都可以求出当前所有观测值的平均数，而且这个公式和常见的梯度下降法的更新公式</p>
<p>也十分相近，其中的等同于学习率 _α_ ，而等同于目标函数的梯度当然，这样的相似并不是偶然的，在后面的章节我们还会介绍类似形式的公式。</p>
<p>上面的代码中我们实现了一次游戏的交互，下面来比较蒙特卡罗法和策略迭代法两种算法的优劣。代码如下所示：</p>
<p>最终的结果为：</p>
<p>.</p>
<p>可以看出，蒙特卡罗的效果比策略迭代差一些，由于使用蒙特卡罗法时我们对状态转移是未知的，因此存在一些差距是可以理解的。那么除此之外还有没有别的差距呢？下面就来分析其他的原因。</p>
<h3 id="7-3-探索与利用"><a href="#7-3-探索与利用" class="headerlink" title="7.3 探索与利用"></a>7.3 探索与利用</h3><p>7.2节介绍了如何使用交互得到的序列进行价值估计，本节就来讨论模型未知情况下的另一个问题，也是前面提到的关键问题之一：如何得到这些游戏序列。</p>
<p>当模型已知时，我们可以根据状态转移直接计算得到收敛的值函数，但当模型未知时，我们只能通过与环境交互得到交互序列。这里就存在一个问题：交互序列可以真实反映状态转移概率吗？在一些极端条件下这是可能的。我们假设通过与环境交互，所有的状态和行动组合全部被Agent经历过，而且每种情况都经历了足够多次，那么通过统计计算这些序列的长期回报，可以得到接近状态转移概率已知时得到的值函数。</p>
<p>但是对常见的问题来说，实现这个目标比较困难。主要原因在于可行的状态行动序列太多，想要对其进行一一尝试不太现实。虽然蛇棋的状态空间比较少，但棋盘中还是可能存在一些位置，这些位置几乎没有被玩家走到过，不是在此之前就遇到了梯子，就是投掷时刚好错过。由于缺乏数据，这一位置的长期回报无法精确统计。</p>
<p>对蛇棋这样的问题，我们还有一些更“暴力”的方法。因为状态是有限可数的，行动也是有限可数的，所以所有的状态-<br>行动对是可以模拟出来的，我们可以通过设定不同的游戏起始状态，使玩家将所有的位置经历一遍。这样数据缺乏的问题就得到了比较好的解决。既然蛇棋共有99个可行位置，那么我们让Agent依次从这些位置出发，经过反复的尝试，就可以收集到所有状态行动的交互信息。</p>
<p>不幸的是，这样的好事并不存在于每一个问题上，还是有很多问题，它们的状态是近乎无限的。以Atari游戏为例，它的游戏界面是指定尺寸的图像，但并不是所有指定尺寸的图像都是Atari的游戏。因此，我们可以把所有Atari游戏画面所在的空间想象成指定尺寸内所有图像空间的一个子空间，更麻烦的是，这个子空间的概率分布很难得到，因此遍历所有可能的画面是不可行的，我们必须采取其他方法。</p>
<p>那么，为了达到和基于模型的算法接近的效果，我们首先要做的是确保当前的问题有遍历所有状态-<br>行动对的可能。一般来说，我们认为问题的开始状态是一个集合，我们从中任选一个状态开始，并沿着这个状态序列执行所有可行的行动，那么当这个流程被反复执行到一定次数时，问题的所有状态-<br>行动对一定都会被经历。这段话听上去很拗口，又像是一段废话，但这是解决问题的基本条件。如果由于观察信息的损失的状态，导致无模型算法存在无法被感知和经历，那么无模型算法就很难得到最优解了。</p>
<p>既然我们认为这个前提一定满足，那么只要尽可能地让每一个状态下的行动都有概率执行，是不是就可以弥补模型未知的不足呢？这里可以做一个尝试，为了“雨露均沾”，我们必须让其他没有被选为最优策略的行动也参与到与环境交互的过程中，这样才能让每一个<br>_q_ （s _，_ a）都有值，这样做策略改进也就更有意义。</p>
<p>基于这个想法，我们改进了策略模块，这里采用一种叫 _∈_ -greedy的算法，首先随机生成一个0～1的数，然后用这个随机数进行判断，如果随机数小于某个值<br>_∈_ ，就采用完全随机的方式产生行动，此时每个行动产生的概率是一样的；如果随机数不小于某个值 _∈_<br>，就选择当前的最优策略。可以想象，在使用了这个算法后，很多由于策略而无法到达的状态也可以到达了，这样对状态价值的估计也会更准确，对应的示意图如图7-1所示。</p>
<p>图7-1 _∈_ -greedy效果对比图</p>
<p>图7-1中上图所示为没有使用 _∈_<br>-greedy的交互情况。图中的圆圈表示状态，因策略造成的状态转移用实线箭头表示。如果直接采用当前策略进行交互，由于策略是确定的，很可能有一些状态无法到达。使用<br>_∈_ -greedy算法后，所有的行动都有可能被执行，这样可到达的状态也开始变多。图7-1中下图所示为使用 _∈_<br>-greedy的交互情况，其中虚线箭头表示因随机策略造成的状态转移。 _∈_ -greedy的代码如下所示：</p>
<p>在这里，我们设定的 _∈_ =0 _._ 05，完成了这一步的修改，我们再将算法和策略迭代法进行对比，结果如下所示：</p>
<p>可以看出，虽然两种方法的最终策略有部分不同，但是模拟得到的分数是相同的。说明在增加了对不同方法的尝试后，算法有了大幅的提高，达到了模型已知的水平。</p>
<p>前面提到的 _∈_<br>-greedy算法实际上是在解决强化学习中的一个经典问题：探索与利用。这是两种与环境交互的策略。所谓的探索是指不拘泥于当前的表现，选择一些不同于当前策略的行动；利用就是持续使用当前的最优策略，尽可能地获得更多的回报。我们假设总的资源有限，例如时间有限，可以与环境进行交互的轮数有限，不能无止境地探索。所以，想要像前面所说等到所有的状态-<br>行动对被多次经历是很难实现的。那么，如何在有限的时间达到最佳的效果呢？</p>
<p>如果只探索不利用，对每一个行动分配等量的时间，那么最优的策略可能没有经过充分的尝试，得到的值函数不够准确，会使玩家最终丧失选择它的机会，而对非最优行动花费了大量的时间，实际上也是一种浪费。如果只利用不探索，就容易满足于当前得到的行动，陷入次优解，与最优解擦肩而过。所以单独执行其中任意一种策略都不是最好的，必须同时执行两种策略才能达到良好的结果。上面介绍的<br>_∈_ -greedy就是这样一种方法，它比较好地在两种策略间找到了平衡，用较高的性价比完成了交互的过程，并可以使用它求解出接近真实情况的价值。</p>
<h3 id="7-4-蒙特卡罗的方差问题"><a href="#7-4-蒙特卡罗的方差问题" class="headerlink" title="7.4 蒙特卡罗的方差问题"></a>7.4 蒙特卡罗的方差问题</h3><p>7.2节和7.3节介绍了蒙特卡罗法，它可以比较好地解决无模型场景下的蛇棋问题，在配合了 _∈_<br>-greedy策略后，它的效果和策略迭代法十分相近。当然，它也不是完美的，这些不完美体现在很多方面，接下来我们就来看其中的一个方面：估计值的方差较大。</p>
<p>前面我们已经知道，蒙特卡罗通过与环境交互得到序列的回报信息，然后用这些回报信息求平均，就可以得到估计的值函数。这个方法从理论上看是没有问题的，它能够成立的根本原因在于概率论中的大数定理。大数定理也是没有问题的，可是大数定理只说明了统计量的收敛性，并没有说明收敛的速度。很显然，如果采样集合的方差比较大，那么想让均值收敛就需要更长的时间；如果采样序列的方差较小，那么收敛的速度也会相应地加快。</p>
<p>基于上面的分析，我们就来看看7.3节中蒙特卡罗法的采样存不存在方差方面的问题。我们用蒙特卡罗法完成一次计算，同时保存所有状态值为50，行动值为1的长期回报值，并最终将所有的值综合起来做成一个直方图，如图7-2所示。</p>
<p>图7-2 蒙特卡罗法every-visit方法的长期回报直方图</p>
<p>可以看出，由于持续时间比较长，长期回报的方差比较大，模型在收敛方面相对困难。导致方差大的因素有很多，例如每一次投掷骰子的数字都不同，那么到达终点的时刻也不一样，同一个位置计算得到的长期回报也自然不一样。</p>
<p>除此之外，就是交互序列中的采样频率这个因素。7.3 节的采样方法一般被称为every-visit，也就是说，交互序列中的每一个状态-<br>行动对都会参与到计算中，这样就为统计带来了困难。以蛇棋游戏为例，由于有梯子的存在，交互序列中可能会多次出现同一个状态，例如例子中的位置“50”。当采用every-<br>visit方法时，同一状态的长期回报都会放入值函数中进行计算，在计算的过程中没有区分第一次到达“50”和第二次到达“50”的值，而是将这两个长期回报等权重地加在一起。由于这两次的长期回报存在差异，因此这样的统计方式会让方差增大，从而使期望值难以收敛。上述形式如图7-3所示。</p>
<p>图7-3 使用every-visit方法对重复出现的状态的处理</p>
<p>那么该如何应对这个问题呢？一个解决方案是把every-visit方法换掉，改成first-visit方法。所谓first-<br>visit法就是只统计状态第一次出现时的长期回报，后面出现的同一状态则不再统计。从方法的理念来说，因为它只在一个序列中对相同状态取出一次，所以同一个交互序列内因多次出现同一状态而造成方差增大的问题将得到解决。我们将every-<br>visit换成first-visit，执行与上面相同的实验，可以得到如图7-4所示的直方图。</p>
<p>图7-4 蒙特卡罗法first-visit的return收集直方图</p>
<p>从结果来看，方法first-visit的效果与every-<br>visit相比稍好，但并没有明显的提高。因为前面提到的其他因素依然影响着结果，每次交互的数值差距依旧很大，这是蒙特卡罗法的缺点。但是在数据量足够大时，我们也可以看到它的优点：对期望值的估计是无偏的，这一点可以很轻易地证明。所以最终我们可以得到：蒙特卡罗法在估计价值上具有无偏差但方差较高的特点。当交互的数据量足够大时，蒙特卡罗法是有优势的，但是当交互的数据量不足时，它也会暴露出自己的缺点。</p>
<p>熟悉机器学习的读者一定会想到，这似乎和机器学习中的偏差与方差问题（bias and variance<br>problem）类似，那么我们能不能找到一个低方差的方法呢？</p>
<h3 id="7-5-时序差分法与SARSA"><a href="#7-5-时序差分法与SARSA" class="headerlink" title="7.5 时序差分法与SARSA"></a>7.5 时序差分法与SARSA</h3><p>为了使价值评估的方差进一步降低，本节将介绍另一种方法：时序差分法（Temporal<br>Difference，简称TD法）。TD法是一种结合了蒙特卡罗法和动态规划法的方法。从算法的主体结构来看，它同蒙特卡罗法类似，同样通过模拟交互序列的方式进行求解；但是从算法的核心思想来看，它同时用到了强化学习中的经典公式：Bellman公式进行自迭代更新。</p>
<p>前面提到状态-行动值函数的Bellman公式的形式为</p>
<p>那么利用蒙特卡罗法，可以将公式变为</p>
<p>TD法和蒙特卡罗法有一些不同，蒙特卡罗法在估计价值时使用了完整序列的长期回报，而TD法使用了当前回报和下一时刻的价值估计，但是两个方法的公式在形式上还是有些类似的。如果把TD法像蒙特卡罗法一样写成迭代更新的形式，就可以得到</p>
<p>这个公式就是TD法的基本形式。我们可以看出这个方法和蒙特卡罗法、动态规划法的关系。从形式上看，蒙特卡罗法会通过大量的采样估计出值函数的统计量，而TD法也同样需要采样这个过程；从更新公式上看，TD法通过下一时刻的价值更新前一时刻的价值，似乎是承认了下一时刻的回报和价值足够优秀，从而利用了这个最优子结构进行更新。这和动态规划的思想是一致的。</p>
<p>前面叙述的算法属于TD法中的一种，被称为SARSA算法，这个名字看上去有些奇怪，它来自于这个方法的五个关键因子：S（当前状态）、A（当前行动）、R（模拟得到的奖励）、S（模拟进入的下一个状态）和A（模拟中采取的下一个行动）。从上面的公式也可以看出，更新公式中确实只有这5个变量。蒙特卡罗法和SARSA算法的对比如图7-5所示。</p>
<p>图7-5 蒙特卡罗法和SARSA算法的对比</p>
<p>接下来就来实现这个算法，其实它的实现过程比较简单，这里只展现价值评估的过程，对应的代码和注释如下所示：</p>
<p>那么这个代码的实现效果如何呢？我们同样通过一段代码进行测试。def td demo（）：</p>
<p>为了试验效果，我们将SARSA算法中的迭代轮数增加到2000，最终的结果为：</p>
<p>从结果可以看出，SARSA算法的效果与蒙特卡罗法相比并不够好，那么它和蒙特卡罗法相比有什么优势呢？我们来思考前面提到的方差问题，我们同样取交互过程中状态为50，行动值为1的价值估计值，再来看一看SARSA算法的结果，如图7-6所示。</p>
<p>图7-6 SARSA算法的长期回报收集直方图</p>
<p>可以看出，SARSA算法的数值变动和蒙特卡罗法相比更稳定，跨度比之前小。</p>
<p>那么，为什么TD系列的方法会在方差方面控制得更好呢？答案就在它的更新公式上。前面提到蒙特卡罗的计算方法由于使用了完整的采样得到了长期回报值，所以在价值的估计上的偏差更小，但同时它需要收集完整序列的信息，而序列存在一定的波动，所以价值的方差会比较大。</p>
<p>而TD方法只考虑了当前一步的回报值，其余的计算均使用了之前的估计值，所以当整体系统没有达到最优时，这样的估计都是存在偏差的，但是由于它只估计了一步，所以它在估计值方面受到的波动比较小，因此方差也会相应减小许多。</p>
<p>所以前人发现，蒙特卡罗法和SARSA算法象征着两个极端：一个为了追求极小的误差使方差变大，一个为缩小方差使误差变大。</p>
<h3 id="7-6-Q-Learning"><a href="#7-6-Q-Learning" class="headerlink" title="7.6 Q-Learning"></a>7.6 Q-Learning</h3><p>7.5节我们介绍了基于TD思想的SARSA算法，它的核心公式为</p>
<p>接下来，我们要介绍的另一种TD的算法叫作Q-Learning，它的基本公式为</p>
<p>两个算法的差别只在其中一项，SARSA算法遵从了交互序列，根据下一步的真实行动进行价值估计；Q-Learning算法没有遵循交互序列，而是在下一时刻选择了使价值最大的行动。这个差别使两种算法在效果上产生了一定的不同。我们先来看Q-<br>Learning算法的效果，首先是算法的主体框架，这个框架和蒙特卡罗法、SARSA算法基本一致，策略改进部分完全相同：</p>
<p>对应的策略评估代码为：</p>
<p>使用同前几节相同的评估方法，我们可以得到最终的评估结果：</p>
<p>可以看出，虽然Q-Learning算法和策略迭代法比还有点差距，但是Q-Learning算法比SARSA算法好一些。</p>
<p>前面提到了SARSA算法和Q-Learning算法在公式上的不同，实际上这两种算法代表了两种策略评估的方式，分别是On-Policy和Off-<br>Policy。On-Policy对值函数的更新是完全依据交互序列进行的，我们在计算时认为价值可以直接使用采样的序列估计得到；Off-<br>Policy在更新值函数时并不完全遵循交互序列，而是选择来自其他策略的交互序列的子部分替换了原本的交互序列。从算法的思想上来说，Q-Learning的思想更复杂，它结合了子部分的最优价值，更像是结合了价值迭代的更新算法，希望每一次都使用前面迭代积累的最优结果进行更新。</p>
<p>实际上，这也给Q-<br>Learning算法带来了一些问题。由于它使用最优价值的行动替代了交互时使用的行动，所以在估计时就有可能造成对状态行动的“过高估计”。由于Q-<br>Learning算法本质上还是采用了采样的方法，所以对于被采样的状态行动，Q-Learning会对其“过高估计”，而对于没采样到的状态行动，则无法享受“过高估计”的待遇，那么两者之间就可能产生比较大的差距，这些差距会造成算法的波动，在真实的应用场景中更是如此。</p>
<h3 id="7-7-Q-Learning的收敛性分析"><a href="#7-7-Q-Learning的收敛性分析" class="headerlink" title="7.7 Q-Learning的收敛性分析"></a>7.7 Q-Learning的收敛性分析</h3><p>总的来说，Q-Learning是一个比较优秀的算法，它具有较好的收敛性质。本节我们通过公式推导得出结果。</p>
<p>对于一般的收敛问题，我们通常需要一个框架对问题进行定义。一般认为问题定义的变量存在于一个空间中，空间拥有对于度量的定义，以及空间中某些运算的定义。同时，有的空间中存在一些“收缩算子”，对这些收缩算子来说，存在至少一个稳定点，使得我们从任意一个点出发，经过不断地与这些收缩算子运算，最终无限地靠近稳定点。如果我们可以证明一个算子是收缩算子，那么经过算子计算的点列就具备了一定的收敛性；如果稳定点恰好又具有我们想要的性质，那么这个算子就是我们想要的算子。上面的概念比较抽象，读者可能不太容易理解，下面我们把这些概念替换成Q-<br>Learning中的概念。</p>
<p>· 前面提到 _q_ （s _，_ a）是状态行动值函数，我们定义一个空间，这个空间中的每一个点表示 _q_ （s _，_ a）的一种可能的形式。</p>
<p>·<br>度量可以衡量空间内两个点的距离，现在空间内的每一个点都是一个值函数，和我们常见的点并不相同，这里我们用无穷阶范数作为衡量两个“点”的距离的方法，范数的公式为</p>
<p>也就是说，找出两个值函数中相差最大的那一项作为距离。这个距离的定义十分重要，Q-Learning算法并不是在所有的度量上都能收敛的。</p>
<p>· 空间中的算子H就是Q-Learning的计算公式：</p>
<p>我们可以认为，通过计算，我们将空间中的一个“点”移动到了另一个“点”上。</p>
<p>· 空间中的稳定点就是最终收敛的值函数，我们记为 _q_ ∗ （s _，_ a）。</p>
<p>· 我们需要证明空间中的任意一点经过算子H的运算可以收敛到稳定点。</p>
<p>这个问题需要分两步证明，首先证明算子H是一个收缩算子（Contraction Opera-tor），然后证明采用Q-<br>Learning的计算方法可以收敛到最优价值。</p>
<p>我们先来证明第一步：对于算子H，存在一个 _γ ＞_ 0，使得对任意的值函数 _q_ 1 和 _q_ 2 ，都有</p>
<p>证明过程如下：</p>
<p>根据绝对值的不等式| _a_ + _b_ |≤| _a_ |+| _b_ |进行推导，可以得到</p>
<p>令s∗ _，_ b∗ =argmaxs，b |maxb _q_ 1 （s _，_ b）-maxb _q_ 2 （s _，_<br>b）|，用它表示两个值函数差距的最大值。继续推导可以得到</p>
<p>经过上面的证明，我们得出了想要的结果。下面就来证明第二步：Q-Learning的收敛效果。</p>
<p>我们假设策略π能够覆盖所有的状态-行动组合：</p>
<p>用下面的公式对 _q_ 函数进行更新：</p>
<p>_q_ T+1 （xt _，_ at ）= _q_ T （xt _，_ at ）+ _α_ T （xt _，_ at ）[rt + _γ_ maxb∈A<br>_Q_ T （xt+1 _，_ b） _-Q_ T （xt _，_ at ）]其中，0 _＜α_ t （xt _，_ at ） _＜_ 1</p>
<p>在证明之前，我们需要使用一个定理：</p>
<p>输出在Rn 上的随机过程{∆t }的更新过程定义为</p>
<p>如果下面的条件能够满足，那么它将依概率1收敛到0：</p>
<p>（1）0≤ _α_ t ≤1 _，_ ∑t _α_ t （x）=∞，同时保证（2）‖ _E_ [ _F_ t （x）| _F_ t ]‖W ≤ _γ_<br>‖∆t ‖W ，同时 _γ ＜_ 1。</p>
<p>，对所有的 _C ＞_ 0。</p>
<p>如果我们能按照上面的定理整理出相应的形式，就能使用它得出Q-Learning算法收敛的证明。证明过程如下：</p>
<p>将Q-Learning的更新公式进行重新组合，可以得到</p>
<p>假设值函数可以收敛且最终的收敛值为 _q_ ∗ （st _，_ at ），公式两边同时减去收敛值，可以得到</p>
<p>令∆T （s _，_ a）= _q_ T （s _，_ a） _-q_ ∗ （s _，_ a），可以得到</p>
<p>再令 _F_ T （s _，_ a）= _r_ （s _，_ a _，X_ （s _，_ a））+ _γ_ maxb∈A _q_ T （st+1 _，_<br>b） _-q_ ∗ （st _，_ at ）这样上面定理中随机过程的更新公式就可以得到了</p>
<p>.</p>
<p>下面分别看定理需要满足的三个条件。其中条件一比较简单，我们假设</p>
<p>那么有</p>
<p>这样就得到了证明。与此类似的数列还有很多，它们都适用于这个定理，而且在实践中也具有一定的操作性，这里就不再一一介绍。</p>
<p>对于条件二，我们有</p>
<p>前面我们将Q-Learning运算表示成一个收缩算子H，这里可以用它替换公式，得到</p>
<p>令前面定理的 _W_ 为无穷阶范数‖x‖∞ ，可以得到</p>
<p>这样第二个条件就得到了证明。</p>
<p>关于第三个条件，有</p>
<p>我们知道，一般情况下回报值是有界的，且 _γ ＜_ 1，那么上面公式中两个项目都是有界的，所以对应的方差也必然是有界的，令上式小于某个常数 _C_ 1<br>，我们再找到某个常数 _C_ 2 ，使得</p>
<p>就可以得到</p>
<p>这样第三个条件也被证明可以满足。通过上面的证明可以得到，Q-Learning算法可以使 _Q_ 函数依概率1得到收敛。通过前面复杂的证明，我们证明出Q-<br>Learning的收敛性可以得到保障，当然是有一定条件的。除了SARSA法和Q-Learning算法，还有其他用于估计价值的方法，例如Expected-<br>SARSA方法等，在此不再赘述。</p>
<h3 id="7-8-从表格形式到价值模型"><a href="#7-8-从表格形式到价值模型" class="headerlink" title="7.8 从表格形式到价值模型"></a>7.8 从表格形式到价值模型</h3><p>在前面的章节中，我们对价值模型一直有一个很大的限制，那就是它们需要用表格的形式表示。包括7.7节的证明，都是基于表格形式才能成立的。表格形式对于求解有很大的帮助，但它也有自己的缺点。如果问题的状态和行动的空间非常大，使用表格表示将难以求解，因为我们需要将所有的状态行动价值都求解出来，才能保证对于任意一个状态和行动，我们都能够得到对应的价值。</p>
<p>以5.1节提到的Atari游戏为例，经过裁剪处理后，状态空间数为25684×84<br>个，行动数一般不超过10个，这样算下来想用表格的形式表达所有的价值，对于现在的计算机系统来说是一个很有挑战的事情。因此，我们需要使用其他的方法。于是研究人员就开始研究一些更合适的表达方式，这时机器学习的方法终于登场，我们可以建立一些模型，并用一个模型来表示状态、行动到值函数的关系。</p>
<p>我们令状态为s∈S，行动为a∈A，最终的值函数为 _v_ ∈R，那么我们要建立这样一个映射：</p>
<p>这样我们就把强化学习中求解值函数的问题转换成了监督学习的问题，而监督学习是读者熟悉的学习方法，做起来更得心应手。我们可以把这个问题看作回归问题，于是所有可以用于回归问题的模型都可以被我们用上，例如线性回归（Linear<br>Regression）、支持向量回归（Support Vector Regression）、决策树（Decision Tree），以及神经网络（Neural<br>Network）等。在后面的章节中，我们将经常使用一个深层的神经网络完成这个映射的构建。</p>
<p>为了学习值函数模型，我们需要设定模型的目标函数。最简单的目标函数自然是平方损失函数，已知我们通过Q-Learning算法得到的状态行动价值为目标价值 _v_<br>i ，而模型计算得到的价值为估计价值，那么目标值为</p>
<p>其中也被称为TD-Error。定义了目标函数，下面就可以利用机器学习中经典的梯度下降法求解：</p>
<p>这个公式是不是看上去很眼熟？实际上，如果，那么模型的最优解就等于也就是所有训练数据的平均值，这和表格版的计算公式一致，也就是说表格版的算法实际上也是一种模型，只不过它的梯度处处为1。这样，机器学习方法和表格法得到了统一，我们可以认为表格法是一种参数量为|S|×|A|的模型，它的优点是可以完美拟合任意的模型（因为每一点可以独立收敛），缺点是它需要太多的参数。在实践中，表格法有时是无法进行训练的；而对于拟合得到的模型来说，它的参数数量相对可控，我们可以确保模型训练是可行的，但是它的收敛性就不再有保证。</p>
<p>我们知道不同的模型有不同的拟合能力，参数数量少的模型的拟合能力一般比参数数量多的模型差，所以面对不同的真实的值函数，不同的模型可能会拟合出不同的效果，有的结果靠近最优值，有的远离最优值。我们可以用二维平面上的一个点表示值函数的最优点，用一条线表示某个模型能表示的所有值函数的集合，两者的关系如图7-7所示。有时模型并不能够拟合得到最优价值，只能够得到近似值。</p>
<p>在7.7节中，我们证明了Q-<br>Learning方法可以使值函数得到收敛，那么使用模型进行拟合后，这个结论还能不能成立呢？有时结论是成立的，而有时会得到相反的效果。对于Q-<br>Learning算法，我们可以认为每进行一轮更新，得到的值函数会更靠近最优值，而对于基于最小二乘法的优化，我们认为这是一个将值函数投影到模型拟合范围的方法。如图7-8所示，经过优化，值函数会投影到模型范围上。</p>
<p>图7-7 最优值函数和模型拟合范围的关系图</p>
<p>图7-8 最小二乘法优化后值函数的投影效果</p>
<p>如果只使用Q-<br>Learning算法，值函数最终将收敛至最优价值；如果只使用最小二乘法，模型会收敛到最靠近最优价值的位置上。那么看上去，结合这两种方法，我们能够得到最好的效果，但实际上有时并非如此。我们假设当前的参数模型、最优值和拟合范围如图7-9所示，首先经过Q-<br>Learning的更新，值函数将更新至如图7-10所示的位置，模型的效果得到了提升，然而经过最小二乘法的投影后，模型反倒被投影到了更远的位置，如图7-11所示。</p>
<p>图7-9 收敛反例的状态图</p>
<p>图7-10 TD法更新后的状态图</p>
<p>图7-11 最小二乘法更新后的状态图</p>
<p>所以在一些情况下，这两个步骤并不能保证模型更新的一致收敛，因此模型的能力成了算法收敛的关键。越靠近表格形式的模型，拟合能力越强，也越不容易发生上面所述的情况。深层模型在拟合上的能力很强，这也使它成为了近年来拟合值函数最常见的模型。接下来我们介绍近年来强化学习模型的代表之一：Deep<br>Q Network。</p>
<h3 id="7-9-Deep-Q-Network"><a href="#7-9-Deep-Q-Network" class="headerlink" title="7.9 Deep Q Network"></a>7.9 Deep Q Network</h3><p>在前面的章节中我们介绍了Q-Learning和深层模型的优势，本节就来介绍这个具有极大突破的模型Deep Q Network，简称DQN，方法来自论文<br>_Human-leνel control throughdeep reinforcement learning_ [2]<br>。论文主要介绍了如何使用DQN网络训练Agent在Atari游戏平台上尽可能获得更多的分数。基于前面章节的学习，它的整体算法思路就不那么复杂了：在使用Q-<br>Learning作为优化算法的前提下，使用深层神经网络表示值函数，直接使用游戏图像和得分进行训练，不再使用其他任何领域的知识。</p>
<p>在这篇论文之前，大多数的强化学习问题都需要研究任务的领域知识，同时需要解决同一序列内样本之间相互关联的问题，以及Q-<br>Learning算法中价值估计值与更新值相互关联的问题。这篇论文提出了一系列的解决方案，这些方案虽然并非首创，但是通过将这一系列的设计和改进融入其中，使模型在Atari游戏上获得了更好的效果，模型在部分游戏上达到了媲美专家级玩家的效果，整体的算法结构如图7-12所示。</p>
<p>图7-12 Q-Learning算法的结构</p>
<p>我们可以从图7-12中的序号看出模型的计算流程及作者在模型上的一些小改进。</p>
<p>1.数据预处理</p>
<p>虽然DQN的作者表示算法直接使用了游戏的画面信息进行学习，但实际上为了算法能够更高效地执行，一些预处理工作还是需要的。Atari游戏的原生尺寸为210×160，每个像素有128种颜色，我们首先将其转换成84×84维度的灰度图。以Breakout游戏为例，转换前后的效果如图7-13所示，可以看出变换后的图像依然保留了主要的信息，同时减轻了数据处理的负担。</p>
<p>虽然Atari游戏是一个动态的游戏，但是每一时刻Agent只能从环境中获得1帧信息，而这种静态的图像信息很难表示游戏的动态信息。以乒乓球游戏为例，当画面静止时，我们无法判断球要飞向哪一方。为此，算法将收集从当前时刻起的前<br>_N_ 帧画面，并将这些信息结合起来作为模型的输入。获得了一定时间内集合的状态信息，模型可以学习到更准确的行动价值，在实验中 _N_ 被设置为4。</p>
<p>图7-13 游戏图像预处理效果</p>
<p>除了游戏的画面，游戏的得分也需要做预处理。不同的Atari游戏的得分系统不同，有的得分可以上万，有的得分只有几十。为了模型能够更好地拟合长期回报，我们需要将得分压缩到模型擅长处理的范围中，我们将每一轮得到的回报压缩到[-1<br>_，_ 1]。虽然对游戏来说有些不合理，但这样的数值确实更方便模型处理。</p>
<p>2.环境交互</p>
<p>Atari游戏的可行状态数量非常多，因此如何更好地探索更多的状态变得十分关键。DQN采用了 _∈_<br>-greedy的策略，一开始策略以100%的概率随机产生行动，随着训练的不断进行，这个概率将不断衰减，最终衰减至10%。也就是说，有90%的概率执行当前最优策略。这样，从以探索为主的策略逐渐转变成以利用为主的策略，两者得到了很好的结合。</p>
<p>3.模型结构</p>
<p>前面提到价值模型是从|S|×|A|到R的映射，采用这种方法构建模型自然是可以的，但是它还有一些缺点。当模型需要通过值函数求解最优策略时，我们需要计算|A|次才能求出，在实际中这样的计算方法效率较低。为了简化计算，我们可以将模型变成这样的形式，模型的输出为长度为|A|的向量，向量中的每一个值表示对应行动的价值估计。这样我们只需要一次计算就可以求出所有行动的价值，无论行动有多少，我们评估价值的时间是一样的。两个模型的表示形式差异如图7-14所示。</p>
<p>模型的主体采用了卷积神经网络的结构。</p>
<p>图7-14 模型形式的转变</p>
<p>第一层卷积层的卷积核为8×8，stride=4，输出的通道数为32，此后应用ReLU非线性层。</p>
<p>第二层卷积层的卷积核为4×4，stride=2，输出的通道数为64，此后应用ReLU非线性层。</p>
<p>第三层卷积层的卷积核为3×3，stride=1，输出的通道数为64，此后应用ReLU非线性层。</p>
<p>第四层是全连接层，输出的维度为512，此后应用ReLU非线性层。</p>
<p>最后一层全连接层得到对应行动的价值估计，输出维度与可行的行动数量相关，一般在4～18个。</p>
<p>4.Random Start</p>
<p>有些Atari游戏的起始画面是完全一样的，例如Space<br>Invader，如果每次我们都从一个固定的场景开始做决策，那么Agent总要对这些相同的画面进行决策，这显然不利于我们探索更多的画面进行学习。为了增强探索性同时不至于使模型效果变糟，我们可以设定在游戏开始很短的一段时间内，让Agent执行随机的行动，这样可以最大程度地获得不同的场景样本。在5.2.3节我们分析了Baselines中实现的这部分代码，感兴趣的读者可以回顾相关的实现。</p>
<p>5.Frame-Skipping</p>
<p>除了上面对模型的设计，作者还有一个结合现实的考量。虽然我们可以根据游戏的每一个状态给出一个输出，但是实际游戏中玩家并不需要如此高频率的操作。第1章提到模拟器可以达到每秒60帧的显示速率，但是实际上人类玩家无法实现如此高频率的操作，因此我们可以设定每隔一定帧数执行一次行动，例如每隔4帧进行一次行动选择，那么中间的几帧将重复执行前面选择的行动，这样相当于模仿了人类按下某个按钮并持续一段时间的效果。此外，相邻帧之间的画面存在着极大的相似性，对于十分相似的画面，我们通常可以采用相同的行动，因此这样跳过一定帧数的判断也是合理的。在5.2.3节我们分析了Baselines中实现的这部分代码，感兴趣的读者可以回顾一下相关的实现。</p>
<p>除了上面介绍的这些要点，还有两个十分重要的创新值得用更多的篇幅介绍，下面就来详细分析这两个创新：Replay Buffer和Target Network。</p>
<p>1.Replay Buffer</p>
<p>前面我们已经了解了Q-Learning的计算方法。Q-Learning方法基于当前策略进行交互和改进，更像是一种在线学习（Online<br>Learning）的方法，每一次模型利用交互生成的数据进行学习，学习后的样本被直接丢弃。但如果使用机器学习模型代替表格式模型后再采用这样的在线学习方法，就有可能遇到两个问题，这两个问题也都和机器学习有关。</p>
<p>（1）交互得到的序列存在一定的相关性。交互序列中的状态行动存在着一定的相关性，而对于基于最大似然法的机器学习模型来说，我们有一个很重要的假设：训练样本是独立且来自相同分布的，一旦这个假设不成立，模型的效果就会大打折扣。而上面提到的相关性恰好打破了独立同分布的假设，那么学习得到的值函数模型可能存在很大的波动。</p>
<p>（2）交互数据的使用效率。采用梯度下降法进行模型更新时，模型训练往往需要经过多轮迭代才能收敛。每一次迭代都需要使用一定数量的样本计算梯度，如果每次计算的样本在计算一次梯度后就被丢弃，那么我们就需要花费更多的时间与环境交互并收集样本。</p>
<p>为了解决这两个问题，作者使用了Replay<br>Buffer这个数据结构。这个结构被翻译为“样本回放缓存区”，因为这个名字比较拗口，所以在接下来的内容中我们使用英文表达。Replay<br>Buffer的结构和交互流程如图7-15所示。</p>
<p>图7-15 Replay Buffer的结构图</p>
<p>从图7-15中可以看出，Replay<br>Buffer保存了交互的样本信息，一般来说每一个样本都会保存当前的状态s、行动a和长期累积回报v。对于其他的算法，Replay<br>Buffer还会保存其他的信息（第11章中将介绍）。Replay<br>Buffer的大小通常会设置得比较大，例如，将上限设置为可以存储100万个样本，这样较长一段时间的样本都可以被保存起来。在训练值函数时，我们就可以从中取出一定数量的样本，根据样本记录的信息进行训练。</p>
<p>总的来说，Replay Buffer包含了收集样本和采样样本两个过程。收集的样本按照时间先后顺序存入结构中，如果Replay<br>Buffer已经存满样本，那么新的样本会将时间上最久远的样本覆盖。而对采样来说，如果每次都取出最新的样本，那么算法就和在线学习相差不多；一般来说，Replay<br>Buffer会从缓存中均匀地随机采样一批样本进行学习。</p>
<p>均匀采样的好处是什么呢？前面提到我们交互得到的序列在时间维度上存在一定的相关性。我们希望学习得到的值函数能够表示在当前状态行动下的长期收益的期望，然而每一次交互得到的序列，只能代表当前状态-<br>行动下的一次采样轨迹，并不能代表所有可能的轨迹，这样估计的结果就和期望的结果存在一定的差距。随着交互时间不断拉长，这个差距的累积会越来越大。如果完全使用序列的估计值进行训练，某一轮训练时模型会朝着一个序列的估计训练，另一轮训练又会朝着另一个序列的估计训练，那么模型很容易产生较大波动。采用均匀采样后，每次训练的样本通常来自多次交互序列，这样单一序列的波动就被减轻很多，训练效果也就稳定了很多。同时，一份样本也可以被多次训练，提高了样本的利用率。</p>
<p>2.Target Network</p>
<p>模型不稳定的另外一个原因来自算法本身。从Q-Learning的计算公式可以看出，算法可以分成如下两个步骤。</p>
<p>（1）计算当前状态行动下的价值目标值：∆ _q_ （s _，_ a）= _r_ （s′ ）+maxa ′ _q_ T-1 （s′ _，_ a′ ）。</p>
<p>（2）网络模型的更新：。</p>
<p>可以看出模型通过当前时刻的回报和下一时刻的价值估计进行更新，这既像一场自导自演的电影，又像一场既当运动员又当裁判员的比赛。这里存在一些隐患，前面提到数据样本差异可能造成一定的波动，由于数据本身存在着不稳定性，每一轮迭代都可能产生一些波动，如果按照上面的计算公式，这些波动会立刻反映到下一个迭代的计算中，这样我们就很难得到一个平稳的模型。为了减轻相关问题带来的影响，我们需要尽可能地将两个部分解耦。</p>
<p>为此论文作者引入了目标网络（Target Network），它能缓解上面提到的波动性问题。这个方法引入了另一个结构完全一样的模型，这个模型被称为Target<br>Network，而原本的模型被称为表现模型（Behavior Network）。两个模型的训练过程如下所示。</p>
<p>（1）在训练开始时，两个模型使用完全相同的参数。</p>
<p>（2）在训练过程中，Behavior Network负责与环境交互，得到交互样本。</p>
<p>（3）在学习过程中，由Q-Learning得到的目标价值由Target Network计算得到；然后用它和Behavior<br>Network的估计值进行比较得出目标值并更新Behavior Network。</p>
<p>（4）每当训练完成一定轮数的迭代，Behavior Network模型的参数就会同步给Tar-get Network，这样就可以进行下一个阶段的学习了。</p>
<p>通过使用Target Network，计算目标价值的模型在一段时间内将被固定，这样模型可以减轻模型的波动性。</p>
<p>以上就是DQN模型的主要内容，完整的算法流程如下所示。</p>
<p>算法 DQN算法</p>
<p>Input</p>
<p>初始化容量为 _N_ 的Replay Buffer： _D_</p>
<p>初始化状态行动价值模型 _Q_ 和参数 _θ_</p>
<p>初始化Target Network^ _Q_ 和参数 _θ - _</p>
<p>Start</p>
<p>for episode=1 _,M_ do</p>
<p>初始化环境，得到初始状态s1 ，并预处理得到 _φ_ 1 = _φ_ （s1 ）</p>
<p>for _t_ =1 _,T_ do</p>
<p>以 _∈_ 的概率随机选择一个行动at ，或者根据模型选择当前最优</p>
<p>at =maxa _Q_ ∗ ( _φ_ (st ) _,_ a; _θ_ )</p>
<p>执行行动at ，得到新一轮的状态st+1 和回报rt+1</p>
<p>预处理得到 _φ_ t+1 = _φ_ （st+1 ）</p>
<p>将{ _φ_ t _，_ at _，_ rt+1 _，φ_ t+1 }存储到 _D_ 中</p>
<p>从 _D_ 中采样一批样本（ _φ_ j _，_ aj _，_ rj+1 _，φ_ j+1 ）</p>
<p>根据目标函数（yj _-Q_ （ _φ_ j _，_ aj ； _θ_ ））2 进行梯度下降法求解</p>
<p>每隔 _C_ 轮完成参数更新 _θ_ - ← _θ_</p>
<p>end for</p>
<p>end for</p>
<p>从论文中的介绍和实际的效果看，DQN已经实现了很大的飞跃，效果有了显著的提高，但这还不够，第8章我们将继续介绍DQN的改进算法。</p>
<h3 id="7-10-总结"><a href="#7-10-总结" class="headerlink" title="7.10 总结"></a>7.10 总结</h3><p>本章我们学习了无模型的学习算法，以及其中的经典算法DQN，下面让我们回顾一下。</p>
<p>（1）在无模型问题中，我们将无法使用状态转移概率进行策略求解。</p>
<p>（2）蒙特卡罗法通过统计完整序列的长期回报来完成策略评估的过程。</p>
<p>（3）TD法通过当前时刻的回报和下一时刻开始的长期回报估计来完成策略评估的过程。</p>
<p>（4）SARSA法和Q-Learning法分别代表了On-Policy和Off-Policy的TD法。</p>
<p>（5）DQN结合了Q-Learning的价值估计方法和深层模型较强的拟合效果，同时还结合了Replay Buffer和Target<br>Network这样的结构，在Atari游戏上获得了不错的结果。</p>
<h3 id="7-11-参考资料"><a href="#7-11-参考资料" class="headerlink" title="7.11 参考资料"></a>7.11 参考资料</h3><p>[1] Mnih V,Kavukcuoglu K,Silver D,et al.Playing Atari with Deep Reinforcement<br>Learn-ing[J].Computer Science,2013.</p>
<p>[2] Mnih V,Kavukcuoglu K,Silver D,et al.Human-level control through deep<br>reinforce-ment learning[J].Nature,2015,518(7540):529.</p>
<h2 id="8-DQN的改进算法"><a href="#8-DQN的改进算法" class="headerlink" title="8 DQN的改进算法"></a>8 DQN的改进算法</h2><p>第7章介绍了Q-<br>Learning和DQN的算法细节。本章继续介绍与DQN相关的算法，这些算法都是基于DQN算法的扩展算法，它们从不同的角度解决了DQN中存在的问题，也获得了更好的效果。</p>
<p>为了减少模型的波动性，Double<br>Q-Learning可以使用不同的模型完成最优行动的选择和价值估计两个工作，使价值估计的波动降低；第7章介绍了Replay<br>Buffer的概念和作用，本章将介绍基于它的改进算法Priority Replay<br>Buffer，它可以更高效地利用存储的样本；DQN的模型输出的是状态行动值函数 _q_ （s _，_ a），它用一个单一的数值表示了价值期望，Duel<br>DQN将 _q_ （s _，_ a）拆解成 _v_ （s）+adv（s _，_ a）两部分；而Distributional<br>DQN则以类似直方图的形式表示了价值的分布；Noisy DQN则通过增加参数的随机性增强模型的探索性能；Rainbow模型则将前面的改进融合在了一起。</p>
<h3 id="8-1-Double-Q-Learning"><a href="#8-1-Double-Q-Learning" class="headerlink" title="8.1 Double Q-Learning"></a>8.1 Double Q-Learning</h3><p>本节介绍Double Q-Learning，算法来自论文 _Deep Reinforcement Learning with Double<br>Q-learning_ [1] 。在第7章介绍DQN时，曾提到模型中的一个核心结构Target<br>Network。DQN使用了两个结构相同的网络：Behavior Network和Target Network。通过在一段时间内固定Target<br>Network的参数，Q-Learning方法的目标价值能够得到一定的固定，这样模型也能够获得一定的稳定性。</p>
<p>虽然这个方法提升了模型的稳定性，但它并没有解决另外一个问题：Q-Learning对价值过高估计的问题。我们知道Q-<br>Learning在计算时利用了下一时刻的最优价值，所以它通常在计算时给出了一个状态行动的估计上限。由于训练过程中模型并不够稳定，因此对上限的估计可能会存在一定的偏差。如果偏差是一致的，也就是说，每个行动都拥有相近的偏差，那么偏差对模型效果的影响相对较小；如果偏差是不一致的，那么这个偏差会造成模型对行动优劣的判断偏差，这样就会影响模型的效果。</p>
<p>我们已经知道Target Network求解价值目标值的公式</p>
<p>其中 _θ_ - 表示Target Network的参数。将公式进一步展开，可以得到更详细的公式内容：</p>
<p>公式展开后，我们发现采用Target<br>Network后，模型在选择最优行动和计算目标值时依然使用了同样的参数模型，这样必然会造成对价值的过高估计。为了尽可能地减少过高估计的影响，一个简单的办法就是把选择最优行动和估计最优行动两部分的工作分离，我们用Behavior<br>Network完成最优行动的选择工作，这样就可以得到</p>
<p>通过这样的变化，算法在三个环节的模型安排如下。</p>
<p>（1）采样：Behavior Network _Q_ （ _θ_ ）。</p>
<p>（2）选择最优行动：Behavior Network _Q_ （ _θ_ ）。</p>
<p>（3）计算目标价值：Target Network _Q_ （ _θ_ - ）。</p>
<p>经过这样的变换，模型在过高估计的问题上得到了缓解，稳定性也就得到了提高。</p>
<h3 id="8-2-Priority-Replay-Buffer"><a href="#8-2-Priority-Replay-Buffer" class="headerlink" title="8.2 Priority Replay Buffer"></a>8.2 Priority Replay Buffer</h3><p>Priority Replay Buffer是一种针对Replay Buffer的改进结构，它来自论文 _Priori-tized Experience<br>Replay_ [2] 。在7.9节我们了解了Replay<br>Buffer，它能够在提高样本利用率的同时减少样本的相关性。那么它还存在什么问题吗？在采样样本时，每一个样本都会以相同的概率被采样出来。也就是说，每一个样本，都会以相同的频率被学习。但实际上，每一个样本的难度是不同的，学习每一个样本得到的收获也不同。有的样本相对简单，表现相对较好，学习得到的收获不会太高；有的样本则有些困难，表现也相对较差，经过学习后模型会有显著的提高。如果平等地看待每一个样本，就会在那些简单的样本上花费比较多的时间，而学习的潜力没有被充分挖掘出来。</p>
<p>Priority Replay<br>Buffer则很好地解决了这个问题。它会根据模型对当前样本的表现情况，给样本一定的权重，在采样时样本被采样的概率就和这个权重有关。交互时表现得越差，对应的权重越高，采样的概率也就越高；反过来，交互时表现得越好，那么权重也就越低，采样的概率也就越低。</p>
<p>这样，那些使模型表现不好的样本可以有更高的概率被重新学习，模型就会把更多的精力放在这些样本上，模型的学习效率就会有一定的提升。现在，Priority<br>Replay Buffer已经被广泛应用在Q-Learning的算法中，而且这个思想也被神经科学界证实，在人脑上也存在类似的学习机制。</p>
<p>接下来，我们来看看它在代码中的具体实现方法。Baselines项目的baselines/deep-q/replay_buffer.py中实现了Replay<br>Buffer和Priority Replay Buffer两个结构。从算法原理上看，两者的差别在以下两个方面。</p>
<p>（1）为每一个存入Replay Buffer的样本设定一个权重。</p>
<p>（2）使用这个权重完成采样过程。</p>
<p>我们先来解决第2个问题。假设权重已经得到，我们就可以采用轮盘赌的方法进行样本的采样。如果像Replay<br>Buffer一样直接使用数组保存每一个样本和权重，它的更改样本和采样的流程如图8-1所示。</p>
<p>图8-1 利用数组实现Priority Replay Buffer功能的流程</p>
<p>流程分为3步：在添加样本到Replay Buffer时，直接将其写入而不做任何处理，同时更新Replay<br>Buffer中所有样本的权重总和sum；在采样时随机得到一个[0，sum]之间的数，那么从Replay<br>Buffer算起，累积权重和超过随机值的第一个样本就被选择为待训练的样本，它的运算复杂度如下。</p>
<p>· 更改复杂度： _O_ （1），只需要在指定的位置写入信息。</p>
<p>· 采样复杂度： _O_ （ _N_ ），需要按序扫描指定的位置才能结束，最坏的情况下可能会扫描整个数组。</p>
<p>从上面的分析可以看出，采样的复杂度有些高，Replay<br>Buffer的大小一般比较大，第7章曾提到很多任务都会将其设置为100万，那么扫描整个数组的代价就会比较大。为了能快速实现样本集合中一部分样本的权重修改和集合的采样，我们可以采用线段树这个数据结构实现这个功能。线段树能够以比较快的速度更新局部的统计信息，同时可以利用这些信息快速查找满足某些特征的数据。线段树的结构如图8-2所示。</p>
<p>图8-2 线段树示意图</p>
<p>线段树的主体是由一棵树构成的。它的叶子节点存放着每一个元素个体的信息，例如它的权重，而线段树的非叶子节点则存放着其子节点的权重汇总。如果这是一颗求和的线段树，那么非叶子节点存放着其叶子节点的权重总和；如果这是一颗求最小值的线段树，那么非叶子节点存放着其叶子节点的最小值。使用线段树后的更新与采样的流程如图8-3所示。</p>
<p>图8-3 利用线段树实现Priority Replay Buffer功能的流程</p>
<p>它的流程同样有3步。在添加样本到Replay<br>Buffer时，不光记录样本信息，还要沿着样本叶子出发，依次找到叶子节点的父节点，更新父节点的权值；在采样时同样得到一个随机数，然后利用树的结构二分查找就可以快速定位被选到的样本，它的运算复杂度如下。</p>
<p>· 更改复杂度： _O_ （log _N_ ），需要更新叶子节点的所有父节点的信息。</p>
<p>· 采样复杂度： _O_ （log _N_ ），只需要从根出发访问指定的叶子。</p>
<p>对比发现，线段树在采样上明显快于数组，而更新的代价又没有增加太多，因此它可以替代数组使用。这样第二个问题就得到了解决。</p>
<p>让我们回到第一个问题。Priority Replay<br>Buffer的主要思路是增加有价值样本出现的频率。那么我们该如何定义“有价值”这个概念呢？最直观的想法是，能让模型学习到更多的内容，这个样本一定更有价值。那么，什么样的样本能让模型学习到更多呢？当然是能产生更大的TD-<br>Error的，即估计值与目标值的差距越大，样本越有价值。所以我们可以将TD-Error作为样本重要性的衡量指标。TD-Error越高，样本出现的概率就越高。</p>
<p>虽然TD-Error是一个很合理的指标，但是由于我们的模型处在学习的过程中，在样本放入Priority Replay Buffer时样本的TD-<br>Error和它被取出时的TD-Error有可能不一样。所以可能会出现一种情况：一个样本在放入时TD-<br>Error很高，但是随着模型的不断学习，模型对这个样本的实际TD-<br>Error已经降低，但它还是会以很高的概率出现。一个最合理的方法，是在我们从Priority Replay Buffer取出样本时，将它的TD-<br>Error重新计算一遍，但是这样做的代价实在太大，所以我们只好使用样本放入Priority Replay Buffer时的TD-Error作为近似。</p>
<p>既然我们已经知道这个指标并不完全可靠，就需要其他的方法辅助。因此我们要确保TD-Error较高的样本出现的概率更高，同时也要使那些TD-<br>Error较低的样本以一定的概率出现。论文中定义的一个计算样本出现概率的方法为</p>
<p>其中 _p_ i 表示每个样本在计算时的TD-Error。 _α_ 可以调整样本TD-Error的重要性。当 _α_ =1时，相当于我们直接使用TD-<br>Error数值；当 _α ＜_ 1时，我们就可以削弱高TD-Error样本的影响，增强低TD-<br>Error样本的影响。这样我们就可以通过这个参数调整Priority Replay Buffer的表现。</p>
<p>除了上面提到的两个不同，实际上Priority Replay<br>Buffer还做了第三个改变，它提供了一个参数，用于调整每个样本对模型更新的影响。对Replay<br>Buffer来说，每一个样本是被等概率取出的，它们对模型的更新也是等权重的；而Priority Replay<br>Buffer的样本是非等概率取出的，它的样本服从另一个难以清楚地描述的分布，所以我们对模型的更新是有偏差的。为了使我们的更新无偏，可以采用重要性采样的方法使更新变得无偏，对应的公式为</p>
<p>其中 _N_ 表示Replay Buffer存放的样本数量。所以我们可以在每一个被学习的样本前增加一个权重，这样就可以使更新变得无偏。</p>
<p>总感觉哪里有点不对劲……我们使用Priority Replay<br>Buffer的目的不正是让更新变得有偏吗？为什么还要削弱它的优势呢？实际上，这个纠正是为了说明我们可以通过一些设定让它变回Replay<br>Buffer那样的更新方式，这样虽然没有带来任何好处，但也没有任何坏处。那么我们就可以根据实际问题调整这个权重，让它在两种更新效果之间做一个权衡，既能确保提升样本的利用率，又能确保结果不会带来太大的偏差，新的权重更新公式变为</p>
<p>这样当 _β_ =1时，更新效果实际上等同于Replay Buffer；当 _β ＜_ 1时，Priority Replay Buffer就能够发挥作用了。</p>
<p>从另一个角度讲，很多理论推导都是基于Replay<br>Buffer的，我们所做的改进主要希望训练速度能够增加，至于方法的收敛性却并不确定。所以我们还是希望Priority Replay<br>Buffer能够最终变成Replay Buffer，所以可以让 _β_ 在训练开始时赋值为一个小于1的数，然后随着训练迭代数的增加，让 _β_<br>数不断变大，并最终到达1。这样我们既可以确保训练速度能够增加，又可以让模型的收敛性得到保证。</p>
<p>以上是Priority Replay Buffer的算法原理，在此我们可以做一个总结。</p>
<p>（1）在样本存入Replay Buffer时，计算。</p>
<p>（2）在样本取出时，以第1步计算得到的概率进行采样。</p>
<p>（3）在更新时，为每一个样本添加的权重。</p>
<p>（4）随着训练的进行，让 _β_ 从某个小于1的值渐进地靠近1。</p>
<p>从算法的思想到最终实现，我们经历了许多波折，可以看出很多算法在设计时需要同时兼顾“求新”和“求稳”两个目标，这样才能在更大程度上提高算法的效果。如果去看代码实现，读者一定会发现在实现中Priority<br>Replay Buffer还包含其他未提到的结构（例如最小值线段树），这些内容欢迎读者进行探索。</p>
<h3 id="8-3-Dueling-DQN"><a href="#8-3-Dueling-DQN" class="headerlink" title="8.3 Dueling DQN"></a>8.3 Dueling DQN</h3><p>Dueling DQN是一种基于DQN的改进算法，它的主要突破点在于利用模型结构将值函数表示成更细致的形式，这使得模型能够拥有更好的表现。它来自论文<br>_Dueling network architectures for deep reinforcement learning_ [3]<br>。接下来，我们就来看看它的改进形式。</p>
<p>经过前面的介绍，我们可以给出这个公式并定义一个新的变量：</p>
<p>也就是说，基于状态和行动的值函数 _q_ （以下简称为 _q_ ）可以分解成基于状态的值函数 _v_ （以下简称为 _v_ ）和优势函数（Advantage<br>Function） _A_ （以下简称为 _A_ ）。由于存在</p>
<p>所以如果所有状态行动的值函数不相同，一些状态行动的价值 _q_ （s _，_ a）必然会高于状态的价值 _v_<br>（s），当然也会有一些状态行动对低于价值，于是优势函数可以表示出当前行动和平均表现之间的区别：如果优于平均表现，那么优势函数为正；反之则为负。</p>
<p>既然概念上有这样天然的分解，那么在设计模型时能不能也采用这样的结构呢？当然可以。于是我们在保持网络主体结构不变的基础上，将原本网络中的单一输出变成两路输出，一个输出用于输出<br>_v_ ，它是一个一维的标量；另一个输出用于输出 _A_ ，它的维度和行动数量相同。最后将两部分的输出加起来，就是原本的 _q_<br>值。改变输出结构后，我们只需要对模型做很少的改变即可实现功能：模型前面的部分可以保持不变，模型后面的部分从一路输出改变为两路输出，最后再合并成一个结果。模型结构的改变如图8-4所示。</p>
<p>图8-4 Dueling DQN结构变化图</p>
<p>难道我们单纯地将它们分解就可以了吗？当然不行。我们还需要对这两部分的输出做一定的限制。如果我们不对这两个输出做限制，那么当 _q_ 值一定时， _v_ 和<br>_a_ 有无穷种可行组合，而实际上只有很小一部分的组合是合乎情理、接近真实数值的。为了解决这个问题，我们可以对 _A_ 函数做限定。我们知道 _A_<br>函数的期望值为0：</p>
<p>就可以对输出的 _A_ 值进行约束，例如将公式变成</p>
<p>让每一个 _A_ 值减去当前状态下所有 _A_ 值的平均数，就可以保证前面提到的期望值为0的约束，从而增加了 _v_ 和 _A_ 的输出稳定性。</p>
<p>进行这样的输出分解有什么好处呢？首先，通过这样的分解，我们不但可以得到给定状态和行动的 _q_ 值，还可以同时得到 _v_ 值和 _A_<br>值。这样如果在某些场景需要使用 _V_ 值时，我们同样可以获得 _v_ 值而不用再训练一个网络。同时，通过显式地给出 _v_<br>函数的输出值，每一次更新时，我们都会显式地更新 _v_ 函数。这样 _V_ 函数的更新频率就会得到确定性的增加。对于单一输出的 _q_<br>网络来说，它的更新就显得有些晦涩。</p>
<p>其次，从网络训练的角度来看，我们从原本需要训练| _A_ |个取值为[0 _，_ ∞]的数值，变成了训练一个取值为[0 _，_ ∞]的数值，和| _A_<br>|个均值为0，实际取值为[ _-C，C_ ]的数值，对网络训练来说，后者显然是更友好且容易的。同时，对一些强化学习的问题来说， _A_ 值的取值范围远比<br>_v_<br>值小，这样将两者分开训练更容易保持行动之间的排列顺序。由于我们需要从所有的行动中挑选出价值最高的行动，因此不同行动之间的价值需要保持一定的区分度。由于<br>_A_<br>值的数值范围比较小，因此它对模型更新更敏感，这样模型在更新时会更容易考量与其他行动的相对变化量，也就不会因为某一次的更新使得原本的行动排序被意外打破。如果采用<br>_q_ 值进行更新，由于 _q_ 值相对于 _A_ 值可能会很大，因此 _q_<br>值对微小的更新不敏感，某一次的更新可能会影响行动的原本排序，从而对策略造成一定的波动。</p>
<p>将值函数分解后，每一部分的结果都具有实际的意义，我们也可以从中挖掘出很多有价值的信息。从论文中给出的实验效果图可以看出，将模型分为 _A_ 函数和 _v_<br>函数后，两个函数会展示出不同的特点。通过反卷积操作得到两个函数值对原始图像输入的梯度后，可以发现 _v_ 函数对游戏中的所有关键信息都十分敏感，而 _A_<br>函数只对和行动相关的信息敏感，如图8-5所示。</p>
<p>图8-5 Dueling Network的反卷积敏感区域图</p>
<p>图8-5的上面两幅图中，玩家附近没有障碍物时，价值敏感区域比较单一，由于行动之间对结果的影响差距不大，所以优势函数中没有太敏感的区域；而图8-5的下面两幅图中，玩家附近有一些障碍物，因此优势函数的敏感区域变得多了起来。两个输出敏感图的差异说明了价值和优势之间的差别。</p>
<h3 id="8-4-解决DQN的冷启动问题"><a href="#8-4-解决DQN的冷启动问题" class="headerlink" title="8.4 解决DQN的冷启动问题"></a>8.4 解决DQN的冷启动问题</h3><p>本节要解决DQN网络冷启动的问题。一直以来，强化学习问题都遇到了“冷启动”的问题。对于以值函数为核心的Q-<br>Learning算法来说，前期的算法迭代很难让模型快速进入一个相对理想的环境，更何况由于前期值函数估计存在较大偏差，与环境交互得到的采样与最优策略存在一定的差别，这更增加了学习的难度。那么有没有能提高训练初期的学习效率的办法呢？论文<br>_Deep Q-learning from Demonstrations_ [4] 为我们探索了强化学习和监督学习结合的一种方案。</p>
<p>作者的主要思路是利用预先准备好的优质采样轨迹加快模型前期的训练速度。这个思路是非常直观的。对一个未经任何训练的模型来说，模仿优质采样轨迹的行动是一个不错的选择，因为模型训练初期Agent的策略相对较差，从交互序列中学习的效果不会很好，而优质行动本身来自较强的策略，这样就相当于直接站在巨人的肩膀上，学习速度自然会快很多。只要我们有办法取得一定数量的优质轨迹，就可以通过监督学习完成与环境交互前的预训练。</p>
<p>除了使用监督学习预训练，我们还要使用强化学习完成DQN模型原本方法中的训练。最终模型的目标函数变成了多个学习目标结合的形式：</p>
<p>这个目标函数包含了4个子目标。</p>
<p>（1） _J_ DQ （ _q_ ）：Deep Q-Learning的目标函数。</p>
<p>（2） _J_ n （ _q_ ）：以 _n_ 步回报估计法为目标的Q-Learning目标函数。</p>
<p>（3） _J_ E （ _q_ ）：利用准备数据进行监督学习的目标函数。</p>
<p>（4） _J_ L2 （ _q_ ）：L2正则的目标函数。</p>
<p>其中， _λ_ n 用于平衡不同目标函数之间的权重。相信读者对第1和第4个子目标比较熟悉，下面就来重点介绍第2和第3个子目标。首先介绍 _J_ n （<br>_q_ ）。Q-Learning在计算对某个状态-行动的值函数的估计值时，采用了如下估计法：</p>
<p>前面曾介绍过，对于序列比较长的问题，这种方法有一个缺陷，那就是价值需要一定时间的训练才能跳过前面波动较大的时期，进入相对稳定的更新时期。为了使模型的更新进行得更快，我们可以将上面的公式进一步展开，不仅使用下一时刻的回报，而是将此后更多时刻的回报加入目标值中。公式如下所示：</p>
<p>这个估计方式被称为 _n_<br>步回报的估计法，可以看出这个方法结合了蒙特卡罗法的特点，在公式中使用了更多真实的交互回报，这样更容易在模型训练早期将值函数做快速更新，一般来说， _n_<br>可以设置为10。这种更新方式还会在后面的章节中再次出现。</p>
<p>其次介绍 _J_ E （ _q_<br>）。一般来说，事先准备好的数据是比较有限的，很难支撑一个完整模型的训练，因此它必然只能影响很小一部分的状态行动值。如果它不能尽可能地覆盖更多的状态，那么这些数据反而有可能对模型造成不好的影响。同时，这些准备好的数据中也可能存在一些噪声，其中的行动并不是真正的行动。为了避免上面提到的问题，监督学习的目标函数被定义为如下形式：</p>
<p>其中aE 表示当前状态下专家给出的行动， _l_ （ _x，y_ ）函数是一个指示函数，当模型选择的行动与aE<br>相同时，函数值为0；当选择不同时，函数值为某个固定值。当a=aE 时，目标函数等于0，表示模型的决策与准备数据相同；如果a/=aE<br>，则说明其他某个行动的价值至少不弱于专家行动太多，这对模型来说是一个比较合理的约束。</p>
<p>这样我们就了解了目标函数4个部分的含义。4个子目标涵盖了监督学习和强化学习各方面的目标。除此之外，算法还有一些其他的设定：</p>
<p>· 由于为预训练准备的样本质量比较高，我们可以对其进行反复利用，因此它们在训练过程中不会被换出来，全部永久存在于Replay Buffer中。</p>
<p>· 同样，由于准备数据和交互数据的来源和质量不同，因此在从Replay<br>Buffer中随机抽取样本时，准备数据和交互得到的数据拥有不同的采样概率；模型一开始只接受准备数据的学习，不进行模型模拟采样，这样确保了模型前期的快速成长。</p>
<p>· 算法也使用了常见的Target Network、Priority Replay Buffer等技巧。</p>
<p>模型结果显示，该方法在早期训练上明显超过其他方法，同时在最终结果上也超过了很多当前表现最优的模型，获得了很不错的成绩。完整的算法流程如下所示：</p>
<p>算法 Deep Q-learning from Demonstrations</p>
<p>Input</p>
<p>_D_ replay ：专家行动数据作为初始化的Replay Buffer</p>
<p>_θ_ ：Behaviour Network的参数</p>
<p>_θ_ ′ ：Target Network的参数</p>
<p>_τ_ ：更新Target Network的频率</p>
<p>_k_ ：预训练的轮数</p>
<p>Start</p>
<p>For steps _t_ ∈{1 _,_ 2 _,…,k_ }do</p>
<p>从 _D_ replay 中进行优先采样，得到一个mini-batch的数据</p>
<p>使用Target Network计算损失函数 _J_ （ _Q_ ）</p>
<p>对网络 _θ_ 进行梯度下降更新参数</p>
<p>当 _t_ mod _τ_ =0时，进行Target Network的更新： _θ_ ′ ← _θ_</p>
<p>for steps _t_ ∈{1 _,_ 2 _,…_ }do</p>
<p>利用Behaviour Network _θ_ 进行交互，获得执行数据（s _，_ a _，_ r _，_ s′ ）</p>
<p>将数据存入 _D_ replay ，同时根据容量情况替换老的数据</p>
<p>从 _D_ replay 中进行优先采样，得到一个mini-batch的数据</p>
<p>使用Target Network计算损失函数 _J_ （ _Q_ ）</p>
<p>对网络 _θ_ 进行梯度下降更新参数</p>
<p>当 _t_ mod _τ_ =0时，进行Target Network的更新： _θ_ ′ ← _θ_</p>
<h3 id="8-5-Distributional-DQN"><a href="#8-5-Distributional-DQN" class="headerlink" title="8.5 Distributional DQN"></a>8.5 Distributional DQN</h3><p>本节介绍Distributional DQN，这个算法同Dueling DQN类似，都是要对价值模型的结构进行改进。它来自论文 _A<br>Distributional Perspectiνe on Reinforcement Learning_ [5]<br>。在此之前，我们介绍了DQN的基本思想，那就是通过神经网络完成值函数的拟合。状态行动值函数本质上计算了基于某个特定策略对未来的长期回报的期望，这个期望值的估计既不容易，也不细致。Distributional<br>DQN模型希望通过建立更复杂细致的值函数，让估计结果更细致可信。</p>
<h4 id="8-5-1-输出价值分布"><a href="#8-5-1-输出价值分布" class="headerlink" title="8.5.1 输出价值分布"></a>8.5.1 输出价值分布</h4><p>让我们先回忆Bellman公式的形式：</p>
<p>由此可见，目前模型的估计结果是基于策略和状态转移两个概率分布的价值期望，由于综合了太多的信息，这个数值就显得有些不够具体，如果想要得到更详细的信息，那么模型就不能仅输出一个期望值，而需要将价值的分布估计出来。这样模型的结果就会做出如图8-6所示的变化。</p>
<p>如果我们能将价值的分布估计出来，这个结果将变得更有意义。但是分布中拥有太多的信息，精确地表示分布并不是一件容易的事。如果我们把价值分布想象成一个比较常见的分布，那么原本的价值模型同样可以表示为分布的形式。例如，假设价值分布是一个方差固定的高斯分布，那么我们只需要估计出分布的均值，就可以知道分布的形式。但是这样的假设可能和真实情况存在一定差距，所以我们要使用更弱的假设求出分布。</p>
<p>图8-6 分布估计变化</p>
<p>为了确保分布不受太多的限制，同时又可以减少分布的计算量，我们选择一种简单直观的方法：直方图。假设绝大多数的价值最终值落在区间[ _V_ min _，V_<br>max ]之内，同时限定区间内每一段的价值范围相等，那么我们只需让模型输出给定值函数区间的概率，也就是 _P_ （ _V_<br>），就可以表示出直方图分布的形式。这样我们的价值分布估计又经历了如图8-7所示的转变。</p>
<p>图8-7 用直方图表示分布</p>
<p>采用直方图的方法有一个好处，那就是它可以近似很多不太常规的分布形式，分布的表示范围比起前面提到的正态分布要大很多。前面提到直方图的最大值和最小值是限定的，其中的区间也是等距表示的。假设直方图一共有<br>_N_ 段，那么每一段的间隔为</p>
<p>于是直方图中表示的数值采样集合为{zi = _V_ MIN + _i_ ∆z ∶0≤ _i＜N_ }，这个集合共有 _N_ 个元素。我们的模型要输出 _N_<br>个值的向量，每一个值表示这其中一个价值采样点出现的概率，于是模型可以用下面的映射表示</p>
<p>不同于DQN输出实数，直方图模型会通过Softmax层输出每一个价值采样点的概率，这样可以得到价值为zi<br>时，对应的概率为，模型的输出结构也就变得十分清晰。如果目标价值分布和估计价值分布的价值范围和价值采样点完全相同，那么两者的比较就相当于同一个概率分布结构下不同概率形式的比较，也就可以使用Cross<br>Entropy Loss作为价值模型的目标函数。</p>
<h4 id="8-5-2-分布的更新"><a href="#8-5-2-分布的更新" class="headerlink" title="8.5.2 分布的更新"></a>8.5.2 分布的更新</h4><p>解决了输出的表示问题，还要解决另一个十分重要的问题，那就是更新。在前面的DQN模型中，我们可以使用Q-<br>Learning公式计算目标价值，得到的结果为目标期望价值；而在Distributional DQN模型中，我们的计算过程则变得有些复杂。</p>
<p>（1）计算 _t_ +1时刻每一个行动的价值分布。</p>
<p>（2）将第1步的价值期望和 _t_ +1时刻的回报相加，并选出期望最大的行动。</p>
<p>（3）将期望最大的行动的价值分布表示出来。</p>
<p>我们发现第3步计算后，得到的分布可能和原始的分布形式不同。不光是价值范围不同，价值采样点也会发生偏移，这样目标价值分布和估计分布将很难比较。如果允许这样的不同，我们就不能使用Cross<br>Entropy<br>Loss作为目标函数，选择其他的函数将会使模型变得更复杂，为了简化计算，我们可以选择一种近似计算的方法，那就是将价值范围和采样点对齐。只要对齐了直方图，就可以使用之前的目标函数进行计算。</p>
<p>直方图分布对齐的过程涉及几个子过程，其中的变换过程如图8-8所示。</p>
<p>图8-8 价值分布更新过程</p>
<p>在对齐前，我们要先计算出最优的行动。由于价值的范围和采样点已经确定，那么对于不同的行动，可以计算出每一个价值采样点zi 的概率 _p_ （zi<br>），这样也可以求出当前行动a的价值期望：</p>
<p>我们从所有的行动中选择使期望最高的行动，可以得到</p>
<p>然后我们可以开始对齐的计算了。从图8-8中可以看出，计算过程分为三步。</p>
<p>第1步计算出真实的目标价值分布。由于价值采样点是确定的，所以我们可以直接使用公式计算出目标价值的价值点，即进行下面的计算：</p>
<p>由于每个采样点是固定的，所以这一步对于每一步更新来说都是相同的。每一个新的价值点的概率和旧的价值点相同，也就是说</p>
<p>第2步要完成价值限定的操作。其中落入价值范围之外的值将被强制投影到范围之内。大于最大值的价值采样点会被投影到最大价值处，小于最小值的价值采样点会被投影到最小价值处。</p>
<p>第3步要完成价值采样点的投影。最终我们需要把价值采样点投影回原始的采样点zi<br>。这里的投影方法比较简单，由于第2步我们已经将采样点限定在价值范围内，所以就可以找到离采样点最近的两个原始采样点zi<br>，那么下一步就可以以采样点到原始采样点的距离为权重将采样点的概率分配到原始采样点上，这样就得到了基于原始采样点的目标概率。</p>
<p>再通过模型得到对当前时刻的分布估计，就可以使用Cross Entropy Loss求出目标函数了。完整的更新算法如下所示：</p>
<p>算法 Distributional DQN模型更新</p>
<p>Input</p>
<p>xt _,_ at _,_ rt _,_ xt+1 _,γ_ t ∈[0 _,_ 1]</p>
<p>Start</p>
<p>第一步：选择最优行动</p>
<p>_q_ (xt+1 _,_ a)=∑i zi _p_ (xt+1 _,_ a),a∗ ←argmaxa _q_ (xt+1 _,_ a)</p>
<p>第二步：计算对齐的概率</p>
<p>for _i_ =0 to _N_ -1,do:</p>
<p>_l_ ←floor( _b_ i ), _u_ ←ceil( _b_ i )</p>
<p>end for</p>
<p>第三步：计算目标函数</p>
<p>通过使用分布的形式表示值函数后，模型可以学习到更丰富的信息，这样可以使模型做出更细致的决策。</p>
<h3 id="8-6-Noisy-Network"><a href="#8-6-Noisy-Network" class="headerlink" title="8.6 Noisy Network"></a>8.6 Noisy Network</h3><p>增强探索能力是强化学习中经常遇到的问题，在前面我们曾接触过 _∈_ -greedy的方法，这个算法中以一定的概率 _∈_ 随机执行行动，而在剩下的1<br>_-∈_ 中执行最优行动，这相当于在执行策略的环节增加一定的噪声，使得模型具备一定的探索能力。本节介绍另一种增强探索能力的方法：Noisy<br>Network，来自论文 _Noisy Networks for Exploration_ [6] 。不同于 _∈_<br>-greedy的方法，它使用了一种更平滑的手段增加探索能力。</p>
<p>那么，算法是如何实现想要的效果的呢？我们以一个简单的函数为例，来看看它的效果。这个函数的形式为</p>
<p>其中 _x_ 表示输入、 _y_ 表示输出， _w_ 和 _b_ 是函数的参数。也就是说，如果输入是 _x_ ，那么经过函数的变换，输出的结果就会变成<br>_y_ 。这样，我们就可以使用这个函数表示自然界中一组 _x_ 和 _y_<br>的关系。当然，自然界中存在着一定的噪声，我们无法直接使用这个函数进行表示，于是给函数加入一个噪声项，于是函数变为</p>
<p>其中 _∈_ 服从均值为0，方差为 _σ_ 2 的高斯分布</p>
<p>_σ_ 是一个固定值，表示噪声带来的方差。这样我们也可以认为 _y_ 服从如下的高斯分布：</p>
<p>可以看出由于噪声的存在，我们可以从同一个 _x_ 映射到多个 _y_<br>，这相当于增加了输出的不确定性。不确定性对于探索来说十分重要，由于不确定性的存在，我们可以选择确定行动之外的其他行动，因此我们发现噪声和探索存在某些类似的特性，我们可以利用噪声增加模型的探索能力。</p>
<p>一种添加噪声的方法是在参数上增加噪声。对于上面函数中的参数 _w_ ，我们可以定义参数来自均值为 _µ_ w ，方差为 _σ_ w 的高斯分布。同理，参数<br>_b_ 服从均值为 _µ_ b ，方差为 _σ_ b 的高斯分布，这样函数就变成了下面的形式：</p>
<p>这个形式理解起来并不难，前向计算也并不困难，但是反向计算却有点困难，如何将得到的反向梯度传递到高斯分布中的分布参数呢？为了简化计算，我们需要将参数的表现形式做一定的变换，变成固定部分和随机部分的和，形式如下所示：</p>
<p>_∈_ 代表参数中的随机部分，它不属于参数，服从确定的统计分布，例如均值为0，方差为1的高斯分布 _N_ （0 _，_<br>1）。在完成采样后，它可以被当成一个常量对待，这样另外两个参数就可以使用前向后向计算优化了。</p>
<p>当然，上面这种方法也只是为模型添加噪声的方法之一，如果利用这种方法增加噪声，那么如果函数原本有 _N_<br>个参数，为了实现噪声的效果，我们需要把参数数量增加一倍，对于小型网络来说，使用这样的方法添加噪声是可行的，但对较大的网络来说，增加一倍的网络参数会给计算带来不小的负担。</p>
<p>为了减少噪声参数的数量，我们还可以从函数参数的结构入手。在我们熟悉的全联接运算中，参数w一般是一个二维的矩阵，假设它的维度为 _p_ × _q_<br>，那么我们可以只生成 _p_ + _q_ 个噪声参数，于是对于上述函数中每一个参数，可以用下面这种全新的形式表示：</p>
<p>其中参数数值的第一项保持不变，后面一项的表示有些变化。我们将添加的 _p_ + _q_ 个参数分成两部分，一部分的维度为 _p_ ， _σ_ p [ _i_<br>]表示其中的第 _i_ 个噪声参数；另一部分的维度为 _q_ ， _σ_ q [ _j_ ]表示其中第 _j_ 个噪声参数。这里的 _f_ （ _x_<br>）=sgn（ _x_ ） _x_<br>。通过这样的设定，我们在噪声效果和噪声参数数量两方面得到了很好的平衡。对于更复杂的模型，我们也可以采用类似的方法添加噪声参数来实现对噪声信息的拟合。</p>
<p>完成了对添加噪声基本思想的介绍，下面就来介绍这个噪声的参数如何融入我们已经介绍的DQN算法，我们知道基于Target<br>Network的DQN算法的目标函数公式为</p>
<p>其中 _θ_ 表示Behavior Network的模型参数， _θ_ - 表示Target<br>Network的模型参数。我们可以在值函数中加入一定的噪声，由于噪声会影响最终的价值输出，也会影响最终的行动，于是噪声的大小影响了模型的探索特性，噪声越小表示探索能力越小，噪声越大表示探索能力越大。我们可以为两个模型参数分别加入噪声随机变量<br>_∈_ 和 _∈_ - ，以及噪声参数 _σ_ 和 _σ_ - ，此时新的目标函数变为</p>
<p>在原本的目标函数中噪声项并不存在，因此此时噪声的加入使得目标函数产生了偏差。为了消除这个偏差，我们可以对噪声求期望，由于噪声的期望值为0，求解期望后目标函数不再有偏，但是模型依然拥有一定的探索能力。对于这个公式的实现方法，读者一定已经非常熟悉了，我们只需要将其以蒙特卡罗的形式计算即可，公式中的两个期望可以同时转化为求平均的操作，这样这个算法就变得可以计算了。</p>
<p>最后，由于噪声的引入，我们需要考虑噪声参数的初始化。根据论文中的介绍，噪声参数为 _p_ + _q_ 个时，令 _N_ = _p_ × _q_ ，参数<br>_µ_ 按照范围为的均匀分布进行初始化，参数 _σ_ 将初始化为常量。</p>
<p>Noisy Network的思想相对简单，我们通过添加噪声为模型赋予探索的特性，与之前的 _∈_ -greedy的探索不同，Noisy<br>Network的探索方式更平滑，对探索的粒度控制更细腻，因此在实验中它也获得了不错的成绩。</p>
<h3 id="8-7-Rainbow"><a href="#8-7-Rainbow" class="headerlink" title="8.7 Rainbow"></a>8.7 Rainbow</h3><h4 id="8-7-1-Rainbow的模型特点"><a href="#8-7-1-Rainbow的模型特点" class="headerlink" title="8.7.1 Rainbow的模型特点"></a>8.7.1 Rainbow的模型特点</h4><p>本节介绍Rainbow模型，它是一个集多长所长的模型，来自论文 _Rainbow：Combin-ing Improνements in Deep<br>Reinforcement Learning_ [7]<br>。Rainbow模型的名字让人一头雾水，它的名字并不代表它使用了某个技术，而是一种文艺的表示。由于它集成了很多模型的特色算法，如果把每一个特色算法想象成一种色彩，那么这些特点组合在一起就形成了五彩颜色的物体，而彩虹刚好是由多种颜色组成的，于是模型就有了彩虹这个名字。</p>
<p>那么，Rainbow模型融合了哪些模型的技术呢？实际上这些技术都在前面的章节中介绍过，下面我们来回顾这些技术。</p>
<p>1.Double Q-Learning</p>
<p>Double Q-Learning构建了两个结构相同但是参数数值不同的模型：Behavior Net-work和Target<br>Network。在模型更新时，首先由Behavior Network选出 _t_ +1时刻的最优行动，然后由Target Network得到估计 _t_<br>+1时刻最优行动的目标价值估计。通过将这两个步骤解耦，我们可以减少Q-Learning方法对价值过高估计带来的影响，其中的核心公式为</p>
<p>2.Prioritized Replay Buffer</p>
<p>Prioritized Replay Buffer通过对Replay<br>Buffer中不同的样本赋予不同的权重，使得模型有更高的概率训练对自己有更多提升的样本上，同时以较低的概率训练对自己提升有限的样本。样本出现的权重和样本采样时的TD-<br>Error有关，样本的学习率和更新权重还可以根据参数进行调整。</p>
<p>3.Dueling Networks</p>
<p>Dueling Networks 将状态行动价值模型 _q_ （s _，_ a） 分解成状态价值 _v_ （s） 和价值优势 _A_ （s _，_<br>a）两个部分，分解后的两部分具有明确的含义，而这样的分解也同时降低了训练的难度，其中的核心公式为</p>
<p>4.Multi-step Learning</p>
<p>前面介绍的Q-Learning大多通过下一时刻的回报和价值估计得到目标价值，这种方法在前期具有学习速度较慢的弱点，为了克服这个弱点，Multi-step<br>Learning使用了更多步的回报，这样在训练前期目标价值可以估计得更准确，从而加快训练速度，其中的核心公式为</p>
<p>5.Distributional Network</p>
<p>DQN网络只输出了期望形式的价值，而对价值缺少更细致的刻画。Distributional<br>Network的模型结构可以输出价值的分布形式。我们可以设定价值模型可能的输出范围[ _V_ MIN _，V_ MAX<br>]，并在范围内以直方图的形式表示价值对应的出现概率，这使模型的表现能力有了很大的提升，分布形式模型的表示形式如下所示：</p>
<p>6.Noisy Network</p>
<p>模型的探索能力一直是一个需要提高的方面。为了更优雅、灵活地提高模型的探索能力，Noisy<br>Network为模型参数加入了一定的噪声，通过噪声的随机性改变参数的数值，进而改变模型输出的数值，对应的更新公式为</p>
<p>可以看到，Rainbow中使用了很多基于DQN算法的改进技术，这些技术在本章都已经介绍。如果说DQN模型是一个经典算法，那么Rainbow就是站在DQN肩膀上，集多家改进算法于一身的又一个经典算法。对算法中6个技术不够熟悉的读者可以回到前面的章节复习其中的内容。</p>
<h4 id="8-7-2-Deep-Q-Network的实现"><a href="#8-7-2-Deep-Q-Network的实现" class="headerlink" title="8.7.2 Deep Q Network的实现"></a>8.7.2 Deep Q Network的实现</h4><p>前面已经了解了Rainbow模型，接下来介绍Baselines中关于DQN的实现，对应的代码在baselines/deepq文件夹中。这个子项目实现了一个类似“Rainbow”的算法，但是它缺少了一些“颜色”，例如Distributional<br>DQN结构等。为了灵活地组合DQN算法的各种特性，项目中包含一些比较灵活的配置，例如开启或者关闭其中的某个特性，这样我们可以根据需要配置不同的算法。</p>
<p>算法的主体流程可以用如下伪代码表示：</p>
<p>让我们先来看算法的主要流程。在第2行算法完成了几个模型的构建：act用于和环境交互，train用于模型训练，update_target用于将Behavior<br>Network的参数同步到Target Network。第3行创建了Replay Buffer，如果使用Priority Replay<br>Buffer，那么在后续的训练过程中，需要考虑每个样本的采样权重和更新权重。第5行用于初始化噪声，使用噪声网络时它会派上用场。接下来的第6～10行实现了Agent与环境交互并将交互样本保存至Replay<br>Buffer的过程。接下来的第11～15行实现了模型更新的过程，第16～17行实现了Target<br>Network更新的过程。在主体流程中，我们看到了Priority Replay Buffer结构。</p>
<p>从上面的流程可以看出，第2行的build_train方法完成了所有模型的构建，让我们进一步深入这个函数，其中的流程可以用如下伪代码表示（实际代码的顺序与伪代码不同）：</p>
<p>上面的函数主要介绍了与Q-Learning有关的部分，其中第7～13行实现了目标价值的计算，变量obs_tp1实际上是时刻为 _t_<br>+1的观测值，所以变量名展开为“observation t plus 1”。第7行实现了 _t_ +1时刻的价值估计，接下来如果采用本章介绍的Double<br>Q-Learning算法，那么最优行动的选择将由Behavior Network选出，反之则直接由Target<br>Network选出，其中one_hot方法用于选出目标行动的价值。第13行目标价值由 _t_ 时刻的回报和 _t_<br>+1时刻的价值估计相加得到。第15～16行则实现了价值模型的估计值，接下来在第18～19行可以计算出td_error，并以此进行优化。第20行完成了两个网络的参数同步，这样主体的模型更新部分就完成了。</p>
<p>最后只剩下第2～5行关于策略部分的实现，其中build_act方法比较简单，对应的简化版伪代码如下所示：</p>
<p>这部分的代码比较简单，实际上实现了 _∈_ -greedy算法，同时拥有更新 _∈_ 的功能，在实际的代码中，我们可以选择使用 _∈_<br>-greedy算法或者确定的算法。而另外一个方法build_act_with_param_noise就比较复杂了，它用到了Noisy<br>DQN的特性，对应的伪代码如下所示：</p>
<p>这个函数的第2～8行代码与前一个函数类似，不同的是这里使用了噪声网络。噪声网络的产生就像前面介绍的那样，使用Behavior<br>Network的参数加上噪声得到。从第12行起，就需要对噪声的过程做更多的控制。第12～14行完成了对参数的噪声的重置，每一个序列结束后，参数的噪声都会重新生成。第15～16行用于设置衡量噪声范围指标的上限。这里采用KL散度对噪声影响力进行测定，在接下来的代码中可以看到衡量指标计算的全过程。</p>
<p>首先，我们将使用相同的噪声设定生成另一个模型behavior_network_adaptive，并使用这个模型得到的价值作为估计价值。我们可以认为行动对应的价值估计越高，行动产生的概率就越高，于是价值向量可以作为输入经过Softmax层的计算，将价值转换成行动对应的概率。这样没有加入噪声的模型可以得到行动概率q_prob，而加入了噪声的模型behavior_network_adaptive计算得到了行动概率q_prob_adaptive，得到了这两个概率分布，就可以使用KL散度计算两者的距离。最后我们根据KL散度值进行判断，如果值大于上限，那么噪声范围就应该适当缩小；如果值小于上限，那么噪声范围就应该适当扩大。</p>
<p>上面的算法没有提到价值模型的具体形式，在本项目中我们可以选择经典的DQN实现，也可以选择Dueling DQN的实现。DQN的实现相对简单，而Dueling<br>DQN则是在DQN的基础上增加一些操作完成的，两个模型的具体结果如图8-9所示。</p>
<p>图8-9 DQN和Dueling DQN模型</p>
<p>这样，我们就了解了这个项目的主要流程，剩下的内容相对比较简单，希望读者自行了解。</p>
<h3 id="8-8-总结"><a href="#8-8-总结" class="headerlink" title="8.8 总结"></a>8.8 总结</h3><p>本章介绍了关于DQN模型的一些改进算法，并介绍了集成众多算法的Rainbow模型，让我们回顾一下。</p>
<p>（1）Priority Replay Buffer可以更高效地使用其中的样本，让模型更多地选择能让模型提高的样本。</p>
<p>（2）Dueling DQN将值函数拆解成了两个部分，使模型更容易训练的同时，可以表达更多有价值的信息。</p>
<p>（3）DQN from Demonstration解决了Q-Learning的冷启动问题，也平衡了监督学习和强化学习两部分对模型的影响力。</p>
<p>（4）Distributional DQN将模型的输出从计算价值期望改为计算价值分布，使模型可以学习更多有价值的信息。</p>
<p>（5）Noisy DQN通过给参数增加噪声的方式为模型增加一定的探索能力，这种方式更具可控性。</p>
<p>（6）Rainbow模型集成了上述模型的优势，给出了多种特性融合的可能性。</p>
<h3 id="8-9-参考资料"><a href="#8-9-参考资料" class="headerlink" title="8.9 参考资料"></a>8.9 参考资料</h3><p>[1] Van Hasselt H,Guez A,Silver D.Deep Reinforcement Learning with Double<br>Q-learning[J].Computer Science,2015.</p>
<p>[2] Schaul T,Quan J,Antonoglou I,et al.Prioritized Experience<br>Replay[J].Computer Sci-ence,2015.</p>
<p>[3] Wang Z,Schaul T,Hessel M,et al.Dueling network architectures for deep<br>reinforcement learning[J].2015:1995-2003.</p>
<p>[4] Hester T,Vecerik M,Pietquin O,et al.Deep Q-learning from<br>Demonstrations[J].2018.</p>
<p>[5] Bellemare M G,Dabney W,Munos R.A Distributional Perspective on<br>Reinforcement Learning[J].2017.</p>
<p>[6] Fortunato M,Azar M G,Piot B,et al.Noisy Networks for Exploration[J].2017.</p>
<p>[7] Hessel M,Modayil J,Van Hasselt H,et al.Rainbow:Combining Improvements in<br>Deep Reinforcement Learning[J].2017.</p>
<h1 id="第三部分-基于策略梯度的算法"><a href="#第三部分-基于策略梯度的算法" class="headerlink" title="第三部分 基于策略梯度的算法"></a>第三部分 基于策略梯度的算法</h1><p>第三部分将介绍基于策略梯度的强化学习算法，不同于第二部分，本部分介绍的算法将直接基于长期回报期望进行优化，使用的也是在机器学习领域常见的基于梯度的优化方法。策略梯度法一直受到广泛关注，基于这种思想的算法也非常多，从其中很多算法身上都能看到其他领域算法的影子。作为一类经典算法，策略梯度法在学习策略模型过程中表现得相对稳定，同时也可以处理离散和连续行动空间的计算，基于策略模型的改进方案也使算法有了很大的提高。</p>
<p>第三部分共包含三章：</p>
<p>· 第9章介绍了策略梯度法的基本原理，以及其改进算法Actor Critic，并介绍了A2C算法的实现。</p>
<p>· 第10章介绍了使策略单调提升的算法，其中包括置信区域策略优化（TRPO）和近端策略优化（PPO）。</p>
<p>· 第11章介绍了提高样本利用率的策略梯度算法，其中包括基于经验回放的Actor Critic（ACER）和确定策略梯度法（DPG）。</p>
<h2 id="9-基于策略梯度的算法"><a href="#9-基于策略梯度的算法" class="headerlink" title="9 基于策略梯度的算法"></a>9 基于策略梯度的算法</h2><p>前面的章节中我们介绍了基于最优价值的强化学习算法，这些方法的核心在于对值函数的估计，如果值函数能够很好地估计得到，我们就可以通过最优值函数得到对应的策略。这个思路在实际中得到了很好的应用，但是它也存在一些问题，例如模型训练的稳定性等。这也使得我们去探索其他的方法，例如本章介绍的策略梯度（Policy<br>Gradient）法并不采取迂回的方式更新策略，而是直接计算策略可能更新的方向。本章，我们将介绍策略梯度方法的原理，同时介绍由此扩展得到的方法：Actor<br>Critic法，最后介绍在实战中获得比较好结果的方法：A3C与A2C。</p>
<h3 id="9-1-策略梯度法"><a href="#9-1-策略梯度法" class="headerlink" title="9.1 策略梯度法"></a>9.1 策略梯度法</h3><p>本节将介绍策略梯度法，这种算法和前面介绍的算法有很大不同。实际上，第6章～第8章介绍的算法都使用了类似的优化思想，优化的重点都落在了值函数上，无论是策略迭代，还是Q-<br>Learning，只要能够得到精确的值函数，就可以使用Bellman公式求出最优策略，也就是说，我们的最优策略 _π_ ∗ 需要满足</p>
<p>策略梯度法则使用了另外一种思路，这个思路实际上更容易被读者理解。让我们回到问题的本源，强化学习的目标是最大化长期回报期望，于是目标也可以写作</p>
<p>其中τ 表示使用策略进行交互得到的一条轨迹， _r_<br>（τ）表示这条轨迹的总体回报。由于值函数实际上也是一个函数，强化学习的目标是最大化这个函数，如果这个函数性质良好，我们是不是可以用求导的方法对其进行优化呢？如果我们可以将值函数表示为策略参数的某个函数，就可以求出值函数关于策略参数的梯度，并使参数沿着梯度上升的方向更新，也就可以提升策略了。下面就使用优化的思想介绍这个算法如何对策略进行优化。</p>
<h4 id="9-1-1-算法推导"><a href="#9-1-1-算法推导" class="headerlink" title="9.1.1 算法推导"></a>9.1.1 算法推导</h4><p>下面开始公式的推导，我们用 _J_ （ _θ_ ）表示前面提到的目标函数，将轨迹的期望回报展开，可以得到</p>
<p>下面对公式求导，由于策略函数通常是定义良好的函数，所以求导运算可以和积分运算互换，这样可以得到</p>
<p>因为积分的缘故，这个形式并不方便直接计算，于是我们对其做一个变换，这里用到对数求导的基本公式</p>
<p>经过变换可以得到</p>
<p>我们将 _y_ 替换为 _π_ θ （τ），将 _x_ 替换为 _θ_ ，同时将公式的左右两边互换，就可以得到</p>
<p>带入前面的公式，得到</p>
<p>求出的梯度虽然看上去形式很简洁，但其中还是有一些不易计算的部分，例如∇θ log _π_ θ （τ）。下面就来看看这些内容如何做进一步拆解。前面我们给出了<br>τ 的基本形式，假设轨迹的总长度为 _T_ ，将其展开可以得到</p>
<p>对其求导，可以得到</p>
<p>得到这样的结果，我们就可以暂时停止进一步展开公式的步伐了，因为此时的公式已经和大家常见的最大log似然的公式没有区别。最后，我们再使用已经尝试了多次的蒙特卡罗法，将公式中的期望用蒙特卡罗近似的方式进行替换，可以得到求解梯度的“最终”形式：</p>
<p>完成了对梯度的求解，剩下的工作就是参数更新。所以总结起来Policy Gradient方法分为两步：</p>
<p>（1）计算∇θ _J_ （ _θ_ ）。</p>
<p>(2) _θ_ ~= _θ_ + _α_ ∇θ _J_ ( _θ_ ).</p>
<p>实际上，策略梯度的推导还有另外一种更复杂的形式，它主要使用了Bellman公式表示交互序列的细节，推导如下所示：</p>
<p>这里的s′ 表示状态s下一时刻得到的状态，下面的s′′ 含义类似，继续推导，可以得到</p>
<p>将公式整理，可以得到</p>
<p>为了化简公式，我们定义 _P r_ （s→x _，k，π_ ）为经过 _k_ 时刻环境从状态s到状态x的概率，那么公式就变为</p>
<p>将这个公式无限展开，就会变为</p>
<p>随着 _n_ →∞， _γ_ n →0，这个公式展开的项目就会趋近于0，于是我们将尾项忽略，并将项目的其他部分合并，就可以表示为</p>
<p>其中的</p>
<p>可以表示某个状态s′ 出现的全部次数的期望，一般可以用变量 _d_ π （s）表示，于是最终公式就变为</p>
<p>公式的形式就与前面求得的 _E_ τ～πθ（τ） [∇θ log _π_ θ （τ） _r_ （τ）]相同了。</p>
<h4 id="9-1-2-算法分析"><a href="#9-1-2-算法分析" class="headerlink" title="9.1.2 算法分析"></a>9.1.2 算法分析</h4><p>由于策略梯度算法的求解形式和我们熟悉的机器学习方法十分相近，所以我们可以从机器学习的角度理解这个方法。其实它和最大似然法十分类似，如果将强化学习问题变成一个监督学习问题，那么我们希望策略模型得到的行动概率分布能够尽可能地与采样数据的行动概率分布一致。我们可以按照这个思路给出模型的求解方法。</p>
<p>首先是准备训练数据，我们将一个序列折开，每一个状态都与下一个状态解耦，同时使用每一步的行动当作模型的目标输出值，这样就把一个交互序列</p>
<p>拆解成由状态-行动组成的训练数据集合：</p>
<p>这样就可以进行监督学习了。我们采用最大似然法进行建模，目标函数可以设立为</p>
<p>令等号右边的公式为 _J_ （ _θ_ ），对其求梯度，可以得到</p>
<p>我们将这个公式与前面得到的Policy<br>Gradient的梯度进行对比，可以看出两个公式的梯度值十分相近，唯一的不同就是公式后面的一项，这一项表示了序列的整体回报。关于这个差异，我们可以从以下两个角度分析。</p>
<p>（1）从策略梯度法的角度看，最大似然法没有考虑长期回报，或者说最大似然法将长期回报值设定为固定值1，即每一个样本对训练的影响相同。</p>
<p>（2）从最大似然法的角度看，最大似然对每个样本使用了相同的权重，对策略梯度法来说，它使用序列的回报作为样本的加权权重。也就是说，对于序列回报为正的训练样本，我们要最大化它的似然；对于序列回报为负的训练样本，我们要最小化它的似然。</p>
<p>这样策略梯度算法就和最大似然法在思想上得到了统一，序列回报的功能和样本权重比较类似，我们可以用类似的思路看待这个新事物。</p>
<h4 id="9-1-3-算法改进"><a href="#9-1-3-算法改进" class="headerlink" title="9.1.3 算法改进"></a>9.1.3 算法改进</h4><p>看上去我们的算法介绍已经告一段落，可以进入实战阶段，但实际上这个公式还存在一些需要解决的问题。主要问题来自和最大似然法不同的这一部分：序列回报部分。</p>
<p>首先，我们再回顾一下求解策略价值梯度的公式：</p>
<p>从这个公式中我们看出了一个问题，不论是哪个时间段，我们都要用策略的梯度乘以所有时刻的回报值总和，这样的设计显然是不合理的。因为理论上，在 _t_<br>时刻我们完成了决策后，它最多只能影响 _t_ 时刻之后的所有回报，并不会影响 _t_<br>时刻之前的回报，因为我们无法通过现在的决策影响已经发生过的事情，所以这一部分的回报值不应该计算在梯度中。我们的公式应该改写成</p>
<p>现在的公式中用两个不同的变量 _t_ 和 _t_ ′ 表示两部分需要计算的时间，这样公式就变得更合理了。</p>
<p>其次，从上面的分析来看，策略梯度法更像是加权版的最大似然优化法。“权重”将直接影响梯度的更新量，这样就会带来以下两个问题。</p>
<p>（1）如果计算得出的序列回报数值较大，那么对应的参数更新量就会比较大，这样我们的优化就可能出现一定波动，这些波动很可能影响优化效果。</p>
<p>（2）在一些强化学习问题中，环境给予的回报始终为正，那么不论我们的决策如何，最终累积的长期回报值都是一个正数。换句话说，我们会增强所有的策略，只是对于实际效果并不好的策略，我们为其提升的幅度会有所降低。这样的更新方法和我们的初衷并不一致，我们降低不好的行动的概率，而不是轻微提升不好的行动的概率。</p>
<p>这两个问题使我们回到了强化学习的目标：提高能最大化长期回报策略的概率，降低无法最大化长期回报策略的概率。将上面的思想转化成策略梯度问题的表述形式，就会变成：让能够最大化长期回报策略的“权重”为正且尽可能大，让不能最大化长期回报策略的“权重”为负且尽可能小。</p>
<p>为了实现这个目标，我们可以调整权重的数值和范围，一个简单的方法就是给所有时刻的长期累积回报减去一个偏移量，这个偏移量也被称为Baseline，我们用变量b表示，于是公式就变为</p>
<p>这个变量可以设计为同一起始点的不同序列在同一时刻的长期回报均值，它的公式形式如下所示：</p>
<p>经过这样的处理，所有时刻的权重均值变为0，就会存在权重为正或为负的行动，同时权重的绝对值也得到了一定的缩小，这对于算法的稳定性来说都有一定的帮助。更重要的是，加入的这个偏移量并不会使原本的计算值变得有偏，我们可以得到</p>
<p>所以它可以在不影响期望值的同时降低算法的波动性。这个方法也被广泛应用于Policy<br>Gradient算法中。当然，在现在常见的方法中，我们已经有了更好的替代方法，这部分内容将在9.2节介绍。</p>
<h3 id="9-2-Actor-Critic算法"><a href="#9-2-Actor-Critic算法" class="headerlink" title="9.2 Actor-Critic算法"></a>9.2 Actor-Critic算法</h3><h4 id="9-2-1-降低算法的方差"><a href="#9-2-1-降低算法的方差" class="headerlink" title="9.2.1 降低算法的方差"></a>9.2.1 降低算法的方差</h4><p>在9.1节中我们介绍了策略梯度法，这个方法已经能够利用交互的样本进行训练了，但是它还存在着一些问题。我们再来看看下面这个公式：</p>
<p>公式的问题仍然在等号右边的最后一项。前面提到，我们用轨迹的回报表示整个序列的价值，这个表示是准确无偏的，但是在真实的训练过程中，为了尽可能地控制学习时间，我们无法完成非常多次的交互，往往只会进行有限次数的交互，这些交互有时并不能很好地代表轨迹真实的期望。每一个交互得到的序列都会有一定的差异，对应的回报也会有一定的差异，因此不充足的交互会给轨迹回报带来较大的方差，这和前面提到的蒙特卡罗法遇到的问题类似，如图9-1所示。</p>
<p>由于强化学习中Agent与环境交互基于MDP，每一时刻Agent基于当前的状态可以有很多的选择，而采样得到的序列只是其中之一，所以一条或几条序列的回报和所有路径的回报期望存在一定的差距。</p>
<p>图9-1 交互序列的差异</p>
<p>机器学习的一个核心问题是平衡偏差和方差。对策略梯度模型来说，它的方差相对较大，为了模型的稳定，我们可以牺牲一定的偏差来换取方差变小。这其中的一种方法就是采用Actor-<br>Critic算法，其主要特点就是用一个独立的模型估计轨迹的长期回报，而不再直接使用轨迹的真实回报。类似于基于模型的Q-<br>Learning算法，在估计时使用模型估计轨迹价值，在更新时利用轨迹的回报得到目标价值，然后将模型的估计值和目标值进行比较，从而改进模型。</p>
<p>公式中可以被模型替代的部分有两个，其中代表从 _t_ 时刻出发所获得的长期回报，bi<br>代表待减去的偏移量。根据这两个部分被模型替代的情况，我们可以得到以下几种方案。</p>
<p>（1）采用策略梯度法的方法：公式由表示。</p>
<p>（2）使用状态值函数估计轨迹的回报： _q_ （s _，_ a）。</p>
<p>（3）直接使用优势函数估计轨迹的回报： _A_ （s _，_ a）= _q_ （s _，_ a） _-V_ （s）。</p>
<p>（4）使用TD-Error估计轨迹的回报：公式由 _r_ （st _，_ at ）+ _v_ （st+1 ） _-v_ （s）表示。</p>
<p>可以看出四种方法都可以在一定程度上降低算法的方差，实际中Actor<br>Critic算法最终选择了第4种方案，这种方案在计算量和减少方差方面具有一定的优势。由于引入了状态价值模型，算法整体包含了两个模型，一个是策略模型，一个是价值模型，所以这个算法被称为Actor-<br>Critic，其中Actor表示策略模型，Critic表示价值模型。</p>
<h4 id="9-2-2-A3C算法"><a href="#9-2-2-A3C算法" class="headerlink" title="9.2.2 A3C算法"></a>9.2.2 A3C算法</h4><p>真正将Actor-Critic应用到实际中并得到优异效果的是A3C（Asynchronous Advan-tage Actor-<br>Critic）算法，从算法的名字中可以看出，算法突出了异步和优势两个概念。另一个优化算法A2C（Advantage Actor-<br>Critic），采用了同步的方法进行优化，并同样获得了比较好的效果。</p>
<p>Asynchronous表示了A3C算法的重要特点之一：并行的交互采样和训练。通过9.1节我们已经知道，策略梯度法和Actor-<br>Critic算法都通过目标函数的梯度进行策略更新，而计算梯度需要基于当前的策略模型，所以每一次计算梯度时，我们需要使用当前最新的策略模型重新进行交互采样，得到相应的序列样本，然后使用这些样本完成梯度计算；当梯度计算完成，模型得到更新后，这些使用过的样本将被丢弃，我们需要采集新的样本。这种训练方式通常被称为On-<br>Policy Learning，也就是说我们要基于样本进行学习，第7章中的SARSA算法也是这样的算法。</p>
<p>第7章和第8章介绍的DQN方法，由于采用Replay Buffer存储了一段时间的交互样本，模型学习时使用的样本不一定是由当前模型交互得到的，而且Q-<br>Learning在学习时假设在 _t_ +1时刻将采用当前最优的行动，因此它的目标价值并不是由真实的交互序列得到的。这样的学习方式通常被称为Off-<br>Policy Learning。</p>
<p>由于Off-Policy算法可以使用非当前策略产生的样本，因此它对样本的重复使用度比较高。而对On-<br>Policy算法来说，每一次模型更新都需要一定量的“新样本”，所以On-<br>Policy算法需要花费更多的时间收集样本。所以为了更快地收集样本，我们需要用并行的方法收集。在A3C方法中，我们要同时启动 _N_ 个线程，Agent将在<br>_N_<br>个线程中同时进行环境交互。只要保证每一个线程中的环境设定不同，线程间交互得到的序列就不会完全一样，这样得到的样本就很有意义。收集完样本后，每一个线程将独立完成训练并得到参数更新量，并异步地更新到全局的模型参数中。下一次训练时，线程的模型参数将和全局参数完成同步，再使用新的参数进行新一轮的训练。</p>
<p>Advantage表示了算法关于“权重”部分的计算方法。前面提到Actor-Critic算法并不是真的直接使用优势函数表示权重，而是采用TD-<br>Error的形式：rst，at + _v_ （st+1 ） _-v_ （st<br>），公式通过当前时刻的回报和下一时刻的价值估计得到了目标价值，然后减去当前时刻的价值估计。这个方法虽然增加了模型的稳定性，但是模型的偏差也相应变大，为了更好地平衡模型的偏差和方差，A3C方法使用了8.4节介绍的多步回报估计法，这个方法可以在训练早期更快地提升价值模型。对应的公式变为</p>
<p>最后，为了增加模型的探索性，模型的目标函数中加入了策略的熵。由于熵可以衡量概率分布的不确定性，所以我们希望模型的熵尽可能大一些，这样模型就可以拥有更好的多样性。这样，完整的策略梯度计算公式就变为</p>
<p>其中 _β_ 为策略的熵在目标中的权重。价值模型的目标函数与DQN类似，这里不再赘述。A3C的完整算法如下所示：</p>
<p>A3C算法 单一线程执行过程</p>
<p>Input</p>
<p>_T_ ←0</p>
<p>全局和线程内部的策略模型参数： _θ，θ_ ′</p>
<p>全局和线程内部的价值模型参数：</p>
<p>两个模型的梯度更新量：d _θ，_ d _θ_ v</p>
<p>Start</p>
<p>repeat until _T ＞T_ max</p>
<p>将梯度清零：d _θ_ ←0 _，_ d _θ_ v ←0</p>
<p>同步模型参数：</p>
<p>用策略模型 _π_ （at |st ； _θ_ ′ ）完成一次轨迹采样，得到{s0 _，_ a0 _，_ r0 _，…_ }</p>
<p>_T_ ← _T_ + _n_</p>
<p>计算每一个采样的价值：</p>
<p>for _i_ ∈{ _n_ -1 _,_ 0}do</p>
<p>_R_ ←ri + _γR_</p>
<p>积累策略模型梯度</p>
<p>积累价值模型梯度</p>
<p>end for</p>
<p>对全局模型 _θ_ 和 _θ_ v 进行异步更新： _θ_ ←d _θ_ ， _θ_ v ←d _θ_ v</p>
<p>由于其十分优异的效果，A3C算法的影响力极大。但是大家一直存在一个疑问：算法中的异步更新是否是必要的？</p>
<p>算法作者的实验环境和我们常见的基于GPU的环境不同，它主要使用了CPU进行模拟和训练，并且使用了“Hogwild！”[3]<br>这样的更新方法，这和深度学习中常见的基于同步更新的方法并不一样。凭直觉，异步或者同步更新并不是决定算法优劣的主要因素，那么为什么不尝试使用同步更新方法呢？于是大家开始尝试非异步的方法，这就是A2C的来源。A3C和A2C更新流程的对比如图9-2所示。可以看出，在A3C中，每个线程各自完成参数更新值的计算，并异步完成参数的同步操作；而A2C将决策和训练的任务集中到一处，其他进程只负责环境模拟的工作。</p>
<p>图9-2 A3C与A2C的更新区别</p>
<p>在Baseline项目中，我们可以看到A2C的实现，而且OpenAI的官方博客中也提到 [5]<br>A2C的效果要优于A3C，而且A2C在并发性和系统简洁性上都优于A3C。那么我们就来介绍A2C的实现细节。</p>
<h4 id="9-2-3-A2C算法实战"><a href="#9-2-3-A2C算法实战" class="headerlink" title="9.2.3 A2C算法实战"></a>9.2.3 A2C算法实战</h4><p>本节我们介绍A2C算法在实战中的效果。A2C的代码在Baselines项目中的baseli-<br>nes/a2c文件夹，读者可以直接执行其中的run_atari.py文件执行算法。</p>
<p>首先我们介绍算法与环境交互的实现。这个算法的代码采用了Master-<br>Slave的结构，其中的Master用于执行Agent的模型，在Env给出状态观测值后判断应该执行的Action；而Slave用于模拟Env，在收到Agent的行动后，将后续的观测值、回报等信息返回给Master，Master将Slave接收不了的数据收集并进行训练。这个架构比较简洁清晰，代码中的Master-<br>Slave结构如图9-3所示。</p>
<p>然后我们介绍算法的模型结构。在代码的policies.py文件中，我们可以看到三个表示策略的模型：LnLstmPolicy、LstmPolicy和CnnPolicy。从名字上可以看出，它们采用了不同的模型结构，其中LnLstm代表Layer<br>Normalization LSTM。三个模型使用了相同的卷积神经网络结构Nature<br>CNN用于从Atari的游戏画面中提取特征，这个网络名字来源于DQN的网络结构，因为发明DQN模型的论文发表在Nature上。Nature<br>CNN的网络结构如图9-4所示。</p>
<p>图9-3 Master-Slave的结构</p>
<p>图9-4 Nature CNN的网络结构</p>
<p>CNNPolicy的结构相对简单，使用Nature CNN提取特征后再经过全连接层的计算，就可以得到观测值的行动概率和价值，其结构如图9-5所示。</p>
<p>图9-5 CNNPolicy的网络结构</p>
<p>LstmPolicy则相对复杂，除了使用Nature CNN提取特征，我们将时长为 _T_<br>的特征转换成序列的形式，并通过LSTM得到转换后的序列特征，再经过全连接层的处理，就可以得到行动概率和价值，其结构如图9-6所示。</p>
<p>LnLstmPolicy的结构和LstmPolicy类似，这里就不再介绍。了解了这两部分，我们就可以看看模型的核心部分了，主体训练流程如下面的伪代码所示：</p>
<p>图9-6 LstmPolicy的网络结构</p>
<p>从伪代码中可以看出，整体流程分为两个部分：与环境交互和基于样本训练模型。在与环境交互时，我们使用step_model进行交互，这个模型接收 _N_<br>个Slave发来的观测值，并给出单步的行动决策。为了方便计算，每一个轨迹只选取前steps进行交互，交互完成后，这一时刻的obs、reward和这一时刻的价值估计value将被保存。当数据收集完成后，下一步就是计算每一时刻的目标价值估计值，并更新到data[reward]中。</p>
<p>接下来就是训练过程，代码中分别计算了三部分的目标函数。</p>
<p>· 策略Loss：∑A dv×log _π_ （a|s）。</p>
<p>· 价值模型Loss：。</p>
<p>· 策略的熵Loss：-∑a _π_ （a|s）log _π_ （a|s）。</p>
<p>由于代码中实际上要完成梯度下降的操作，所以每一项的符号和前面概念中介绍的刚好相反。得到了目标值后，我们直接使用TensorFlow的反向计算就可以完成梯度更新。以上就是A2C算法的流程，其中更多细节内容欢迎读者自行阅读。</p>
<h3 id="9-3-总结"><a href="#9-3-总结" class="headerlink" title="9.3 总结"></a>9.3 总结</h3><p>本章介绍了基于策略梯度的算法：策略梯度法和Actor Critic法，让我们回顾一下。</p>
<p>（1）策略梯度法的主要思想是直接计算目标函数关于策略的梯度，通过梯度优化策略。</p>
<p>（2）在策略梯度法中，轨迹的回报起到了类似样本权重的作用。</p>
<p>（3）Actor Critic法引入了价值模型，并使用TD-Error的计算结果作为样本权重。</p>
<p>（4）A3C方法使用了并发收集交互样本的方式快速收集样本，并采用异步更新参数的方式。</p>
<h3 id="9-4-参考资料"><a href="#9-4-参考资料" class="headerlink" title="9.4 参考资料"></a>9.4 参考资料</h3><p>[1] Sutton R S.Policy Gradient Methods for Reinforcement Learning with<br>Function Ap-proximation[J].Submitted to Advances in Neural Information<br>Processing Systems,1999,12:1057-1063.</p>
<p>[2] Konda V R,Tsitsiklis J N.On Actor-Critic Algorithms[M].Society for<br>Industrial and Applied Mathematics,2003.</p>
<p>[3] Feng N,Recht B,Re C,et al.HOGWILD!:A Lock-Free Approach to Parallelizing<br>Stochastic Gradient Descent[J].Advances in Neural Information Processing<br>Systems,2011,24:693-701.</p>
<p>[4] Mnih V,Badia A P,Mirza M,et al.Asynchronous Methods for Deep Reinforcement<br>Learning[J].2016.</p>
<h2 id="10-使策略单调提升的优化算法"><a href="#10-使策略单调提升的优化算法" class="headerlink" title="10 使策略单调提升的优化算法"></a>10 使策略单调提升的优化算法</h2><p>在第7章和第8章中，我们了解了DQN算法，这个算法具有很多很好的特性，但是在训练时模型存在一定的波动性；第9章介绍的A3C算法平衡了模型方差和偏差，使得模型能够尽量减少波动，但波动性的问题依然存在。于是我们不禁在想：能不能找到一种算法尽可能地减少波动？能不能找到一种训练方法，使模型在保持前面各种优点的同时，效果能稳步上升呢？本章介绍的算法会给我们一些启示。</p>
<p>第3章已经了解了自然梯度法这个优化方法，本章介绍以此为基础的方法：置信区域策略优化（Trust Region Policy<br>Optimization，TRPO）和近端策略优化（Proximal Policy<br>Optimization，PPO）方法，它们的总体优化目标比较相近，主要区别在于具体的计算方法。本章的涉及大量的数学知识，请读者根据自身的能力进行学习。</p>
<h3 id="10-1-TRPO"><a href="#10-1-TRPO" class="headerlink" title="10.1 TRPO"></a>10.1 TRPO</h3><p>TRPO算法来自论文 _Trust Region Policy Optimization_ [1]<br>。TRPO算法的计算过程比较复杂，但是它可以确保策略模型在优化时单调提升。为了达到算法想要的效果，我们需要进行一系列的分析和推导，其中主要的思路是找到一种衡量策略之间优劣的计算方法，并以此为目标最大化新策略与旧策略相比的优势。虽然算法的整体思路比较简单，但想要完成这些计算和证明却有些困难，让我们一步一步完成目标公式的推导与求解。</p>
<h4 id="10-1-1-策略的差距"><a href="#10-1-1-策略的差距" class="headerlink" title="10.1.1 策略的差距"></a>10.1.1 策略的差距</h4><p>TRPO算法的目标是确保每一轮更新后策略算法都能够有进步，或者说至少不退步。为了确保这一点能够成立，我们首先要得出衡量策略间效果对比的公式，这样才能进行后面的计算。我们先定义基于某个策略的期望价值：</p>
<p>其中s0 ～ _ρ_ 0 （s0 ），at ～ _π_ （at |st ），st+1 ～ _P_ （st+1 |st _，_ at<br>），公式中的变量相信读者已经比较熟悉。接下来给出状态行动值函数、状态值函数和优势函数的定义：</p>
<p>通过上面的定义，我们可以给出表示两个策略的期望价值差距的公式：</p>
<p>它的证明过程如下，由于</p>
<p>我们可以得到</p>
<p>将等式左右进行变换，就可以得到最终结果。</p>
<p>经过前面的推导，我们找到了两个策略差距的基本形式。上面的公式并不能直接计算，我们还要对公式做进一步变换。</p>
<p>令 _ρ_ π （s）表示任意时刻状态s的访问概率和，它的形式为</p>
<p>上面的公式可以变形为</p>
<p>这样我们就求出了策略之间的差距。从这个公式可以看出，当</p>
<p>保持非负值，可以得到 _η_ （ _π_ ）≤ _η_ （～ _π_ ）。也就是说，我们从某个策略 _π_ 0 出发，通过计算找到一个策略 _π_ 1<br>，使得</p>
<p>那么我们可以确定策略 _π_ 1 在总体上优于 _π_ 0<br>，依此类推，我们可以不断地找到这样的策略，使策略模型的效果不断增强，直至达到目标，这就是算法单调提升的基本原理。</p>
<h4 id="10-1-2-策略提升的目标公式"><a href="#10-1-2-策略提升的目标公式" class="headerlink" title="10.1.2 策略提升的目标公式"></a>10.1.2 策略提升的目标公式</h4><p>10.1.1节得到了策略提升的目标，但是受目标公式所限，这样寻找策略的方法在实际中是几乎不可行的。因为公式中包含 _ρ_ ～π<br>（s），也就是说，我们需要按照新策略与环境交互得到状态的访问频率，而这个新策略正是我们要求解的策略。所以想要求解这个策略，需要先确定新策略的形式，然后使用这个策略得到一定量的样本，并最终通过这些样本统计判断这个策略是否满足需求。这样我们就需要不断地尝试验证每一个可能的新策略，更新过程就会慢到难以忍受，变得在实际问题中无法执行。</p>
<p>因此，为了让计算变得可行，我们需要找到与上面公式的近似且可解的形式，于是等式被近似为下面这个公式：</p>
<p>可以看到，前后两个公式唯一的差别在状态访问频率上。前面的公式需要知道基于新策略的状态访问概率 _ρ_ ～π （s）进行采样，后面的公式只需要基于旧策略<br>_ρ_ ～π （s）进行采样即可，这样一来，在寻找新策略时这一项已经变成了一个已知项，公式变得相对容易求解。那么这个公式能否满足我们的需求呢？</p>
<p>实际上通过证明我们发现，对当前策略 _π_ 来说，两个公式在数值和导数上是相同的：</p>
<p>由于数值相同，导数方向相同，所以在有限的范围内，两个目标函数的数值相差比较有限，我们就可以用近似函数替代原始目标。那么，我们沿着近似目标的导数方向做有限步长的变化，也同样可以提升策略。</p>
<p>既然我们知道在策略附近的区域，两个目标函数可以相互近似，就可以加入另外一个约束，用于限定新策略与旧策略的差异。如果我们把策略模型想象成一个概率分布，那么我们可以使用KL散度表示两个分布的距离。其中</p>
<p>～ _π_ （·|s）），经过推导可以证明，当～ _π_ =argmaxπ ′ _L_ π （ _π_ ′ ）时，满足</p>
<p>这部分的证明比较复杂，这里不再给出。我们令</p>
<p>为第 _i_ 轮迭代得到的值，通过推导得到</p>
<p>根据上面两个公式，可以得到</p>
<p>如果 _π_ i+1 =argmaxπ _M_ i （ _π_<br>），那么上面不等式右边就是一个非负值，这样我们就能确保策略对应的期望价值非负上升。现在目标函数得到了一定的化简，我们就可以围绕这个公式进行优化。</p>
<h4 id="10-1-3-TRPO的目标定义"><a href="#10-1-3-TRPO的目标定义" class="headerlink" title="10.1.3 TRPO的目标定义"></a>10.1.3 TRPO的目标定义</h4><p>下面开始介绍TRPO的优化算法，它的实现有一些复杂，经过前面的推导，我们已经知道算法的目标函数为</p>
<p>在实践中，这个公式还有很多问题，下面我们将一一解决。</p>
<p>第一个优化的目标是KL散度。首先在实践中上式虽然可以得到比较好的效果，但是由于上式的理论验证过于保守，每一轮迭代更新的步长都偏小，导致优化的速度很慢。为了解决这个问题，我们将优化形式进行转变，得到了有约束条件的优化问题：</p>
<p>当问题变成了这个形式后，原公式中较复杂的部分都被转移到了约束中。在公式中我们需要对最大值进行约束，而最大值表示为KL散度的上界，这实际上相当于对所有状态的KL散度进行约束，这样约束条件会变得多而复杂。为了简化运算，这里将最大值变成均值，虽然约束条件有所放宽，但这样可以降低计算的难度，而且从实际效果看这样不会造成效果下降。我们令</p>
<p>为两个策略的平均KL散度，于是问题又变成了如下形式：</p>
<p>第二个优化和新策略 _π_ （a|s）有关，我们可以将目标公式的第二项写作如下形式：</p>
<p>如果我们想采用蒙特卡罗法对行动进行采样，就需要事先知道新策略的形式，这对优化同样造成了阻碍。为了方便计算，我们需要对其进行化简。这里采用重要性采样的方法，由于新策略不会发生特别大的变化，所以两个策略的概率分布十分接近，使用重要性采样也可以获得比较好的效果。那么公式就变为</p>
<p>有了这个形式，采样困难的问题就得到了解决，现在可以基于旧策略进行采样计算。2.4节介绍过重要性采样的原理，也提到如何提高重要性采样的效果这个问题。当原始概率分布和新的概率分布十分接近时，重要性采样的效果会比较好，由于前面我们限定新旧策略的差距不会太大，所以这里的重要性采样效果不会差。</p>
<p>此时我们已经完成了主要的公式推导，下面就将TRPO目标函数转换成3.3节介绍的自然梯度法的形式。我们可以回顾一下，自然梯度法的目标函数为</p>
<p>其中w表示参数， _f_ 表示待优化的函数，Ifw<br>表示Fisher信息矩阵。对这部分内容不太熟悉的读者可以回顾3.3节的内容。如果我们将TRPO的目标函数变换成上面的形式，就能使用自然梯度法进行求解。幸运的是，我们可以用一些方法将问题转换成这样的形式。令策略<br>_π_ 的参数为 _θ_ ，首先，对目标函数进行一阶泰勒展开，可以得到</p>
<p>其次，对约束条件进行变换，首先根据3.3节证明得到的公式</p>
<p>可以对约束条件做第一步变换：</p>
<p>再根据3.3节的结论：，继续推导得到</p>
<p>这样我们就从求解新策略的目标函数变成了求解策略参数更新量的目标函数：</p>
<p>当我们完成了更新量的计算，就可以将更新量添加到原策略参数上，完成一轮迭代的优化：</p>
<p>经过这么长时间的推导，我们完成了TRPO问题的定义，然而这只是将问题定义的工作完成，后面还要继续介绍它的求解方法。在我们介绍求解之前，先来总结前面的推导思路。</p>
<p>·<br>在10.1.1节，我们确立了策略单调提升的概念，同时给出了两个策略差距的计算公式，我们发现如果能保证新旧策略差距项始终为正，就能确保策略模型单调非递减地更新。</p>
<p>·<br>在10.1.2节，为了方便计算，我们用旧策略的状态访问概率代替新策略的状态访问概率。由于只有在新旧策略相近时，两个公式才表现得相近，因此我们额外加入了衡量策略的差异KL散度约束。</p>
<p>·<br>本节，我们首先将无约束问题拆解成有约束问题，同时将KL散度的最大值改为平均值，以方便计算。接着，采用重要性采样的方法，将待采样的行动由新策略换为旧策略。最后，使用泰勒展开和Fisher信息矩阵的定理将公式变为可以由自然梯度法求解的目标形式。</p>
<h4 id="10-1-4-自然梯度法求解"><a href="#10-1-4-自然梯度法求解" class="headerlink" title="10.1.4 自然梯度法求解"></a>10.1.4 自然梯度法求解</h4><p>本节笔者就来介绍求解的具体过程。自然梯度法的求解方法同样采用拉格朗日乘子法，求解出最优的策略模型参数更新量为</p>
<p>如果直接采用这样的公式进行计算，就需要计算Fisher信息矩阵的逆矩阵，而逆矩阵的计算量比较大，直接计算会降低模型的训练速度，因此我们需要寻找一种方法减少这部分的计算量。对TRPO算法来说，它采用共轭梯度法求解更新量，从而避免了Fisher信息矩阵逆矩阵的计算。</p>
<p>我们知道共轭梯度法可以用来求解Ax=g这样的问题，而参数更新量也可以表示成这样的形式：</p>
<p>幸运的是，在共轭梯度法的计算过程中我们只需要矩阵A参与运算，这样我们就不需要计算逆矩阵了。而且在共轭梯度法的每一轮迭代中，它只参与了一次计算，就是在计算的优化步长这一步：</p>
<p>虽然我们节省了一定的计算时间，但是我们依然要计算出矩阵A，这个矩阵对深层神经网络这样复杂的模型来说同样是难以计算的，因此我们仍然需要找方法进行化简。幸运的是，矩阵A同旁边的方向向量p进行乘法会得到一个向量，这是简化算法的突破口。矩阵和向量乘积的结果的每一个元素都是由两个向量得到的，对应的计算公式为</p>
<p>对于我们现在要解决的问题来说，求导和求和是可以互换的，所以得到</p>
<p>这样一来，我们先进行小括号内部的计算，然后对结果进行求导，这样整体的复杂度将会大大降低。为了展示这个方法的正确性，我们举一个简单的例子。例如，我们现在有一个目标函数：</p>
<p>给定一个向量p为[p1 _，_ p2 ]T ，计算函数的矩阵A与向量p的乘积。</p>
<p>如果采用直接计算矩阵A的方法，则可以得到</p>
<p>与向量p相乘得到</p>
<p>如果采用另一种方法，那么在得到一阶导数后，直接进行向量乘法，可以得到</p>
<p>对其进行求导，得到</p>
<p>可见两者的结果是相同的，这也证明了这个方法的可行性。这样原本的二阶求导变成了求两次一阶导数，运算量得到了大幅降低，运算速度得到了很大的提升。解决了这个大问题，我们就可以使用共轭梯度法完成对优化方向的求解，令∇π<br>_L_ πold （ _π_ ； _θ_ old<br>）表示残差，令为共轭梯度法的A，就可以完成对优化方向s的计算。有关共轭梯度法的细节，请读者回顾3.2节的内容。</p>
<p>完成了方向的计算，还需要计算在当前方向上的更新步长。由于有约束条件的存在，优化的最大步长一定存在，这样约束条件相当于为我们划定了一个置信区域（Trust<br>Region），以保证我们的优化满足策略提升的要求。前面我们利用共轭梯度法求出了方向s，现在令最大步长为 _β_ ，也就是说，参数更新的最大值为 _β_<br>s。我们前面提到了问题的约束：</p>
<p>使用 _β_ s代替∆ _θ_ ，就可以得到</p>
<p>将公式进行整理，可以得到</p>
<p>这样我们就知道了满足约束条件下的最大步长。与前面的方法类似，这里同样可以将Fisher信息矩阵和右边的方向向量相乘，这样可以用两个一阶导计算代替原本的二阶导计算，方法和前面介绍的类似。</p>
<p>最后，我们使用backtrack的线搜索方法找到满足优化条件的合适步长，也就是找到一个 _π_ （ _θ_ ）= _π_ （ _θ_ old +∆ _θ_<br>），使得它比 _π_ old 产生足够的提升。具体方法为：先尝试以 _β_<br>为步长的情况下，策略提升是否可以满足，如果已经满足则步长选择结束；如果无法满足，则将步长减少一半，再进行测试，直到满足为止。</p>
<p>完成了一切算法推导，我们将TRPO的完整算法总结出来。</p>
<p>算法 TRPO 算法</p>
<p>Input</p>
<p>策略网络 _π_ （a|s； _θ_ ）、旧策略网络 _π_ ′ （a|s； _θ_ ′ ）</p>
<p>价值网络 _v_ （s； _θ_ ）</p>
<p>Start</p>
<p>for iter=1 to _N_ ∶</p>
<p>通过交互得到交互样本</p>
<p>根据实际的序列计算当前状态的目标价值 _q_ （s _，_ a）</p>
<p>利用 _v_ 计算st 的估计价值 _v_ （st ），并计算出优势值adv= _q_ （st _，_ at ） _-v_ （st ）</p>
<p>根据目标函数更新价值模型</p>
<p>根据上面提到的策略目标更新策略模型</p>
<h4 id="10-1-5-TRPO的实现"><a href="#10-1-5-TRPO的实现" class="headerlink" title="10.1.5 TRPO的实现"></a>10.1.5 TRPO的实现</h4><p>TRPO的实现代码在Baselines项目的baselines/trpo_mpi中，这里我们重点关注策略模型的更新过程。有关价值模型的更新算法将在10.2节介绍，算法的核心代码在子项目的trpo_mpi.py中。从项目名称中可以看出，代码采用了MPI完成分布式训练，有关MPI的知识读者可以回顾4.3节的内容。这里我们只关注策略模型的目标函数构建与优化过程。</p>
<p>在模型方面，代码构建了两个策略模型pi和oldpi，分别代表待优化的策略和当前的策略。每一次优化时我们都会固定oldpi优化pi。策略更新量的计算可以分为下面几个步骤。</p>
<p>（1）计算目标函数的梯度：∇π _L_ πold （ _π_ ； _θ_ old ）。</p>
<p>（2）准备好矩阵A。</p>
<p>（3）使用共轭梯度法求解更新方向。</p>
<p>（4）求出步长的上限 _β_ 。</p>
<p>（5）求出真实的步长。</p>
<p>下面就来看看这5个步骤的计算过程。首先是第1步，目标函数的计算图结构如图10-1所示，其中方框框住的变量为placeholder。</p>
<p>图10-1 目标函数的计算图</p>
<p>图10-1所示为目标公式的计算过程，首先目标由TRPO算法中的目标函数surrgain和新策略的熵entbonous组成。熵的计算比较简单，它由新策略pi的熵和系数entcoeff组成。我们将surrgain继续拆解成新旧策略的比例ratio和优势函数atarg两部分，其中atarg会在前一步算好并传入其中。ratio又可以继续拆解成两个策略的对数似然，这样只要计算对应行动ac的对数似然即可。</p>
<p>第2步需要计算矩阵A，我们可以先求出KL散度的一阶导，计算流程图如图10-2所示。由于是KL散度的均值，因此计算过程比较简单。为了后续计算的方便，我们可以先把矩阵的一阶导计算好，得到klgrads。</p>
<p>图10-2 矩阵A的一阶导计算图</p>
<p>得到了KL散度的一阶导后，我们就可以构建一阶导和优化方向p的乘积。这个函数的计算图如图10-3所示。由于我们需要让矩阵A的一阶导和向量p进行点乘，因此需要把向量p变换成需要的形式。同时，方法U.flatgrad表示求出导数的同时将参数转换成一维向量的形式，这样可以继续在共轭梯度法中计算。</p>
<p>图10-3 矩阵A和向量p的乘积的计算图</p>
<p>第3步就是使用共轭梯度法，这部分计算在计算图定义之外。现在我们已经有了梯度和Ap的计算方法，直接调用共轭梯度法就可以计算优化方法。第4步和第5步的计算也在实际优化中，这两部分的计算比较简单，在此不再赘述。</p>
<p>以上就是TRPO算法实现的全过程，由于这部分的内容比较复杂，希望读者能够仔细阅读代码，了解更多的细节。</p>
<h3 id="10-2-GAE"><a href="#10-2-GAE" class="headerlink" title="10.2 GAE"></a>10.2 GAE</h3><p>在10.1节介绍的TRPO算法中，我们主要关注了策略网络的改进方法，并没有介绍价值网络的改进方法。本节介绍这部分的改进方案泛化优势估计（Generalized<br>Advan-tage Estimation，简称GAE）的具体实现。</p>
<h4 id="10-2-1-GAE的公式定义"><a href="#10-2-1-GAE的公式定义" class="headerlink" title="10.2.1 GAE的公式定义"></a>10.2.1 GAE的公式定义</h4><p>我们已经知道在策略梯度法中，如果直接使用On-<br>Policy的方法交互采样，并用每一时刻的回报作为梯度中的长期回报估计，会使算法产生较大的波动，换句话说，梯度的方差会比较大。如果采用Actor-<br>Critic的方法，通过模型估计状态的价值，那么模型优化的方差会减小，但是由于函数拟合的问题，这个方法会产生一定偏差。因此问题的关键就在于如何平衡偏差和方差带来的影响。</p>
<p>Actor Critic的价值梯度可以表示为</p>
<p>其中</p>
<p>总的来说， _A_ π，γ （st _，_ at<br>）已经可以做到在保持无偏差的情况下，尽可能地降低方差值。如果我们能通过学习得到一个完美的优势函数，模型就可以得到很好的表现。但实际中直接学习优势函数比较困难，我们往往需要组合其他函数得到优势函数，同时还需要考虑偏差和方差对模型的影响，为此我们给出了一个定义：<br>_γ_ -just。当一个函数 _A_ ^t 满足 _γ_ -just条件时，它就满足下面的公式：</p>
<p>_E_ s0,a0,…~τ [ _A_ ^t (s0∶∞ _,_ a0∶∞ )∇θ log _π_ θ (at |st )]= _E_<br>s0,a0,…~τ [ _A_ π,γ (st _,_ at )∇θ log _π_ θ (at |st )]</p>
<p>如果我们找到的估计函数能够满足上面的公式，它就可以用来替换优势函数。经过推导分析我们发现，rt + _γV_ π，γ （st+1 ） _-V_ π，γ<br>（st ）满足上述条件，是一个合格的替换项，于是后面的工作将围绕它替换进行。</p>
<p>令 _V_ 为一个近似的值函数，我们定义，这里的可以作为at 的一个优势价值估计。如果上面的公式中 _V_ = _V_ π，γ ，那么就是一个 _γ_<br>-just的估计函数，它可以得到 _A_ π，γ 的一个无偏估计：</p>
<p>接下来我们考虑 _n_ 步的优势函数估计，并用表示，可以得到</p>
<p>依此类推，可以得到</p>
<p>我们知道 _γ_ 是一个小于1的数，随着 _k_ 趋近于无穷大，最终 _γ_ ∞ _V_ （st+∞ ）→0，所</p>
<p>以这一项相当于用蒙特卡罗法对优势函数进行估计。</p>
<p>此时我们看到，随着估计步数的增加，估计值的偏差逐渐变小，方差逐渐变大。如果我们能将这些估计值同时考虑在内，是不是就可以在偏差和方差之间找到更好的平衡呢？基于这样的想法，我们可以得到多个估计值加权平均后的公式，公式的计算过程如图10-4所示。</p>
<p>图10-4 GAE计算公式图解</p>
<p>我们发现这个公式的最终形式比较简洁，虽然我们引入了一个新的超参数，但是公式并没有复杂太多。此时我们的估计值在偏差和方差方面得到了更好的平衡，我们可以分别计算<br>_λ_ 等于0和1时的值</p>
<p>可以看出，当 _λ_ =0时，算法等同于计算TD-Error，这是一个方差较低但偏差较高的算法；当 _λ_<br>=1时，算法变成蒙特卡罗目标值和价值估计的差，这是一个偏差较低但方差较高的算法。我们可以通过调整 _λ_<br>值使模型在两者之间得到更好的平衡。因此，我们可以用它代替前面公式中的优势函数，此时计算的公式就变为</p>
<h4 id="10-2-2-基于GAE和TRPO的值函数优化"><a href="#10-2-2-基于GAE和TRPO的值函数优化" class="headerlink" title="10.2.2 基于GAE和TRPO的值函数优化"></a>10.2.2 基于GAE和TRPO的值函数优化</h4><p>在介绍GAE的论文 _High-Dimensional Continuous Control Using Generalized Adνan-tage<br>Estimation_ [2] 中，算法除了将TRPO方法应用在策略的训练上，还将TRPO应用到了价值网络的训练上。价值网络的目标函数为</p>
<p>我们可以使用类似Trust<br>Region的方法对目标值函数进行限定。限定的核心思想是希望新的价值模型与旧模型不要相差过大，那么我们首先可以得到原始值函数和GAE方法的估计值的均方误差<br>_σ_ 2 ：</p>
<p>我们希望新旧模型的价值估计差不要过大，为了使约束具有一定的自适应性，我们将限定值设定为和 _σ_ 2 相关的一个值，于是我们的问题变为</p>
<p>这个问题的描述和TRPO中对策略网络的描述很类似，约束条件也和TRPO中的KL散度约束类似，这样我们同样可以把问题转变为一阶目标函数和二阶约束条件的形式：</p>
<p>其中。得到了这样的形式，我们就可以用Fisher信息矩阵近似值函数的二阶导，同时利用共轭梯度法进行优化，通过计算矩阵和向量的乘积来减少计算。由于这部分的计算过程和TRPO类似，这里就不再赘述。通过这种估计方法，我们在使用TRPO方法估计时可以获得更稳定的模型，效果也比原始模型更好。</p>
<h4 id="10-2-3-GAE的实现"><a href="#10-2-3-GAE的实现" class="headerlink" title="10.2.3 GAE的实现"></a>10.2.3 GAE的实现</h4><p>在 Baselines 项目的 trpo_mpi 目录中，我们可以看到 GAE<br>算法的应用，算法的实现在trpo_mpi.py文件的add_vtarg_and_adv方法上。通过与环境交互，我们得到了每一时刻模型对状态 s的价值估计值<br>_v_ （s），这样就可以计算出每一时刻的TD Error：δt =rt + _γv_ （st+1 ） _-v_ （st<br>），然后应用GAE的公式求解出优势函数的估计值。为了快速求出序列中所有时刻的估计值，代码采用了倒序的计算方式，通过 _t_ +1时刻的优势估计值辅助计算<br>_t_ 时刻的估计值。我们可以通过公式展开得到</p>
<p>这样就完成了基于GAE的目标价值计算，虽然整个过程有些复杂，但是我们获得了更大的灵活度，我们可以根据具体问题找到更适合的价值计算方法。</p>
<h3 id="10-3-PPO"><a href="#10-3-PPO" class="headerlink" title="10.3 PPO"></a>10.3 PPO</h3><h4 id="10-3-1-PPO介绍"><a href="#10-3-1-PPO介绍" class="headerlink" title="10.3.1 PPO介绍"></a>10.3.1 PPO介绍</h4><p>PPO是一个基于TRPO算法的改进算法，前面我们已经知道，TRPO算法的问题定义如下所示：</p>
<p>TRPO算法使用共轭梯度法进行求解更新，这样可以减少Fisher信息矩阵的计算量，但是实际上它的计算量仍然比较大，算法的速度和其他基于策略梯度的算法相比仍有一定的差距。同时，共轭梯度法和梯度下降法相比略显复杂，在实现上也更困难，因此论文的作者希望将目标函数进行一定的变换，并使用常见的梯度下降法进行求解，这样可以在实现难度和求解效率上得到提升。</p>
<p>由于实际中我们希望采用蒙特卡罗法完成对期望的近似，因此目标TRPO的函数也可以变为</p>
<p>此时我们用一个变量 _r_ t （ _θ_ ）表示公式中的，它代表了两个策略概率的比率。这样上面的目标函数就变为</p>
<p>这样的变换有什么意义呢？从TRPO的约束条件中可以看出，算法对模型每一轮的优化都划定了置信区域，为了保证优化的稳定性，每一步优化都不能超过这个范围，而范围的限定就是由约束条件中的KL散度给出的。直观地看，约束条件要求新的策略概率分布不能与旧的概率分布差距太大，于是我们可以认为，TRPO的约束条件和要求<br>_r_ t （ _θ_ ）的值靠近1是相似的，于是作者提出了另一种目标函数：</p>
<p>可以看出目标函数为了确保算法能达到TRPO中置信区域的效果，为算法上了两把“锁”：第一把锁是对新旧策略概率比率 _r_ t （ _θ_<br>）的限制，这个比率只能限制在[1 _-∈，_ 1+ _∈_<br>]，这确保了每一次的更新不会有太大的波动；第二把锁就是min函数，它将在两个结果中选择一个较低值作为结果。如果我们能够将较低值优化到令人满意的程度，那么对于其他的情况，模型一定会表现得更好。</p>
<p>除了上面的目标，作者又给出了另一种目标函数。在这个目标函数中，公式将采用另一个变量控制约束项和目标项之间的权重关系，这个方法被称为Adaptive KL<br>Penalty方法，其中新加入的参数 _β_ 被称为Adaptive KL Penalty Coefficient。新的目标函数就变为</p>
<p>那么 _β_ 该如何确定呢？我们可以使用两个分布的KL散度值进行调节。这里设定一个目标值 _d_ targ<br>为计算训练数据平均KL散度的阈值，我们可以计算出KL散度的平均值：</p>
<p>接下来就要根据 _d_ 和 _d_ targ 的关系调整 _β_ ：</p>
<p>· 如果 _d＜d_ targ _，β_ ← _β_ /2，相当于放松KL约束限制。</p>
<p>· 如果 _d＞d_ targ _，β_ ← _β_ ×2，相当于增强KL约束限制。</p>
<p>更新后的 _β_ 值将在下一轮优化中发挥它的作用。</p>
<p>这就是论文作者提出的两种目标函数。经过试验对比，作者最终选择了作为TRPO目标函数的替代。在实际实现过程中，PPO方法还可以结合其他的方法对目标公式中的优势函数做近似，例如在9.2节介绍的A3C算法中实用的<br>_N_ 步回报价值估计法：</p>
<p>以及10.2节介绍的GAE方法：</p>
<p>由于这些方法已经在前面介绍过，这里不再赘述。除了关于策略的目标函数，PPO方法还将值函数的目标和策略模型的熵加入目标，于是完整的目标函数变为</p>
<p>其中</p>
<p>由于 PPO 算法采用常见的梯度下降法求解，因此求解过程比较简单，这里不再赘述。</p>
<h4 id="10-3-2-PPO算法实践"><a href="#10-3-2-PPO算法实践" class="headerlink" title="10.3.2 PPO算法实践"></a>10.3.2 PPO算法实践</h4><p>Baselines项目包含了两个PPO算法的项目：baselines/ppo1和baselines/ppo2，接下来介绍ppo1中的内容。这里代码的结构和TRPO中的代码结构类似，其中的cnn_policy.py和mlp_policy.py中包含了两种策略模型。模型的结构和A2C模型的结构类似：</p>
<p>· 输入为状态s，策略模型和价值模型共享部分模型。</p>
<p>· 策略部分的输出为行动的概率分布参数 _θ_ ，我们可以根据分布 _π_ （a|s； _θ_ ）采样得到行动。</p>
<p>· 价值部分的输出为当前状态的价值 _v_ （s）。</p>
<p>核心的算法实现在pposgd_simple.py中，算法同样使用MPI进行分布式训练，在构建计算图的部分，代码构建了类似于TRPO的目标函数，构建过程如图10-5所示。</p>
<p>图10-5 PPO的计算图</p>
<p>实际上PPO的计算图和TRPO的类似，在10.1节中已经详细分析过，这里不再赘述，项目的其他代码请读者自行阅读。</p>
<h3 id="10-4-总结"><a href="#10-4-总结" class="headerlink" title="10.4 总结"></a>10.4 总结</h3><p>本章介绍了使策略单调提升的优化方法，下面让我们来总结一下。</p>
<p>（1）为了确保策略能够单调提升，我们需要找到比较策略间差异的公式。</p>
<p>（2）TRPO算法构建了类似自然梯度法的目标公式，并使用共轭梯度法减少Fisher信息矩阵的计算量。</p>
<p>（3）GAE算法通过融合多种TD-Error使目标价值的计算可以在偏差和方差之间做到更好的平衡。</p>
<p>（4）PPO算法通过化简目标函数并使用梯度下降法加快了学习速度。</p>
<h3 id="10-5-参考资料"><a href="#10-5-参考资料" class="headerlink" title="10.5 参考资料"></a>10.5 参考资料</h3><p>[1] Schulman J,Levine S,Moritz P,et al.Trust Region Policy<br>Optimization[C]//ICML.2015.</p>
<p>[2] Schulman J,Moritz P,Levine S,et al.High-Dimensional Continuous Control<br>Using Generalized Advantage Estimation[J].Computer Science,2015.</p>
<p>[3] Schulman J,Wolski F,Dhariwal P,et al.Proximal Policy Optimization<br>Algorithms[J].2017.</p>
<h2 id="11-Off-Policy策略梯度法"><a href="#11-Off-Policy策略梯度法" class="headerlink" title="11 Off-Policy策略梯度法"></a>11 Off-Policy策略梯度法</h2><p>我们已经介绍了很多有关策略梯度的算法，这些算法也取得了不错的成绩，但是它们仍然要面对策略梯度的两个软肋。这两个软肋已经在前面提到，笔者总结如下。</p>
<p>· 高方差：由于每一次交互我们都能得到一条轨迹，基于这条轨迹的信息进行策略更新时会造成回报估计的波动，影响最终效果。为了减少方差对算法的影响，Actor<br>Critic方法增加了一个模型，用于估计当前状态的价值，通过引入一定的偏差换取方差的降低。</p>
<p>· 样本使用率：这个问题是所有On-Policy算法都要解决的。前面提到策略梯度针对当前策略的价值希望进行优化，所以需要使用On-<br>Policy的方法进行优化，这带来了两个问题。首先，On-<br>Policy方法并不是一个节约样本的方法，每一次策略发生改变，都要丢弃前面产生的样本，这将带来很大的样本浪费。对于通过计算机模拟进行的任务来说，这点浪费似乎不算什么，但是对于物理环境中的任务来说，机器人与真实环境的交互是十分耗时的，每次丢掉大量样本并不是一个好主意，所以我们需要考虑用Off-<br>Policy的算法进行学习。</p>
<p>第10章介绍了用TRPO和PPO方法解决第一个问题，减少算法的波动，提高了算法的稳定性，甚至为策略模型提供了单调上升的性质；而本章我们要解决第二个问题，即使用Off-<br>Policy的方法。第7章和第8章介绍了以DQN为核心思想的基于价值模型的Off-Policy算法，以及包括Replay<br>Buffer在内的一系列结构，本章还会见到这些熟悉的身影。</p>
<h3 id="11-1-Retrace"><a href="#11-1-Retrace" class="headerlink" title="11.1 Retrace"></a>11.1 Retrace</h3><p>从本节开始，我们要使用由其他策略交互得到的样本对当前的模型进行训练，由于样本并非来自同一个样本，两者的概率分布也并不相同，因此我们无法直接使用这些样本。2.4节介绍了“使用概率分布<br>_A_ 为概率分布 _B_ 采样”的方法——重要性采样，我们可以使用这样的方法解决概率分布不匹配的问题。本节，我们先来介绍如何估计Off-<br>Policy的轨迹价值，并介绍Retrace算法。</p>
<h4 id="11-1-1-Retrace的基本概念"><a href="#11-1-1-Retrace的基本概念" class="headerlink" title="11.1.1 Retrace的基本概念"></a>11.1.1 Retrace的基本概念</h4><p>基于Off-Policy的价值估计方法主要使用重要性采样的方法实现，我们可以用一个R表示这一类计算方法的基本形式：</p>
<p>R _Q_ π (x _,_ a)= _Q_ π (x _,_ a)+ _E_ µ _c_ s )(rt + _γQ_ π (xt+1 _,_ ·)<br>_-Q_ µ (xt _,_ at ))]</p>
<p>其中 _Q_ （x _，_ a）表示值函数估计值， _µ_ 表示参与交互的策略， _π_ 表示待学习的策略， _γ_ 表示回报的打折率：</p>
<p>表示新旧策略的概率比率。</p>
<p>我们先考虑 _µ_ 和 _π_ 完全相同的情况，此时公式中的等于1。当 _t_ =0时，上面的公式就变成了Actor Critic中TD-<br>Error的计算公式：</p>
<p>如果时间长度进一步拉长，我们可以得到</p>
<p>此时的公式形式和10.2节中GAE的计算公式比较接近。</p>
<p>当两个策略变得不同时，这个项目就显得比较重要了。将上面的公式展开就可以得到</p>
<p>重要性采样算法本身是无偏的，但是当两个分布相差比较大时，重要性采样算法的效果就会很差，而且在数值上可能出现一些问题。当 _µ_ （as |xs ）非常小而<br>_π_ （as |xs<br>）非常大时，这个比值就会变得非常大，从而使模型产生较大的波动。所以直接使用重要性采样进行计算不能保证算法的稳定性，我们需要对这个比率做更多的限制。</p>
<p>为了解决这个问题，Retrace（ _λ_ ）算法（在本节中假设 _λ_ =1）采用了对比率进行限定的方式，使算法更新变得更稳定：</p>
<p>通过这样的设定，比率既存在一个上限，不至于因为比率差距过大而对模型产生过大的波动，又可以保持重要性采样的基本形式，使计算不会有太大的偏差。我们可以证明得到Retrace算法同样具有良好的收敛性质，并使用它进行价值更新。更多的证明过程请读者阅读论文<br>_Safe and efficient off-policy reinforcement learning_ [1] 了解更多的细节。</p>
<h4 id="11-1-2-Retrace的算法实现"><a href="#11-1-2-Retrace的算法实现" class="headerlink" title="11.1.2 Retrace的算法实现"></a>11.1.2 Retrace的算法实现</h4><p>Retrace算法该如何实现呢？在Baselines项目的baselines/acer/acer_simple.py文件中可以看到它的实现。这个子项目实现了ACER算法的全过程，其中包含了Retrace的实现。在Retrace的代码中，我们可以看到如下参数介绍：</p>
<p>代码中已经假设Retrace算法中的 _λ_ 值为1，代码中Retrace（ _λ_<br>=1）算法和论文中提到的公式有一些不同，我们需要重新做推导来说明实现过程采用的公式。首先，我们使用Expected SARSA的价值更新公式：</p>
<p>SARSA是一个On-Policy的算法，我们可以通过重要性采样的方法将其变成一个Off-Policy的方法：</p>
<p>用 _ρ_ t+1 代替，并将原本的值函数包含在公式中，可以得到</p>
<p>在计算值函数时我们就可以使用这个公式进行更新。下面就来考虑具体实现中的一些问题。现实中，我们需要对模型实际的轨迹长度进行限制，即使真实的轨迹长度超过了我们的限定，我们也会将其提前终止，所以对于每一个状态，我们必须使用额外的变量<br>_d_ t 表示时刻 _t_ 的状态是否为终止状态。其中 _d_ t =1表示该轨迹终止， _d_ t<br>=0表示该轨迹仍在继续，那么我们就可以把公式拆解成两个部分：</p>
<p>实际上第二部分的计算是为第一部分服务的，最终要保留的是第一部分的结果。了解了这个原理，就可以把这两部分应用到下面的代码中。以下是对代码的详细介绍：</p>
<p>这样我们就完成了Retrace的计算过程。</p>
<h3 id="11-2-ACER"><a href="#11-2-ACER" class="headerlink" title="11.2 ACER"></a>11.2 ACER</h3><p>本节我们介绍的方法叫作Actor Critic with Experience Replay（简称ACER），来自论文 _Sample Efficient<br>Actor-Critic with Experience Replay_ [3] ，这是一种Off-Policy的Actor-Critic算法。由于Off-<br>Policy在一些约定上与第9章及更前面的约定不同，同时也为了让读者对前面的内容做一个回顾，本节我们将重新对问题进行定义。</p>
<h4 id="11-2-1-Off-Policy-Actor-Critic"><a href="#11-2-1-Off-Policy-Actor-Critic" class="headerlink" title="11.2.1 Off-Policy Actor-Critic"></a>11.2.1 Off-Policy Actor-Critic</h4><p>本节我们依然沿用第6章介绍的MDP框架。对于一个强化学习任务，我们用S表示状态集合，A表示行动集合，那么状态转移概率分布可以表示为 _P_<br>：|S|×|S|×|A|→[0 _，_ 1]，在公式中我们用 _p_ （s′ |s _，_ a）表示由状态s、行动a转移到状态s′<br>的概率，用r表示Agent与环境交互过程中获得的反馈与回报。 _γ_ 表示长期回报中未来回报对当前的打折率。状态价值可以用下面的公式表示：</p>
<p>这里我们假设任务从状态st 出发，经过有限的时间长度 _T_ ，最终到达任务的终点。根据状态价值，我们还可以得出状态行动价值公式：</p>
<p>Off-Policy的策略梯度法涉及两个不同的策略。我们定义 _µ_ 为交互的序列样本{s0 _，_ a0 _，_ r0 _，µ_ 0 _，_ · ·<br>·}使用的策略，这些样本在交互后保存下来，并在未来的时刻应用于对某个策略 _π_ 的训练中。</p>
<p>我们再定义状态出现的频率 _d_ µ （s）=limt→∞ _p_ （st =s|s0 _，µ_ ），它表示了在遵循策略 _µ_ 时，从任务的起始状态s0<br>出发，任意时刻到达状态s的概率之和。这样我们就可以定义目标：最大化从交互样本的每一状态出发的长期回报，令 _θ_ 为策略 _π_<br>的参数，那么目标公式可以写作</p>
<p>如果我们的问题是一个On-Policy的问题，那么目标公式中的 _d_ µ （s）将被换作 _d_ π<br>（s），如9.1节所示，我们已经了解过它的求解算法。这里使用与9.1节类似的求解方法，对目标函数直接求导，可以得到</p>
<p>对公式来说，∇θ _Q_ π （s _，_ a）是一个比较难求解的项目，尤其是在它和 _d_ µ （s）混合表示的情况下。因此，对Off-<br>Policy的方法来说，我们可以忽略这一项，于是公式就变为</p>
<p>实际上即使忽略掉那一部分的公式，我们依然可以在一定的条件下确保策略的提升。我们定义参数的更新公式为</p>
<p>当 _α_ 被限定为一个比较小的值时，例如存在一个 _∈＞_ 0，使得对所有的 _α ＜∈_ ，依然可以得到</p>
<p>这个结论要如何证明呢？我们知道现在的近似梯度 _g_ （ _θ_ ）实际上求解了策略 _π_ 的梯度。也就是说，在 _θ_ 附近的区域内，我们可以沿着<br>_g_ （ _θ_ ）的方向使策略得到提高。于是我们可以通过设置 _α_ ，满足 _π_ （a|s； _θ_ ′ ）≥ _π_ （a|s； _θ_<br>），于是就可以得到</p>
<p>进一步替换，还可以得到</p>
<p>这样不断地展开，就可以得到</p>
<p>基于这样的证明，我们发现这个简化的梯度同样可以对目标函数进行优化，因此我们可以选择这个方法。将梯度公式进一步变换，可以得到更易于计算的形式，用 _β_ 表示<br>_d_ µ ，可以得到</p>
<p>其中，表示了两个策略的比值。现在公式中的状态和行动完全遵循策略 _µ_ ，我们就可以采用蒙特卡罗法进行采样，并使用上面的公式进行梯度计算。</p>
<p>看上去核心工作已经完成，而实际上同11.1节类似，两个策略的比值可能对训练造成不好的影响。如果两个策略的概率比值差异很大，那么梯度更新的步长也会很大，模型就会产生较大的波动，接下来介绍的ACER算法就是要解决这方面的问题。</p>
<h4 id="11-2-2-ACER算法"><a href="#11-2-2-ACER算法" class="headerlink" title="11.2.2 ACER算法"></a>11.2.2 ACER算法</h4><p>在11.1节，我们介绍了Retrace算法，它通过对新旧策略比值进行限定减少重要性采样带来的不稳定性。ACER算法也要使用类似的方式，首先，我们将公式分解成On-<br>Policy的部分和Off-Policy的部分。公式计算如下所示：</p>
<p>如果我们用一个经过限定的比率 _ρ_ ～t 替换其中一部分的 _ρ_ t ，同时限定 _ρ_ ～t =max（ _ρ_ t _，c_ ），就有</p>
<p>我们就可以将上面的公式替换为ACER算法的公式：</p>
<p>这个公式中的两个项目对算法起到不同的作用。第一项被称为截断的重要性采样（Truncated Importance<br>Sampling），和Retrace算法的思想类似，它限制了重要性采样的比率上限，我们可以确保模型不会造成太大的波动。第二项被称为偏差纠正项（Bias<br>Correction for the<br>Truncation），作为前面一项的纠正项，它可以保证算法是无偏的，这样算法就在偏差和方差之间得到了一定的平衡。实际上，只有 _ρ_ t _＞ c_<br>时，第二项才会发挥作用，而此时第一项的比率被限定为 _c_ ，这时第二项会对第一项的限制做一定的补充，从而保证算法没有较大的偏差。</p>
<p>完成了这一步的变换，下面就要对值函数做一定的替换。对上面公式的第一项，由于是Off-Policy的期望计算，可以用11.1节介绍的Retrace（ _λ_<br>）算法计算价值估计值 _Q_ ret （st _，_ at ），而第二项由于是On-Policy的价值计算，因此直接使用价值模型的估计值即可，模型的参数为<br>_θ_ v 。这样公式就变成了下面的形式：</p>
<p>最后一步是Actor-Critic算法中常见的步骤，为算法添加一个Baselines，以降低目标函数的方差，我们可以用价值模型估计当前的状态值函数 _V_<br>（s），最终的模型变为</p>
<p>完成了上面对目标函数的定义，下面就来介绍求解的具体过程。10.1节已经介绍了TRPO方法，它能够比较好地解决Actor-<br>Critic算法中模型效果波动较大的问题，于是我们也采用同样思想的方法进行求解。但是由于两个算法要解决的问题不同，同时希望加快模型训练的速度，ACER算法采取了两个改变：采用更保守的参数更新方式和相对简单的目标函数。</p>
<p>第一个改变是参数更新方式。ACER维护了一个滑动平均的策略网络，每一次模型更新时，新的参数仅会以一个很小比例进行更新。这样我们就可以确保每一次更新后的参数和之前的参数保持较近的距离。这样算法可以以较小的代价实现类似TRPO约束的效果。我们令平均策略的参数为<br>_θ_ a ，优化后的策略参数为 _θ_ ，那么平均策略的更新公式为</p>
<p>第二个改变是目标函数。ACER目标函数中计算一阶梯度的部分与TRPO比较相似，只要将前面得到的公式求导就可以得到</p>
<p>ACER的约束条件与TRPO不同，没有使用二阶导数做约束，而是使用了KL散度的一阶导数，这样目标函数可以变为下面的形式：</p>
<p>可以看出，这个目标函数虽然也拥有基于KL散度的约束条件，但是它与TRPO的形式完全不同。经过这样的变换，我们不再需要计算复杂的Fisher信息矩阵。最终的目标是求解的参数更新量z，我们希望它在满足约束条件的同时尽可能地靠近值。上面提到的对目标函数的两个改变实际上为更新量的求解提供了很大的便利。为了后面求解公式时的便利，我们做如下的变量替换：</p>
<p>这个目标函数实际上是一个二次规划的问题。由于问题中目标函数的二阶导为半正定矩阵（在这个问题中，二阶导实际上是一个对角阵），我们可以证明这个问题满足KKT条件，它的对偶问题具有强对偶的特性，原问题和它的对偶问题的解相同。于是我们就可以构造这个问题的拉格朗日乘子法形式：</p>
<p>首先对z进行求导，可以得到</p>
<p>令导数为0，可以得到z的最优值</p>
<p>将其带回上面的拉格朗日公式中，可以得到</p>
<p>对其进行求导，可以得到</p>
<p>令导数为0，可以得到</p>
<p>因为朗格朗日的乘子被限定为不小于0，所以 _λ_ ∗ 不小于0，将其带回前面z∗ 表示的公式，可以得到</p>
<p>从结果可以看出，当约束条件可以得到满足时，最终的更新值就是梯度g；如果约束无法满足，那么z就会沿着k的方向进行一定的缩减，使更新量满足约束，这样就得到了我们想要的更新量。</p>
<h4 id="11-2-3-ACER的实现"><a href="#11-2-3-ACER的实现" class="headerlink" title="11.2.3 ACER的实现"></a>11.2.3 ACER的实现</h4><p>ACER的完整算法流程如下所示，流程主要包含两个子过程，一个是采用On-Policy的优化，也就是基于当前的策略与环境交互采集样本；另一个是采用Off-<br>Policy的优化，直接使用保存起来的样本进行训练。这个训练流程和DQN的训练方法十分类似，ACER和DQN都需要定期使用新的策略采集样本，以保证Replay<br>Buffer中采样样本的策略和当前的策略差距不大。</p>
<p>由于在ACER中需要计算两个策略的概率比率，因此在Replay Buffer中需要将采样策略的行动概率值保存，因此Replay<br>Buffer中就需要额外保存这个信息。</p>
<p>算法 ACER算法的主体</p>
<p>Repeat</p>
<p>Call ACER (on-policy=True)</p>
<p>for _i_ ∈{1 _,_ · · · _,n_ }do</p>
<p>Call ACER (on-policy=False)</p>
<p>end for</p>
<p>until最大的迭代轮数满足</p>
<p>子算法 离散行动的ACER （on-policy）</p>
<p>重置参数梯度：d _θ_ ←0，d _θ_ v ←0</p>
<p>初始化参数：If on-policy==True：</p>
<p>利用policy和环境交互得到，并将其放入Replay Buffer，其中：</p>
<p>_µ_ (·|si )← _π_ (a|si ; _θ_ ′ )</p>
<p>else:</p>
<p>从Replay Buffer中采样得到</p>
<p>end if</p>
<p>for _i_ ∈{0 _,_ · · · _,k_ }do</p>
<p>end for</p>
<p>计算Trust Region目标需要的部分公式：</p>
<p>计算策略参数 _θ_ ′ 的更新量：</p>
<p>计算价值参数的更新量：</p>
<p>更新Retrace目标：</p>
<p>end for</p>
<p>将d _θ_ ′ 和更新到原始的参数 _θ_ 和 _θ_ v 上</p>
<p>更新平均策略参数： _θ_ a ← _αθ_ a +（1 _-α_ ） _θ_</p>
<p>介绍完ACER的算法流程，再介绍它在Baselines中的实现。这部分代码在baseli-<br>nes/acer项目中，核心代码在acer_simple.py中。由于涉及的内容比较多，模型架构相对复杂，如图11-1和图11-2所示。</p>
<p>图11-1 ACER的流程图1</p>
<p>图11-2 ACER的流程图2</p>
<p>图11-1所示为 _Q_ ret 的计算过程。算法的模型部分和A2C的模型有些不同，它要输出 _π_ （a|s）和 _q_ （s _，_<br>a）两个值，在图中分别用pi和 _q_<br>表示。这两个值进行乘加操作，可以得到价值v。代码中有一个strip操作，用于将数据转换成算法中易于处理的形式，pi变形成 _f_ ，并与Replay<br>Buffer中存储的代表 _µ_ （a|s）的mu相除，得到策略的比值rho。</p>
<p>当我们知道了回报 R，终止状态 D，价值 q、v，策略比值rho后，就可以利用Retrace方法计算出 _Q_ ret 值qret。</p>
<p>图11-2所示为ACER计算公式中的两部分。首先是截断的重要性采样部分，它由 _Q_ ret<br>和价值的估计值的差、策略的对数似然和比率阶段值三部分组成，公式为</p>
<p>其中后两项的计算结果分别为logf和adv，由于这三项中只有第二项参与梯度计算，因此第一、第三项需要被tf.stop_gradient包含，以关闭它们的反向计算。</p>
<p>ACER公式的第二部分，也就是偏差纠正部分，同样是由比率值、对数似然和公式及价值差组成，对应的公式为</p>
<p>其中后两项的计算结果分别为logf_bc和adv_bc，这个公式同样只有第二项参与梯度计算，因此处理方式与前面一致。这样得到了两部分的loss值：loss_f与loss_bc，将两部分加起来就可以得到loss_policy。</p>
<p>上面的目标值和新策略的熵加起来，就得到了完整的目标函数的形式，对其进行求导，就可以得到导数值 _g_ 。</p>
<p>接下来则是KL散度的梯度，对应的公式计算可以转换为</p>
<p>所以我们可以用两个策略比值的相反数表示KL散度梯度。在完成了对策略梯度的计算后，还需要计算值函数的梯度，而值函数使用了比较简单的目标函数，计算比较简单，具体过程请读者自行阅读。</p>
<p>可以看出，只要能够理解Retrace算法和ACER中形似TRPO目标的目标函数，整体的算法流程是比较容易理解的。前面我们介绍的只是离散行动的解法，关于连续行动空间的解法，欢迎读者阅读论文进行了解，其主体思想与离散方法基本一致，只是在一些细节上有所不同。</p>
<h3 id="11-3-DPG"><a href="#11-3-DPG" class="headerlink" title="11.3 DPG"></a>11.3 DPG</h3><p>11.2节从Experience Replay的角度实现了Actor-Critic方法，本节要介绍另外一个Off-<br>Policy的算法，这个方法被称为Deterministic Policy<br>Gradient（简称DPG），这个算法经过几篇论文的不断打磨改进，效果变得越来越好。这个算法的名称看上去和Policy<br>Gradient相似，但其实际计算过程并不相同，从名字中可以看出，这个方法使用了确定的策略，这不同于之前提到的输出分布的策略形式。下面我们就来看看算法的计算过程。</p>
<h4 id="11-3-1-连续空间的策略优化"><a href="#11-3-1-连续空间的策略优化" class="headerlink" title="11.3.1 连续空间的策略优化"></a>11.3.1 连续空间的策略优化</h4><p>在前面的章节中，我们主要介绍了离散行动空间的任务，例如Atari游戏，由于游戏手柄的操作形式有限，我们可以使用探索算法尽可能地将状态行动枚举出来。对于行动连续的任务，想要枚举所有的行动变得更困难，而如何将所有可行的行动逐一尝试出来也变得不那么可能。我们先回顾前面介绍过的两种强化学习算法的计算公式。首先是DQN算法的更新公式：</p>
<p>DQN算法遵循了泛化策略梯度的思想，先完成策略评估的工作，再进行策略改进。这个过程需要计算出下一时刻状态下所有行动的价值，并从中选出最优的行动价值。如果行动数量是有限的，那么这个公式比较好计算；如果行动空间是连续的，我们该如何找出价值最优的行动呢？这就成为了一个问题。</p>
<p>接下来是策略梯度法，它的核心计算公式为</p>
<p>策略梯度法由于直接对轨迹的价值期望求导，因此它不需要完成最优行动选取的操作。因此连续型行动空间的问题可以使用策略梯度算法求解，但是策略梯度法是一个On-<br>Policy的方法，它非常依赖与环境交互的过程，而DQN方法直接对值函数进行优化，可以使用Off-<br>Policy的方法进行训练。所以，如果我们想在连续行动空间使用Off-<br>Policy算法进行优化，可以考虑结合两种算法的特点，创造出一种全新的算法。不同于11.2节使用重要性采样进行求解，本节介绍的DPG算法将采用另外一种方式。</p>
<p>在与环境交互时，DQN算法一般使用 _∈_<br>-greedy的策略，策略梯度是从一个概率分布中采样得到的，而DPG的交互方式结合了前面两种算法。从形式上看，DPG使用了 _∈_<br>-greedy的策略，以一定的概率使用随机策略，而在剩下的情况下使用最优行动；从策略产生的行动上看，DPG将先得到一个确定的行动，这个行动由确定的策略得到，不需要从概率分布中采样，相当于当前状态下的最优行动。如果决定使用随机策略，那么就在求出的确定行动基础上加上一定的噪声，反之则没有噪声。</p>
<p>虽然确定策略的思想和DQN相近，但实际上，DPG也可以看作是策略梯度法的一种特殊情况。我们知道随机策略梯度的输出是行动分布形式，对于离散行动空间，模型输出的是一个Category的分布，也就是每一个取值的概率；而对于连续行动空间，我们一般会输出一个高斯分布，其中一部分值表示分布的均值，另一部分值表示分布的方差，然后我们可以使用这些分布的参数采样出行动值。DPG的输出也可以想象成一个连续的分布，只不过这个分布的方差为0。这样我们就把DPG和策略梯度法统一起来了，前面介绍的策略梯度法也可以称为随机策略梯度法。</p>
<p>下面我们就利用这个思路建立目标。DPG的算法本身还是采用策略梯度法的方法，直接对轨迹价值求导。但是由于策略产生的行动是确定的，于是行动就可以直接被替换为策略函数，公式将变为下面的形式：</p>
<p>其中 _µ_ 表示生成确定行动的策略函数。将上面的公式进一步推导，利用链式法则可以得到</p>
<p>同随机策略梯度法对比，这个梯度公式中没有了与行动相关的期望求解项。下面我们就围绕这个公式进行计算。</p>
<h4 id="11-3-2-策略模型参数的一致性"><a href="#11-3-2-策略模型参数的一致性" class="headerlink" title="11.3.2 策略模型参数的一致性"></a>11.3.2 策略模型参数的一致性</h4><p>在进一步介绍确定策略的目标推导前，我们需要简单介绍一个概念：策略模型参数的一致性。在随机策略梯度方法的介绍过程中，我们曾提到直接使用一个函数来近似状态行动价值，即定义一个函数<br>_f_ w ∶ S×A → R，用来近似真实的值函数 _Q_ π ，函数的参数为 _w_ 。如果我们用L2损失作为近似函数 _f_ w 和真实价值 _Q_<br>π 的损失函数，那么近似函数的梯度为</p>
<p>其中^ _Q_ π （s _，_ a）是状态行动的目标价值。如果函数 _f_ w 最终收敛，即下面的公式成立：</p>
<p>那么，在论文 _Policy Gradient Methods for Reinforcement Learning with Function Ap-<br>proximation_ [6] 中，作者证明了，当 _f_ w 满足上面的公式，同时满足和策略函数参数的某种一致性，即 _f_ w<br>关于参数的梯度等于策略的对数似然关于参数的梯度：</p>
<p>那么，在计算梯度时我们就可以用 _f_ w 代替 _Q_ π ，并得到相同的结果。具体的计算过程为</p>
<p>上面定理实现的关键是参数的一致性，这一点容易做到吗？由于策略模型的计算结果是对数似然概率，所以这样的形式很容易被满足。我们令策略模型是一个最简单的线性模型，当模型经过Softmax层计算后，可以得到</p>
<p>对其取对数求导，可以得到</p>
<p>上式的第二项是一个期望项，在实际中它是一个常量，我们可以将其忽略，只要将输入数据进行归一化即可抵消掉这一项。因此为了满足参数的一致性，我们可以将价值估计函数的形式设定为</p>
<p>实际上，神经网络同样可以以这样的形式表示出来。因此，采用神经网络同样可以达到这样的一致性效果。读者看到这里一定充满疑惑，这个证明看上去有些无聊，那么它的意义何在呢？对于DPG，作者在论文中给出了类似的证明与结论。</p>
<p>给定一个值函数 _Q_ w （s _，_ a）与确定的策略函数 _µ_ θ （s），当下面两个条件满足时：</p>
<p>(1)∇a _Q_ w (s _,_ a)|a=µθ(s) =∇θ _µ_ θ (s)T _w_</p>
<p>（2）参数 _w_ 是近似值函数的局部最优值，它使得下面的目标函数取得最小值：</p>
<p>其中</p>
<p>我们可以得到下面的定理，也就是说可以用值函数近似真实的价值并得到正确的策略梯度：</p>
<p>我们可以简单证明一下，根据前面的一致性定义，可以得到</p>
<p>继续展开前面的公式可以得到当 _w_ 等于局部最优时，目标函数的导数为0：</p>
<p>接着展开可以得到</p>
<p>于是得证。实际上通过前面大段的证明，我们可以得出一个结论：对于目标函数中的价值估计部分，我们可以使用一个值函数模型进行拟合，这样价值模型不需要遵从某个策略。这样我们就可以使用Off-<br>Policy的方法进行计算了。</p>
<h4 id="11-3-3-DDPG算法"><a href="#11-3-3-DDPG算法" class="headerlink" title="11.3.3 DDPG算法"></a>11.3.3 DDPG算法</h4><p>接下来介绍DPG的实现。实际上，DPG的思想仅仅是使用确定的策略输出，算法并不包含其他的限定，例如是否为On-Policy或者Off-<br>Policy。对于On-Policy的Deter-ministic Actor-Critc算法，值函数为 _Q_ w （s _，_ a），确定策略为 _µ_<br>θ （s），我们可以建立如下目标函数：</p>
<p>对其进行求解，可以得到</p>
<p>对于Off-<br>Policy的算法，我们同样可以建立目标函数。由于我们使用了确定的策略，同时值函数不依赖任何策略，那么在计算时我们就不需要向随机策略那样进行重要性采样计算。假设样本来自策略<br>_β_ ，我们的目标函数为</p>
<p>对其进行求解，可以得到类似的结果：</p>
<p>由于本章的主题是介绍Off-<br>Policy的算法，因此接下来我们只关心这部分的算法。虽然DPG从名字上看属于策略梯度法的一种，但是它的求解过程有很多与DQN相近的地方，正如论文<br>_Continuous control with deep reinforcement learning_ [6]<br>中介绍的，DQN的优化过程中的两个常见的优化方法也被应用到了Deep DPG上。</p>
<p>1.Replay-Buffer</p>
<p>由于连续的序列中存在的相关性会使DQN的优化充满不稳定性，因此DQN采用了Replay-<br>Buffer，将一些采样样本收集起来，每次优化时从中随机取出一部分进行优化，就可以减少一些不稳定性。这个方法同样适用于DPG。</p>
<p>2.Target Network</p>
<p>这也是DQN中常见的方法，通过设置一个不会频繁或大幅更新的模型，使模型计算的值函数在一定程度上减少波动，令计算更稳定。在论文中，作者介绍采用滑动平均的方法更新Target<br>Network：，τ 一般设置为非常接近1的数，这样Target网络的参数 _θ_ 不会发生太大的变化，每次只会受一点训练模型 _θ_ ′<br>的影响。这个更新方法和11.2节的ACER算法十分相似。</p>
<p>除了这两个方法外，作者还对模型的探索做了一些改变。由于DPG采用确定策略，如果它在与环境进行交互时只采用确定的策略，那么必然会导致对环境的探索不够充分，因此需要为策略增加一定的探索性。前面已经提到<br>DPG 可以使用基于 _∈_ -greedy的探索方法，通过为确定的行动增加噪声提高探索性。在DDPG模型中，作者采用Ornstein-<br>Uhlenbeck噪声增加模型的探索能力。Ornstein-Uhlenbeck噪声是一种基于Ornstein-<br>Uhlenbeck过程的随机变量，它可以被用于模拟与时间有关联的噪声数据。它的生成公式为</p>
<p>其中 _x_ 是要生成的数据， _µ_ 是设定的随机变量的期望值， _W_ 是一个由Wiener过程生成的随机变量，一般我们用一个简单的随机函数代替就可以，<br>_θ_ 和 _σ_ 是随机过程中的参数，d _x_<br>是每一时刻数据的变化量，真正的采样值等于上一时刻的采样值加上求出的变化量。从公式中可以看出，每一时刻数据的变化量和当前时刻存在关联，公式右边的第一项将为随机数据提供朝向均值<br>_µ_ 的变化，第二项才是常见的随机变化。</p>
<p>为了更直观地理解这个噪声生成器，我们举一个例子。设定 _θ_ =0 _._ 01， _µ_ =0， _σ_ =0 _._ 02， _W_<br>由均值为0，方差为1的高斯随机函数生成。我们设定三个随机变量，它们的初始值为2、0和-2，对其进行一定时间的采样，得到图11-3所示的结果。</p>
<p>图11-3 Ornstein-Uhlenbeck的采样结果图</p>
<p>从结果可以看出，三个随机变量在波动中靠近了均值0。这就可以确保噪声的增加不会对策略产生太大的影响，一旦噪声在某个维度造成了影响，一定会尽快给予补偿。</p>
<p>除了上面提到的一些改进方法，我们还可以从前面章节的内容中找到更多改进模型的灵感。第8章介绍了许多改进DQN的方法，这些方法都可以以某种形式应用到Deep<br>DPG中。例如，Priority Replay Buffer、Pre-train from Demonstration等。读者可以阅读论文<br>_Leνeraging Demonstrations for Deep Reinforcement Learning on Robotics<br>Problems with Sparse Rewards_ ，进一步了解关于这部分的改进过程。</p>
<p>完整的DDPG算法如下所示。</p>
<p>算法 DDPG算法</p>
<p>初始化critic网络 _Q_ （s _，_ a| _θ_ Q ），actor网络 _µ_ （s| _θ_ µ ）</p>
<p>用上面的两个网络初始化对应的目标网络 _Q_ ′ ← _Q_ ， _µ_ ′ ← _µ_</p>
<p>初始化Replay Buffer：R</p>
<p>for episode=1 _,M_ do</p>
<p>初始化噪声分布 _N_</p>
<p>s1 =env.reset()</p>
<p>for _t_ =1, _T_ do</p>
<p>at = _µ_ (st | _θ_ µ )+ _N_ t</p>
<p>st+1 _,_ reward_t _,_ terminate _,_ _=env.step(at )</p>
<p>_R.save_ ((st _,_ at _,_ rt _,_ st+1 ))</p>
<p>//训练</p>
<p>(si _,_ ai _,_ ri _,_ si+1 )= _R._ sample( _N_ )</p>
<p>′ _y_ i =r i + _γQ_ ′ (s i+1 , _µ_ ′ (s i+1 | _θ_ µ )| _θ_ Q′ )</p>
<p>根据critic loss更新critic网络：</p>
<p>根据actor的梯度更新actor网络：</p>
<p>更新目标网络：</p>
<p>_θ_ Q′ ←τ _θ_ Q +(1-τ) _θ_ Q′</p>
<p>_θ_ µ′ ←τ _θ_ µ +(1-τ) _θ_ µ′</p>
<h4 id="11-3-4-DDPG的实现"><a href="#11-3-4-DDPG的实现" class="headerlink" title="11.3.4 DDPG的实现"></a>11.3.4 DDPG的实现</h4><p>在Baselines项目中的baselines/ddpg文件夹中，可以看到DDPG的实现。文件夹中各文件的含义如下所示。</p>
<p>· ddpg.py：DDPG模型的实现。</p>
<p>· main.py：启动的主程序。</p>
<p>· memory.py：Replay Buffer的实现。</p>
<p>· models.py：Actor和Critic的实现。</p>
<p>· noise.py：随机噪声的实现。</p>
<p>· training.py：训练过程的实现。</p>
<p>我们可以从一些独立的部分入手进行分析，首先是noise.py，其中介绍了几种噪声生成方法。</p>
<p>· NormalActionNoise：从高斯分布 _N_ （ _µ，σ_ ）中采样噪声。</p>
<p>· AdaptiveNoise：自适应方差的噪声，可以根据随机的效果调整采样的方差。</p>
<p>· OrnsteinUhlenbeckNoise：前面提到的噪声实现。</p>
<p>memory.py中包含Replay Buffer的实现，这里实现了最基本的Replay<br>Buffer。在阅读了前面几章的代码后，我们对这部分的代码已经很熟悉了。</p>
<p>在models.py中介绍了两个模型，由于我们解决的问题是连续的状态和行动空间，不像前面介绍的Atari游戏那样以画面为状态输入，因此模型只需要全连接层就可以。两个模型都采用3层全连接层实现，其中Actor模型的输出和行动的维度相同，Critic模型的输出维度为1。</p>
<p>ddpg.py中介绍了训练的主要内容。由于我们采用了回归模型进行模型训练，数值的范围变得不太可控，为了让训练过程变得更稳定、更快捷，需要对输入和输出进行归一化。对于输入的观测值，我们首先经过一个滑动平均的结构记录输入值的均值和方差，然后在输入值进入模型前对其进行归一化，也就是减去它的均值和方差。同样，对于输出值，我们得到的也是被归一化之后的数值，在模型输出后，这个数值再被恢复到原来的数值，这个过程如图11-4所示。</p>
<p>图11-4 输入和输出的归一化与恢复</p>
<p>模型在训练时需要从Replay Buffer中读取数据，我们首先使用Q-Learning计算更新后的价值。我们从数据中找到 _t_<br>+1时刻的状态obs1、回报reward和状态是否为终结状态的标识符terminate，使用Target<br>Network中的target_actor和target_critic两个模型就可以计算得到：</p>
<p>得到它之后，就可以计算价值模型的目标了。我们再使用 _t_ 时刻的状态obs0，就可以得到：</p>
<p>我们的目标就是最小化上面的公式。上面的公式里我们忽略了归一化相关的操作，在代码中我们会看到这部分的内容。而对策略模型，我们的目标为：</p>
<p>只要对这些目标进行求导就可以了，这部分的计算如图11-5和图11-6所示。</p>
<p>图11-5 目标计算图1</p>
<p>图11-6 目标计算图2</p>
<p>代码中使用了MPI进行梯度的通信，这样增强了模型的并发度。以上就是代码的核心部分，对具体内容感兴趣的读者可以阅读其中的细节。</p>
<h3 id="11-4-总结"><a href="#11-4-总结" class="headerlink" title="11.4 总结"></a>11.4 总结</h3><p>本章我们主要介绍了基于Off-Policy的Actor Critic算法，在此我们总结如下。</p>
<p>（1）ACER算法继承了Off-Policy Actor<br>Critic的算法，通过对重要性采样比例的限制与偏差纠正，减少了因重要性采样造成的优化波动，同时使用Retrace方法减少价值估计的波动。</p>
<p>（2）DDPG算法吸收了DQN的思想，采用确定的策略函数，使问题在高维度的连续空间上能够发挥更好的效果。同时，DDPG算法吸收了大量DQN的改进方案，使模型在训练频率和效果上有了保证。</p>
<h3 id="11-5-参考资料"><a href="#11-5-参考资料" class="headerlink" title="11.5 参考资料"></a>11.5 参考资料</h3><p>[1] Munos R,Stepleton T,Harutyunyan A,et al.Safe and Efficient Off-Policy<br>Reinforce-ment Learning[J].2016.</p>
<p>[2] Degris T,White M,Sutton R S.Off-Policy Actor-Critic[J].2012.</p>
<p>[3] Wang Z,Bapst V,Heess N,et al.Sample Efficient Actor-Critic with Experience<br>Re-play[J].2016.</p>
<p>[4] Silver D,Lever G,Heess N,et al.Deterministic policy gradient<br>algorithms[C]//In-ternational Conference on International Conference on<br>Machine Learning.JMLR.org,2014:387-395.</p>
<p>[5] Lillicrap T P,Hunt J J,Pritzel A,et al.Continuous control with deep<br>reinforcement learning[J].Computer Science,2015,8(6):A187.</p>
<p>[6] Večerík M,Hester T,Scholz J,et al.Leveraging Demonstrations for Deep<br>Reinforcement Learning on Robotics Problems with Sparse Rewards[J].2017.</p>
<h1 id="第四部分-其他强化学习算法"><a href="#第四部分-其他强化学习算法" class="headerlink" title="第四部分 其他强化学习算法"></a>第四部分 其他强化学习算法</h1><p>第四部分将介绍其他强化学习算法。由于强化学习本身十分复杂，需要解决的问题很多，前面介绍的算法并不能涵盖所有的内容，仍然有很多优秀的算法和有意义的问题值得我们探索。笔者挑选了两个比较经典的子领域介绍给读者，希望能够扩展读者对强化学习认知的广度。</p>
<p>第四部分共包含两章：</p>
<p>· 第12章将介绍解决回报稀疏问题的算法，包括层次强化学习（HRL）和Hindsight经验回放（HER）。</p>
<p>· 第13章将介绍结合状态转移概率的算法，包括棋类博弈常见的蒙特卡罗决策树（MCTS）和iLQR算法。</p>
<h2 id="12-稀疏回报的求解方法"><a href="#12-稀疏回报的求解方法" class="headerlink" title="12 稀疏回报的求解方法"></a>12 稀疏回报的求解方法</h2><p>在前面的章节中曾经介绍过Atari游戏环境，它的状态观测值是游戏画面，行动是离散且有限的游戏柄操作。在大家的印象中，电子游戏可以给予玩家丰富的反馈，每当玩家取得一点点成果，游戏环境就会给予玩家一定的回报，因此电子游戏环境是非常适合强化学习的。然而，一些游戏的回报反馈十分缓慢，需要玩家执行一系列很长、很复杂的操作，才能获得明确的回报反馈，而在获得回报之前，玩家很难获得任何反馈。实际上，很多强化学习的任务都存在着反馈稀疏的问题，本章就尝试解决这类问题。</p>
<h3 id="12-1-稀疏回报的困难"><a href="#12-1-稀疏回报的困难" class="headerlink" title="12.1 稀疏回报的困难"></a>12.1 稀疏回报的困难</h3><p>第5章曾介绍过一款称为蒙特祖玛的复仇（Montezuma’s<br>Revenge）的游戏，它属于Atari游戏系列中的一款，游戏界面如图12-1所示。这是一款RPG游戏，玩家通过操纵主角的行动通过一个个房间，找到迷宫的终点并完成游戏。为了通过一些房间，玩家需要操纵角色获取房间内的一些钥匙。当玩家获得了钥匙或者通过一个房间时，才能获得一定的分数，而其他操作不能获得任何分数。</p>
<p>这个游戏的特点是反馈比较稀疏。除了获得钥匙和进入下一个房间这两个情境外，其他的行动都不会获得任何正向反馈。同时主角如果从高空坠落，或者触碰到骷髅头时，将会丢失一条生命。因此，绝大多数的操作都不会获得环境的认可或者否定，如果玩家采取常规的探索手段对环境进行探索，很可能快速丢失所有生命，无法获得任何正向反馈。</p>
<p>图12-1 蒙特祖玛的复仇</p>
<p>听到这里，相信读者一定会想：既然触碰骷髅头和从高空坠落这两个操作会丢失生命，那么我们能不能为这两个操作赋予一个负向的反馈呢？这样当然可以，但是这样做可能会出现两个问题：首先在一些场景下，为了通关，我们需要主动牺牲生命来简化游戏，例如，主角触碰骷髅头之后，虽然会损失一条生命，但是骷髅头也会随之消失。因此我们实际上是用一条生命扫清了障碍，这也不失为一种策略；其次，这种方法也被称为回报重塑（Reward<br>Shaping），也就是根据任务的具体情况对奖励进行改造。这个方法往往会获得不错的效果，但也需要我们对问题有一定的了解，这样学习到的策略也都与任务相关，并不是通用的策略。这与我们构建通用的人工智能模型的目标有些冲突。</p>
<p>棋类的博弈游戏也存在这样的问题，对围棋来说，自己走出的每一颗棋子并不能得到具体的反馈，局面的形式只能通过自己的经验估计，而按照规则，只有双方结束放子，才会得出胜负结果，所以很可能需要在完成一个很长的轨迹之后得到反馈。和蒙特祖玛的复仇类似，这对学习并不是很友好，我们需要额外的工作确定回报究竟来源于这段时间内的哪些操作，这个问题也被称为Credit<br>Assignment Problem，即一局棋下完之后，我们要分析究竟哪些招法是妙招，哪些招法是昏招。</p>
<p>可是令人沮丧的是，Credit Assignment<br>Problem并不是一个很容易解决的问题。即使是前面介绍的那些反馈频繁的问题，也存在着相同的问题，只不过当反馈足够频繁时，这个问题造成的影响并不大。只要反馈足够频繁，每一个行动的优劣都会随着训练样本的增加变得逐渐明确，所以这个问题一直以来没有被我们当成一个很大的问题；但是对于反馈稀缺的任务，行动的价值很难精确地衡量，这个问题也就被放大了。如果我们想学习较好的策略，那么一个直观的想法是解决反馈稀疏的问题：让反馈变得“稠密”起来。只要反馈变稠密，我们就可以使用前面章节介绍的算法求解，并能获得很好的策略模型。</p>
<p>实际上，对于反馈稀疏的任务，还有一个解决方法，就是增强Agent的探索能力，前面我们介绍了 _∈_<br>-greedy的随机策略方法，它可以提高策略的探索能力，但是这种方法有一定的限制，因为它只能在当前状态附近进行无目标的随机探索，对于持续时间比较短的任务来说，足够数量的随机行动会产生“瞎猫碰上死耗子”的效果，一旦通过随机探索遇到了一些有效的回报，模型就会很快地学习到一些有用的信息，获得反馈的频率也会不断提高；但对更复杂的任务来说，单纯的探索也不一定能学习到很好的策略。</p>
<p>笔者曾关注过一个十分小众的游戏：仙剑98速通。玩家要在仙剑98这款曾经十分有名的游戏上尽可能快速地完成游戏。据笔者所知，完成游戏的最快时间在142分钟左右，比起初玩游戏时花费数天才能完成游戏，这个速度让许多人感到惊叹。玩家想要做到这个速度，需要具备以下条件。</p>
<p>（1）通过所有迷宫时都做到不绕路，不执行多余的操作。图12-2所示的地图，是游戏中一个很经典、很难的地图，站在“上帝”视角我们也会觉得这个迷宫比较复杂，更别说玩家在游戏时只能看到很小一部分的场景，很容易被地图中一些支路带回起始出发点，因此很多玩家都会在这里迷路。想要快速通过这个迷宫，玩家甚至需要利用地图中留下的Bug快速通关。这样复杂的操作难以通过单纯探索实现。</p>
<p>图12-2 仙剑98将军冢二层地图</p>
<p>（2）超长的思考路径。为了获得最快的通关速度，玩家需要在游戏前期获取一些道具，并将这些道具使用到游戏后期。例如，从前面的怪物处偷窃一些攻击性道具，然后投掷到后期的Boss身上。或者为了节省游戏时间，缩减一些冗长的剧情，提前购买装备。这些时间跨度非常长的策略也是难以通过单纯的探索得到的。</p>
<p>（3）备用策略。游戏中存在一定的随机性，而这是最考验每一个速通者的环节。若没有获得一个减少时间的关键道具，玩家就需要采取其他的策略弥补。弥补的方法很多，玩家需要快速确定一个最优的弥补方案。除此之外，有时玩家还需要根据游戏的形势采取一些冒险的行动，例如放弃存档、放弃补血等。这些都需要经过不断地探索才能掌握。</p>
<p>读者如果对这个游戏有一定的了解，会明白上面提到的三个难点。读者也许会想，既然这个游戏存在这么多难点，人类是如何探索出来的呢？虽然最终的衡量目标是游戏的总时间，人类却可以找到很多游戏内在的小目标，通过这些小目标的辅助，使游戏的探索更高效。强化学习中的<br>_∈_ -greedy方法显然不能适应这样复杂的游戏环境，如果我们想让机器也获得这样的能力，就需要让它拥有更智能、更理性的探索方法。</p>
<p>通过上面的分析我们发现，解决反馈稀疏的方法主要是通过为Agent增加一些内在的目标使反馈变得不再稀疏。本章将介绍两个方法，它们的核心思想来源于上面的两个解决方法。其中一个采用层次强化学习的方法，通过为任务设立一些“小目标”加快学习；另一个则干脆修改了目标，使有效回报的数量变多。</p>
<h3 id="12-2-层次强化学习"><a href="#12-2-层次强化学习" class="headerlink" title="12.2 层次强化学习"></a>12.2 层次强化学习</h3><p>本节介绍论文 _Hierarchical Deep Reinforcement Learning：Integrating Temporal Ab-<br>straction and Intrinsic Motiνation_ [1] 中介绍的方法：层次强化学习（Hierarchical Reinforce-<br>ment<br>Learning，简称HRL），它的目标就是解决像蒙特祖玛的复仇这样反馈稀疏的问题。我们已经知道这个游戏的反馈比较稀少，交互的轨迹比较长，而HRL解决问题的方法就是设定一些符合直觉的“小目标”。</p>
<p>12.1节已经介绍了设立“小目标”（goal）的缘由，那么让我们直接看小目标的设定方法。我们以到达某个状态作为完成这个小目标的条件。那么，整个任务就变成：到达某个状态1→到达某个状态2→……到达某个状态<br>_N_ →完成任务，得到反馈。</p>
<p>在完成一个小目标的过程中，Agent将不再关注环境本身的反馈，而是专注于完成这个小目标。这样我们的任务就有了层次结构，完成这个任务的Agent仿佛不是一个人，而是一个团队。团队的领导者负责制定小目标，而团队的执行者负责完成小目标。经过这样的分解，我们发现一个单独的模型已经无法满足我们的需求，我们需要构建两个模型。我们把团队领导称为元控制器（Meta<br>Controller），它负责接收环境的状态和反馈，并根据这些信息产生小目标。执行人员称为控制器（Controller），它接收环境状态和小目标，并根据这些信息产生行动。两个模型的结构与交互流程如图12-3和图12-4所示。</p>
<p>图12-3 层次强化学习结构图</p>
<p>图12-4 层次强化学习交互图</p>
<p>我们整理出的层次强化学习完整流程分如下5步。</p>
<p>（1）环境提供状态s和外在反馈f。</p>
<p>（2）元控制器 _π_ g 得到内部目标g，并将状态和内部目标g发送给控制器。</p>
<p>（3）控制器 _π_ ag 接收到信息，产生真实的行动a。</p>
<p>（4）环境接收行动a，并产生新的状态s和外在反馈f。</p>
<p>（5）元控制器根据新的状态判断内部目标g是否完成，如果完成，就生成了一个新的内部目标g；同时根据目标完成情况给出内部反馈r，并回到第3步。</p>
<p>从时间维度看，控制器在每一个时刻都要根据状态和目标给出行动；而元控制器只在一段时间内执行一次行动。这样两个模型在时间维度上具有一定的层次性。论文中给出了基于Q-<br>Learning的学习方法，对控制器来说，它的目标函数为</p>
<p>对元控制器来说，它的目标为</p>
<p>其中 _N_<br>表示执行小目标经历的时间。根据第7章和第8章的介绍，我们可以使用两个模型分别近似这两个控制器的价值估计结果。除此之外，DQN中的所有训练技巧都可以运用在其中，例如Replay<br>Buffer、Target Network等。控制器的价值模型的目标函数可以定义为</p>
<p>其中（s _，_ a _，_ g _，_ r _，_ s′ ）是保存在Replay Buffer _D_ 1 中的样本， _i_<br>表示模型的迭代编号。公式中的 _y_ 1，i 就是Q-Learning的计算结果。对目标函数求导，可以得到</p>
<p>而元控制器的价值模型的目标函数可以定义为</p>
<p>其中（s _，_ g _，_ f _，_ s′ ）是保存在Replay Buffer _D_ 2 中的样本。对目标函数求导，可以得到</p>
<p>以下是层次强化学习的算法。</p>
<p>算法 层次强化学习算法</p>
<p>Input</p>
<p>元控制器和控制器两个模型需要的Replay Buffer{ _D_ 1 _，D_ 2 }，以及模型的参数{ _θ_ 1 _，θ_ 2 }</p>
<p>控制器探索概率 _∈_ 1，g =1，元控制器探索概率 _∈_ 2 =1</p>
<p>Start</p>
<p>for _i_ =1,num_episodes do</p>
<p>获取游戏初始状态s</p>
<p>由s、 _Q_ θ2 和 _∈_ 2 得到目标g</p>
<p>while s不是终止状态do</p>
<p>F ←0</p>
<p>s0 ←s</p>
<p>while s不是终结状态and目标g未完成do</p>
<p>由s、g、 _Q_ θ1 和 _∈_ 1，g 得到行动a</p>
<p>执行行动a，并从环境获得下一个状态s′ 和外部回报f</p>
<p>从内部的评判模型得到内部回报 _r_ （s _，_ a _，_ s′ ）</p>
<p>将样本（{s _，_ g} _，_ a _，_ r _，_ {s′ _，_ g}）存入 _D_ 1 中</p>
<p>使用 _D_ 1 的样本更新模型参数 _θ_ 1 ，目标函数为 _L_ 1 （ _θ_ 1 ）</p>
<p>使用 _D_ 2 的样本更新模型参数 _θ_ 2 ，目标函数为 _L_ 2 （ _θ_ 2 ）</p>
<p>F ←F+f</p>
<p>s←s′</p>
<p>end while</p>
<p>将样本（s0 _，_ g _，_ F _，_ s′ ）存入 _D_ 2 中</p>
<p>if s不是终结状态then</p>
<p>由s、 _Q_ θ2 和 _∈_ 2 得到目标g</p>
<p>end if</p>
<p>end while</p>
<p>衰减 _∈_ 2 ，并按照目标g的完成情况衰减 _∈_ 1</p>
<p>end for</p>
<p>笔者以论文中的一个例子为例，介绍层次强化学习的实际应用。这个例子的环境如图12-5所示。图中共有6个状态，从左到右依次为S1 到S6<br>，Agent共有两个操作，向左移动和向右移动。当Agent决定向左移动时，它会移动到左边的状态；当Agent决定向右移动时，它有50%的概率移动到右边的状态，也有50%的概率移动到左边的状态。任务的终结状态是S1<br>，在任务过程中，我们不会得到任何反馈，直到任务终止，才会根据任务完成情况获得反馈。最终奖励和Agent是否经过某个状态有关，如果Agent首先经过S6<br>才到达S1 ，那么Agent将获得1单位奖励；如果没有经过则只会获得0.01单位的奖励。</p>
<p>图12-5 随机过程环境</p>
<p>为了最大化奖励，我们当然希望Agent首先来到环境的最右边，然后回到左边，但由于Agent在交互时并不知道这样的规则，无法从中总结出规律，因此直接进行交互时，它的行为会显得有些盲目。如果将这个任务改造成层次强化学习的问题，就会变得完全不一样。我们让元控制器先从6个状态中选择一个状态作为目标点，于是当Agent到达这个状态时，它会获得一份奖励。这样虽然我们不明确指出真正有价值的状态，Agent还是可以通过小目标的建立更快速地找到其中的规律。</p>
<p>除了上面这个小例子，我们回到本章开始处介绍的Atari游戏：蒙特祖玛的复仇。采用层次强化学习法，问题将被转换成一个层次的问题，元控制器首先负责定义一个状态作为小目标，控制器通过探索与交互完成目标。只不过这个问题不同于前面的例子，我们无法用抽象的形式表示状态，而且图像中的可行状态有很多，将所有的状态找出来也不太可能。为此，我们采用另外一种办法，利用图像自身的特点，从图像中利用无监督的方法找到一些显著的物体，例如图像中的钥匙、梯子、骷髅头等。我们可以使用很多图像处理的方法找到这些实体（例如，基于图像分割的方法、实体检测的方法、显著物体检测的方法等），找到了这些实体，再使用元控制器决定当前的小目标是什么。选择小目标的效果如图12-6所示。</p>
<p>图12-6 选择小目标的效果</p>
<p>一旦确定了小目标，控制器就可以操纵Agent完成小目标。我们使用元控制器选定了一个小目标区域后，可以使用一个mask作为单独的通道加入图像输入中，在这个mask中，只有小目标所在的区域数值为非0，这样就强调了这个局部区域的重要性。我们将组合后的图像传入卷积神经网络模型中，就可以得到行动的概率分布，并使用概率分布采样得到行动。对应的目标函数也比较好建立，这里不再赘述。</p>
<p>我们通过使用层次强化学习，使任务有了小目标，这样Agent就可以更容易地找到一些探索方向，从而加快解决问题的速度。对一些回报稀疏的问题来说，这样的方法具有很好的效果。</p>
<h3 id="12-3-HER"><a href="#12-3-HER" class="headerlink" title="12.3 HER"></a>12.3 HER</h3><p>7.8节和8.2节分别介绍了Replay Buffer、Priority Replay<br>Buffer两个存储训练样本的数据结构。它们都具有减少序列样本相关性、提高样本利用率的功能，同时在算法中被广泛应用。本节介绍另外一个存储样本的结构Hindsight<br>Experience Replay（简称HER），来自同名论文 [2]<br>。它的主体结构和前面介绍的两种结构基本一致，主要的不同之处在于它对样本回报和价值的设定。HER可以用在回报稀疏的环境中，为Agent提供更多的回报信息。</p>
<h4 id="12-3-1-渐进式学习"><a href="#12-3-1-渐进式学习" class="headerlink" title="12.3.1 渐进式学习"></a>12.3.1 渐进式学习</h4><p>前面已经提到了一些反馈稀疏的任务，为了介绍HER这个新结构，论文介绍了一个十分特殊的任务。在任务中，环境会初始化产生两个长度为 _N_<br>，由0和1组成的数组，其中一个为初始值，另一个为目标值。Agent在每一时刻可以翻转任意一位数字，将其从0变成1，或者从1变成0。当Agent操纵的数组和目标数组完全一致时，Agent就会获得奖励；其他情况下，Agent不会获得任何奖励。任务的时间长度有一定的限制，到达指定的时间后任务结束。这个任务的流程如图12-7所示。</p>
<p>图12-7 数字翻转任务示意图</p>
<p>可以看出，任务的反馈十分稀疏，而且数组的长度越长，反馈越稀疏。如果策略不够优秀，Agent无法将数组移动为目标值，也无法获得任何反馈。如果持续保持这样的学习状态，Agent需要很长时间才能学习到最优策略，或者无法学习到。</p>
<p>既然问题出自反馈，那么能不能增加反馈出现的频率呢？假设初始状态为s0 ，目标状态为st ，经过 _T_ 轮的行动操作，得到状态sT<br>，虽然现在的方法没有办法在 _T_ 时刻内将s0 变换到st ，但是可以变换到sT<br>。所以我们不应该抱怨模型的效果不好，没有让我们完成既定目标，而应该埋怨目标设置得不合理，如果把目标换成sT ，策略模型不就完成任务了吗？</p>
<p>读者一定会对这样的思想产生质疑，我们的目标明明是st ，为什么要把它修改到sT<br>呢？这种行为相当于纵容我们的模型，对它错误的表现提出了鼓励？这就好像学生参加考试，考试的满分是100分，通过分数是60分。一位同学经过几个小时的努力，终于考出了50分的“好成绩”，于是大家纷纷抨击考试制度：明明50分已经很优秀了，为什么要将门槛设置为60分？你知道这位同学多努力学习吗？他在发烧的状态下还坚持每天学习25个小时……但是在读者的眼中，成绩没考过60分，就是没有通过，任何解释都没有意义。这样的想法并不奇怪。</p>
<p>如果这个考试是一个正常的考试，维持标准无可厚非。如果有些同学的考试分数超过了60分，那么至少证明达到通过的分数是可行的，那些没达到分数的同学也没什么好抱怨的。但是如果给一群小学生出了高等数学题目，让小学生使用泰勒公式、欧拉定理和勒贝格积分，估计家长会说老师教学方式有问题。如果老师真的出了这样一张卷子给小学生，那么小学生答多少分都情有可原，就算不到60分也没关系；如果小学生竟然蒙对了一些题目，说不定家长还会觉得小学生具有数学天分。</p>
<p>所以到这里，我们发现了问题的关键：卷子的难度和考生的能力是否匹配。如果基本匹配，那么常规的标准设定是合情理的；如果完全不匹配，很多设定就无法按照常理进行。我们知道小学生几乎无法解决高等数学问题，所以60分的通过线是不合理的。同理，对一个几乎没有反馈的强化学习任务来说，“通过反馈影响决策”这个机制根本无法建立，我们也不能保证模型能学到多少有用的东西。既然这样，我们为什么还要要求模型一定要达到原本规定的目标呢？我们能不能降低一点难度呢？</p>
<p>机器学习的研究问题中有一个与上面提到的问题相关的领域，叫作课程学习（Cur-riculum<br>Learning）。Curriculum的含义是“课程”，而Curriculum<br>Learning在机器学习中代表一种渐进式地学习策略。这种学习思想同样来源于人类自身，我们不可能让一个刚出生的婴儿直接参加高考并取得优异的成绩，因为这对婴儿来说太难了。直接挑战这样的任务不但十分困难，也更容易在学习的过程中学习到“投机取巧”的解法，例如大家熟知的“三长一短选最短，三短一长选最长”、“不会就蒙C”等。这些基于“大数据”分析得到的经验在一些“数据集（考试卷）”上会有良好的效果，但是它实际上是一种过拟合的表现，一旦出题人改变出题思路，结果将变得很糟糕。所以一般来说，我们要采取渐进式的学习方法，在学习的道路上多设置一些“小目标”，让自己在不断完成小目标的过程中成长。</p>
<p>那么这样的小目标该如何设定呢？一般来说，我们可以从几个方向实现。以打靶为例，一个人手持气枪瞄准一个靶子射击，靶子上有一些标识，给不同的区域设定了各自的分数，同时人和靶子要保持一定的距离，射击还包括时间和其他因素的限制。如果训练一个射击选手，那么我们可以使用以下几个思路进行训练。</p>
<p>（1）任务分解。我们将任务分解成一些更小粒度的任务，例如先练习持枪，如何稳定地握住枪而不发抖；其次练习瞄准，可以在枪的前方安装一个激光器，让选手练习把激光射向想要的分数区域；最后让选手适应射击时的后坐力，将后坐力造成的震动考虑到射击的过程中。通过完成这样一个个的小任务，大任务可以完成得更好。实际上12.1节介绍的方法和上面介绍的方法更相近。</p>
<p>（2）调整难度。一开始我们缩短人和靶子的距离，同时扩大高分的区域，让选手在一开始也可以获得一定的分数，这样可以使选手建立最初步的射击策略；选手的水平又提高后，将距离变长，同时减少高分区域，进一步提高选手的水平。渐渐地，选手的能力不断提高，而选手的射击环境和真实的环境一致，选手完成了渐进式地学习。这种方法就是课程学习的主要思想。</p>
<p>介绍了这么多关于课程学习的内容，让我们回到前面的例子。我们把最终目标变为sT<br>，实际上相当于降低了任务的难度。我们可以通过调整任务难度让模型渐进式地学习，不断增强策略的能力。这就是HER背后的思想。Hindsight可以翻译为“后见之明”。读者一定听说过“先见之明”，先见之明（Foresight）是指在遇见一个事物之前就对它有准确判断，而后见之明是指遇到一个事物之后的准确判断。论文中也提到，人类与模型相比有一个明显的长处，那就是在无法得到任何明确的反馈时，依然可以从中得到一定的收获并积累经验；而计算机就不能获得任何的收获。人类的这个特点被称为“后见之明”，用俗语来说就是“事后诸葛亮”。</p>
<p>我们可以用“后见之明”的思想重新描述HER方法：当我们经过操纵得到状态sT 时，虽然我们没有得到反馈，但通过操作我们可以得到状态sT<br>，这个经验还是可以保留下来的，万一下一次目标状态就是sT 呢？</p>
<h4 id="12-3-2-HER的实现"><a href="#12-3-2-HER的实现" class="headerlink" title="12.3.2 HER的实现"></a>12.3.2 HER的实现</h4><p>介绍完HER的基本思想，下面来看看HER的具体实现，代码在baselines/her/中，项目通过使用HER实现了DDPG算法。关于DDPG的实现已经在11.3节介绍过，这里不再赘述。我们将目光集中在数据收集和采样这两个环节，其中涉及三个代码文件的内容。</p>
<p>首先是Replay Buffer的数据结构，代码在replay_buffer.py中。代码中构建的数据结构如图12-8所示。</p>
<p>采样数据存在一个字典（dict）中，Replay Buffer存储了如下一些key。</p>
<p>_o_ ：代表观测值（observation）。</p>
<p>_u_ ：代表行动值（action）。</p>
<p>_g_ ：代表目标值（goal）。由于算法可以通过目标计算得到回报，因此这里将不再存储回报值。</p>
<p>_ag_ ：代表已经完成的目标（achieved goal），也就是交互终止时的状态值。</p>
<p>除了以上4个信息，在采样时，Replay Buffer还将生成另外两个key。</p>
<p>o_2：代表下一时刻的观测值。</p>
<p>ag_2：代表下一时刻已完成的目标值。</p>
<p>为了更好地实现HER算法，Replay<br>Buffer以序列（episode）为单位进行存储，也就是说，将一整条序列完整地存储，序列内每一时刻的状态和行动也会如交互时一样排列好。那么，Replay<br>Buffer可存储的样本容量为 _N_ × _T_ ，其中 _N_ 表示Buffer容纳的序列数量， _T_<br>表示每一个序列的长度，其他的结构与前面介绍的Replay Buffer类似。</p>
<p>在采样交互的过程中，策略模型同环境交互，将得到一个完整的交互序列{s0 _，_ a0 _，_ g0 _，…，_ st _，_ at _，_ gt<br>}。由于每一次交互我们都会得到下一时刻的状态，所以可以将其保存为前一时刻的achieved goal，意思为已经达到的“目标”。这样序列就变成了{s0<br>_，_ a0 _，_ g0 _，_ ag0 _，…，_ st _，_ at _，_ gt _，_ agt }，我们将这个序列完整地保存到Replay<br>Buffer中。这部分内容在代码rollout.py中可以看到，这部分过程如图12-9所示。</p>
<p>图12-8 Hindsight Replay Buffer的结构图</p>
<p>图12-9 样本采样与处理</p>
<p>在训练的过程中，我们需要对从Replay<br>Buffer中采样出的样本，设定一定的概率将它的目标从原定的目标g变成其他容易实现的目标。这里我们采用论文中提到的future采样模式，假设采样得出样本的时刻为<br>_i_ ，那么我们将从 _j_ ∈[ _i_ +1 _，T_ ]的时间区间选择一个时刻的achieved goal作为样本的目标，也就是说：</p>
<p>那么，完整的样本选取过程将会变成下面的流程。</p>
<p>（1）从Replay Buffer的 _N_ 个序列中采样 _b_ 个序列。</p>
<p>（2）从 _b_ 个序列中各选出一个时刻的样本，得到 _b_ 个样本。</p>
<p>（3）每一个样本有一定的概率将 achived_goal 设置为当前时刻后的任一时刻的状态。</p>
<p>样本选取的过程如图12-10所示。</p>
<p>这样我们就了解了HER的全部内容。HER使我们从课程学习的角度解决了反馈稀疏的问题，并获得了一定的成功。最后给出HER算法的全过程伪代码：</p>
<p>算法 Hindsight Experience Replay算法（基于future采样方法）</p>
<p>Input</p>
<p>off-policy RL算法 _A_ ：例如DQN（7.9节介绍）、DDPG（11.2节介绍）</p>
<p>回报函数 _r_ ：S×A×G→R</p>
<p>Replay Buffer:R</p>
<p>Start</p>
<p>初始化A，初始化R</p>
<p>for episode=1, _M_ do</p>
<p>采样目标g和初始状态s0</p>
<p>for _t_ =0, _T_ -1 do</p>
<p>从算法 _A_ 中的策略 _π_ b 得到行动at ：at ← _π_ b （st ‖g）</p>
<p>执行行动at 并获得新的状态st+1</p>
<p>end for</p>
<p>for _t_ =0, _T_ -1 do</p>
<p>rt ∶= _r_ (st _,_ at _,_ g)</p>
<p>将样本（st ‖g _，_ at _，_ rt _，_ st+1 ‖g）保存到R中</p>
<p>end for</p>
<p>for _t_ =1, _N_ do</p>
<p>从R中采样一个minibatch B，每个样本有一定的概率将gt 修改为{st+1 _，…，_</p>
<p>sT-1 }中的任意一个元素</p>
<p>使用minibatch B对A进行优化训练</p>
<p>end for</p>
<p>end for</p>
<h3 id="12-4-总结"><a href="#12-4-总结" class="headerlink" title="12.4 总结"></a>12.4 总结</h3><p>本章我们介绍了解决反馈稀疏任务的强化学习算法，让我们总结一下。</p>
<p>（1）当反馈比较稀疏时，强化学习有可能无法完成学习。</p>
<p>（2）HRL方法通过构建层次式的决策模型和设立“小目标”的形式增加了环境给Agent的反馈。</p>
<p>（3）HER通过设立相对可行的目标，使Agent的学习难度变得相对平滑。</p>
<h3 id="12-5-参考资料"><a href="#12-5-参考资料" class="headerlink" title="12.5 参考资料"></a>12.5 参考资料</h3><p>[1] Kulkarni T D,Narasimhan K R,Saeedi A,et al.Hierarchical Deep Reinforcement<br>Learn-ing:Integrating Temporal Abstraction and Intrinsic Motivation[J].2016.</p>
<p>[2] Andrychowicz M,Wolski F,Ray A,et al.Hindsight Experience Replay[J].2017.</p>
<h2 id="13-Model-based方法"><a href="#13-Model-based方法" class="headerlink" title="13 Model-based方法"></a>13 Model-based方法</h2><p>第6章介绍了强化学习最基本的概念，那时我们假设自己处于上帝视角，对环境的运转情况完全了解，这样我们就可以采用动态规划的方法，使用表格式的建模形式解决问题，使用的算法有策略迭代法、价值迭代法和广义策略迭代法。后来我们的视角就转移到了环境未知的问题上，为了解决这些问题，我们引出了一系列的方法，如Q-<br>Learning和Policy Gradient。</p>
<p>我们发现前面的很多方法在面对环境未知的问题时采用了比较“被动”的方法：既然环境的状态转移是未知的，那么我们就不去关注它。本章我们要采取更积极的方法，也就是Model-<br>based的方法，它通过对状态转移概率进行建模，使我们对强化学习的整个流程进行整体优化。本章先介绍影响全世界的Alpha<br>Zero模型，其次介绍连续状态行动空间下的经典算法LQR。</p>
<h3 id="13-1-AlphaZero"><a href="#13-1-AlphaZero" class="headerlink" title="13.1 AlphaZero"></a>13.1 AlphaZero</h3><h4 id="13-1-1-围棋游戏"><a href="#13-1-1-围棋游戏" class="headerlink" title="13.1.1 围棋游戏"></a>13.1.1 围棋游戏</h4><p>AlphaZero<br>是一个世人皆知的人工智能模型，经过几个版本的迭代，模型不断接近完美，从最初的以监督学习为核心的训练方法，到使用强化学习为唯一训练的方法，AlphaZero完成了人工智能的革命，也将人工智能的棋类游戏带入了一个新的高度，相信读者对这段传奇故事已经有所了解。</p>
<p>在介绍AlphaZero模型之前，让我们先看看围棋游戏的定义。围棋游戏是一个信息完全公开的零和博弈。参与游戏的主体有两方，如果把一方看作Agent，那么棋局和另一方可以看作环境。在棋局中，双方交替在棋盘上落子，当双方无法落子时，游戏会按照规则判定最终的获胜方是谁。如果Agent胜利，则获得1分回报，反之则获得-1分，这个过程如图13-1所示。</p>
<p>图13-1 围棋游戏示意图</p>
<p>虽然我们知道对手所有可能的行动，但并不知道对手真正会走到哪个位置。因此，我们可以把对手的策略想象成环境的状态转移概率，这样围棋的问题形式就和前面的问题很相近了。因为游戏的目标是获得最后的胜利，所以我们的目标可以写作</p>
<p>理论上我们可以采用前面章节介绍的方法进行学习训练，但是这样的方法很难学习到很好的策略模型，因为游戏中的对手可能是一个水平很高的选手，一旦玩家发生一点点失误，对手都可能抓住机会赢得胜利。因此，我们需要在前面算法的基础上进行扩展，获得更多对棋局有利的信息。</p>
<p>我们常说“下棋看三步”，即在落子之前，玩家要考虑落子之后对手的出招，然后考虑应对对手的措施。这种经过深思熟虑的招法往往攻守兼备，招法具有一定威力的同时也避免给对手留下机会。棋类游戏的高手基本上都使用了这种思维方式，他们可能会向后思考四步、五步，甚至更多步。</p>
<p>那么玩家该如何向后思考更多步呢？此时玩家并不知道对手的情况，实际上很难对对手的招法进行估计。那么有没有什么方法解决这个问题呢？对于棋类高手来说，他们一般会把自己当作对手，以自己的棋力估计对手的招法。这个方法虽然不一定能猜中对手的真实招法，但确实是目前最好的方法。如果我们能够运用自己的棋力模拟完成双方的对弈，就可以更准确地估计招法对棋局的效果。这样的方法被称为自我对弈（英文为Self-<br>Play），即自己和自己下棋。</p>
<p>基于这样的假设，我们可以认为围棋这个游戏在玩家的眼中是一个模型已知的问题。玩家不再将对手想象成环境，因为此时对手也是玩家本身，只不过在不同的时刻，玩家需要完成不同的目标。玩家在奇数时刻要完成的目标，和在偶数时刻要完成的目标相反。玩家要通过不断地切换自己的身份估计局面的走向，并判断每一个行动最终可能的结果。这个过程如图13-2所示，从当前状态出发，所有的棋局分析可以形成一棵树。</p>
<p>图13-2 局面与策略估计</p>
<h4 id="13-1-2-Alpha-Beta树"><a href="#13-1-2-Alpha-Beta树" class="headerlink" title="13.1.2 Alpha-Beta树"></a>13.1.2 Alpha-Beta树</h4><p>从图13-2中可以看出，为了知道招法的真实效果，我们可以将当前局面下所有的情况都列出来，也就是说先把当前局面下的所有招法列出来，然后把下一轮对手的所有招法列出来，依此类推直到棋局结束，但是这样做的代价实在非常大，这个计算量是计算机不可接受的。既然列出所有的招法不可能，那能不能只列出有限的招法呢？虽然棋盘上有很多的位置可以落子，但有价值的招法应该是有限的。既然我们拥有策略模型，就可以根据策略计算出有价值的前<br>_N_ 个招法，在树的宽度上进行了裁剪。这样计算量在一定程度上减少了。</p>
<p>除此之外，我们还希望能在树的深度上做削减。如果对每一个棋局使用策略模型计算接下来的招法，那么当树的层数为 _d_ ，每一层展开 _N_<br>个局面时，我们仍然需要计算这么多的局面：</p>
<p>可以看出，策略的使用次数随着树的深度呈指数形式增长，因此控制树的深度是必需的。这时我们想到了价值模型，可不可以采用一种折中的方法，当使用策略计算到第 _d_<br>层时，即使我们没法确定局面的最终结果，也可以停止进一步训练，使用一个价值模型判断双方的局势，并估计该局面下最终的期望结果呢？这相当于我们放弃了一定的精度来换取时间，对应的思路如图13-3所示。</p>
<p>图13-3 包含价值模型的树</p>
<p>通过上面的两种方法，我们可以很好地减少棋局计算的数量，还可以通过调节树的宽度和深度调整策略计算的效果。增加宽度相当于增加策略的探索能力，可以发掘到更多的招法；增加深度相当于降低价值模型估计的难度，使我们对最终局面的估计更准确。以上思路通常被称为Alpha-<br>Beta搜索法，其中需要策略模型和价值模型的合作，对应的伪代码如下所示：</p>
<p>代码中的min和max代表搜索到当前节点时对局面估计的得分范围。如果我们认为最终胜利为1分，失败为-1分，那么初始局面下min=-1，max=1。随着搜索与计算的不断深入，这个范围会不断缩小，min和max最终变成一个相同的数值。代码中的choose_n_move方法表示使用策略模型对局面进行分析，得出概率最大的前<br>_n_ 个行动，我们可以通过计算 _p_<br>（a|s）的结果进行比较；代码中的evaluation方法表示使用价值模型对局面进行估计，得到对局面期望的估计。由于我们需要对树进行搜索，因此方法采用了递归的形式，读者需要注意：每当算法要进入下一层迭代时，对结果的估计需要反过来，因为围棋是一个零和博弈游戏，一方的优势就是另一方的劣势。希望读者能够理解这个算法的精髓。</p>
<h4 id="13-1-3-MCTS"><a href="#13-1-3-MCTS" class="headerlink" title="13.1.3 MCTS"></a>13.1.3 MCTS</h4><p>前面介绍了Alpha-<br>Beta树的基本形式，这个方法在博弈问题中被广泛使用，也获得了不错的效果，而本节介绍的蒙特卡罗树搜索方法，在实际中更具有灵活性。它的核心搜索过程主要分为以下四步。</p>
<p>（1）选择（Selection）：从已知的树节点中选出一个最有“价值”的节点，这个价值由节点的“重要性”决定。</p>
<p>（2）扩展（Expand）：在第一步选择的节点上随机扩展一个或者更多的节点。</p>
<p>（3）仿真（Simulation）：从第二步扩展的节点中选择一个节点，采用某种策略随机生成新的状态，并在这个新的状态上继续利用这种policy生成新的状态，这样不断进行下去直到最终状态。</p>
<p>（4）回传（Back-propagation）：将最终的计算结果反向传导给上层的所有父节点，更新父节点的相应信息。</p>
<p>其伪代码和流程如下所示：</p>
<p>function MCTSSEARCH(s0 )</p>
<p>创建根节点v0 ，其中包含状态s0</p>
<p>while有时间和资源继续计算do</p>
<p>vl ←TREEPOLICY(v0 )</p>
<p>∆←DEFAULTPOLICY(s(vl ))</p>
<p>BACKUP(vl _,_ ∆)</p>
<p>return a(BESTCHILD(v0 ))</p>
<p>对应的流程图如图13-4所示。</p>
<p>图13-4 MCTS的计算步骤</p>
<p>下面笔者详细介绍这四个步骤。</p>
<p>（1）选择。在每次进行蒙特卡罗演绎前，我们需要选定一个树中的节点作为蒙特卡罗过程的出发点。这个过程十分重要，而且蒙特卡罗树搜索的过程和强化学习中的经典问题——探索-<br>利用问题很相似。</p>
<p>前面的章节中曾介绍过，探索是尝试之前没有尝试过的行动，如果能找到一些更好的行动，最终的目标也可以完成得更好。我们希望能多尝试一些不同招式的棋局和之前判定胜率低的棋局，因为对手有可能走出我们没有想到的招式，而之前判定为胜率低的棋局也可能是随机采样的次数不够导致的，一旦采样达到一定数量，它就能够真实反映招式的效果，那时也许我们会发现它其实是一个胜率很高的招式。利用是指利用当前已知的情况实施策略。对MCTS来说，如果已知一个招式的胜率很高，为了进一步明确这个招式真的胜率很高，就需要把有限的采样次数尽可能地用在这种局面上，使我们更确定这个招式的胜率。</p>
<p>前面章节中已经介绍，探索和利用似乎是一对矛盾体，当总资源一定时，将更多的资源分配给当前表现优秀的招式，就会忽略一些潜力股，反之亦然。这就是强化学习中提到的探索-<br>利用困境。只要资源有限，就无法摆脱这个困境。经过大量的实验，人们总结出了一套启发式的策略：在学习过程的初期，策略将向探索方向倾斜，而到了学习过程的后期，利用将占据主导地位。</p>
<p>为了平衡探索与利用，我们还需要将树中每一个节点的“重要性”量化。研究人员发明了很多基于MCTS的算法，其中比较经典的算法是Upper Confidence<br>Bound for Trees算法，简称UCT，其中计算节点权值的公式UCB1为</p>
<p>其中 _X_ -j 表示这个节点当前所有蒙特卡罗过程后的平均得分，这象征了这个节点的招法经过蒙特卡洛演绎后的效果； _C_ p<br>是一个参数，用于调整模型在探索和利用之间的权重关系； _n_ 为当前已经进行的蒙特卡罗的轮数； _n_ j<br>表示该节点已经进行的蒙特卡罗的轮数。从这个公式可以看出，想让一个节点被选中，要么是因为它最终获胜的概率高，也就是公式右边的第一项大；要么是它被选中的次数少，这样公式右边的第二项就会大些。这个公式并不复杂，它在实际应用中的效果也很不错，所以被广泛采用。选择过程的目标是从根节点出发找到第一个没有被完全扩展的节点，所以真正的选择过程可能要持续很多轮。</p>
<p>（2）扩展过程。找到符合条件的节点后，就可以对节点进行扩展。扩展实际上是和选择过程结合执行的，选定好待扩展的节点后，算法会从这个节点选择一个没被执行过的行动，执行这个行动并得到下一个状态，然后根据这个状态创建节点，这样树的扩展就算完成了。</p>
<p>（3）模拟过程。这个过程出现在很多棋类博弈算法及初代AlphaGo算法中，它会使用一个相对简单的策略模型快速执行双方的招式，每一时刻只从策略中随机选取一个招式并执行，就这样不断地模拟双方的招式直到游戏结束，并记录最终的结果。一般来说，为了保证双方的出招不至于太过离谱，策略需要有一定的棋力，但基于时间的限制，模型又不能过于复杂。</p>
<p>（4）回传过程。当新节点完成模拟后，无论我们采用蒙特卡罗法还是价值估计法，都会得到当前状态的一个结果估计值。由于这个棋局是其父节点可能产生的局面之一，因此可以把得到的结果回传给前面所有的父节点，并更新它们的估计值。</p>
<p>以上就是MCTS算法的主要流程。下面我们以UCT为例，介绍它的详细流程。方法中树中的每一个节点都保存了两个值：访问该节点的次数 _N_ 和累计的奖励数量<br>_Q_ ，将两个数字相除就可以得到这个节点的平均奖励，也就是期望胜率。基于上面的分析，我们可以给出UCT的具体算法。</p>
<p>算法 UCT算法</p>
<p>function UCTSEARCH(s0 )</p>
<p>创建基于状态s0 的节点v0</p>
<p>while有时间和资源继续计算do</p>
<p>vl ←TREEPOLICY(v0 )</p>
<p>∆←DEFAULTPOLICY(s(vl ))</p>
<p>BACKUP(vl _,_ ∆)</p>
<p>return a(BESTCHILD(v0 ,0))</p>
<p>-function TREEPOLICY(v)</p>
<p>while v不是树的叶子节点do</p>
<p>if v没有被充分扩展then</p>
<p>return EXPAND(v)</p>
<p>else</p>
<p>v←BESTCHILD(v _,Cp_ )</p>
<p>return v</p>
<p>-function EXPAND(v)</p>
<p>从节点v未尝试的行动中选择一个行动a</p>
<p>为v增添一个叶子节点v′ 使得 _s_ （v′ ）= _f_ （ _s_ （v） _，_ a）， _a_ （v′ ）=a</p>
<p>return v′</p>
<p>-function BESTCHILD(v _,c_ )</p>
<p>return argmax</p>
<p>-function DEFAULTPOLICY(s)</p>
<p>while s是非终结状态do</p>
<p>根据策略选择行动a</p>
<p>得到下一个状态：s← _f_ （s _，_ a）</p>
<p>return reward for state s</p>
<p>-function BACKUP(v,∆)</p>
<p>while v is not null do</p>
<p>_N_ (v)← _N_ (v)+1</p>
<p>_Q_ (v)← _Q_ (v)+∆(v _,_ p)</p>
<p>v←v的父节点</p>
<p>对于像围棋这样的零和博弈问题，对弈双方的得分互为相反数，因此在回报结果回传的过程中，同样需要对上一层的父节点的回报取反，如果结果是我方胜利，且给我方的记分是1分，那么给对方的记分就应该是-1分。</p>
<h4 id="13-1-4-策略价值模型"><a href="#13-1-4-策略价值模型" class="headerlink" title="13.1.4 策略价值模型"></a>13.1.4 策略价值模型</h4><p>前面我们介绍了MCTS的主要流程，下面就来介绍它是如何被应用到AlphaZero中的。从前面的介绍中读者已经了解了UCT的算法流程，对AlphaZero来说，它的主体流程与其类似，但是在细节上有以下两个不同。</p>
<p>（1）在选择步骤上，AlphaZero使用了不同的公式对重要性进行判断：</p>
<p>其中 _Q_ （s _，_ a）为处于状态s、行动a下的平均价值积累量，这个值由当前节点的所有叶子节点的估计价值统计产生。 _P_ （s _，_<br>a）是在状态s下执行行动a的概率，这个值由策略模型产生。</p>
<p>（2）在仿真这一步，AlphaZero中并没有像UCT那样使用快速策略的生成招法，而是使用复杂的深层网络直接预测当前棋局的期望结果，这就像价值网络模型预测长期回报一样，如果游戏规则中获胜方获得1分，失败方获得-1分，那么网络预测的结果就应该在-1到1之间。从前面的学习经验来看，这种方法和仿真方法相比方差较小。蒙特卡罗法可以精确地得到一次采样的结果，但是这种方法同其他使用完整轨迹的方法一样，具有高方差的特点，不充分的采样会使价值期望产生波动；而价值网络使用了函数近似的方法，这种方法会存在一定的偏差，但同时会减少结果的波动。如果价值网络可以达到足够的精度，那么它的实际效果就会优于蒙特卡罗模拟。</p>
<p>当完成了对结果的估计后，这个结果会回传给所有的父节点。不同于前面提到的强化学习问题，这里我们不需要对价值进行打折，只要将估计值直接回传即可，过程如图13-5所示。</p>
<p>图13-5 AlphaZero的MCTS实现方式</p>
<p>我们发现，实现AlphaZero的MCTS需要两个模型：策略模型和价值模型，分别用 _π_ （a|s； _θ_ ）和 _v_<br>（s；w）表示。下面笔者就来分析两个模型的结构。</p>
<p>两个模型的输入相同，都是以棋盘为相对位置的特征。由于围棋的棋盘由19×19的交叉点组成，因此模型输入的长和宽都是19。与Atari游戏类似，围棋也包含另外一个维度：时间。通过对比不同时刻棋盘的信息，我们可以知道棋盘最新落子的位置，这样我们可以更好地将落子位置这个信息考虑在内。因此，AlphaZero考虑当前棋局前八招的招法信息，同时为了区分敌我双方，双方的棋子被拆分到不同的通道中，这样就有了16个通道的信息，加上一个用于判断是否为己方出招的通道，输入一共由17个通道组成。一个棋局的输入总大小为17×19×19。</p>
<p>由于围棋的棋盘存在空间上的相邻关系和时间上的相邻关系，从两个维度来看它们都具有一定的局部相关性，因此我们可以使用卷积神经网络将这些特征提取出来。同时，由于两个模型都需要提取棋盘特征，因此这部分的运算可以共享，所以两个模型的前几层是共享的。策略模型的输出维度是19×19，和棋盘的维度相同，每一个输出值表示了棋子落在对应位置的概率。价值模型只输出一个值，用于表示当前局面下的结果估计。</p>
<p>从输出结果可以看出，策略模型相当于一个361类的分类，价值模型是一个回归模型，因此可以根据二者的模型类型定义对应的目标函数。策略模型的目标函数是一个Cross<br>Entropy Loss函数，价值模型的目标函数是一个平方损失函数，这两个目标函数都是十分经典的函数。对于一个输入棋局来说，它的目标值是什么呢？</p>
<p>对于价值模型来说，目标值相对容易考虑，我们就以一局比赛结束后的结果作为整局比赛所有状态的目标输出。我们可以简单地认为如果一局对弈失败，那么整局所有招式都有问题，都应该给予一定的惩罚。这样的做法存在一定的不合理性，对棋类游戏有一定了解的读者一定会认为这种判定方法存在很大的问题。一局对弈持续的招式数可能很多，其中有的招式很优秀，有的招式很糟糕。博弈的结果往往和木桶原理相近，一局失利并不等于没有走出精妙的招式，只是存在一定的失误，那么决定棋局最终结果的应该是少数的几个招式，而不是所有的招式，我们应该只对个别招式进行惩罚，而不是惩罚所有招式。</p>
<p>这个思想并没有错，如果我们能将这个问题搞清楚，模型的学习进度将大幅提升。但如果站在模型的角度，或者说，将自己置身于某个问题中：自己本身就是一个新手，还不能保证能够完美地完成任务，那又如何能分析出其中的问题呢？这本身就是一个有挑战的事情。实际上，这个问题就是前面章节曾经提到的Credit<br>Assignment Problem。既然这件事情很难办到，我们就只能退而求其次，使用不那么完美但同样有效的方法。</p>
<p>以上是从悲观的角度分析最优方法的不可行，从乐观的方向分析，我们可以通过大数据将不同招式的结果期望区分开。假设经过足够多次数的对弈，同一个招式会在不同的棋局中出现，那么对那些不好的招式来说，在它们出现的棋局中，除它们之外，即使其他的招式都非常好，获胜的概率也相对较低；而对那些较好的招式来说，在它们出现的棋局中，除它们之外，其他招式只要不太差，获胜的概率会更高。这样经过更多局面的演绎，好招式和坏招式逐渐区分开，它们的结果期望也就有了差异，模型就能学到有用的信息。所以在大数据的环境下，这个问题可以得到一定程度的解决，读者也不用对此太担心。</p>
<p>对策略模型来说，目标值是MCTS的结果，也就是经过自我对弈分析后得到的结果。这个方法和Q-<br>Learning有一些相似的地方，它们在计算目标值时都使用了模型自身计算的信息。从直觉上考虑，策略模型和价值模型接近最优时，经过自我对弈得到的结果应该比直接由策略模型得到的结果更严谨更可靠；而价值模型可以通过棋局结果优化提升，因此在学习的过程中，策略模型和价值模型总体上都在不断地提升，这样经过MCTS计算得到的行动会逐渐强于策略模型直接得到的行动。可以认为，以MCTS的结果作为目标值，能够提升模型策略，而再一次遇到类似的状态时，新的策略模型经过MCTS的计算后可以获得更优秀的行动，通过这个机制，模型可以做到自我提升，从而摆脱训练数据的束缚。</p>
<p>在AlphaZero的前一代实现AlphaGo中，策略模型和价值模型仍然需要使用训练数据进行训练。实际上这是一种很不错的方法，笔者在第8章中也介绍了将准备好的数据放入Replay<br>Buffer进行训练的方法，这种方法取得了不错的效果。使用训练数据进行训练虽然可以快速提高模型的能力，但是它也存在一定的隐患。因为围棋游戏毕竟是一个尚未完美解决的问题，真正的最优策略仍未找寻到，所以我们并不知道某一棋局的最优招法。虽然我们可以找到一些优秀的对弈棋谱，但是谁也无法证明这些棋谱的每一招都是最优的，于是对于采用监督学习的模型来说，棋谱中的优秀招式和不好的招式统统被学习到了模型中，模型的水平很难超越人类的既有水平。若使用MCTS提升模型的方法可以成立，那么虽然在一开始模型的表现会很差，但随着不断地学习，模型会变得越来越好，这样也就有了超越人类的可能。</p>
<p>模型完整的目标函数可以表示为</p>
<p>目标中的 _t_ 表示训练过程中的样本编号，zt 表示棋局对弈的最终结果，玩家胜利为1分，失败为-1分；～ _π_<br>表示经过MCTS计算得到的行动概率。公式最后两项是对参数的正则约束， _c_ 为控制正则项的权重。可以看出，我们需要在对弈过程中使用Replay<br>Buffer保存棋局状态、MCTS行动结果对应的概率，以及棋局的最终结果三个内容。完整的对弈过程如图13-6所示。</p>
<p>图13-6 自我对弈训练过程示意图</p>
<p>前面关于模型提升的思想也只是一些直观的想法，并没有严格的理论证明，所以随着学习不断推进，模型效果下降的情况是可能发生的。为了解决这个问题，AlphaZero中还包含了第三个模块，这个模块的存在使模型的单调提升变得更有保证。</p>
<h4 id="13-1-5-模型的对决"><a href="#13-1-5-模型的对决" class="headerlink" title="13.1.5 模型的对决"></a>13.1.5 模型的对决</h4><p>为了让模型能够拥有接近单调提升的性质，AlphaZero中包含了模型之间对决的环节。俗话说“是骡子是马，拉出来遛遛”，对于如此复杂的问题，最好的检验方法就是在真实对弈中看效果。于是每完成一轮迭代，新的模型都要和旧的模型进行一定轮棋局的对弈。新的模型只有获得更多的胜利（超过55%的胜率）才被认为超过了旧模型，才能替换旧的模型，否则就抛弃这个新模型，重新进行新的训练。</p>
<p>虽然这个思想并不复杂，但其中包含了大量的计算，而且算法中的模型比较大，在对决时需要将其部署到多台机器上同时进行多局对弈，因此其实现需要工程上的考量。相对而言，这个方法在思想上比较简单，而AlphaZero中的自我博弈已经具备一定的使策略提升的功能，因此这里的设计可以简单一些。</p>
<p>以上就是AlphaZero的核心流程。它的三个核心模块：自我对弈、深层模型和新旧模型对决组成了完整的模型，为棋类游戏构建了一个新的框架范本。很多与围棋形式接近的棋类也可以应用这个框架实现对应的策略。在论文中作者也介绍了AlphaZero在国际象棋和日本将棋上的应用，很多人也尝试在其他棋类上应用，并且收到了很好的效果，这也证明了通用学习框架对人工智能发展起到的作用。更多关于模型训练的细节请读者参阅相关论文，这里不再赘述。</p>
<h3 id="13-2-iLQR"><a href="#13-2-iLQR" class="headerlink" title="13.2 iLQR"></a>13.2 iLQR</h3><p>iLQR算法的全称为iterative Linear Quadratic Regulator，是一种Model-<br>based的规划方法，它使用确定的策略函数和状态转移函数建立起序列的全过程。由于这部分知识最早与控制论相关的内容有关，所以它的变量表示法和前面章节中强化学习的表示方法有所不同，它的变量的对应方式如下所示。</p>
<p>观测状态：s→x。</p>
<p>行动：a→u。</p>
<p>回报（损失）函数： _r_ （s _，_ a）→ _c_ （x _，_ u）。</p>
<p>状态转移函数：st+1 ～ _p_ （st+1 |st _，_ at ）→xt+1 = _f_ （xt _，_ ut ）。</p>
<p>在控制论中，损失函数 _c_ （x _，_ u）<br>表示执行行动后产生的代价。因此强化学习的目标——“最大化长期回报”变成了“最小化长期损失”。它要解决的问题形式如下所示：</p>
<p>因为状态转移函数已知，所以目标函数也可以写作</p>
<p>第6章介绍了状态转移概率已知的问题。那么这个问题和第6章介绍的问题有什么区别呢？第6章介绍的问题在状态和行动空间上都是离散的，而本章介绍的问题在状态和行动空间上都是连续的，而且状态转移模型是一个确定的模型。在现实中我们也会经常遇到这样的问题，例如操纵一个机器人在一个环境中实现某个任务，由于任务的全部物理过程是已知的，所以状态转移函数是确定的；同时，我们也可以构建确定的策略函数，这样完整的交互序列可以由公式表示。可以看出这个问题和第6章提到的蛇棋问题的形式不同，当然二者的算法也不一样。</p>
<h4 id="13-2-1-线性模型的求解法"><a href="#13-2-1-线性模型的求解法" class="headerlink" title="13.2.1 线性模型的求解法"></a>13.2.1 线性模型的求解法</h4><p>我们先从一个比较简单的问题入手，介绍在计算过程上相对简单一些的算法LQR。假设损失函数是一个二次函数，而状态转移函数是一个一次函数，那么两个公式可以写作</p>
<p>这样定义后，目标函数中唯一未知的变量就是行动ut<br>，我们可以通过求解公式的极值找到最优的行动。为了让问题看上去不那么复杂，我们先不考虑较长序列的问题，而是把行动轨迹限定在两个时间段内，那么整个序列为（x1<br>_，_ u1 _，_ x2 _，_ u2 ），我们要推导出当x1 已知时使损失函数最小化的行动，即求出公式的最优解</p>
<p>如果可以从这个问题中找到规律，就可以把求解公式推广到更长的行动序列上。将公式完全展开，可以得到</p>
<p>公式中的C和c可以展开为</p>
<p>其中矩阵C是一个对称矩阵，也就是说CxT，uT =CuT，xT 。F 也可以展开为</p>
<p>公式待求的是u1 _，_ u2 的值，对其分别进行求导，并使导数为0，就可以得到函数取得极值时两个变量的解，首先对u2 求导可以得到</p>
<p>再对u1 进行求导，可以得到</p>
<p>虽然我们得到了这两个变量的导数，但是它们的形式十分烦琐。两个行动的求解计算已经如此复杂，两个以上行动的计算应该会更复杂，而且每一时刻行动的导数计算公式都不相同，因此我们不能使用这样的方法对公式直接展开求解，而是需要使用一种更简洁且易操作的方法实现。这就是LQR方法要解决的问题。</p>
<p>LQR方法包含反向计算部分和前向计算部分，其中反向计算用于确定计算每一个时刻行动的参数值，而前向计算则根据初始状态和反向计算的参数得到所有时刻的行动。反向计算比较复杂，它通过迭代的形式完成计算，每一个迭代中，我们要完成一个时刻行动参数的计算，当这些参数计算完成后，我们就可以通过前向计算得到具体的行动值。迭代内的推导过程分为如下几个步骤。</p>
<p>（1）根据 _T_ 时刻的目标公式对u求导，计算使函数最优的u值。</p>
<p>（2）用x替换u，得到关于x的优化公式。</p>
<p>（3）将公式转换成 _t_ -1时刻变量x、u的形式。</p>
<p>下面就来介绍具体过程。第一步，在 _T_ =2时刻通过最小化 _C_ （x2 _，_ u2 ）求出u2 ，根据公式，可以得到</p>
<p>对其进行求导，可以得到</p>
<p>令函数的导数为0，可以得到关于u2 的公式：</p>
<p>为了简化表达式，用变量K2 和k2 代替公式中的表达式，可以得到</p>
<p>于是u2 的值可以表示为关于x2 的公式：</p>
<p>这样第一步就完成了。第二步的目标是得到关于x2 的优化公式，将上面关于u2 的公式代入损失函数中，可以得到</p>
<p>将其展开可以得到</p>
<p>这个公式展开后也非常复杂，我们同样需要对公式中的变量进行替换，使用变量V2 和v2 ，可以得到</p>
<p>公式又重新变成了十分简洁的形式：</p>
<p>这样第二步就完成了。接下来是第三步，即将目标函数转换成以x1 _，_ u1<br>表示的形式，将原公式中第二时刻的损失函数替换成前面计算的结果，就可以得到下面的形式：</p>
<p>可以看出公式等号右边包含三个项目，第一项和第二项是 _t_ =1时刻的行动损失，第三项是前面计算得到的 _V_ （x2<br>），将上面公式的第二项完全展开，可以得到</p>
<p>展开后的第二项和原公式中的前两项存在可合并的同类项，这样就可以做进一步的变量替换和整理，得到新的公式</p>
<p>实际上，到这里我们已经完成了一轮迭代的推演。我们发现上面的公式变成了只包含x1 和u1 的公式，可以使用第一步的计算方法求出使公式最优的u1 值：</p>
<p>而此时x1 已知，我们就可以直接得到u1 的值，然后根据状态转移函数得到x2 ，再利用参数 _K_ 2 和 _k_ 2 进行计算得到行动u2<br>。这其实就是前向计算的过程。理解了长度为2的序列，再去推导长度为 _N_ 的序列也变得不那么复杂，完整的算法过程如下。</p>
<p>算法 LQR反向计算算法</p>
<p>算法 LQR前向计算算法</p>
<p>for _t_ =1 to _T_ :</p>
<p>ut =Kt xt +kt</p>
<p>xt+1 = _f_ (xt _,_ ut )</p>
<h4 id="13-2-2-非线性模型的解法"><a href="#13-2-2-非线性模型的解法" class="headerlink" title="13.2.2 非线性模型的解法"></a>13.2.2 非线性模型的解法</h4><p>本节我们介绍iLQR算法。13.2.1节介绍的方法存在一个假设：损失函数为二次项函数，状态转移函数为一次函数。然而在很多真实场景下，我们得到的损失函数和状态转移函数并不一定满足上面的条件，例如它们并不是二次函数或者一次函数，而且问题的形式往往也会有一些不同的设定，例如最后一个状态并不需要给出行动，而是采用另一个损失函数直接计算最终状态的花费。由于LQR的推导相对复杂，在此我们可以借助前面的经验，对问题进行重新求解。</p>
<p>我们会对最终状态单独设定一个花费函数 _c_ N （x），用于计算其花费，而其他时刻的花费仍然使用 _c_ （x _，_ u）计算，这样 _N_<br>个时刻总体的花费就可以计算成</p>
<p>其中U<br>代表序列中执行的全体行动为了使用13.2.1节的方法进行求解，我们需要将问题转化为类似的形式，这就要用到泰勒展开这个工具。对于任意的损失函数和状态转移函数，我们可以将其近似地展开，首先是损失函数：</p>
<p>假设xt _，_ ut 和^xt _，_ ^ut 足够接近，那么两者的梯度也可以近似，我们因此可以得到</p>
<p>同理，我们可以对状态转移函数进行展开：</p>
<p>同损失函数类似，我们可以得到近似梯度的公式：</p>
<p>经过这样的转换，我们发现当函数的表示形式发生变化时，我们从直接求解xt _，_ ut<br>，变成了已知序列的情况下求解δx和δu。转换后的求解并不困难，我们可以初始化一个行动序列，然后从初始状态s1<br>出发通过状态转移函数得到这样的序列。为了展示公式的推导过程，我们还是假设一个相对简单的序列{^x1 _，_ ^u1 _，_ ^x2 _，_ ^u2 _，_<br>^x3 }，从这个序列上得到的公式很容易应用到解决复杂的问题上。</p>
<p>下面就来使用和13.3.1节类似的求解方法进行计算。首先对x3 进行优化，可以直接得到</p>
<p>由于我们直接得到了关于x3 的目标函数，所以直接进行13.2.1节中反向计算的第3步，将目标函数表示为x2 _，_ u2 的形式，此时的目标函数为</p>
<p>对其求导，将公式右边的第一项简写为c，第二项简写为f，可以得到上面函数的导数：</p>
<p>继续求二阶导，可以得到</p>
<p>知道了这些导数，就可以进行13.2.1节中反向计算的第1步，求出当前时刻的最优值u，这里我们实际求出的是最优的δu，令∇δu2 Q=0，可以得到</p>
<p>将公式进行整理，可以得到</p>
<p>接下来就是13.2.1节反向计算的第2步，整理公式得到关于x2 的目标函数：</p>
<p>同样，我们可以求出公式的一阶导和二阶导供后续使用：</p>
<p>有些文献也喜欢把这两个公式进行一定的变换，将其变为</p>
<p>感兴趣的读者可以证明前后两组公式的等价性。这样，一个完整循环的计算内容就结束了。对于更长的循环，我们也可以使用这样的方式进行计算。需要注意的是，这一次我们得到的结果不是行动本身，而是最优行动对于当前行动的偏差。由于偏差采用梯度的形式表示，因此在更新时我们可以进行多轮迭代更新，这样算法就变成了iterative<br>LQR。</p>
<h4 id="13-2-3-iLQR的实现"><a href="#13-2-3-iLQR的实现" class="headerlink" title="13.2.3 iLQR的实现"></a>13.2.3 iLQR的实现</h4><p>下面我们介绍iLQR的实现，这里参考https：//github.com/neka-nat/ilqr-gym中的代码实现。这个项目主要实现了两部分功能。</p>
<p>1.基于连续行动的平衡车环境</p>
<p>Gym中实现了经典的控制任务CartPole，环境中有一个小车，这个车只能左右移动，车的中心有一根钉子，这个钉子钉住了一个棍子的一头，对应的界面如图13-7所示。我们的目标是控制小车使棍子竖立在空中并保持一定时间，如果可以做到这一点，那么Agent就可以获得分数；如果棍子失去平衡掉落，那么Agent将无法获得分数。整个环境由一些物理规则实现，每一时刻Agent将获得4个状态观测值：小车的位置、小车的速度、棍子对应的角度，以及棍子的角速度。Agent根据状态值给出对小车施加的力的大小。</p>
<p>图13-7 CartPole界面图</p>
<p>2.iLQR算法</p>
<p>这里我们直接将源代码展现出来，代码中同时包含对变量的介绍。首先是反向计算部分，这一部分比较复杂。</p>
<p>代码函数介绍</p>
<p>其次是前向计算。我们可以将反向计算求解出的参数带入前向计算中，这样就得到了更新后的行动：</p>
<p>经过一段时间的训练，模型就可以控制小车将棍子稳定地竖立在空中。感兴趣的读者可以亲自尝试。</p>
<p>LQR属于微分动态规划（Differential Dynamic<br>Programming，简称DDP）中的一种算法，这样我们就能把第6章中的动态规划法和本节介绍的方法统一起来，两者是针对不同空间的动态规划方法。关于DDP的内容还有很多，其中还会涉及很多很复杂的内容，由于篇幅的原因这里不做深入介绍。</p>
<h3 id="13-3-总结"><a href="#13-3-总结" class="headerlink" title="13.3 总结"></a>13.3 总结</h3><p>本章我们主要介绍了模型已知问题的求解方法，总结如下。</p>
<p>（1）AlphaZero采用了基于MCTS的自我学习方法进行策略学习。</p>
<p>（2）iLQR通过依时间迭代计算的方式对控制类问题进行规划求解。</p>
<h3 id="13-4-参考资料"><a href="#13-4-参考资料" class="headerlink" title="13.4 参考资料"></a>13.4 参考资料</h3><p>[1] Silver D,Huang A,Maddison C J,et al.Mastering the game of Go with deep<br>neural networks and tree search[J].Nature,2016,529(7587):484-489.</p>
<p>[2] Silver D,Schrittwieser J,Simonyan K,et al.Mastering the game of Go without<br>human knowledge[J].Nature,2017,550(7676):354-359.</p>
<p>[3] Silver D,Hubert T,Schrittwieser J,et al.Mastering Chess and Shogi by Self-<br>Play with a General Reinforcement Learning Algorithm[J].2017.</p>
<p>[4] Browne C B,Powley E,Whitehouse D,et al.A Survey of Monte Carlo Tree Search<br>Methods[J].IEEE Transactions on Computational Intelligence&amp;Ai in<br>Games,2012,4:1(1):1-43.</p>
<p>[5] Tassa Y,Erez T,Todorov E.Synthesis and stabilization of complex behaviors<br>through online trajectory optimization[C]//Ieee/rsj International Conference<br>on Intelligent Robots and Systems.IEEE,2012:4906-4913.</p>
<h1 id="第五部分-反向强化学习"><a href="#第五部分-反向强化学习" class="headerlink" title="第五部分 反向强化学习"></a>第五部分 反向强化学习</h1><p>第五部分将介绍反向强化学习的知识。反向强化学习同样基于马尔可夫决策过程，不同的是过程中的回报是未知的，需要通过最优策略的信息推断出回报的形式。很多任务都存在回报形式不明，最优策略形式未知的问题，这些任务都可以通过强化学习与反向强化学习结合的方式求解。</p>
<p>第五部分共包含两章：</p>
<p>· 第14章将介绍反向强化学习的基础，其中包括问题的基本求解方法。</p>
<p>· 第15章将介绍更高级的算法，包括基于最大熵的反向强化学习（MEIRL）和生成对抗模仿学习（GAIL）。</p>
<h2 id="14-反向强化学习入门"><a href="#14-反向强化学习入门" class="headerlink" title="14 反向强化学习入门"></a>14 反向强化学习入门</h2><p>看名字就知道，反向强化学习和强化学习有很密切的关系。本章我们要解决的问题将变得更极端，在与环境交互的过程中，我们无法感知回报，但幸运的是，我们可以使用反向强化学习来解决这个问题。本章我们将介绍反向强化学习的基本概念及基本的解决方法，包括反向强化学习的定义和基本解法。虽然这些方法在实际问题中已经很少使用，但它是了解更高级方法的基础，值得我们深入理解。</p>
<h3 id="14-1-基本概念"><a href="#14-1-基本概念" class="headerlink" title="14.1 基本概念"></a>14.1 基本概念</h3><p>反向强化学习也被称为模仿学习（Imitation Learning）或学徒学习（Apprentice Learn-<br>ing），它是解决另一类强化学习问题的方法。既然如此，我们就回到强化学习的问题范畴中。在介绍强化学习的过程中，我们了解了它的基本结构：Agent、环境、状态、行动和回报。通过这些概念，我们将整个强化学习的基础搭建好，本书前面介绍的算法都在这个框架中执行。</p>
<p>那么这个框架究竟好不好呢？当然是好的，它已经帮助我们解决了很多实际问题。例如，很多基于强化学习的电子游戏机器人已经达到了游戏高手的水平；但是我们也必须看到，游戏是强化学习应用比较广泛的一个领域。那么我们不禁去思考一个问题：为什么强化学习在游戏方面斩获颇丰呢？</p>
<p>这就要回到问题的本源回报上来。在游戏设计的世界中，回报是一个极其关键的因素，游戏之所以吸引人，其中一个很重要的原因就是它会为玩家每一个小阶段的表现赋予一个评价或者得分，这些得分可以很及时地给予玩家被量化的回报，而这些量化的回报也恰恰是强化学习算法十分需要的。</p>
<p>回到现实生活中，我们生活的真实世界——到目前为止，还无法用量化的回报清晰地表示一切行动。很多时候，我们需要人为地制定一些量化回报的规则，才能进行强化学习训练。但实际上，这些规则有时并不容易制定。</p>
<p>以游泳为例，人类是如何学习游泳的呢？如果我们报名游泳辅导班，老师会先带领我们进行监督学习——在岸上划臂蹬腿，然后直接指出我们的动作和标准动作的差距。随着我们不断地学习，我们的姿势也在不断提高。</p>
<p>而独立学习游泳时，我们就进入了另外一种学习模式。如果把独立学习游泳想象成强化学习，那么我们每完成一次游泳的动作都应该获得一定的反馈，但事实上，在真实的游泳过程中，这种反馈是很难获得的。我们还可以通过和小伙伴进行游泳比赛，知道谁游得快、谁游得慢，从而了解自己在游泳时的长处与不足，但是这个过程就像第12章介绍的问题一样，整个过程的反馈十分稀疏，甚至不足以为学习提供足够的信息。这样看来，独立学习游泳实在是一件事倍功半的事情。</p>
<p>那么反向强化学习能不能解决这个问题呢？反向强化学习的思路和强化学习相反，强化学习通常是在回报已知的情况下求出值函数和策略，但如果回报不容易得到，我们就得创造一些回报，但如果回报完全无法获得，学习就会困难很多。反向强化学习使用了逆向思维：我们不通过回报求策略，而是反过来，通过策略求回报。</p>
<p>这听上去有些荒谬，我们想通过交互获得回报，就是为了求解更好的策略；现在有了策略，还回去求回报干什么呢？当然，很多时候最优或者接近最优的策略是存在的，但是它不属于我们；或者说虽然我们无法具体表示出这种策略，但是最优策略的采样是可以得到的。回到游泳学习上，虽然我们不像游泳健将那样拥有超高的游泳“策略”，可以通过肌肉的控制力和爆发力完成完美的泳姿，但是我们可以拿起手中的拍摄装置，或者打开电视和视频网站，观看这些人游泳比赛的慢动作回放，从而掌握这个策略与环境的“交互数据”。这些交互数据就可以帮助我们提高游泳的能力。</p>
<p>实际上，很多人也是用这种方法学习游泳的：在家认真观看高手的动作要领，并铭记于心，最后回到泳池一试身手。实际上，这就是一个完整的从反向强化学习到强化学习的过程。首先通过最优策略的交互样本学习到回报函数——什么样的姿势是正确的，应该获得更高的回报；什么样的姿势不正确，应该获得较低的回报。接下来就可以利用学习到的回报函数进行尝试，使用强化学习的方法学习到策略。如果再进行一次反向强化学习的过程，通过分析高手的动作和自己动作的差异，我们就可以得到新的回报函数。新的回报函数将再次用于改进策略，这样不断迭代，就可以完成策略的学习。</p>
<p>将上面的过程整理出来，可以得到下面的流程：</p>
<p>（1）随机生成一个策略作为初始策略。</p>
<p>（2）通过比较“高手”的交互样本和自己交互样本的差别，学习得到回报函数。</p>
<p>（3）利用回报函数进行强化学习，提高自己策略的水平。</p>
<p>（4）如果两个策略的差别已经不大，就可以停止学习了，否则回到第（2）步。</p>
<p>以上是从通俗的角度对反向强化学习的介绍，下面将从数学视角出发，对这个问题进行深入分析。</p>
<h3 id="14-2-从最优策略求解回报"><a href="#14-2-从最优策略求解回报" class="headerlink" title="14.2 从最优策略求解回报"></a>14.2 从最优策略求解回报</h3><h4 id="14-2-1-求解回报的目标函数"><a href="#14-2-1-求解回报的目标函数" class="headerlink" title="14.2.1 求解回报的目标函数"></a>14.2.1 求解回报的目标函数</h4><p>14.1节介绍了反向强化学习的基本概念，也介绍了它和强化学习的区别。对于一个强化学习的问题，如果状态和行动空间已知，状态转移概率确定，可以通过回报函数求出最优策略；而对于反向强化学习来说，则通过最优策略或最优策略的行动采样，求出回报函数。本节将介绍反向强化学习中最简单的问题：已知最优策略，求回报函数。</p>
<p>如果我们已知了状态和行动空间、状态转移概率和最优策略 _π_ ∗<br>，且状态和行动的空间都是离散的，那么我们该如何求解呢？我们使用表格的形式表示策略与值函数，这种表示形式在第6章和第7章已经介绍过。根据最优策略的定义我们知道，最优策略<br>_π_ ∗ 和其他策略 _π_ 相比，对于每一个状态，其对应的值函数都应不弱于其他策略的值函数。由此可以得到</p>
<p>如果令S为所有的状态集合，A为所有的行动集合，那么对于任意的状态，可以得到</p>
<p>我们知道最优策略的每一步行动都应该不弱于其他策略的行动，那么可以得到对于任意的状态，完全使用最优策略的价值，应该不弱于“在当前时刻使用最优策略外的行动，此后使用最优策略进行决策”的价值。这个条件可以通过反证法得到，如果这一条件不能满足，那么实际上最优策略就不是最优了。上面的条件可以写作：</p>
<p>我们可以利用Bellman公式将上面的不等式展开，得到</p>
<p>如果用列向量Ps，a 表示由 _P_ （s′ |s _，π_ ∗ （s））组成的向量，用列向量V 表示由 _V_ π ∗ （s′<br>）组成的向量，那么公式就可以变为</p>
<p>这个公式里的值函数 _V_ 是未知的，但好在可以用Bellman公式进行求解：</p>
<p>将这个公式转成矩阵和向量计算的形式，用 _π_ 表示行动概率向量 _P_ （a|s），就可以得到</p>
<p>由于我们在考虑表格式策略的值函数，所以策略是确定的，公式可以进一步化简为</p>
<p>当值函数收敛时，公式中的两个价值向量是相等的，于是公式可以整理为关于价值向量的形式：</p>
<p>把这个公式带回前面的不等式中，就得到了我们初步的目标函数：</p>
<p>于是这个问题变成了一个线性规划问题，我们要找到合适的回报向量满足这个不等式。经过上面的推导，我们终于完成了问题定义的第一步，通过最优策略的定义和Bellman公式，我们将其表达出来。</p>
<h4 id="14-2-2-目标函数的约束"><a href="#14-2-2-目标函数的约束" class="headerlink" title="14.2.2 目标函数的约束"></a>14.2.2 目标函数的约束</h4><p>虽然上面的目标函数清晰地阐述了求解的思路，但由于这个目标只是一个不等式，对待求的回报向量来说，这个约束有点弱。显然我们可以找出很多种回报函数组合满足这个不等式，其中还有一些是平凡解，例如全部为0回报函数，这样的解同样可以满足不等式，但实际上它是没有意义的。于是我们需要思考如何增强问题的约束。</p>
<p>我们将为目标函数增加两个约束：1）限制回报的范围；2）只考虑最优策略和次优策略的差异。</p>
<p>首先是限制回报的范围。由于这是一个线性规划问题，回报向量可以在成比例扩大的同时满足约束条件，因此我们应该对回报数值的上界做一定的限制，于是就有了这样的约束：</p>
<p>这种增加约束的方法在机器学习中普遍存在，这和L1、L2正则的功能比较相似，都是为了防止参数太大。由于这种方法在机器学习的很多算法上都得到了不错的效果，所以它在这里的效果应该也不会差。</p>
<p>其次考虑最优策略的比较对象。在前面的目标不等式中，我们需要最优策略强于所有的策略，这个约束并没有错，但在实际中，如果我们以相同的权重考量其他策略和最优策略的差距，就显得不那么合理了。实际上，最优策略和次优策略的价值差会显得更重要，而最优策略和其他很差的策略的价值差并没那么重要，所以实际上最优策略只要尽可能地比次优策略强就可以了，和其他的策略进行比较并不应该拥有很高的权重。</p>
<p>我们以一个棋类AI的表现为例解释价值差异的权重问题。假设有两个策略，分别为 _A_ 和 _B_<br>，它们分别与普通玩家和高级玩家对弈，最终的战绩如表14-1所示。</p>
<p>表14-1 策略 _A_ 和 _B_ 分别与普通玩家和高级玩家对弈的战绩</p>
<p>这里的分数表示了两者的差距，分数为正表示AI最终胜利。可以看出，策略 _A_ 获得了更高的分数，却和高级玩家打平；策略 _B_ 的总得分虽然不及策略 _A_<br>，但是它获得了两场胜利，这说明策略 _B_ 与策略 _A_ 相比，更像一个最优策略，即使它并不能从普通玩家手上获得太多的分数。</p>
<p>根据上面的分析，我们将目标函数变为</p>
<p>从公式中可以看出，我们先通过内层的min运算选出与最优策略差距最小的策略，再通过外层的max运算求解Reward使最优策略与它的差距变大，这样Reward的求解就变得更明确了。</p>
<p>将两个约束加入原问题，此时完整的问题就变为</p>
<p>这样我们就得到了这个问题的完整目标，14.3节将介绍这个问题的具体求解方式。</p>
<h3 id="14-3-求解线性规划"><a href="#14-3-求解线性规划" class="headerlink" title="14.3 求解线性规划"></a>14.3 求解线性规划</h3><h4 id="14-3-1-线性规划的求解过程"><a href="#14-3-1-线性规划的求解过程" class="headerlink" title="14.3.1 线性规划的求解过程"></a>14.3.1 线性规划的求解过程</h4><p>14.2节介绍了反向强化学习的公式形式，接下来完成求解。这个问题本质上是一个线性规划的问题，但目前的公式形式还不是一个易于求解的形式，于是我们要将公式转变，采用变量替换的方式，将问题变换成标准的线性规划形式。这里涉及两个变量替换：</p>
<p>于是可以得到</p>
<p>当我们将公式变成这个形式后，就可以进行求解了。我们可以从项目https：//github.com/MatthewJA/Inverse-<br>Reinforcement-<br>Learning中找到关于这个问题的求解，具体的实现在irl/linear_irl.py中的irl函数，下面笔者就来介绍它是如何进行求解的。</p>
<p>代码中使用了cvxopt这个库，它用来实现常见的凸优化算法，其中也包括线性规划的求解方法。想要使用这个算法库进行求解，就需要将问题整理成下面的形式：</p>
<p>其中A为矩阵，b、c为向量。最终我们将A、b、c三部分输入函数中，就可以利用算法库中的线性规划算法进行求解。</p>
<p>那么，上面的公式该怎么变成这个样子呢？为了方便计算，此时我们共有三组变量R、T 和U，三组变量的长度都是|S|，所以我们的目标函数变为</p>
<p>接下来按照类似的形式，我们得到下面的条件约束：</p>
<p>上面的公式无法详细展示计算过程，于是我们画出了问题的具体计算形式和每一个计算量的维度，如图14-1所示，其中 _T_ （a _，_ s）表示公式 （ _P_<br>s，π ∗ （s） （ _i_ ） _-P_ s，a （ _i_ ））（I _-γ_ Pπ ∗ ）-1<br>R的计算值，对于每一个状态，我们都要让最优策略的行动和其他行动进行比较，因此这个公式维度为|S|×（|A|-1）。</p>
<p>按照图14-1所示将数据组织好后，就可以求解得到回报向量了。</p>
<p>图14-1 线性规划运算的数据结构</p>
<h4 id="14-3-2-实际案例"><a href="#14-3-2-实际案例" class="headerlink" title="14.3.2 实际案例"></a>14.3.2 实际案例</h4><p>看完了上面的算法，我们再来看一个具体的例子，这个例子是Grid World。Grid World是一个很简单的问题，我们给定一个 _N_ × _N_<br>的棋盘格，这个棋盘格上的每一个格子都可以作为落脚点。我们以5×5的格子为例，如图14-2所示。</p>
<p>站在棋盘的格子上会产生不同的效果，棋盘格的右下角是一个会得到奖励的格子，站在上面会获得1分，游戏结束；站在其他的格子上不会获得奖励，游戏仍会继续。</p>
<p>玩家在初始状态下会随机站在某个格子上，接下来的每一个时刻，玩家都可以做出决定，选择上下左右4个方向行进。世事无常，有时玩家并不能到达想去的地方，游戏中有一股“妖风”，它会以一定的概率触发。触发后玩家的行进将不受控制，会任意选择一个方向前进，如果撞到棋盘的边缘，玩家将被阻挡在那里，停止前进。</p>
<p>整个游戏的规则并不复杂，从在玩家的角度看，因为右下角是得分点，因此只要不停地向那个方向前进就可以了；如果已经到达了那里，那么保持停留就可以。</p>
<p>图14-2 Grid World的游戏形式</p>
<p>我们换一个思路：我们并不知道游戏的得分规则，只能看到一个老玩家的操作。我们能不能通过老玩家的操作推断出得分规则呢？这就是基于Grid<br>World的反向强化学习问题。</p>
<p>由于这个问题并不复杂，我们可以用表格的方式表示问题中的策略、回报和价值向量，运用上面的算法，我们可以得到最终的结果，如图14-3所示。</p>
<p>图14-3 经过计算得到的Grid World回报图</p>
<p>从结果可以看出，我们复原的这个5×5的Reward结果和真实结果相差很小，或者说这点差距并不能阻碍我们选出正确的策略。以上就是反向强化学习的基本解法。</p>
<h3 id="14-4-无限状态下的求解"><a href="#14-4-无限状态下的求解" class="headerlink" title="14.4 无限状态下的求解"></a>14.4 无限状态下的求解</h3><p>14.3节介绍了反向强化学习的基本解法，但是它所应用的场景比较有限，以状态空间为例，我们需要状态空间中状态的数量是有限的，行动的数量也是有限的。遇到无限状态空间的问题时，该如何确定回报函数呢？</p>
<p>首先要解决的是回报的表示问题。回报函数是一个从状态到回报的映射函数，在14.3节的问题中，由于状态数量是有限的，所以这个函数可以以向量的形式表示出来；但是现在状态数量不再是有限的，显然不能继续用表格的形式表示，我们必须做出改变。</p>
<p>为了解决这个问题，我们可以定义一个映射函数 _φ_<br>（s），它可以把状态映射到一个低维空间，再进行计算。映射函数的具体形式可以根据问题的具体情况定义，也可以把这部分想象成特征工程要完成的工作：将无限维的状态用有限维的特征表示出来，这样回报函数可以用一个线性函数表示，这个函数的形式为</p>
<p>于是我们的问题就变成了通过最优策略求解参数 _w_ 来确定回报函数。</p>
<p>读者可能会有一个疑问，为什么要把这个函数定义为一个线性函数，而不是其他的函数呢？当然是有原因的：这个函数的形式对后面的计算具有一定的简便性。我们可以将上面的公式展开，假设每一个状态被映射到了一个<br>_d_ 维的向量上，那么求解回报就相当于进行如下计算：</p>
<p>我们又知道值函数的计算方式为</p>
<p>将两个公式融合，可以得到</p>
<p>根据上面的变换，我们发现值函数被整理成了一个比较简单的形式，其中</p>
<p>表示某一维映射特征的累积值。我们最终的目标同样是希望找到最佳的回报函数，使最优策略的价值最大化，也就是说：</p>
<p>最终的求解目标为</p>
<p>这个目标函数与14.3节的目标函数有3个不同。</p>
<p>第1个不同是状态的数量，由于状态空间变得非常大，我们无法将所有的状态纳入考虑范畴，因此只能考虑有限个状态，在公式中用S0<br>表示所有参与优化的状态，这是全部状态的子集。</p>
<p>第2个不同是对回报的约束。在14.3节的算法中，我们直接对回报的值进行设定，而对本节的目标函数来说，我们再对回报值做设定将变得有些困难，因此这里直接对回报函数的参数的大小进行约束，这样约束就更像L1范数约束了。</p>
<p>第3个不同是由于函数形式变换带来的修改。第7章介绍过表格式模型和函数式模型的特点，这些特点在这里同样适用。表格式模型虽然在一些问题上难以使用，但它可以完美地拟合真实的回报函数；而函数式模型由于可变量有限，并不能保证完美拟合真实的回报函数。一旦无法完美拟合，函数式模型就有可能出现违背约束条件的情况，即可能出现“最优策略”不是最优的情况。为了解决这个问题，我们需要将约束项转变为惩罚项，对模型中违反约束条件的部分进行高权重的惩罚。这里我们使用了惩罚函数<br>_p_ （ _x_ ），它的形式为</p>
<p>接下来，我们就要对这个问题进行求解。从目标公式上看，这个问题和上面的问题形式差不多：同为线性规划，可以用同样的方法求解。我们可以从项目https：//github.com/MatthewJA/Inverse-<br>Reinforcement-<br>Learning中找到关于这个问题的求解，具体的实现在irl/linear_irl.py中的large_irl函数，下面笔者就来介绍它是如何进行求解的。</p>
<p>为了将目标函数转变成优化库可以求解的形式，我们要先解决 _p_ （gaps，a ）这个函数的表示问题，其中。它包含一个分支条件，因为我们有| _S_ 0<br>|个状态，| _A_ |-1个非最优策略的行动，共有| _S_ 0 |×（| _A_ |-1）个状态价值比较对，于是我们就需要设立2| _S_ 0 |×（|<br>_A_ |-1）个变量表示这些比较对可能的绝对值。我们令表示gap 为正时的绝对值，也就是说表示gaps，a 为负的绝对值，即，这样就产生了下面五个约束：</p>
<p>完成这一步的变换，下一步要做的就是描述最优比次优好的条件，我们用另一个变量 _u_ s 表示mina∈{a2，…，ak} { _p_ （gaps，a<br>）}，根据最小值的约束，可以得到</p>
<p>由于前面约束的存在，我们可以将其拆成两个约束：</p>
<p>最后我们的目标就是最大化每一个 _u_ s ，所以完整的线性规划问题变为</p>
<p>得到了这个完整的公式后，我们就可以进行进一步地计算了。代码的作者在函数中实际上计算了gaps，a<br>的相反数，但约束条件与上面的推导类似，请读者自行分析其中的不同。</p>
<h3 id="14-5-从样本中学习"><a href="#14-5-从样本中学习" class="headerlink" title="14.5 从样本中学习"></a>14.5 从样本中学习</h3><p>前面几节提到了两种反向强化学习方法，虽然我们已经将问题的规模扩展到了无限维的状态空间，但这和我们的真实情况还有一定的差距。毕竟在问题中，我们已经知道了最优策略的细节，而在很多情况下，我们最终的目标是求解最优策略，也就是说，最优策略是未知的。</p>
<p>本节将不再直接获得问题的最优策略，而是获得最优策略的交互样本。就像笔者提到过的：我们并不知道最佳的游泳姿势策略是怎样的，但是我们可以获得最佳游泳姿势的样本——世界冠军的游泳姿势，而这才是我们最常见的问题形式。所以从本节起，我们才真正开始了对真实反向强化学习问题的探索。</p>
<p>对于每一个最优策略的采样序列，我们可以将其列为</p>
<p>我们可以设定它是有限的。通过对序列进行值函数的计算，我们得到了这个序列的值函数：</p>
<p>如果有某个策略函数 _π_ ，从状态s0 开始，令策略 _π_ 和环境进行交互，最终产生了如下状态序列：</p>
<p>这个序列的值函数为</p>
<p>得到了两个序列的值函数，就可以使用和前面章节一样的目标：最优策略的价值不低于其他策略。由于我们只有策略 _π_ ，于是就让策略 _π_<br>作为“其他策略”的代表。我们令最优采样的序列为D，那么根据最优策略的定义，可以得到如下目标函数：</p>
<p>看到这个目标函数，相信读者已经有点熟悉了。在14.4节中，我们也得到了类似的目标函数，使用了一个映射函数，将无限维的状态映射到有限维的空间中。当然，这个公式和14.4节的形式存在一些不同。在14.4节中，我们需要让最优策略尽可能地大于次优策略，但是现在我们无法找到次优策略，只能使用策略<br>_π_ 。</p>
<p>14.1节介绍了反向强化学习和强化学习结合的算法，也列出了算法的详细步骤。我们从一个随机采样得到的策略出发，让这个策略从最优策略采样的起始状态出发进行行动决策，我们用这个随机策略得到了一系列行动轨迹；有了行动轨迹，就可以用反向强化学习的模型求解回报函数了；当我们得到了回报函数，就可以利用强化学习对现有的策略进行改进，前面的章节已经了解了很多改进的方法，这里不再赘述。策略完成改进，就可以以这个策略重新完成一轮优化，也就是再进行一次采样、回报函数优化和强化学习改进。这样一轮一轮地不断改进，就可以让策略的效果不断提升。随着策略的不断提升，它产生的序列的回报总和会越来越靠近最优策略轨迹的回报，策略模型也会越来越接近“次优策略”，甚至超过次优策略，因此对应的回报函数也会估计得更准确，这个过程可以用图14-4表示。</p>
<p>图14-4 基于序列样本的强化学习流程</p>
<p>最终，我们得到了两个结果。</p>
<p>（1）精确的回报函数，它使得最优策略采样不弱于此前的所有策略函数。</p>
<p>（2）一个无限接近最优策略的策略函数，经过若干轮的迭代优化，它的效果不断增强，直至无法提高。</p>
<p>以上就是针对这类问题的解决方案，更多反向强化学习的算法将会在第15章介绍。</p>
<h3 id="14-6-总结"><a href="#14-6-总结" class="headerlink" title="14.6 总结"></a>14.6 总结</h3><p>本章介绍了反向强化学习的基本知识，让我们总结一下。</p>
<p>（1）反向强化学习主要用于求解环境回报的问题，为了求解回报，我们需要知道最优策略或者最优策略的交互序列。</p>
<p>（2）为了学习状态回报，我们需要确保最优策略下的状态价值不小于其他策略。</p>
<p>（3）为了使求解出的回报更有意义，我们需要限定回报的上限，同时关注最优策略和次优策略在状态价值上的关系。</p>
<p>（4）当需要基于最优序列样本学习策略时，我们可以结合反向强化学习和强化学习，共同提高回报函数的精确度和策略的效果。</p>
<h3 id="14-7-参考资料"><a href="#14-7-参考资料" class="headerlink" title="14.7 参考资料"></a>14.7 参考资料</h3><p>[1] Ng A Y,Russell S J.Algorithms for Inverse Reinforcement<br>Learning[C]//International Conference on Machine Learning.2000:663-670.</p>
<p>[2] Abbeel P,Ng A Y.Apprenticeship learning via inverse reinforcement<br>learning[C]//In-ternational Conference on Machine Learning.ACM,2004:1.</p>
<h2 id="15-反向强化学习算法2-0"><a href="#15-反向强化学习算法2-0" class="headerlink" title="15 反向强化学习算法2.0"></a>15 反向强化学习算法2.0</h2><p>第14章是对反向强化学习的基础进行介绍，相信读者对反向强化学习已经有了一定的了解。第14章介绍的解法相对简单，可以解决小规模的问题，而解决大规模的问题时就有些吃力了。本章的目标是学习更好的反向强化学习算法，它们可以解决更复杂的问题。这些方法并不是这个领域独有的，而是在其他机器学习领域得到了一定的验证，获得了一定的成功后，才被应用到这个领域上的。由于“站在巨人的肩膀上”，这些方法的效果也得到了肯定。首先要介绍的是基于最大熵的反向强化学习方法（Max<br>Entropy Inverse Reinforcement<br>Learning），它能够从多个满足限定条件的策略中寻找一个更合理的策略。由于最大熵的算法有一点复杂，因此我们要先对它的基本原理进行介绍。然后介绍基于生成对抗的模仿学习（Generative<br>Adversarial Imitation<br>Learning），它使用了近年来大获成功的生成对抗模型的思想和模型结构，通过挖掘它和反向强化学习的相似性实现了类似的方法。</p>
<h3 id="15-1-最大熵模型"><a href="#15-1-最大熵模型" class="headerlink" title="15.1 最大熵模型"></a>15.1 最大熵模型</h3><p>想要了解基于最大熵的反向强化学习，先要了解最大熵模型。实际上，最大熵的思想是一个很常见、也很符合直觉的算法。为了更深刻地理解最大熵模型，我们还要介绍一个和最大熵模型相伴的概念——指数家族。指数家族是一个包含我们常见的概率分布的分布族，当我们理解了这个神奇的家族后，最大熵也就变得不那么神秘了。</p>
<h4 id="15-1-1-指数家族"><a href="#15-1-1-指数家族" class="headerlink" title="15.1.1 指数家族"></a>15.1.1 指数家族</h4><p>第2章介绍了一些概率分布模型，其中包括离散概率分布的代表伯努利分布和连续概率分布的代表高斯分布。虽然这两个分布的形式不同，但它们存在着一定的相似性，那就是——它们都属于指数家族。这个家族是一个经典且独具特色的家族，其中包含了很多我们常见的模型。这个家族中的分布拥有一些很有趣的性质，这些性质有时可以帮助我们解决一些大问题。</p>
<p>首先我们来看看这个大家族的样子。既然它可以表示很多概率分布，那么它一定拥有一个十分抽象的形式。这个形式抽象到让我们很难将它和它的成员联想到一起的地步。指数家族的基本公式形式为</p>
<p>公式中的每一项都有自己的名字。让我们从左往右依次介绍： _h_ （ _x_ ）一般被认为是一个简单的乘子项，很多时候它只是一个常量。 _θ_<br>表示模型中的参数， _φ_ （ _x_ ）被称为函数的充分统计量（Sufficient Statistic）， _A_ （ _θ_<br>）被称为对数分割函数（Log Partition Function），一般被用来归一化所有的概率项，它的具体形式为</p>
<p>读者一定觉得这个形式十分陌生，它怎么会和我们常见的模型产生联系呢？这个公式如何与我们常见的模型对应呢？我们就以上面的两个分布为例，通过变换找出它们与指数分布的联系。首先是伯努利分布，它的形式为</p>
<p>其中 _p_ 表示 _X_ =1时的概率。将这个公式转变，可以变成类似指数分布的形式：</p>
<p>和指数分布的形式对比，我们发现其中的每一项可以做如下替换：</p>
<p>这样我们可以用指数家族的标准形式表示伯努利分布。虽然看上去这样的表示有些复杂，但是它们在形式上还是完成了统一。同样地，高斯分布也可以变换成类似的形式：</p>
<p>和指数分布的形式对比，我们发现其中的每一项可以做如下替换：</p>
<p>我们发现，本身有些复杂的高斯分布经过变换变成了更复杂的指数家族通用形式。这时读者一定会感到疑惑：为什么要将这些分布变换成这样的形式呢？因为指数分布具有一些非常好的性质，有时它们可以帮我们解决大问题。下面就来看两个简单的性质。</p>
<p>第一个性质与对数分割函数有关： _A_ （ _θ_ ）的一阶导等于 _φ_ （ _x_ ）的期望。听上去有些不可思议，让我们对这个性质进行证明，可以得到</p>
<p>第二个性质同样与对数分割函数有关： _A_ （ _θ_ ）的二阶导等于 _φ_ （ _x_ ）的方差。这听上去就更不可思议了，但是它确实成立，可以得到</p>
<p>为了证明这个性质的效果，我们以伯努利分布为例进行求解，已知 _A_ （ _φ_ ）=log（1+exp（ _θ_ ））， _φ_ （ _x_ ）= _x_<br>，可以得到</p>
<p>我们发现利用上面的性质得到了正确的结果，这两个性质告诉我们，将概率分布以指数家族的形式表达后， _φ_ （ _x_<br>）的期望与方差实际上拥有两种解法，一种是直接使用这个函数进行求解，另一种则是采用对数分割函数进行求解。在真实的问题中，我们可以根据实际情况选择一种更简单的方法进行求解。</p>
<p>在接下来的章节里，我们要使用指数家族的另一种简单的形式，即：</p>
<p>可以看出这里的 _Z_ （ _θ_ ）就是由 _A_ （ _θ_ ）转变得到的。感兴趣的读者可以研究这两个公式的关系。</p>
<h4 id="15-1-2-最大熵模型的推导"><a href="#15-1-2-最大熵模型的推导" class="headerlink" title="15.1.2 最大熵模型的推导"></a>15.1.2 最大熵模型的推导</h4><p>介绍完指数家族的基本概念，让我们回到最大熵模型。2.4节介绍过熵的概念，它表示随机变量的不确定性。换句话说，它表示了知道随机变量结果的“代价”。熵的值越大，代表随机变量越“捉摸不定”，反之则越确定。人类在面对未知事物时，往往也倾向于产生怀疑的态度。</p>
<p>以最经典的投掷骰子的问题为例。当我们以“正常的手法”投掷出一枚骰子时，会认为六个面朝上的概率是相等的，因为没有任何证据表明其中的某一个面将会以更高的概率出现。这种平等看待所有未知情况的思想其实暗含了最大熵的思想，因为使熵最大的方法就是让所有的情况具有相等的概率，这样也最难推断出结果。如果我们知道其中的某一面会以1/3的概率出现，那么剩下的五个面将共享2/3的概率。此时，由于我们没有其他关于另外五面的信息，因此按照最大熵的思想，我们只能认为剩下的五个面将以等概率出现。</p>
<p>对机器学习来说，我们同样需要这种思想来帮助我们确定模型的形式。在我们建立一个概率模型时，往往会伴随一些约束，例如对数值的直接约束，要求与训练数据尽可能地接近等。有时，这些约束的存在并不能得到一个唯一的模型，满足这些条件的模型有很多种，这些模型在约束上的表现是基本一致的，而在没有约束的子空间上则表现得不尽相同。那么，根据最大熵的思想，没有约束的子空间应该拥有均等的概率。这样我们就可以确定一个唯一的概率分布，而这个概率分布就是我们想要的。</p>
<p>介绍完最大熵的思想，下面介绍模型的具体构建形式。假设一组数据（ _x，y_ ），它的输出是 _y_ ， _y_ 属于集合Y；它的输入是 _x_ ， _x_<br>来自集合X。我们的目标是构建一个模型，来表示 _x_ 和 _y_ 之间的关系，这里采用条件概率分布 _p_ （ _y_ | _x_<br>）进行构建。假设数据是离散的，最简单直接的方法就是基于这些数据，利用直方图计数的方式构建一个分布。对应的公式为</p>
<p>其中 _N_<br>表示数据总量，count（）表示给定的数据样本出现了多少次。这其实就是最简单的计算频率的公式。如果数据量十分充足，根据大数定理，直接用这个运算结果作为模型的参数就可以了，它可以很好地表达数据的真实分布。</p>
<p>在实际中，由于问题的规模一般很大，数据总是不够，我们仍然需要用建模的方法达到更好的模型效果。所谓更好的模型效果指的是机器学习中经常提到的泛化性，我们希望在数据缺失的区域也能达到很好的效果。由于数据可能来自高维的空间，为了将数据空间进行压缩，我们要通过特征的形式将数据进行转换。特征的一般形式为</p>
<p>我们可以根据问题的具体形式设立不同的特征，为每个特征定义不同的满足条件，这样每一对原始数据就可以通过这些特征转换成有限维空间中的数据。特征在训练数据上也具有一定的统计特性，我们定义“特征<br>_f_ （ _x，y_ ）关于训练样本分布 _p_ ～（ _x，y_ ）的期望”为</p>
<p>模型要建立概率分布 _p_ （ _y_ | _x_ ）与训练样本分布的特征期望对应，我们可以得到特征关于模型分布的期望值：</p>
<p>我们希望模型能学习到数据中的分布特性，于是我们可以设立一个约束：让模型分布的特征期望和训练样本分布的特征期望相等，于是我们就有了约束：</p>
<p>公式展开后可以得到</p>
<p>除了保持这个约束，我们还希望模型的概率分布能够满足最大熵的约束，于是我们的目标函数就是尽可能地增加概率分布的熵，于是可以得到公式</p>
<p>把所有的约束和目标完整地列出来，就得到了最大熵模型的完整形式：</p>
<p>完成了问题的定义，下面就来解决这个问题。我们采用朗格朗日乘子法，为每一个特征引入一个参数 _λ_ i ，得到拉格朗日函数 _ξ_ （ _p，_ Λ _，γ_<br>）：</p>
<p>其中Λ表示乘子 _λ_ 1 _，…，λ_ n 的集合。我们假设Λ和 _γ_ 不变，对 _p_ 求导，可以得到</p>
<p>令导数为0，可以得到关于 _p_ 的极值：</p>
<p>对公式进行整理，可以得到</p>
<p>如果我们再次使用前面关于概率总和为1的约束：</p>
<p>将公式进行带入求解，就可以得到</p>
<p>将这个公式带入上面的公式，得到</p>
<p>令 _Z_ （ _x_ ）为</p>
<p>模型的最优解为</p>
<p>看到最大熵模型的最优解形式时，相信读者会有种似曾相识的感觉，这不就是我们前面见到的指数家族的结构形式吗？经过前面拉格朗日乘子法的计算，变量由模型 _p_<br>变成了乘子 _λ_ ，而此时这个乘子更像是每一个特征的权重项，我们通过为每个特征乘以一个权重判断数据所属的 _y_ 的概率值。</p>
<p>既然我们已经得到了概率分布的形式，下面就使用最大似然法求解模型中的参数。将模型的结构展开，可以得到</p>
<p>我们的目标是最大化对数似然log _p_ （ _y_ | _x，λ_ ），可以得到</p>
<p>对公式进行求导，可以得到</p>
<p>可以看出，最大似然的梯度等于训练数据分布的特征期望与模型的特征期望的差。当梯度为0时，得到的概率分布刚好满足约束条件：</p>
<p>所以我们可以采用梯度下降的方法不断迭代更新参数 _λ_ 。</p>
<p>最后让我们验证前面留下的一个疑问，采用这种计算方法是否能保证求解结果唯一呢？下面我们来证明。我们已经通过优化得到了一个概率模型 _p_<br>，假设存在另外一个指数家族的概率模型 _p_ ^（ _y_ | _x_ ）=exp（^ _λ_ T _f_ （ _x，y_ ） _-A_ （ _λ_<br>））（我们将 _Z_ 转换成了 _A_ ），也能够在满足约束的条件下最大化目标函数，且参数与 _p_ 不同，那么可以得到</p>
<p>因为两个模型都达到了最优，所以可以得到两个模型的特征期望与训练数据相同：</p>
<p>所以可以得到</p>
<p>因为KL（ _p_ （ _x，y_ ）‖ _p_ ^（ _x，y_ ））≥0，所以 _H_ （ _p_ ）≥ _H_ （ _p_ ^），且当 _p_ =<br>_p_ ^时，两者的熵相等，这样就可以证明最优解是唯一的。</p>
<h4 id="15-1-3-最大熵模型的实现"><a href="#15-1-3-最大熵模型的实现" class="headerlink" title="15.1.3 最大熵模型的实现"></a>15.1.3 最大熵模型的实现</h4><p>经过前面的推导，我们已经求出了最大熵模型的目标梯度。接下来我们将给出它的实现算法。最大熵模型的梯度下降法实现起来相对简单。令 _X_ 的数量为dimx ，<br>_Y_ 的数量为dimy ，于是我们可以定义dimx ×dimy 个特征，每一个特征可以指示一组数据是否存在：</p>
<p>模型拥有dimx ×dimy 个参数 _λ_ i，j ，每一个参数表示对应特征的权重。如果一个样本拥有 _N_ 个特征，那么我们可以得到某个样本 _x_<br>关于结果 _y_ j 的概率：</p>
<p>知道了这个概率，就能计算出一个样本对所有特征的积累量，其中每一个特征积累量为</p>
<p>那么，也就可以求出一批样本对所有特征的积累量，其中每一个特征积累量为</p>
<p>求解出所有的特征累积量，就可以轻松求出模型的梯度，完整的算法如下所示。</p>
<p>算法 最大熵似然法</p>
<p>Input</p>
<p>模型参数 _λ_ i，j ，特征fi，j</p>
<p>Start</p>
<p>for _i_ =1 _,…,N_ do</p>
<p>初始化训练样本特征期望矩阵f～和模型特征期望矩阵f，二者的维度为| _F_ |×| _Y_ |。</p>
<p>取出一个批次的数据，根据公式计算特征期望，其中 _i_ 和 _j_ 分别表示 _x_ 和 _y_ 的</p>
<p>下标：</p>
<p>f~i,j =∑(x,y)~(X,Y) _f_ ~i,j ( _x,y_ )</p>
<p>求出每一个特征在模型的积累量：</p>
<p>计算每一个参数的更新量并对参数进行更新：</p>
<p>end for</p>
<h3 id="15-2-最大熵反向强化学习"><a href="#15-2-最大熵反向强化学习" class="headerlink" title="15.2 最大熵反向强化学习"></a>15.2 最大熵反向强化学习</h3><p>经过了15.1节的铺垫，本节介绍最大熵模型在反向强化学习中的应用，算法来自论文 _Maximum entropy inνerse reinforcement<br>learning_ [1] 。再次回顾反向强化学习的问题定义：对于某个强化学习的任务，我们观察到专家的状态-行动轨迹：ζ={si _，_ ai }N<br>，但我们并不知道任务中回报函数的形式。我们希望使用这些轨迹信息学习到回报函数 _r_ （fsj ； _θ_ ），其中fsj 是由状态转换得到的特征， _θ_<br>是回报函数的参数。如果回报函数是一个线性函数，那么回报函数可以写作</p>
<p>那么轨迹的长期回报就可以写作</p>
<p>这样就可以把fζ 称为特征积累量，每一条轨迹被称为ζ～i ，我们假设所有的轨迹起始于同一个状态，并将所有的轨迹聚合起来，就可以用这些轨迹得到价值期望的估计：</p>
<p>所以f～也可以表示特征积累的期望。</p>
<p>定义策略模型为 _P_ π （ζi ），同最大熵模型中的约束条件类似，我们希望交互得到的轨迹的长期回报等同于专家的轨迹的长期回报，于是可以得到</p>
<p>由于等式两边的 _θ_ 相同，可以得到</p>
<p>这样我们就得到了一个和最大熵模型类似的约束：策略的轨迹特征积累量的期望应该与专家的相等。这个条件并不是一个很强的约束，满足这个条件的策略模型可能有很多，而这些策略在完整的问题空间中可能各有优劣，所以我们仍然有可能学习出一个较差的模型。为了解决这个问题，我们需要再添加最大熵的约束。如果有两个策略<br>_π_ 1 和 _π_ 2<br>，它们的特征累积量期望是相同的，但它们的参数不同，那么至少存在一条轨迹，使得两个策略对它的“喜好”不同，一个策略有较高的概率产生这条轨迹，而另一个策略产生这条轨迹的概率较低。如果我们可以将轨迹对某个策略的概率用形式化的方法表示，就可以将目标函数定义为最大化概率分布的熵，从而区别这些策略。</p>
<p>如果MDP是一个确定的MDP，也就是说当策略选定了行动后，环境会转换到确定的状态，那么轨迹的长期回报只和策略对轨迹的概率有关。于是我们可以使用轨迹的长期回报表示轨迹出现的概率，这个表示是非常合理的，如果轨迹获得的长期回报比较高，那么策略应该以较高的概率出现这条轨迹，这样也更符合最大化长期回报的目标。于是轨迹的概率可以表示为</p>
<p>其中 _Z_ （ _θ_ ）=∑i eθTf ζi ，得到了轨迹的概率形式，我们就可以构建基于最大熵的目标函数了。确定的MDP状态转换图如图15-1所示。</p>
<p>当然，在实际问题中，我们也会遇到非确定的MDP问题。也就是说，当策略确定了行动后，下一个状态并不确定，仍然需要环境的状态转移概率确定，这样我们就必须把这部分随机性考虑进去。轨迹的概率由策略和状态转换概率共同决定。那么，我们使用某个策略与环境交互，在环境随机性的影响下，一条轨迹可能是多个行动序列的结果。这就是非确定的MDP问题的特点，如图15-2所示。</p>
<p>图15-1 确定的MDP状态转换</p>
<p>图15-2 非确定的MDP状态转换</p>
<p>要想表示一条轨迹的长期回报期望，首先要找到轨迹对应的所有行动序列。令一条行动序列为o，所有行动序列的集合为O，就可以使用指示变量 _I_ ζ∈o<br>表示轨迹是否可以通过某一条行动序列生成。当这个值等于1时，表示轨迹可以通过行动序列生成，反之则无法生成。同时，我们用 _P_ T<br>（ζ|o）表示当行动序列确定时，状态最终会转换成序列ζ的概率。于是轨迹的概率可以表示为</p>
<p>其中 _Z_ （ _θ，_<br>o）的定义也随之改变。上面的公式虽然是正确的，但是由于有指示变量的存在，这个公式很难计算。我们需要将上面的公式近似为一个可解的形式：</p>
<p>经过上面的推导，得到了模型对轨迹的概率，于是我们可以采用最大似然法进行优化，假设问题是确定的MDP，对应的目标函数为</p>
<p>由于公式的形式和最大熵模型十分一致，于是我们省略了推导过程，直接给出参数梯度的计算公式：</p>
<p>这样的形式实际上仍不容易计算， _p_ （ζ| _θ，T_ ）和fζ<br>这两项都显得有些复杂。下面我们来化简其中一个项目，并将复杂的计算“转移”到另一个项目。我们要考虑的问题是确定MDP的问题，即行动可以直接决定下一时刻的状态。这样当初始状态、策略和回报函数确定时，未来的轨迹也就确定下来。所以我们可以把每一条轨迹的状态分别列出来，再将其中重复的状态合并，将这些状态的概率与状态对应的特征相乘，得到的结果也是相同的。这一过程的变换如图15-3所示。</p>
<p>图15-3 期望状态访问频率转换</p>
<p>图15-3中包含3个序列，每个序列都包含4个状态。我们以状态S1<br>为例，它出现在4个位置上，在计算期望时，我们实际上要让这4个位置的状态概率分别乘以状态对应的特征。由于这4个位置的特征值相同，我们可以将它们合并为同一项。只要能计算出4个位置的状态概率和，再将其乘以特征值，就可以求出序列中关于这个状态的特征期望，其他的状态也可以依次得到。</p>
<p>这样我们就得到了一个新的计算公式</p>
<p>其中 _D_ si 被称为状态访问频率期望（Expected State Visitation<br>Frequency），我们将计算的重点放在这一项。从任意一个状态出发，根据策略的频率和确定的状态转换概率推演，可以得到任意一个时刻状态出现的概率，之后将每一个时刻状态出现的概率加起来，就可以得到<br>_D_ si 。</p>
<p>从上面的分析来看，目前我们已经知道了状态转换概率，唯一不清楚的就是策略，于是我们先求解策略。最大熵反向强化学习的论文 [1]<br>中介绍了一种基于前向后向计算的方法，它可以先通过反向计算求出策略模型，再进行前向计算得到状态访问频率期望。</p>
<p>首先设定在最后一刻 _T_ 所有状态的出现值为1，就可以计算 _T_ -1时刻某个状态下执行某个行动的概率 _P_ T-1 （ai，j |si ），可以得到</p>
<p>前面我们提到轨迹出现的概率和回报成正相关，于是我们可以使用回报替代概率值，这样就可以得到</p>
<p>如果令 _Z_ ai，j =∑k ereward（s i |θ） _p_ （sk |si _，_ ai，j ），公式就可以变为</p>
<p>从公式中可以发现，分子和分母存在一定的重复，这部分计算可以共享。当时刻转换到 _T_ -2时，我们也可以得到类似的结果：</p>
<p>这样就得到了一个迭代的公式形式，随着迭代的轮数不断增加，策略估计值会变得越来越稳定，越来越接近真实的策略，这样我们就可以完成策略的计算，计算过程如图15-4所示。</p>
<p>通过反向计算得到策略值后，再进行前向计算，就可以得到每一个时刻状态的访问频率，这部分的计算过程如图15-5所示。</p>
<p>图15-4 反向计算过程</p>
<p>图15-5 前向计算过程</p>
<p>这样就可以得到状态访问频率期望 _D_ si ，使接下来梯度的计算变得容易。前向后向算法如下所示。</p>
<p>算法 状态访问频率期望计算</p>
<p>反向计算：</p>
<p>（1）设置 _Z_ si =1</p>
<p>（2）重复计算 _N_ 个时间迭代过程</p>
<p>（3）概率计算</p>
<p>（4）前向计算</p>
<p>设置 _D_ si，t = _P_ （si =sinitial ）</p>
<p>for _t_ =1 to _N_ ;</p>
<p>_D_ si,t+1 =∑ai,j ∑k _D_ sk,t _P_ (ai,j |si ) _P_ (sk |ai,j _,_ si )</p>
<p>（5）加和频率</p>
<p>_D_ si =∑t _D_ si,t</p>
<p>了解了最大熵算法的核心部分，就可以得到完整的算法。剩下的部分和最大熵似然法类似，这里不再展开。论文 _Maximum Entropy Deep<br>Inνerse Reinforcement Learning_ [2]<br>中还介绍了深度最大熵反向强化学习算法，整体思想与最大熵算法比较相近，主要的改进是将其中的模型换成了深层模型，感兴趣的读者可以自行阅读。</p>
<h3 id="15-3-GAIL"><a href="#15-3-GAIL" class="headerlink" title="15.3 GAIL"></a>15.3 GAIL</h3><p>本节介绍另一个反向强化学习的方法，它的名称叫作GAIL（Generative Adversarial Imitation Learning），来自同名论文<br>[3] 。从名字上可以看出，这个方法使用GAN（Gen-erative Adversarial Network）完成Imitation<br>Learning的工作。近一两年，与GAN相关的研究发展十分迅速，也催生了许多应用方面的扩展，GAIL就是其中之一。本节先介绍GAN的基础知识，然后介绍GAN与反向强化学习结合的方法。</p>
<h4 id="15-3-1-GAN的基本概念"><a href="#15-3-1-GAN的基本概念" class="headerlink" title="15.3.1 GAN的基本概念"></a>15.3.1 GAN的基本概念</h4><p>GAN是一个实现生成模型的网络结构，它的结构比较复杂，其中包含了一对子模型：生成模型和判别模型。生成模型用于生成目标概率分布中的样本，而判别模型用于判断样本是否来自真实数据分布。GAN最早被应用于生成图像，由于图像数据本身比较复杂，它的分布也比较复杂，因此很多常规的生成模型很难生成令人满意的图像。而GAN通过隐式概率的形式生成图像，同时配合判别模型，可以生成更高质量的图像。</p>
<p>我们用字母 _D_ 代表判别模型，它的输入是数据空间内的任意一张图像 _x_ ，输出是一个概率值，表示这张图像属于真实数据的概率。字母 _G_<br>代表生成模型，它的输入是一个随机变量 _z_ ， _z_ 服从某种分布，输出是一张图像 _G_ （ _z_ ），如果它生成的图像经过模型 _D_<br>后的概率值很高，就说明生成模型已经比较好地掌握了数据的分布模式，可以产生符合要求的样本；反之则没有达到要求，还需要继续训练。</p>
<p>两个模型的目标公式如下所示。</p>
<p>（1）判别模型的目标是最大化公式 _E_ x [ _D_ （ _x_ ）]+ _E_ z [1 _-D_ （ _G_ （ _z_<br>））]，也就是甄别出哪些图来自真实数据分布，哪些是由生成模型生成的。</p>
<p>（2）生成模型的目标是最大化公式 _E_ z [ _D_ （ _G_ （ _z_ ））]，也就是让自己生成的图被判别模型判断为来自真实数据分布。</p>
<p>从目标公式可以看出两个模型之间的联系，如果生成模型生成的图像和真实的图像有区别，判别模型要给前者判定比较低的概率。这里可以举个形象的例子，假设 _x_<br>是一种商品， _D_ 是商品的检验方，负责检验商品是否是正品； _G_ 是一家山寨公司，希望根据拿到手的一批产品 _x_ 研究出生产山寨商品 _x_<br>的方式。对 _D_ 来说，不管 _G_ 生产出来的产品多像正品，都应该被判定为赝品。 _G_<br>的技术水平不高，生产出来的产品漏洞百出，只有不断地提高技术水平，才有可能迷惑检验方。</p>
<p>基于上面的例子，两个模型的目标可以统一成一个充满硝烟味的目标函数。</p>
<p>minG maxD _V_ ( _D,G_ )= _E_ x [log _D_ ( _x_ )]+ _E_ z [log(1 _-D_ ( _G_ (<br>_z_ )))]</p>
<p>上面这个公式对应的模型架构如图15-6所示。</p>
<p>图15-6 GAN的基本形式</p>
<p>对应的模型学习算法的伪代码如下所示：</p>
<p>上面的代码从宏观层面介绍了模型的优化方法，其中 _K_ 表示判别模型 _D_ 的迭代次数， _K_<br>一般大于等于1。从上面的公式可以看出，两个模型的目标是对立的。生成模型希望最大化自己生成的图像的似然，判别模型希望最大化原始数据的似然的同时，能够最小化<br>_G_<br>生成的图像的似然。既然是对立的，两个模型经过训练产生的能力就可能有很多种情况。它们既可能上演“魔高一尺，道高一尺”的竞争戏码，在竞争中共同成长，最终产生两个强大的模型；也可能产生一个强大的模型，将另一方完全压倒。</p>
<p>（1）如果判别模型太过强大，那么生成模型会有两种情况：一种情况是发现自己完全被针对，模型参数无法优化；另一种情况是发现判别模型的一些漏洞后，它的模型将退化，不管输入是什么，输出统一变成之前突破了判别模型防线的那几类结果。这种情况被称为Mode<br>Collapse，有点像一个复杂强大的模型崩塌成一个简单弱小的模型，这样的模型即使优化结果很好，也不能拿去使用。</p>
<p>（2）如果判别模型不够强大，无法精确判别一个样本是否来自真实数据，而生成模型又是按照它的判别结果进行调节，那么生产出的产品质量就不太有保证，这同样不是我们想看到的结果。</p>
<p>总而言之，对抗是GAN这个模型要面对的一个大问题。后续基于GAN的很多改进模型也都在试图解决这个问题，每一个模型有自己的改进策略，这些改进方法的详细信息请读者自行查阅了解。</p>
<h4 id="15-3-2-GAN的训练分析"><a href="#15-3-2-GAN的训练分析" class="headerlink" title="15.3.2 GAN的训练分析"></a>15.3.2 GAN的训练分析</h4><p>我们从目标函数出发对GAN的训练过程进行分析。我们先证明第一步：当生成模型固定时，判别模型的最优形式。首先将目标函数做变换：</p>
<p>由于组成公式的两部分积分的区域不同，会对后面的计算造成困难，所以先将两个积分区域统一。我们将生成图像 _G_ （ _z_ ）的分布与真实图像 _x_<br>的分布做一个投射，只要判别式能够在真实数据出现的区域保证判别正确即可，于是公式就变为</p>
<p>只要让积分内部的公式最大化，整个公式就可以实现最大化。这样问题就转变为最大化下面的公式：</p>
<p>对它进行求导取极值，可以得到</p>
<p>令上面的式子为0，可以得到结果</p>
<p>这就是理论上判别式的预测结果，如果一张图像在真实分布中出现的概率大而在生成分布中出现的概率小，那么最优的判别模型会认为它是真实图像，反之则认为不是真实图像。如果生成模型已经达到了完美的状态，也就是说对每一幅图像都有<br>_p_ r （ _x_ ）= _p_ g （ _x_ ），那么</p>
<p>.</p>
<p>可以利用上面的结果，计算当生成模型达到完美状态时，损失函数的值。我们将的结果代入，可以得到</p>
<p>也就是说，生成模型损失函数的理论最小值为-2log2。那么一般情况下，它的损失函数是什么样子呢？假设在某一时刻判别式经过优化已经达到最优，所以 _D_ ∗<br>（ _x_ ）=，将这个公式代入之前的公式，可以得到</p>
<p>后面的两个KL散度的计算公式可以转化为Jenson-Shannon散度，简称JS散度，于是公式变为</p>
<p>JS散度与KL散度类似，都被用于衡量概率分布之间的距离。不同的是，JS是一个对称的度量，也就是说下面的公式成立：</p>
<p>对称能带来什么好处呢？它能让散度度量更准确。接下来将用一段代码展示这其中的道理。首先给出两个离散随机变量的KL散度和JS散度的计算方法：</p>
<p>下面将用3组实验看看两个散度的计算结果，实现的目标是两个比较简单的离散分布。首先是第一个实验，我们直接求出两个分布在KL散度和JS散度下的距离：</p>
<p>它的结果如下：</p>
<p>在第二个实验中，我们将第二个分布稍做修改，将这个分布中的某个概率减小，这样两个分布的散度值将会增加，对应的代码如下所示：</p>
<p>它的结果如下所示：</p>
<p>可以看出KL散度的波动比较大，而JS的波动相对小。</p>
<p>在第三个实验中，我们将修改第一个分布，同样是将其中一个概率值降低，代码如下所示：</p>
<p>它的结果如下所示：</p>
<p>如果将第二个实验和第三个实验做对比，可以发现KL散度在衡量两个分布的差异时具有很大的不对称性。当第二个分布在某一个值上缺失时，KL散度值就会发生很大的变化；而第一个分布在某一个值上缺失时，KL散度并没有太大的波动。从这个例子可以很清楚地看出KL不对称性带来的问题。因为JS散度具有对称性，所以第二个实验和第三个实验中JS散度实际上是相同的。</p>
<p>从这个小例子可以看出，有时KL散度下降的程度和两个分布靠近的程度不成比例，而JS散度靠近的程度更令人满意，这也是GAN模型的一大优势。</p>
<h3 id="15-4-GAIL实现"><a href="#15-4-GAIL实现" class="headerlink" title="15.4 GAIL实现"></a>15.4 GAIL实现</h3><p>15.3节介绍了GAN的基本知识，让我们来看看GAIL的具体实现。经过前面章节的介绍，我们知道回报函数的目标是使专家轨迹的长期回报尽可能地高于其他策略生成的轨迹。如果把这个目标放到GAN的场景下，就可以做一个对应。在GAN中，我们希望生成分布尽可能靠近真实分布；而在反向强化学习中，我们希望策略模型轨迹尽可能靠近专家轨迹。所以我们可以使用类似的方法，由策略模型充当GAN中的生成模型，以状态为输入生成行动；而回报函数模型可以充当判别模型，用于判别行动近似专家行动的程度。</p>
<p>令回报模型为 _D_ ，其参数为 _w_ ，策略模型为 _π_ ，其参数为 _θ_ ，这样我们的目标函数就变为</p>
<p>对于策略模型，我们希望最大化目标函数；对于回报模型，我们希望最小化目标函数。除了前面提到的GAN模型外，我们还可以把策略的熵添加进来，以保证策略分布相对平均。当我们对GAN有了一定的了解后，再来看GAIL的实现就变得容易理解了，对应的算法如下所示。</p>
<p>算法 Generative Adversarial Imitation Learning</p>
<p>Input</p>
<p>专家交互轨迹：τE ～ _π_ E ；初始化策略和评判模型的参数： _θ_ 0 _，w_ 0</p>
<p>Start</p>
<p>for i=0,1,2,…do</p>
<p>使用策略 _π_ θi 与环境交互得到轨迹τi</p>
<p>使用下面的梯度计算公式将评判模型的参数由 _w_ i 更新到 _w_ i+1 ：</p>
<p>^ _E_ τi [∇w log( _D_ w (s _,_ a))]+^ _E_ τE [∇w log(1 _-D_ w (s _,_ a))]</p>
<p>使用TRPO算法将策略模型的参数由 _θ_ i 更新到 _θ_ i+1 ，其中梯度为：</p>
<p>^ _E_ τi [∇θ log _π_ θ (a|s) _Q_ (s _,_ a)] _-λ_ ∇θ _H_ ( _π_ θ )</p>
<p>其中 _Q_ （s _-，_ -a）=^ _E_ τi [log（ _D_ wi+1 （s _，_ a））|s0 =s _-，_ a0 =-a]</p>
<p>GAIL的实现代码在baselines/gail文件夹中，其中包含了第1章提到的Behavior<br>Cloning的实现和GAIL的实现，这里我们只关注GAIL的实现。</p>
<p>其中的判别模型由adversary.py文件中的TransitionClassifier实现，模型的输入为状态观测值和行动，对观测值进行标准化后，将其传入三层全连接层网络中进行计算，得到对行动判断的logit值。判别模型的目标函数有三部分：最大化来自专家轨迹的行动概率、最小化来自Agent策略轨迹的行动概率和最大化判别模型的熵。完整的计算图如图15-7所示。</p>
<p>图15-7 判别模型计算图</p>
<p>其中求解伯努利分布的熵的公式相对晦涩，笔者在此推导一下，伯努利分布的熵的公式为</p>
<p>而要想得到伯努利分布的概率，需要用sigmoid函数对神经网络中得到的logit值做变换：</p>
<p>综合两个公式，可以得到</p>
<p>GAIL中的策略模型训练算法选择了TRPO算法，代码结构和TRPO项目的结构比较类似。完整的模型训练如下面的伪代码所示：</p>
<p>可以看出训练过程分为两部分：训练TRPO算法中的策略模型和训练GAIL中的判别模型。策略模型同Actor<br>Critic算法的模型结构相同，对于输入的状态，可以同时得到行动的概率分布和价值估计，因此这两部分都需要参与目标函数的计算，并进行优化，所以在代码的6～8行要完成两部分的优化，在代码的9～11行才进行判别模型的优化，其计算方法需要用到交互轨迹和专家轨迹两部分的数据。</p>
<p>如果读者对TRPO有一定的了解，就不必痛苦地了解GAIL代码的逻辑，其中更多的细节请读者自行阅读。</p>
<h3 id="15-5-总结"><a href="#15-5-总结" class="headerlink" title="15.5 总结"></a>15.5 总结</h3><p>本章介绍了两个反向强化学习算法，总结如下。</p>
<p>（1）最大熵反向强化学习以最大化序列概率分布熵为目标，同时限定基于策略模型的特征期望与最优策略的特征期望相等进行求解。</p>
<p>（2）生成对抗模仿学习基于生成对抗模型，构建了代表生成模型的策略模型和代表判别模型的回报模型，并通过交替训练策略模型和回报模型进行学习。</p>
<h3 id="15-6-参考资料"><a href="#15-6-参考资料" class="headerlink" title="15.6 参考资料"></a>15.6 参考资料</h3><p>[1] Ziebart B D,Maas A,Bagnell J A,et al.Maximum entropy inverse reinforcement<br>learn-ing[C]//National Conference on Artificial Intelligence.AAAI<br>Press,2008:1433-1438.</p>
<p>[2] Wulfmeier M,Ondruska P,Posner I.Maximum Entropy Deep Inverse Reinforcement<br>Learning[J].2015.</p>
<p>[3] Ho J,Ermon S.Generative Adversarial Imitation Learning[J].2016.</p>
<p>[4] Goodfellow I J,Pouget-Abadie J,Mirza M,et al.Generative Adversarial<br>Networks[J].Advances in Neural Information Processing<br>Systems,2014,3:2672-2680.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      <div>    
       
       
      <ul class="post-copyright">
        <li class="post-copyright-author">
            <strong>本文作者：</strong>hac_lang
        </li>
        <li class="post-copyright-link">
          <strong>本文链接：</strong>
          <a href="/2018/07/22/book-《强化学习精要-核心算法与TensorFlow实现》-冯超/" title="book_《强化学习精要 核心算法与TensorFlow实现》_冯超">2018/07/22/book-《强化学习精要-核心算法与TensorFlow实现》-冯超/</a>
        </li>
        <li class="post-copyright-license">
          <strong>版权声明： </strong>
          许可协议，请勿用于商业，转载注明出处！
        </li>
      </ul>
      
      </div>
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/计算机/" rel="tag"># 计算机</a>
          
            <a href="/tags/人工智能/" rel="tag"># 人工智能</a>
          
            <a href="/tags/自评/" rel="tag"># 自评</a>
          
            <a href="/tags/books/" rel="tag"># books</a>
          
            <a href="/tags/续更/" rel="tag"># 续更</a>
          
            <a href="/tags/算法/" rel="tag"># 算法</a>
          
            <a href="/tags/豆瓣6/" rel="tag"># 豆瓣6</a>
          
            <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          
            <a href="/tags/强化学习/" rel="tag"># 强化学习</a>
          
            <a href="/tags/工程/" rel="tag"># 工程</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/11/book-《内容算法：把内容变成价值的效率系统》-闫泽华/" rel="next" title="book_《内容算法：把内容变成价值的效率系统》_闫泽华">
                <i class="fa fa-chevron-left"></i> book_《内容算法：把内容变成价值的效率系统》_闫泽华
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/25/book-《21个项目玩转深度学习：基于TensorFlow的实践详解》/" rel="prev" title="book_《21个项目玩转深度学习：基于TensorFlow的实践详解》">
                book_《21个项目玩转深度学习：基于TensorFlow的实践详解》 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
    <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="hac_lang">
  
  <p class="site-author-name" itemprop="name">hac_lang</p>
  <div class="site-description motion-element" itemprop="description">小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理</div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    

    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>







  <div class="links-of-author motion-element">
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://github.com/HACLANG" title="GitHub &rarr; https://github.com/HACLANG" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://stackoverflow.com/yourname" title="StackOverflow &rarr; https://stackoverflow.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://gitter.im" title="Gitter &rarr; https://gitter.im" rel="noopener" target="_blank"><i class="fa fa-fw fa-github-alt"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.jianshu.com/u/442ddccf3f32" title="简书 &rarr; https://www.jianshu.com/u/442ddccf3f32" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="Quora &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://plus.google.com/yourname" title="Google &rarr; https://plus.google.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="mailto:haclang.org@gmail.com" title="E-Mail &rarr; mailto:haclang.org@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="skype:haclang?call|chat" title="Skype &rarr; skype:haclang?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://twitter.com/haclang2" title="Twitter &rarr; https://twitter.com/haclang2" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="FaceBook &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
    
  </div>








          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第一部分-强化学习入门与基础知识"><span class="nav-text">第一部分 强化学习入门与基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-引言"><span class="nav-text">1 引言</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-强化学习的概念"><span class="nav-text">1.1 强化学习的概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-巴浦洛夫的狗"><span class="nav-text">1.1.1 巴浦洛夫的狗</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-俄罗斯方块"><span class="nav-text">1.1.2 俄罗斯方块</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-站在被实验者的角度看问题"><span class="nav-text">1.2 站在被实验者的角度看问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-强化学习效果的评估"><span class="nav-text">1.3 强化学习效果的评估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-不断试错"><span class="nav-text">1.3.1 不断试错</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-看重长期回报"><span class="nav-text">1.3.2 看重长期回报</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-强化学习与监督学习"><span class="nav-text">1.4 强化学习与监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-强化学习与监督学习的本质"><span class="nav-text">1.4.1 强化学习与监督学习的本质</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-模仿学习"><span class="nav-text">1.4.2 模仿学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-强化学习的实验环境"><span class="nav-text">1.5 强化学习的实验环境</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-1-Arcade-Learning-Environment"><span class="nav-text">1.5.1 Arcade Learning Environment</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-2-Box2D"><span class="nav-text">1.5.2 Box2D</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-3-MuJoCo"><span class="nav-text">1.5.3 MuJoCo</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-4-Gym"><span class="nav-text">1.5.4 Gym</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-本书的主要内容"><span class="nav-text">1.6 本书的主要内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-参考资料"><span class="nav-text">1.7 参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-数学与机器学习基础"><span class="nav-text">2 数学与机器学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-线性代数基础"><span class="nav-text">2.1 线性代数基础</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-对称矩阵的性质"><span class="nav-text">2.2 对称矩阵的性质</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-特征值与特征向量"><span class="nav-text">2.2.1 特征值与特征向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-对称矩阵的特征值和特征向量"><span class="nav-text">2.2.2 对称矩阵的特征值和特征向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-对称矩阵的对角化"><span class="nav-text">2.2.3 对称矩阵的对角化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-概率论"><span class="nav-text">2.3 概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-概率与分布"><span class="nav-text">2.3.1 概率与分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-最大似然估计"><span class="nav-text">2.3.2 最大似然估计</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-重要性采样"><span class="nav-text">2.4 重要性采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-信息论基础"><span class="nav-text">2.5 信息论基础</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-KL散度"><span class="nav-text">2.6 KL散度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-凸函数及其性质"><span class="nav-text">2.7 凸函数及其性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-机器学习的基本概念"><span class="nav-text">2.8 机器学习的基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-9-机器学习的目标函数"><span class="nav-text">2.9 机器学习的目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-10-总结"><span class="nav-text">2.10 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-优化算法"><span class="nav-text">3 优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-梯度下降法"><span class="nav-text">3.1 梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-什么是梯度下降法"><span class="nav-text">3.1.1 什么是梯度下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-优雅的步长"><span class="nav-text">3.1.2 优雅的步长</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-动量算法"><span class="nav-text">3.2 动量算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-共轭梯度法"><span class="nav-text">3.3 共轭梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-精妙的约束"><span class="nav-text">3.3.1 精妙的约束</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-共轭"><span class="nav-text">3.3.2 共轭</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-优化步长的确定"><span class="nav-text">3.3.3 优化步长的确定</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-4-Gram-Schmidt方法"><span class="nav-text">3.3.4 Gram-Schmidt方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-5-共轭梯度"><span class="nav-text">3.3.5 共轭梯度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-自然梯度法"><span class="nav-text">3.4 自然梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-基本概念"><span class="nav-text">3.4.1 基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-Fisher信息矩阵"><span class="nav-text">3.4.2 Fisher信息矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-3-自然梯度法目标公式"><span class="nav-text">3.4.3 自然梯度法目标公式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-总结"><span class="nav-text">3.5 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-TensorFlow入门"><span class="nav-text">4 TensorFlow入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-TensorFlow的基本使用方法"><span class="nav-text">4.1 TensorFlow的基本使用方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-TensorFlow原理介绍"><span class="nav-text">4.2 TensorFlow原理介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-创建变量的scope"><span class="nav-text">4.2.1 创建变量的scope</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-创建一个Variable背后的故事"><span class="nav-text">4.2.2 创建一个Variable背后的故事</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-3-运算操作"><span class="nav-text">4.2.3 运算操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-4-tf-gradients"><span class="nav-text">4.2.4 tf.gradients</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-5-Optimizer"><span class="nav-text">4.2.5 Optimizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-6-TensorFlow的反向传播技巧"><span class="nav-text">4.2.6 TensorFlow的反向传播技巧</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-7-arg-scope的使用"><span class="nav-text">4.2.7 arg_scope的使用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-TensorFlow的分布式训练"><span class="nav-text">4.3 TensorFlow的分布式训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-基于MPI的数据并行模型"><span class="nav-text">4.3.1 基于MPI的数据并行模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-MPI的实现：mpi-adam"><span class="nav-text">4.3.2 MPI的实现：mpi_adam</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-基于TensorFlow实现经典网络结构"><span class="nav-text">4.4 基于TensorFlow实现经典网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-1-多层感知器"><span class="nav-text">4.4.1 多层感知器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-2-卷积神经网络"><span class="nav-text">4.4.2 卷积神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-3-循环神经网络"><span class="nav-text">4.4.3 循环神经网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-总结"><span class="nav-text">4.5 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-参考资料"><span class="nav-text">4.6 参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Gym与Baselines"><span class="nav-text">5 Gym与Baselines</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Gym"><span class="nav-text">5.1 Gym</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-Gym的安装"><span class="nav-text">5.1.1 Gym的安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-Gym的基本使用方法"><span class="nav-text">5.1.2 Gym的基本使用方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-3-利用Gym框架实现一个经典的棋类游戏：蛇棋"><span class="nav-text">5.1.3 利用Gym框架实现一个经典的棋类游戏：蛇棋</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Baselines"><span class="nav-text">5.2 Baselines</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-Baselines中的Python-3新特性"><span class="nav-text">5.2.1 Baselines中的Python 3新特性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-tf-util"><span class="nav-text">5.2.2 tf_util</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-3-对Gym平台的扩展"><span class="nav-text">5.2.3 对Gym平台的扩展</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-总结"><span class="nav-text">5.3 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-强化学习基本算法"><span class="nav-text">6 强化学习基本算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-马尔可夫决策过程"><span class="nav-text">6.1 马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-1-MDP：策略与环境模型"><span class="nav-text">6.1.1 MDP：策略与环境模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-值函数与Bellman公式"><span class="nav-text">6.1.2 值函数与Bellman公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-3-“表格式”Agent"><span class="nav-text">6.1.3 “表格式”Agent</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-策略迭代"><span class="nav-text">6.2 策略迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-策略迭代法"><span class="nav-text">6.2.1 策略迭代法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-策略提升的证明"><span class="nav-text">6.2.2 策略提升的证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-3-策略迭代的效果展示"><span class="nav-text">6.2.3 策略迭代的效果展示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-价值迭代"><span class="nav-text">6.3 价值迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-1-N-轮策略迭代"><span class="nav-text">6.3.1 _N_ 轮策略迭代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-2-从动态规划的角度谈价值迭代"><span class="nav-text">6.3.2 从动态规划的角度谈价值迭代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-3-价值迭代的实现"><span class="nav-text">6.3.3 价值迭代的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-泛化迭代"><span class="nav-text">6.4 泛化迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-1-两个极端"><span class="nav-text">6.4.1 两个极端</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-2-广义策略迭代法"><span class="nav-text">6.4.2 广义策略迭代法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-3-泛化迭代的实现"><span class="nav-text">6.4.3 泛化迭代的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-总结"><span class="nav-text">6.5 总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第二部分-最优价值算法"><span class="nav-text">第二部分 最优价值算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Q-Learning基础"><span class="nav-text">7 Q-Learning基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-状态转移概率：从掌握到放弃"><span class="nav-text">7.1 状态转移概率：从掌握到放弃</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-蒙特卡罗方法"><span class="nav-text">7.2 蒙特卡罗方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-探索与利用"><span class="nav-text">7.3 探索与利用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-蒙特卡罗的方差问题"><span class="nav-text">7.4 蒙特卡罗的方差问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-时序差分法与SARSA"><span class="nav-text">7.5 时序差分法与SARSA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-Q-Learning"><span class="nav-text">7.6 Q-Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-Q-Learning的收敛性分析"><span class="nav-text">7.7 Q-Learning的收敛性分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-8-从表格形式到价值模型"><span class="nav-text">7.8 从表格形式到价值模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-9-Deep-Q-Network"><span class="nav-text">7.9 Deep Q Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-10-总结"><span class="nav-text">7.10 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-11-参考资料"><span class="nav-text">7.11 参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-DQN的改进算法"><span class="nav-text">8 DQN的改进算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-Double-Q-Learning"><span class="nav-text">8.1 Double Q-Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-Priority-Replay-Buffer"><span class="nav-text">8.2 Priority Replay Buffer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-Dueling-DQN"><span class="nav-text">8.3 Dueling DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-解决DQN的冷启动问题"><span class="nav-text">8.4 解决DQN的冷启动问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-Distributional-DQN"><span class="nav-text">8.5 Distributional DQN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-1-输出价值分布"><span class="nav-text">8.5.1 输出价值分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-2-分布的更新"><span class="nav-text">8.5.2 分布的更新</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-6-Noisy-Network"><span class="nav-text">8.6 Noisy Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-7-Rainbow"><span class="nav-text">8.7 Rainbow</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-7-1-Rainbow的模型特点"><span class="nav-text">8.7.1 Rainbow的模型特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-7-2-Deep-Q-Network的实现"><span class="nav-text">8.7.2 Deep Q Network的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-8-总结"><span class="nav-text">8.8 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-9-参考资料"><span class="nav-text">8.9 参考资料</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第三部分-基于策略梯度的算法"><span class="nav-text">第三部分 基于策略梯度的算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-基于策略梯度的算法"><span class="nav-text">9 基于策略梯度的算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-策略梯度法"><span class="nav-text">9.1 策略梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-1-算法推导"><span class="nav-text">9.1.1 算法推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-2-算法分析"><span class="nav-text">9.1.2 算法分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-3-算法改进"><span class="nav-text">9.1.3 算法改进</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-Actor-Critic算法"><span class="nav-text">9.2 Actor-Critic算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-1-降低算法的方差"><span class="nav-text">9.2.1 降低算法的方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-2-A3C算法"><span class="nav-text">9.2.2 A3C算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-3-A2C算法实战"><span class="nav-text">9.2.3 A2C算法实战</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-总结"><span class="nav-text">9.3 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-参考资料"><span class="nav-text">9.4 参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-使策略单调提升的优化算法"><span class="nav-text">10 使策略单调提升的优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-TRPO"><span class="nav-text">10.1 TRPO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-1-策略的差距"><span class="nav-text">10.1.1 策略的差距</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-2-策略提升的目标公式"><span class="nav-text">10.1.2 策略提升的目标公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-3-TRPO的目标定义"><span class="nav-text">10.1.3 TRPO的目标定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-4-自然梯度法求解"><span class="nav-text">10.1.4 自然梯度法求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-5-TRPO的实现"><span class="nav-text">10.1.5 TRPO的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-GAE"><span class="nav-text">10.2 GAE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-2-1-GAE的公式定义"><span class="nav-text">10.2.1 GAE的公式定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-2-2-基于GAE和TRPO的值函数优化"><span class="nav-text">10.2.2 基于GAE和TRPO的值函数优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-2-3-GAE的实现"><span class="nav-text">10.2.3 GAE的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-3-PPO"><span class="nav-text">10.3 PPO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-3-1-PPO介绍"><span class="nav-text">10.3.1 PPO介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-3-2-PPO算法实践"><span class="nav-text">10.3.2 PPO算法实践</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-4-总结"><span class="nav-text">10.4 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-参考资料"><span class="nav-text">10.5 参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-Off-Policy策略梯度法"><span class="nav-text">11 Off-Policy策略梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-Retrace"><span class="nav-text">11.1 Retrace</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-1-1-Retrace的基本概念"><span class="nav-text">11.1.1 Retrace的基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-1-2-Retrace的算法实现"><span class="nav-text">11.1.2 Retrace的算法实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-ACER"><span class="nav-text">11.2 ACER</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-2-1-Off-Policy-Actor-Critic"><span class="nav-text">11.2.1 Off-Policy Actor-Critic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-2-2-ACER算法"><span class="nav-text">11.2.2 ACER算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-2-3-ACER的实现"><span class="nav-text">11.2.3 ACER的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-3-DPG"><span class="nav-text">11.3 DPG</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-1-连续空间的策略优化"><span class="nav-text">11.3.1 连续空间的策略优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-2-策略模型参数的一致性"><span class="nav-text">11.3.2 策略模型参数的一致性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-3-DDPG算法"><span class="nav-text">11.3.3 DDPG算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-4-DDPG的实现"><span class="nav-text">11.3.4 DDPG的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-4-总结"><span class="nav-text">11.4 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-5-参考资料"><span class="nav-text">11.5 参考资料</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第四部分-其他强化学习算法"><span class="nav-text">第四部分 其他强化学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#12-稀疏回报的求解方法"><span class="nav-text">12 稀疏回报的求解方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-稀疏回报的困难"><span class="nav-text">12.1 稀疏回报的困难</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-层次强化学习"><span class="nav-text">12.2 层次强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-HER"><span class="nav-text">12.3 HER</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#12-3-1-渐进式学习"><span class="nav-text">12.3.1 渐进式学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-3-2-HER的实现"><span class="nav-text">12.3.2 HER的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-4-总结"><span class="nav-text">12.4 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-5-参考资料"><span class="nav-text">12.5 参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-Model-based方法"><span class="nav-text">13 Model-based方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-AlphaZero"><span class="nav-text">13.1 AlphaZero</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-1-围棋游戏"><span class="nav-text">13.1.1 围棋游戏</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-2-Alpha-Beta树"><span class="nav-text">13.1.2 Alpha-Beta树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-3-MCTS"><span class="nav-text">13.1.3 MCTS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-4-策略价值模型"><span class="nav-text">13.1.4 策略价值模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-5-模型的对决"><span class="nav-text">13.1.5 模型的对决</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-2-iLQR"><span class="nav-text">13.2 iLQR</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#13-2-1-线性模型的求解法"><span class="nav-text">13.2.1 线性模型的求解法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-2-2-非线性模型的解法"><span class="nav-text">13.2.2 非线性模型的解法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-2-3-iLQR的实现"><span class="nav-text">13.2.3 iLQR的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-3-总结"><span class="nav-text">13.3 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-4-参考资料"><span class="nav-text">13.4 参考资料</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第五部分-反向强化学习"><span class="nav-text">第五部分 反向强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#14-反向强化学习入门"><span class="nav-text">14 反向强化学习入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-1-基本概念"><span class="nav-text">14.1 基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-从最优策略求解回报"><span class="nav-text">14.2 从最优策略求解回报</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-2-1-求解回报的目标函数"><span class="nav-text">14.2.1 求解回报的目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-2-2-目标函数的约束"><span class="nav-text">14.2.2 目标函数的约束</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-3-求解线性规划"><span class="nav-text">14.3 求解线性规划</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-1-线性规划的求解过程"><span class="nav-text">14.3.1 线性规划的求解过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-2-实际案例"><span class="nav-text">14.3.2 实际案例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-无限状态下的求解"><span class="nav-text">14.4 无限状态下的求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-5-从样本中学习"><span class="nav-text">14.5 从样本中学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-6-总结"><span class="nav-text">14.6 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-7-参考资料"><span class="nav-text">14.7 参考资料</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-反向强化学习算法2-0"><span class="nav-text">15 反向强化学习算法2.0</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-1-最大熵模型"><span class="nav-text">15.1 最大熵模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-1-1-指数家族"><span class="nav-text">15.1.1 指数家族</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-1-2-最大熵模型的推导"><span class="nav-text">15.1.2 最大熵模型的推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-1-3-最大熵模型的实现"><span class="nav-text">15.1.3 最大熵模型的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-2-最大熵反向强化学习"><span class="nav-text">15.2 最大熵反向强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-3-GAIL"><span class="nav-text">15.3 GAIL</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-1-GAN的基本概念"><span class="nav-text">15.3.1 GAN的基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-2-GAN的训练分析"><span class="nav-text">15.3.2 GAN的训练分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-4-GAIL实现"><span class="nav-text">15.4 GAIL实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-5-总结"><span class="nav-text">15.5 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-6-参考资料"><span class="nav-text">15.6 参考资料</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hac_lang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  





  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="true"></script>









  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  
  

  
  

  


  

  

  

  

  


  


  




  




  




  



<script>
// GET RESPONSIVE HEIGHT PASSED FROM IFRAME

window.addEventListener("message", function(e) {
  var data = e.data;
  if ((typeof data === 'string') && (data.indexOf('ciu_embed') > -1)) {
    var featureID = data.split(':')[1];
    var height = data.split(':')[2];
    $(`iframe[data-feature=${featureID}]`).height(parseInt(height) + 30);
  }
}, false);
</script>


  

  

  


  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":250,"height":500},"mobile":{"show":false,"scale":0.5},"react":{"opacity":0.7},"log":false,"tagMode":false});</script></body>
</html>
