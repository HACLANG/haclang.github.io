<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">






<link rel="stylesheet" href="/css/main.css?v=7.2.0">






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="作者: 赵志勇出版社: 电子工业出版社出版年: 2017-7丛书: 博文视点AI系列ISBN: 9787121313196 内容简介 本书是一本机器学习入门读物，注重理论与实践的结合。全书主要包括6个部分，每个部分均以典型的机器学习算法为例，从算法原理出发，由浅入深，详细介绍算法的理论，并配合目前流行的Python语言，从零开始，实现每一个算法，以加强对机器学习算法理论的理解、增强实际的算法实践能">
<meta name="keywords" content="机器学习,编程,计算机科学,计算机,自评,books,算法,豆瓣6,更毕,Python,python,计算机算法,数学,T-工业技术">
<meta property="og:type" content="article">
<meta property="og:title" content="book_《Python机器学习算法》">
<meta property="og:url" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/index.html">
<meta property="og:site_name" content="Hac_lang">
<meta property="og:description" content="作者: 赵志勇出版社: 电子工业出版社出版年: 2017-7丛书: 博文视点AI系列ISBN: 9787121313196 内容简介 本书是一本机器学习入门读物，注重理论与实践的结合。全书主要包括6个部分，每个部分均以典型的机器学习算法为例，从算法原理出发，由浅入深，详细介绍算法的理论，并配合目前流行的Python语言，从零开始，实现每一个算法，以加强对机器学习算法理论的理解、增强实际的算法实践能">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00366.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00358.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00360.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00369.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00372.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00374.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00376.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00378.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00381.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00383.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00384.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00386.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00388.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00390.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00392.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00394.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00395.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00399.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00402.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00405.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00409.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00411.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00414.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00578.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00421.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00424.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00427.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00432.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00434.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00437.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00440.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00445.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00449.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00454.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00459.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00462.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00464.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00467.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00471.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00002.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00005.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00008.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00013.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00017.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00021.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00025.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00030.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00034.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00039.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00043.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00046.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00049.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00052.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00055.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00060.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00062.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00066.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00070.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00073.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00078.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00485.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00506.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00522.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00536.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00099.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00102.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00105.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00107.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00111.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00114.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00568.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00083.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00741.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00309.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00417.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00585.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00816.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00724.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00861.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00023.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00910.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00256.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00045.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00468.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00439.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00669.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00130.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00157.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00077.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00194.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00305.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00650.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00653.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00656.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00316.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00665.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00018.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00382.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00251.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00355.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00441.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00466.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00295.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00081.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00071.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00084.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00085.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00087.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00090.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00092.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00095.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00096.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00556.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00100.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00575.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00103.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00603.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00106.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00627.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00109.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00647.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00112.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00814.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00115.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00075.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00446.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00450.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00079.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00425.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00703.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00571.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00574.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00756.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00582.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00563.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00591.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00842.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00882.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00888.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00203.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00020.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00418.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00621.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00089.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00110.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00820.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00634.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00180.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00641.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00231.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00469.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00557.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00447.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00765.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00876.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00357.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00196.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00404.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00422.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00688.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00444.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00723.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00645.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00577.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00773.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00791.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00167.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00595.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00602.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00606.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00611.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00614.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00618.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00065.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00626.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00629.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00632.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00182.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00637.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00207.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00646.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00253.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00274.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00293.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00660.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00334.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00668.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00674.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00678.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00682.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00902.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00692.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00713.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00573.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00748.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00580.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00783.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00804.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00593.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00396.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00874.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00900.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00612.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00272.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00777.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00426.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00101.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00119.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00442.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00534.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00643.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00221.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00849.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00132.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00164.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00303.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00327.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00346.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00671.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00393.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00412.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00433.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00690.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00249.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00252.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00255.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00259.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00263.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00266.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00270.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00273.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00275.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00279.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00282.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00284.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00288.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00291.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00761.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00299.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00302.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00306.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00310.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00314.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00320.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00322.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00326.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00328.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00331.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00333.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00336.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00339.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00345.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00349.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00353.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00356.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00413.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00733.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00613.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00764.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00583.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00588.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00129.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00599.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00604.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00465.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00553.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00032.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00054.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00624.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00072.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00189.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00633.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00168.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00639.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00644.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00648.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00264.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00283.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00659.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00663.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00667.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00368.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00676.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00680.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00686.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00455.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00788.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00790.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00794.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00798.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00803.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00808.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00811.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00815.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00818.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00823.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00828.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00833.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00839.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00841.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00844.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00847.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00850.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00854.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00857.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00859.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00864.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00869.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00873.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00877.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00881.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00886.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00891.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00896.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00899.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00903.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00907.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00909.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00570.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00380.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00576.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00895.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00525.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00778.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00829.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00851.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00215.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00610.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00007.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00616.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00620.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00076.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00628.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00631.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00143.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00636.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00193.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00478.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00242.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00652.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00654.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00890.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00091.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00209.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00317.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00423.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00521.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00625.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00731.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00248.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00352.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00461.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00549.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00058.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00757.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00550.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00067.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00184.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00296.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00406.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00505.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00116.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00651.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00286.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00301.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00120.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00243.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00347.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00456.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00544.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00827.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00753.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00858.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00063.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00178.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00290.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00401.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00499.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00594.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00708.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00797.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00121.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00125.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00128.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00131.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00135.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00141.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00145.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00150.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00155.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00158.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00161.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00736.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00169.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00172.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00177.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00181.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00186.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00190.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00195.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00200.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00204.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00208.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00213.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00218.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00222.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00226.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00228.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00233.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00236.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00240.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00244.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00118.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00123.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00126.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00590.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00133.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00139.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00142.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00148.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00153.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00156.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00159.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00162.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00343.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00170.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00174.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00179.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00183.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00187.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00192.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00198.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00202.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00206.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00210.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00214.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00219.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00223.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00227.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00230.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00234.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00237.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00241.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00246.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00458.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00545.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00655.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00754.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00537.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00064.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00701.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00292.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00403.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00500.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00596.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00710.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00799.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00004.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00431.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00239.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00341.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00452.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00541.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00371.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00751.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00856.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00138.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00173.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00287.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00398.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00495.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00589.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00705.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00793.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00000.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00001.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00003.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00006.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00011.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00014.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00019.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00022.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00027.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00031.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00036.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00042.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00044.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00047.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00050.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00053.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00057.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00061.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00397.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00415.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00435.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00460.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00477.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00493.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00512.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00529.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00547.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00566.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00587.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00615.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00635.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00658.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00684.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00789.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00792.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00796.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00801.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00805.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00810.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00813.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00817.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00821.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00826.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00831.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00835.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00840.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00843.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00846.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00569.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00852.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00619.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00638.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00863.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00867.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00871.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00875.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00879.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00883.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00889.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00894.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00898.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00901.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00905.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00908.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00762.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00872.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00074.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00191.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00370.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00885.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00510.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00137.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00721.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00812.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00015.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00127.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00250.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00354.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00463.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00551.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00662.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00759.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00866.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00069.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00704.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00298.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00408.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00507.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00605.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00714.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00806.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00009.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00122.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00245.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00348.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00457.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00832.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00035.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00147.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00166.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00699.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00476.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00565.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00683.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00772.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00887.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00088.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00779.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00428.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00037.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00519.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00623.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00729.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00825.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00029.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00515.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00262.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00670.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00474.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00562.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00677.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00768.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00880.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00082.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00201.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00308.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00416.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00695.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00696.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00700.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00702.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00706.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00709.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00712.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00716.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00720.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00722.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00693.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00727.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00732.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00735.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00738.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00740.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00742.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00744.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00747.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00749.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00752.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00277.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00028.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00760.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00763.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00766.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00769.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00771.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00774.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00188.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00782.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00784.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00475.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00048.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00482.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00484.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00486.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00488.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00490.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00494.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00498.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00503.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00257.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00509.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00511.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00513.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00516.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00520.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00523.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00527.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00528.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00530.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00532.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00535.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00538.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00542.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00026.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00548.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00552.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00555.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00470.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00561.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00564.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00838.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00041.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00152.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00269.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00719.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00480.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00862.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00502.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00775.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00892.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00093.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00211.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00321.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00056.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00584.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00229.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00734.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00830.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00033.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00144.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00265.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00694.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00325.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00154.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00681.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00770.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00884.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00086.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00205.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00313.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00420.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00517.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00271.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00289.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00311.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00332.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00344.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00377.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00400.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00315.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00438.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00518.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00481.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00496.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00514.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00531.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00140.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00261.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00592.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00473.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00560.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00661.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00689.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00707.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00725.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00743.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00758.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00776.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00795.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00819.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00845.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00865.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00893.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00483.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00501.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00664.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00533.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00554.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00572.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00597.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00622.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00642.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00666.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00607.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00717.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00728.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00745.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00124.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00780.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00351.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00824.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00848.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00870.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00897.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00579.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00225.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00051.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00294.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00097.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00504.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00600.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00163.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00802.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00216.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00238.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00448.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00539.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00649.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00868.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00543.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00175.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00691.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00285.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00853.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00492.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00586.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00609.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00280.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00786.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00436.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00232.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00335.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00220.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00739.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00379.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00746.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00685.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00146.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00165.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00281.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00391.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00489.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00581.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00698.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00787.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00906.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00012.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00038.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00059.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00080.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00104.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00330.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00149.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00171.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00199.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00224.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00247.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00267.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00837.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00307.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00329.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00350.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00373.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00098.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00217.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00324.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00430.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00323.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00304.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00737.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00836.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00040.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00151.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00268.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00375.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00479.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00567.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00687.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00024.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00319.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00068.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00094.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00113.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00134.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00160.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00185.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00212.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00235.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00367.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00276.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00297.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00679.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00338.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00359.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00385.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00407.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00312.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00419.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00617.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00254.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00726.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00822.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00016.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00136.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00258.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00361.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00672.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00558.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00673.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00260.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00362.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00300.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00601.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00340.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00363.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00389.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00410.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00429.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00451.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00472.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00487.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00508.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00526.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00540.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00559.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00176.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00608.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00630.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00497.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00675.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00697.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00718.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00807.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00750.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00767.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00785.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00809.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00834.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00855.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00878.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00904.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00715.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00337.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00010.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00491.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00108.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00640.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00278.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00546.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00657.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00755.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00860.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00197.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00387.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00781.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00730.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00524.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00598.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00711.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00800.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00443.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00117.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00318.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00342.jpg">
<meta property="og:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00453.jpg">
<meta property="og:updated_time" content="2020-08-16T10:35:12.286Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="book_《Python机器学习算法》">
<meta name="twitter:description" content="作者: 赵志勇出版社: 电子工业出版社出版年: 2017-7丛书: 博文视点AI系列ISBN: 9787121313196 内容简介 本书是一本机器学习入门读物，注重理论与实践的结合。全书主要包括6个部分，每个部分均以典型的机器学习算法为例，从算法原理出发，由浅入深，详细介绍算法的理论，并配合目前流行的Python语言，从零开始，实现每一个算法，以加强对机器学习算法理论的理解、增强实际的算法实践能">
<meta name="twitter:image" content="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/Image00366.jpg">





  
  
  <link rel="canonical" href="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>book_《Python机器学习算法》 | Hac_lang</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hac_lang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">小白hac_lang的笔记，涉及内容包含但不限于：人工智能   基因工程    信息安全   软件工程   嵌入式   天文物理</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-news">

    
    
      
    

    

    <a href="/news/" rel="section"><i class="menu-item-icon fa fa-fw fa-rss"></i> <br>news</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    
  
  

  

  <a href="https://github.com/HACLANG" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://haclang.github.io/2017/08/20/book-《Python机器学习算法》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hac_lang">
      <meta itemprop="description" content="小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hac_lang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">book_《Python机器学习算法》

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-08-20 22:15:23" itemprop="dateCreated datePublished" datetime="2017-08-20T22:15:23+08:00">2017-08-20</time>
            </span>
          

          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>作者: 赵志勇<br>出版社: 电子工业出版社<br>出版年: 2017-7<br>丛书: 博文视点AI系列<br>ISBN: 9787121313196</p>
<p><strong>内容简介</strong></p>
<p>本书是一本机器学习入门读物，注重理论与实践的结合。全书主要包括6个部分，每个部分均以典型的机器学习算法为例，从算法原理出发，由浅入深，详细介绍算法的理论，并配合目前流行的Python语言，从零开始，实现每一个算法，以加强对机器学习算法理论的理解、增强实际的算法实践能力，最终达到熟练掌握每一个算法的目的。与其他机器学习类图书相比，本书同时包含算法理论的介绍和算法的实践，以理论支撑实践，同时，又将复杂、枯燥的理论用简单易懂的形式表达出来，促进对理论的理解。  </p>
<p><strong>推荐序</strong></p>
<p>志勇是我在新浪微博的同事，刚来的时候坐我的旁边。记得当时志勇喜欢把看过的论文的重点部分剪下来粘到自己的笔记本上，并用五颜六色的笔标注，后来还知道志勇平时会写博客来记录自己在算法学习和实践中的心得。由此可见，志勇是一个非常认真且善于归纳总结的人。2016年志勇告诉我他在写这样一本书的时候，我深以为然，感觉正确的人做了一件正确的事。</p>
<p>志勇的书快完成的时候邀请我写序，这对我绝对是个挑战。幸运的是，书中的内容是我所熟悉的，仿佛是发生在自己身边的事。写作风格也和志勇平时交流时一致。所以，我可以从故事参与者的角度去介绍一下这本书。</p>
<p>说到机器学习算法，这两年可谓蓬勃发展。AlphaGo战胜世界围棋冠军李世石已经成了大家茶余饭后的谈资，无人驾驶汽车是资本竞相追逐的万亿级市场，这些都源于数据收集能力、计算能力的提升，以及智能设备的普及。机器学习也已经在我们身边一些领域取得了成功，例如，现在已经获得上亿用户使用的今日头条。今日头条通过抓取众多媒体的资讯，利用机器学习算法推荐给用户，从而做到了资讯量大、更新快、更加个性化。</p>
<p>作为一个互联网从业者，更是感觉到机器学习算法已经融入到越来越多的产品和功能中。我曾经在一次交流中得知，某个应用为减少用户输入，用了一个团队的力量做输入选项的推荐。这在以前是不曾出现过的，以前的网站更多的是增加功能，让用户选择。现在更多的是推荐给用户，帮助用户选择。这里面既有移动应用屏幕小、操作复杂的原因，也有互联网公司越来越重视用户体验的原因。所以，在此要恭喜这本书的读者，你们选择了一个前途光明的行业。</p>
<p>机器学习算法比较典型的应用是推荐、广告和搜索。我们利用协同过滤技术来推荐商品、利用逻辑回归技术来做点击率的预测、利用分类技术来识别“垃圾”网页等。学好、用好每一种算法都很困难，需要掌握背后的理论基础，以及进行大量的实践，否则就会浮于表面，模仿他人，不能根据自己的业务做出合理的选择。</p>
<p>而市场上的书通常要么只是一些概要性质的介绍，要么是偏向实战，理论基础介绍得比较少。本书是少有的两者兼具的书，每一种算法都先介绍数学基础，再用Python代码做简单版本的实现，并且算法之间循序渐进，层层深入，读来如沐春风。</p>
<p>本书介绍了 LR、FM、SVM、协同过滤、矩阵分解等推荐和广告领域常用算法，有很强的实用性。深度学习更是近期主流互联网公司研究的热门领域。无论对机器学习的初学者还是已经具备一些项目经验的人来说，这都是很好的读本。希望本书对更多的人有益，也希望中国的“人工智能+”蓬勃发展。</p>
<p>新浪微博算法经理 陶辉</p>
<p><strong>前言</strong></p>
<p>起源</p>
<p>在读研究生期间，我就对机器学习算法萌生了很浓的兴趣，并对机器学习中的常用算法进行了学习，利用 MATLAB 对每一个算法进行了实践。在此过程中，每当遇到不懂的概念或者算法时，就会在网上查找相关的资料。也看到很多人在博客中分享算法的学习心得及算法的具体过程，其中有不少内容让我受益匪浅，但是有的内容仅仅是算法的描述，缺少实践的具体过程。</p>
<p>注意到这一点之后，我决定开始在博客中分享自己学习每一个机器学习算法的点点滴滴，为了让更多的初学者能够理解算法的具体过程并从中受益，我计划从三个方面出发，第一是算法过程的简单描述，第二是算法理论的详细推导，第三是算法的具体实践。2014年1月10日，我在CSDN上写下了第一篇博客。当时涉及的方向主要是优化算法和简单易学的机器学习算法。</p>
<p>随着学习的深入，博客的内容越来越多，同时，在写作过程中，博客的质量也在慢慢提高，这期间也是机器学习快速发展的阶段，在行业内出现了很多优秀的算法库，如Java版本的weka、Python版本的sklearn，以及其他的一些开源程序，通过对这些算法库的学习，我丰富了很多算法的知识，同时，我将学习到的心得记录在简单易学的机器学习算法中。工作之后，越发觉得这些基础知识对于算法的理解很有帮助，积累的这些算法学习材料成了我宝贵的财富。</p>
<p>2016年，电子工业出版社博文视点的符隆美编辑联系到我，询问我是否有意向将这些博文汇总写一本书。能够写一本书是很多人的梦想，我也不例外。于是在 2016年9月，我开始了对本书的构思，从选择算法开始，选择出使用较多的一些机器学习算法。在选择好算法后，从算法原理和算法实现两个方面对算法进行描述，希望本书能够在内容上既能照顾到初学者，又能使具有一定机器学习基础的读者从中受益。</p>
<p>在写作的过程中，我重新查阅了资料，力求保证知识的准确性，同时，在实践的环节中，我使用了目前比较流行的Python语言实现每一个算法，使得读者能够更容易理解算法的过程，在介绍深度学习的部分时，使用到了目前最热门的TensorFlow框架。为了帮助读者理解机器学习算法在实际工作中的具体应用，本书专门有一章介绍项目实践的部分，综合前面各种机器学习算法，介绍每一类算法在实际工作中的具体应用。</p>
<p>内容组织</p>
<p>本书开篇介绍机器学习的基本概念，包括监督学习、无监督学习和深度学习的基本概念。</p>
<p>第一部分介绍分类算法。分类算法是机器学习中最常用的算法。在分类算法中着重介绍Logistic回归、Softmax Regression、Factorization Machine、支持向量机、随机森林和BP神经网络等算法。</p>
<p>第二部分介绍回归算法。与分类算法不同的是，在回归算法中其目标值是连续的值，而在分类算法中，其目标值是离散的值。在回归算法中着重介绍线性回归、岭回归和CART树回归。</p>
<p>第三部分介绍聚类算法。聚类是将具有某种相同属性的数据聚成一个类别。在聚类算法中着重介绍K-Means算法、Mean Shift算法、DBSCAN算法和Label Propagation算法。</p>
<p>第四部分介绍推荐算法。推荐算法是一类基于具体应用场景的算法的总称。在推荐算法中着重介绍基于协同过滤的推荐、基于矩阵分解的推荐和基于图的推荐。</p>
<p>第五部分介绍深度学习。深度学习是近年来研究最为火热的方向。深度学习的网络模型和算法有很多种，在本书中，主要介绍最基本的两种算法：AutoEncoder 和卷积神经网络。</p>
<p>第六部分介绍以上这些算法在具体项目中的实践。通过具体的例子，可以清晰地看到每一类算法的应用场景。</p>
<p>附录介绍在实践中使用到的Python语言、numpy库及TensorFlow框架的具体使用方法。</p>
<p>小结</p>
<p>本书试图从算法原理和实践两个方面来介绍机器学习中的常用算法，对每一类机器学习算法，精心挑选了具有代表性的算法，从理论出发，并配以详细的代码，本书的所有示例代码使用 Python 语言作为开发语言，读者可以从 https：//github.com/zhaozhiyong19890102/Python-Machine- Learning-Algorithm中下载本书的全部示例代码。</p>
<p>由于时间仓促，书中难免存在错误，欢迎广大读者和专家批评、指正，同时，欢迎大家提供意见和反馈。本书作者的电子邮箱：zhaozhiyong1989＠126.com。</p>
<p>赵志勇</p>
<p>2017年6月6日于北京</p>
<p><strong>读者服务</strong></p>
<p>轻松注册成为博文视点社区用户（www.broadview.com.cn），扫码直达本书页面。</p>
<p>• <strong>提交勘误</strong> ：您对书中内容的修改意见可在 _提交勘误_ 处提交，若被采纳，将获赠博文视点社区积分（在您购买电子书时，积分可用来抵扣相应金额）。</p>
<p>• <strong>交流互动</strong> ：在页面下方 _读者评论_ 处留下您的疑问或观点，与我们和其他读者一同学习交流。</p>
<p>页面入口：http：//www.broadview.com.cn/31319</p>
<p><img src="Image00366.jpg" alt></p>
<p>三步加入“人工智能交流群”，实时获取资源共享，并有机会与大咖实时交流。</p>
<p>•扫码添加小编为微信好友。</p>
<p>•申请验证时输入“AI”。</p>
<p>•小编带你加入“人工智能交流群”。</p>
<p><img src="Image00358.jpg" alt></p>
<h2 id="0-绪论"><a href="#0-绪论" class="headerlink" title="0 绪论"></a>0 绪论</h2><p>从计算机被发明后，实现人工智能（Artificial Intelligence，AI）成了一代代科学家和技术工作者的奋斗目标，在寻求解决方案的过程中，人们遇到了很多的难题，同时为克服这些难题做了很多的尝试。</p>
<p>随着时代的发展，大量的网络应用出现在人们的生活中，各种智能设备的出现使数据的收集变成现实，同时，计算机的计算能力得到了很大的提高，如何从大量数据中提取出有价值的信息成了非常重要的课题，机器学习就是这样一种能够从无序的数据中提取出有用信息的工具。</p>
<h3 id="0-1-机器学习基础"><a href="#0-1-机器学习基础" class="headerlink" title="0.1 机器学习基础"></a>0.1 机器学习基础</h3><h4 id="0-1-1-机器学习的概念"><a href="#0-1-1-机器学习的概念" class="headerlink" title="0.1.1 机器学习的概念"></a>0.1.1 机器学习的概念</h4><p>机器学习能够从无序的数据中提取出有用的信息，那么什么是机器学习呢？以垃圾邮件的检测为例，垃圾邮件的检测是指能够对邮件做出判断，判断其为垃圾邮件还是正常邮件。</p>
<p>在人工智能技术发展的初期，人们尝试通过手写规则来解决许多问题。例如，在垃圾邮件的检测中，当邮件中出现事先指定的一些可能为垃圾邮件的词时，这条邮件很可能是垃圾邮件，同时，当邮件里出现链接时，它也很可能是垃圾邮件。这些规则在一定程度上对垃圾邮件的检测起到了一些作用，但是随着规则越来越多，这样的检测系统也变得越来越复杂。这时候，人们发现解决这种问题的根本途径是如何自动地从数据的某些特征中学习他们之间的关系，并且随着对数据的不断学习，提升垃圾检测的性能。</p>
<p>机器学习是从数据中学习和提取有用的信息，不断提升机器的性能。那么，对于一个具体的机器学习的问题，很重要的一部分是对数据的收集，我们称这部分数据为训练数据。机器学习的基本工作是从这些数据中学习规则，利用学习到的规则来预测新的数据。</p>
<h4 id="0-1-2-机器学习算法的分类"><a href="#0-1-2-机器学习算法的分类" class="headerlink" title="0.1.2 机器学习算法的分类"></a>0.1.2 机器学习算法的分类</h4><p>在机器学习中，根据任务的不同，可以分为监督学习（Supervised Learning）、无监督学习（Unsupervised Learning）、半监督学习（Semi-Supervised Learning）和增强学习（Reinforcement Learning）。</p>
<p>监督学习（Supervised Learning）的训练数据包含了类别信息，如在垃圾邮件检测中，其训练样本包含了邮件的类别信息：垃圾邮件和非垃圾邮件。在监督学习中，典型的问题是分类（Classification）和回归（Regression），典型的算法有Logistic Regression、BP神经网络算法和线性回归算法。</p>
<p>与监督学习不同的是，无监督学习（Unsupervised Learning）的训练数据中不包含任何类别信息。在无监督学习中，其典型的问题为聚类（Clustering）问题，代表算法有K- Means算法、DBSCAN算法等。</p>
<p>半监督学习（Semi-Supervised Learning）的训练数据中有一部分数据包含类别信息，同时有一部分数据不包含类别信息，是监督学习和无监督学习的融合。在半监督学习中，其算法一般是在监督学习的算法上进行扩展，使之可以对未标注数据建模。</p>
<p>监督学习和无监督学习是使用较多的两种学习方法，而半监督学习是监督学习和无监督学习的融合，在本书中，我们着重介绍监督学习和非监督学习。</p>
<h3 id="0-2-监督学习"><a href="#0-2-监督学习" class="headerlink" title="0.2 监督学习"></a>0.2 监督学习</h3><h4 id="0-2-1-监督学习"><a href="#0-2-1-监督学习" class="headerlink" title="0.2.1 监督学习"></a>0.2.1 监督学习</h4><p>前边简单介绍了监督学习（Supervised Learning）的概念，监督学习是机器学习算法中的一种重要的学习方法，在监督学习中，其训练样本中同时包含有特征和标签信息。在监督学习中，分类（Classification）算法和回归（Regression）算法是两类最重要的算法，两者之间最主要的区别是分类算法中的标签是离散的值，如广告点击问题中的标签为{+1，-1}，分别表示广告的点击和未点击，而回归算法中的标签值是连续的值，如通过人的身高、性别、体重等信息预测人的年龄，因为年龄是连续的正整数，因此标签为y∈N</p>
<ul>
<li>，且y∈[1，80]。</li>
</ul>
<h4 id="0-2-2-监督学习的流程"><a href="#0-2-2-监督学习的流程" class="headerlink" title="0.2.2 监督学习的流程"></a>0.2.2 监督学习的流程</h4><p>监督学习流程的具体过程如图0.1所示。</p>
<p><img src="Image00360.jpg" alt></p>
<p>图0.1 监督学习流程</p>
<p>对于具体的监督学习任务，首先是获取到带有属性值的样本，假设有m个训练样本{（X  （1） ，y （1） ），（X  （2） ，y （2） ），…，（X<br>（m） ，y （m） ）}，然后对样本进行预处理，过滤数据中的杂质，保留其中有用的信息，这个过程称为特征处理或者特征提取。</p>
<p>通过监督学习算法习得样本特征到样本标签之间的假设函数。监督学习通过从样本数据中习得假设函数，并用其对新的数据进行预测。</p>
<h4 id="0-2-3-监督学习算法"><a href="#0-2-3-监督学习算法" class="headerlink" title="0.2.3 监督学习算法"></a>0.2.3 监督学习算法</h4><p>分类问题（Classification）是指通过训练数据学习一个从观测样本到离散的标签的映射，分类问题是一个监督学习问题。典型的问题有：①垃圾邮件的分类（Spam Classification）：训练样本是邮件中的文本，标签是每个邮件是否是垃圾邮件（{+1，-1}，+1表示是垃圾邮件，-1表示不是垃圾邮件），目标是根据这些带标签的样本，预测一个新的邮件是否是垃圾邮件；②点击率预测（Click- through Rate Prediction）：训练样本是用户、广告和广告主的信息，标签是是否被点击（{+1，-1}，+1表示点击，-1表示未点击）。目标是在广告主发布广告后，预测指定的用户是否会点击，上述两种问题都是二分类的问题；③手写字识别，即识别是{0，1，…，9}中的哪个数字，这是一个多分类的问题。</p>
<p>与分类问题不同的是，回归问题（Regression）是指通过训练数据学习一个从观测样本到连续的标签的映射，在回归问题中的标签是一系列连续的值。典型的回归问题有：①股票价格的预测，即利用股票的历史价格预测未来的股票价格；②房屋价格的预测，即利用房屋的数据，如房屋的面积、位置等信息预测房屋的价格。</p>
<h3 id="0-3-无监督学习"><a href="#0-3-无监督学习" class="headerlink" title="0.3 无监督学习"></a>0.3 无监督学习</h3><h4 id="0-3-1-无监督学习"><a href="#0-3-1-无监督学习" class="headerlink" title="0.3.1 无监督学习"></a>0.3.1 无监督学习</h4><p>无监督学习（Unsupervised Learning）是另一种机器学习算法，与监督学习不同的是，在无监督学习中，其样本中只含有特征，不包含标签信息。与监督学习（Supervised Learning）不同的是，由于无监督学习不包含标签信息，在学习时并不知道其分类结果是否正确。</p>
<h4 id="0-3-2-无监督学习的流程"><a href="#0-3-2-无监督学习的流程" class="headerlink" title="0.3.2 无监督学习的流程"></a>0.3.2 无监督学习的流程</h4><p>无监督学习流程的具体过程如图0.2所示。</p>
<p><img src="Image00369.jpg" alt></p>
<p>图0.2 无监督学习流程</p>
<p>对于具体的无监督学习任务，首先是获取到带有特征值的样本，假设有m个训练数据{X  （1） ，X  （2） ，…，X （m）<br>}，对这m个样本进行处理，得到样本中有用的信息，这个过程称为特征处理或者特征提取，最后是通过无监督学习算法处理这些样本，如利用聚类算法对这些样本进行聚类。</p>
<h4 id="0-3-3-无监督学习算法"><a href="#0-3-3-无监督学习算法" class="headerlink" title="0.3.3 无监督学习算法"></a>0.3.3 无监督学习算法</h4><p>聚类算法是无监督学习算法中最典型的一种学习算法。聚类算法利用样本的特征，将具有相似特征的样本划分到同一个类别中，而不关心这个类别具体是什么。如表0-1所示的聚类问题：</p>
<p>表0-1 聚类问题</p>
<p><img src="Image00372.jpg" alt></p>
<p>在表0-1所示的聚类问题中，通过分别比较特征1（是否有翅膀）和特征2（是否有鳍），对上述的样本进行聚类。从表0-1中的数据可以看出，样本1和样本2较为相似，样本3和样本4较为相似，因此，可以将样本1和样本2划分到同一个类别中，将样本3和样本4划分到另一个类别中，而不用去关心样本1和样本2所属的类别具体是什么。</p>
<p>除了聚类算法，在无监督学习中，还有一类重要的算法是降维的算法，数据降维基本原理是将样本点从输入空间通过线性或非线性变换映射到一个低维空间，从而获得一个关于原数据集紧致的低维表示。在本书中，主要介绍聚类算法。</p>
<h3 id="0-4-推荐系统和深度学习"><a href="#0-4-推荐系统和深度学习" class="headerlink" title="0.4 推荐系统和深度学习"></a>0.4 推荐系统和深度学习</h3><p>在机器学习算法中，除了按照上述的分类将算法分成监督学习和无监督学习外，还有其他的一些分类方法，如按照算法的功能，将算法分成分类算法、回归算法、聚类算法和降维算法等。随着机器学习领域的不断发展，出现了很多新的研究方向，推荐算法和深度学习是近年来研究较多的方向。</p>
<h4 id="0-4-1-推荐系统"><a href="#0-4-1-推荐系统" class="headerlink" title="0.4.1 推荐系统"></a>0.4.1 推荐系统</h4><p>随着信息量的急剧扩大，信息过载的问题变得尤为突出，当用户无明确的信息需求时，用户无法从大量的信息中获取到感兴趣的信息，同时，信息量的急剧上升也导致了大量的信息被埋没，无法触达一些潜在用户。推荐系统（Recommendation System，RS）的出现被称为连接用户与信息的桥梁，一方面帮助用户从海量数据中找到感兴趣的信息，另一方面将有价值的信息传递给潜在用户。</p>
<p>在推荐系统中，推荐算法起着重要的作用，常用的推荐算法主要有：协同过滤算法、基于矩阵分解的推荐算法和基于图的推荐算法。</p>
<h4 id="0-4-2-深度学习"><a href="#0-4-2-深度学习" class="headerlink" title="0.4.2 深度学习"></a>0.4.2 深度学习</h4><p>传统的机器学习算法都是利用浅层的结构，这些结构一般包含最多一到两层的非线性特征变换，浅层结构在解决很多简单的问题上效果较为明显，但是在处理一些更加复杂的与自然信号的问题时，就会遇到很多问题。</p>
<p>随着计算机的不断发展，人们尝试使用深层的结构来处理这些更加复杂的问题，但是，同样也遇到了很多的困难，直到2006年，Hinton等人提出了逐层训练的概念，深度学习又一次进入了人们的视野，数据量的不断扩大以及计算机计算能力的增强，使得深度学习技术成为可能。在深度学习中，常用的几种模型包括：①自编码器模型，通过堆叠自编码器构建深层网络；②卷积神经网络模型，通过卷积层与采样层的不断交替构建深层网络；③循环神经网络。</p>
<h3 id="0-5-Python和机器学习算法实践"><a href="#0-5-Python和机器学习算法实践" class="headerlink" title="0.5 Python和机器学习算法实践"></a>0.5 Python和机器学习算法实践</h3><p>在本书中，选择Python作为机器学习算法实践的语言，主要是因为Python语言的天然优势，同时，在机器学习算法中，涉及大量的与线性代数相关的知识，Python中有Numpy函数库可以专门用于处理各种线性代数的问题。</p>
<p>Python语言有很多优势，如：①Python社区有庞大的库，几乎可以解决大部分问题；②Python被称为胶水语言，可以以混合编译的方式使用C/C++/Java等语言的库；③Python语法简单，同时易于操作。</p>
<p>然而，对于 Python 语言来说，其唯一不足就是性能问题，Python 程序的运行效率不如Java或者C语言，但是，在Python中提供了调用Java和C语言的方法，因此，对于计算要求比较高的部分，可以使用 C 语言或者 Java 实现，这样就能同时利用Python的简单易用性和C语言的高效性。Python语言性能不是本书考虑的重点，因此不会在本书中提及Python性能相关的问题。</p>
<p>对于Python语言的各种操作以及对numpy的各种详细操作，可以参见附录A。</p>
<p><strong>参考文献</strong></p>
<p>[1] Peter Harrington.机器学习实战[M].王斌，译.北京：人民邮电出版社.2013.</p>
<p>[2] Wesley J.Chun.Python核心编程（第二版）[M].宋吉广，译.北京：人民邮电出版社.-2008.</p>
<p>[3] 李航.统计学习方法[M].北京：清华大学出版社.2012.</p>
<p>[4] 周志华.机器学习[M].北京：清华大学出版社.2016.</p>
<h1 id="第一部分-分类算法"><a href="#第一部分-分类算法" class="headerlink" title="第一部分 分类算法"></a>第一部分 分类算法</h1><p>分类算法是指根据样本的特征，将样本划分到指定的类别中。分类算法是一种监督的学习算法，在分类算法中，根据训练样本训练得到样本特征到样本标签之间的映射，利用该映射得到新的样本的标签，达到将新的样本划分到不同类别的目的。</p>
<p>第1章介绍最基本的分类算法Logistic Regression算法，基本的Logistic Regression算法是一个二分类的线性分类算法，由于其简单的特点，得到了广泛的应用；第2章介绍 Logistic Regression 算法的推广形式——Softmax Regression 算法，Softmax Regression算法用于处理多分类问题；第3章介绍Logistic Regression算法的另一种推广形式——Factorization Machine算法，该算法在Logistic Regression算法的基础上增加了交叉项；第4章介绍支持向量机SVM分类器，SVM分类算法是被公认的比较优秀的分类算法；第5章介绍一种集成的分类方法——随机森林，在集成方法中，通过组合多个分类器求解复杂的分类问题；第6章介绍BP神经网络算法，该算法可以有效解决线性不可分的问题，同时，BP神经网络是深度学习中算法的基础。</p>
<h2 id="1-Logistic-Regression"><a href="#1-Logistic-Regression" class="headerlink" title="1 Logistic Regression"></a>1 Logistic Regression</h2><p>分类算法是典型的监督学习，其训练样本中包含样本的特征和标签信息。在二分类中，标签为离散值，如{+1，-1}，分别表示正类和负类。分类算法通过对训练样本的学习，得到从样本特征到样本的标签之间的映射关系，也被称为假设函数，之后可利用该假设函数对新数据进行分类。</p>
<p>Logistic Regression算法是一种被广泛使用的分类算法，通过训练数据中的正负样本，学习样本特征到样本标签之间的假设函数，Logistic Regression算法是典型的线性分类器，由于算法的复杂度低、容易实现等特点，在工业界得到了广泛的应用，如：利用Logistic Regression算法实现广告的点击率预估。</p>
<h3 id="1-1-Logistic-Regression模型"><a href="#1-1-Logistic-Regression模型" class="headerlink" title="1.1 Logistic Regression模型"></a>1.1 Logistic Regression模型</h3><h4 id="1-1-1-线性可分VS线性不可分"><a href="#1-1-1-线性可分VS线性不可分" class="headerlink" title="1.1.1 线性可分VS线性不可分"></a>1.1.1 线性可分VS线性不可分</h4><p>对于一个分类问题，通常可以分为线性可分与线性不可分两种。如果一个分类问题可以使用线性判别函数正确分类，则称该问题为线性可分，如图1.1所示；否则为线性不可分问题，如图1.2所示。</p>
<p><img src="Image00374.jpg" alt></p>
<p>图1.1 线性可分</p>
<p><img src="Image00376.jpg" alt></p>
<p>图1.2 线性不可分</p>
<h4 id="1-1-2-Logistic-Regression模型"><a href="#1-1-2-Logistic-Regression模型" class="headerlink" title="1.1.2 Logistic Regression模型"></a>1.1.2 Logistic Regression模型</h4><p>Logistic Regression 模型是广义线性模型的一种，属于线性的分类模型。对于图1.1 所示的线性可分的问题，需要找到一条直线，能够将两个不同的类区分开，如图1.1所示，这条直线也称为超平面。</p>
<p>对于上述的超平面，可以使用如下的线性函数表示：</p>
<p><img src="Image00378.jpg" alt></p>
<p>其中W 为权重，b为偏置。若在多维的情况下，权重W 和偏置b均为向量。在Logistic Regression算法中，通过对训练样本的学习，最终得到该超平面，将数据分成正负两个类别。此时，可以使用阈值函数，将样本映射到不同的类别中，常见的阈值函数有Sigmoid函数，其形式如下所示：</p>
<p><img src="Image00381.jpg" alt></p>
<p>Sigmoid函数的图像如图1.3所示。</p>
<p><img src="Image00383.jpg" alt></p>
<p>图1.3 Sigmoid函数</p>
<p>从Sigmoid函数的图像可以看出，其函数的值域为（0，1），在0附近的变化比较明显。其导函数 f′（x）为：</p>
<p><img src="Image00384.jpg" alt></p>
<p>现在，让我们利用Python实现Sigmoid函数，为了能够使用numpy中的函数，我们首先需要导入numpy：</p>
<p>import numpy as np</p>
<p>Sigmoid函数的具体实现过程如程序清单1-1所示。</p>
<p><strong>程序清单1-1 S igmoid函数</strong></p>
<p><img src="Image00386.jpg" alt></p>
<p>在程序清单1-1中，Sigmoid函数的输出为Simoid值。对于输入向量X，其属于正例的概率为：</p>
<p><img src="Image00388.jpg" alt></p>
<p>其中， _σ_ 表示的是Sigmoid函数。那么，对于输入向量X，其属于负例的概率为：</p>
<p><img src="Image00390.jpg" alt></p>
<p>对于Logistic Regression算法来说，需要求解的分隔超平面中的参数，即为权重矩阵W 和偏置向量b，那么，这些参数该如何求解呢？为了求解模型的两个参数，首先必须定义损失函数。</p>
<h4 id="1-1-3-损失函数"><a href="#1-1-3-损失函数" class="headerlink" title="1.1.3 损失函数"></a>1.1.3 损失函数</h4><p>对于上述的Logistic Regression算法，其属于类别y的概率为：</p>
<p><img src="Image00392.jpg" alt></p>
<p>要求上述问题中的参数W 和b，可以使用极大似然法对其进行估计。假设训练数据集有m个训练样本{（X  （1） ，y （1） ），（X  （2） ，y （2）<br>），…，（X  （m） ，y （m） ）}，则其似然函数为：</p>
<p><img src="Image00394.jpg" alt></p>
<p>其中，假设函数h W，b （X （i） ）为：</p>
<p><img src="Image00395.jpg" alt></p>
<p>对于似然函数的极大值的求解，通常使用Log似然函数，在Logistic Regression算法中，通常是将负的Log似然函数作为其损失函数，即the negative log-likelihood （NLL）作为其损失函数，此时，需要计算的是NLL的极小值。损失函数l W，b 为：</p>
<p><img src="Image00399.jpg" alt></p>
<p>此时，我们需要求解的问题为：</p>
<p><img src="Image00402.jpg" alt></p>
<p>为了求得损失函数l W，b 的最小值，可以使用基于梯度的方法进行求解。</p>
<h3 id="1-2-梯度下降法"><a href="#1-2-梯度下降法" class="headerlink" title="1.2 梯度下降法"></a>1.2 梯度下降法</h3><p>在机器学习算法中，对于很多监督学习模型，需要对原始的模型构建损失函数l，接下来便是通过优化算法对损失函数l进行优化，以便寻找到最优的参数W。在求解机器学习参数W 的优化算法时，使用较多的是基于梯度下降的优化算法（Gradient Descent，GD）。</p>
<p>梯度下降法有很多优点，其中，在梯度下降法的求解过程中，只需求解损失函数的一阶导数，计算的成本比较小，这使得梯度下降法能在很多大规模数据集上得到应用。梯度下降法的含义是通过当前点的梯度方向寻找到新的迭代点，并从当前点移动到新的迭代点继续寻找新的迭代点，直到找到最优解。</p>
<h4 id="1-2-1-梯度下降法的流程"><a href="#1-2-1-梯度下降法的流程" class="headerlink" title="1.2.1 梯度下降法的流程"></a>1.2.1 梯度下降法的流程</h4><p>梯度下降法是一种迭代型的优化算法，根据初始点在每一次迭代的过程中选择下降法方向，进而改变需要修改的参数，对于优化问题min f<br>（w），梯度下降法的详细过程如下所示。</p>
<p>• 随机选择一个初始点w 0</p>
<p>• 重复以下过程</p>
<p>○ 决定梯度下降的方向：<img src="Image00405.jpg" alt></p>
<p>○ 选择步长 _α_</p>
<p>○ 更新：w i+1 =w i + _α_ · d  i</p>
<p>• 直到满足终止条件</p>
<p>具体过程如图1.4所示。</p>
<p><img src="Image00409.jpg" alt></p>
<p>图1.4 梯度下降的过程</p>
<p>在初始时，在点w 0 处，选择下降的方向d  0 ，选择步长 _α_ ，更新w的值，此时到达w 1 处，判断是否满足终止的条件，发现并未到达最优解w ∗ ，重复上述的过程，直至到达w ∗ 。</p>
<h4 id="1-2-2-凸优化与非凸优化"><a href="#1-2-2-凸优化与非凸优化" class="headerlink" title="1.2.2 凸优化与非凸优化"></a>1.2.2 凸优化与非凸优化</h4><p>简单来讲，凸优化问题是指只存在一个最优解的优化问题，即任何一个局部最优解即全局最优解，如图1.5所示。</p>
<p><img src="Image00411.jpg" alt></p>
<p>图1.5 凸函数</p>
<p>非凸优化是指在解空间中存在多个局部最优解，而全局最优解是其中的某一个局部最优解，如图1.6所示。</p>
<p><img src="Image00414.jpg" alt></p>
<p>图1.6 非凸函数</p>
<p>最小二乘（Least Squares）、岭回归（Ridge Regression）和Logistic回归（Logistic Regression）的损失函数都是凸优化问题。</p>
<h4 id="1-2-3-利用梯度下降法训练Logistic-Regression模型"><a href="#1-2-3-利用梯度下降法训练Logistic-Regression模型" class="headerlink" title="1.2.3 利用梯度下降法训练Logistic Regression模型"></a>1.2.3 利用梯度下降法训练Logistic Regression模型</h4><p>对于上述的Logistic Regression算法的损失函数可以通过梯度下降法对其进行求解，其梯度为：</p>
<p><img src="Image00578.jpg" alt></p>
<p>其中，<img src="Image00421.jpg" alt> 表示的是样本X  （i） 的第 j个分量。取w 0 =b，且将偏置项的变量x 0 设置为1，则可以将上述的梯度合并为：</p>
<p><img src="Image00424.jpg" alt></p>
<p>根据梯度下降法，得到如下的更新公式：</p>
<p><img src="Image00427.jpg" alt></p>
<p>利用上述的 Logistic Regression 中权重的更新公式，我们可以实现 Logistic Regression模型的训练，利用梯度下降法训练模型的具体过程，如程序清单1-2所示。</p>
<p><strong>程序清单1-2 Logistic Regression模型的训练</strong></p>
<p><img src="Image00432.jpg" alt></p>
<p><img src="Image00434.jpg" alt></p>
<p>在程序清单1-2中，函数lr_train_bgd使用了梯度下降法对Logistic Regression算法中的损失函数进行优化，其中，lr_train_bgd函数的输入为训练样本的特征、训练样本的标签、最大的迭代次数和学习率，在每一次迭代的过程中，需要计算当前的模型的误差，误差函数为 error_rate，如程序中①所示，error_rate 函数的具体实现形式如程序清单1-3所示。在迭代的过程中，不断通过梯度下降的方法对Logistic Regression算法中的权重进行更新，如程序中②所示。</p>
<p><strong>程序清单1-3 error_rate函数的实现</strong></p>
<p><img src="Image00437.jpg" alt></p>
<p>在程序清单1-3中，error_rate函数的输入为假设函数对训练样本的预测值h和训练样本的标签label，输出值为损失函数的值。损失函数的计算如程序代码中的①所示。</p>
<h3 id="1-3-梯度下降法的若干问题"><a href="#1-3-梯度下降法的若干问题" class="headerlink" title="1.3 梯度下降法的若干问题"></a>1.3 梯度下降法的若干问题</h3><h4 id="1-3-1-选择下降的方向"><a href="#1-3-1-选择下降的方向" class="headerlink" title="1.3.1 选择下降的方向"></a>1.3.1 选择下降的方向</h4><p>为了求解优化问题 f （w）的最小值，我们希望每次迭代的结果能够接近最优值w ∗ ，对于一维的情况，如图1.7所示。</p>
<p><img src="Image00440.jpg" alt></p>
<p>图1.7 下降的方向</p>
<p>若当前点的梯度为负，则最小值在当前点的右侧，若当前点的梯度为正，则最小值在当前点的左侧，负的梯度即为下降的方向。对于上述的一维的情况，有下述的更新规则：</p>
<p><img src="Image00445.jpg" alt></p>
<p>其中， i _α_ 为步长。对于二维的情况，此时更新的规则如下：</p>
<p><img src="Image00449.jpg" alt></p>
<h4 id="1-3-2-步长的选择"><a href="#1-3-2-步长的选择" class="headerlink" title="1.3.2 步长的选择"></a>1.3.2 步长的选择</h4><p>对于步长 _α_ 的选择，若选择太小，会导致收敛的速度比较慢；若选择太大，则会出现震荡的现象，即跳过最优解，在最优解附近徘徊，上述两种情况如图1.8所示。</p>
<p><img src="Image00454.jpg" alt></p>
<p>图1.8 步长太大或者太小</p>
<p>因此，选择合适的步长对于梯度下降法的收敛效果显得尤为重要，如图1.9所示。</p>
<p><img src="Image00459.jpg" alt></p>
<p>图1.9 合适的步长</p>
<h3 id="1-4-Logistic-Regression算法实践"><a href="#1-4-Logistic-Regression算法实践" class="headerlink" title="1.4 Logistic Regression算法实践"></a>1.4 Logistic Regression算法实践</h3><p>有了以上的理论准备，接下来，我们利用已经完成的函数，构建Logistic Regression分类器。我们利用线性可分的数据（如图 1.1 所示）作为训练样本来训练 Logistic Regression 模型，在构建模型的过程中，主要分为两个步骤：①利用训练样本训练模型；②利用训练好的模型对新样本进行预测。</p>
<h4 id="1-4-1-利用训练样本训练Logistic-Regression模型"><a href="#1-4-1-利用训练样本训练Logistic-Regression模型" class="headerlink" title="1.4.1 利用训练样本训练Logistic Regression模型"></a>1.4.1 利用训练样本训练Logistic Regression模型</h4><p>首先，我们利用训练样本训练模型，为了使得Python能够支持中文的注释和利用numpy工具，我们需要在训练文件“lr_train.py”的开始加入：</p>
<p><img src="Image00462.jpg" alt></p>
<p>在训练模型中，其主函数如程序清单1-4所示。</p>
<p><strong>程序清单1-4 训练模型的主函数</strong></p>
<p><img src="Image00464.jpg" alt></p>
<p><img src="Image00467.jpg" alt></p>
<p>在程序清单1-4的主函数中，训练LR模型的主要步骤包括：①导入训练数据，如程序代码中的①所示，导入训练数据的load_data函数如程序清单1-5所示；②利用梯度下降法对训练数据进行训练，以得到Logistic Regression算法的模型，即模型中的权重，如程序代码中的②所示；③将权重输出到文件 weights 中，如程序代码中的③所示，保存最终的模型的save_model函数如程序清单1-6所示。</p>
<p><strong>程序清单1-5 导入训练数据的load_data函数</strong></p>
<p><img src="Image00471.jpg" alt></p>
<p>在 load_data 函数中，其输入为训练数据所在的位置，其输出为训练数据的特征和训练数据的权重。</p>
<p><strong>程序清单1-6 保存最终的模型的save_model函数</strong></p>
<p><img src="Image00002.jpg" alt></p>
<p>在程序清单1-6中，函数save_model将训练好的LR模型以文件的形式保存，其中save_model函数的输入为保存的文件名file_name和所需保存的模型w。</p>
<h4 id="1-4-2-最终的训练效果"><a href="#1-4-2-最终的训练效果" class="headerlink" title="1.4.2 最终的训练效果"></a>1.4.2 最终的训练效果</h4><p>训练的具体过程为：</p>
<p><img src="Image00005.jpg" alt></p>
<p>通过上述的训练，最终得到的Logistic Regression模型的权重为：</p>
<p><img src="Image00008.jpg" alt></p>
<p>最终的分隔超平面如图1.10所示。</p>
<p><img src="Image00013.jpg" alt></p>
<p>图1.10 最终的分隔超平面</p>
<h4 id="1-4-3-对新数据进行预测"><a href="#1-4-3-对新数据进行预测" class="headerlink" title="1.4.3 对新数据进行预测"></a>1.4.3 对新数据进行预测</h4><p>对于分类算法而言，训练好的模型需要能够对新的数据集进行划分。利用上述步骤，我们训练好LR模型，并将其保存在“weights”文件中，此时，我们需要利用训练好的LR模型对新数据进行预测，同样，为了能够使用numpy中的函数和对中文注释的支持，在文件“lr_test.py”开始，我们加入：</p>
<p><img src="Image00017.jpg" alt></p>
<p>主函数如程序清单1-7所示。</p>
<p><strong>程序清单1-7 测试的主函数</strong></p>
<p><img src="Image00021.jpg" alt></p>
<p><img src="Image00025.jpg" alt></p>
<p>在对新数据集的预测中，首先是导入训练好的模型的参数，如程序中的①所示，导入模型的函数load_weight如程序清单1-8所示。其次需要导入测试数据，如程序中的②所示，导入测试数据的函数load_data如程序清单1-9所示。在模型和测试数据都导入之后，需要利用模型对新的数据进行预测，如程序中的③所示，函数predict的实现如程序清单1-10所示。最后需要将预测的结果保存到文件中，如程序代码中的④所示，函数save_result的实现如程序清单1-11所示。</p>
<p><strong>程序清单1-8 导入模型的load_weight函数</strong></p>
<p><img src="Image00030.jpg" alt></p>
<p>在程序清单 1-8 中，首先需要导入 numpy 模块和 lr_train 中的 sig 函数。在load_weight函数中，其输入是权重所在的位置，在导入函数中，将其数值导入到权重矩阵中。</p>
<p><strong>程序清单1-9 导入测试集的load_data函数</strong></p>
<p><img src="Image00034.jpg" alt></p>
<p><img src="Image00039.jpg" alt></p>
<p>在导入测试集的 load_data 函数中，其输入为测试集的位置和特征的个数，其中特征的个数用于判断测试集是否符合要求，如代码中的①所示，若不符合要求，则丢弃。</p>
<p><strong>程序清单1-10 对新数据集进行预测的predict函数</strong></p>
<p><img src="Image00043.jpg" alt></p>
<p>在predict函数中，其输入为测试数据的特征和模型的权重，输出为最终的预测结果。通过特征与权重的乘积，再对其求 Sigmoid 函数值得到最终的预测结果。在此，使用到了文件“lr_train.py”中的 sig 函数，因此，在文件“lr_test.py”中，需要导入sig函数：</p>
<p><img src="Image00046.jpg" alt></p>
<p>在计算最终的输出时，为了将Sigmoid函数输出的概率值转换成{0，1}，通常可以取0.5作为边界，如程序代码中的①和②所示。</p>
<p><strong>程序清单1-11 保存最终预测结果的save_result函数</strong></p>
<p><img src="Image00049.jpg" alt></p>
<p>在程序清单 1-11 中，函数 save_result 实现将预测结果存到指定的文件中，函数save_result 的输入为预测结果保存的文件名 file_name 和预测的结果 result，最终将result中的数据写入到文件file_name中。</p>
<p><strong>参考文献</strong></p>
<p>[1] 李航.统计学习方法[M].北京：清华大学出版社.2012.</p>
<p>[2] 周志华.机器学习[M].北京：清华大学出版社.2016</p>
<p>[3] Peter Harrington.机器学习实战[M].王斌，译.北京：人民邮电出版社.2013.</p>
<p>[4] Chapelle O,Manavoglu E,Rosales R.Simple and Scalable Response Prediction for Display Advertising[J].Acm Transactions on Intelligent Systems&amp;Technology,2014,5(4):1-34.</p>
<h2 id="2-Softmax-Regression"><a href="#2-Softmax-Regression" class="headerlink" title="2 Softmax Regression"></a>2 Softmax Regression</h2><p>由于Logistic Regression算法复杂度低、容易实现等特点，在工业界中得到广泛使用，如计算广告中的点击率预估等。但是，Logistic Regression算法主要是用于处理二分类问题，若需要处理的是多分类问题，如手写字识别，即识别是{0，1，…，9}中的数字，此时，需要使用能够处理多分类问题的算法。</p>
<p>Softmax Regression算法是Logistic Regression算法在多分类问题上的推广，主要用于处理多分类问题，其中，任意两个类之间是线性可分的。</p>
<h3 id="2-1-多分类问题"><a href="#2-1-多分类问题" class="headerlink" title="2.1 多分类问题"></a>2.1 多分类问题</h3><p>在上一章中介绍的Logistic Regression算法主要用于处理二分类问题，其类标签y取值个数为2，即y∈{0，1}或y∈{-1，1}。但是存在一类多分类的问题，即类标签y的取值个数大于2，如手写字识别，即识别是{0，1，…，9}中的数字，手写字如图2.1所示。</p>
<p><img src="Image00052.jpg" alt></p>
<p>图2.1 手写字识别</p>
<p>在图2.1中，手写字选自MNIST数据集。在MNIST手写字识别的数据集中，需要将图2.1中的手写字体划分到0～9的10个类别中。通常对于这样的多分类问题，可以使用多个二分类算法进行划分，同样，也有一些专门用于处理多分类问题的算法，如Softmax Regression算法。</p>
<h3 id="2-2-Softmax-Regression算法模型"><a href="#2-2-Softmax-Regression算法模型" class="headerlink" title="2.2 Softmax Regression算法模型"></a>2.2 Softmax Regression算法模型</h3><h4 id="2-2-1-Softmax-Regression模型"><a href="#2-2-1-Softmax-Regression模型" class="headerlink" title="2.2.1 Softmax Regression模型"></a>2.2.1 Softmax Regression模型</h4><p>Softmax Regression算法是Logistic Regression算法在多分类上的推广，即类标签</p>
<p>y的取值大于或等于2。假设有m个训练样本{（X  （1） ，y （1） ），（X  （2） ，y （2） ），…，（X  （m） ，y （m）<br>）}，对于Softmax Regression算法，其输入特征为：X  （i） ∈R n+1 ，类标记为：y （i）<br>∈{0，1，…，k}。假设函数为每一个样本估计其所属的类别的概率P（y=j X），具体的假设函数为：</p>
<p><img src="Image00055.jpg" alt></p>
<p>其中 _θ_ 表示的向量，且 _θ i _ ∈R n+1 。则对于每一个样本估计其所属的类别的概率为：</p>
<p><img src="Image00060.jpg" alt></p>
<h4 id="2-2-2-Softmax-Regression算法的代价函数"><a href="#2-2-2-Softmax-Regression算法的代价函数" class="headerlink" title="2.2.2 Softmax Regression算法的代价函数"></a>2.2.2 Softmax Regression算法的代价函数</h4><p>类似于Logistic Regression算法，在Softmax Regression算法的损失函数中引入指示函数I （·），其具体形式为：</p>
<p><img src="Image00062.jpg" alt></p>
<p>那么，对于Softmax Regression算法的损失函数为：</p>
<p><img src="Image00066.jpg" alt></p>
<p>其中，I{y （i） =j} 表示的是当 y （i） 属于第 j 类时，I{y （i） =j}=1，否则，I{y （i） =j}=0。</p>
<h3 id="2-3-Softmax-Regression算法的求解"><a href="#2-3-Softmax-Regression算法的求解" class="headerlink" title="2.3 Softmax Regression算法的求解"></a>2.3 Softmax Regression算法的求解</h3><p>对于上述的代价函数，可以使用梯度下降法对其进行求解，首先对其进行求梯度：</p>
<p><img src="Image00070.jpg" alt></p>
<p>最终的结果为：</p>
<p><img src="Image00073.jpg" alt></p>
<p>注意，此处的 _θ j _ 表示的是一个向量。通过梯度下降法的公式可以更新：</p>
<p><img src="Image00078.jpg" alt></p>
<p>现在，让我们一起利用Python实现上述Softmax Regression的更新过程。首先，我们需要导入numpy：</p>
<p><img src="Image00485.jpg" alt></p>
<p>Softmax Regression的更新过程的实现函数如程序清单2-1所示。</p>
<p><strong>程序清单2-1 梯度更新的函数gradientAscent</strong></p>
<p><img src="Image00506.jpg" alt></p>
<p><img src="Image00522.jpg" alt></p>
<p>在程序清单2-1中，梯度更新的函数gradientAscent是Softmax Regression算法的核心程序，实现了Softmax Regression模型中权重的更新。函数gradientAscent的输入为训练数据的特征feature_data，训练数据的标签label_data，总的类别的个数k，最大的迭代次数 maxCycle 以及梯度下降法中的学习步长 alpha。函数的输出为 Softmax Regression的模型权重。函数cost用于计算损失函数的值，如程序代码中的①所示，cost函数的具体实现如程序清单2-2所示。利用梯度下降法，根据计算的误差更新模型中的权重，如程序代码中的②所示。</p>
<p><strong>程序清单2-2 cost函数</strong></p>
<p><img src="Image00536.jpg" alt></p>
<p><img src="Image00099.jpg" alt></p>
<p>在程序清单2-2中，函数cost用于计算当前的损失函数的值，其输入分别为当前的预测值err和样本标签label_data。</p>
<h3 id="2-4-Softmax-Regression与Logistic-Regression的关系"><a href="#2-4-Softmax-Regression与Logistic-Regression的关系" class="headerlink" title="2.4 Softmax Regression与Logistic Regression的关系"></a>2.4 Softmax Regression与Logistic Regression的关系</h3><h4 id="2-4-1-Softmax-Regression中的参数特点"><a href="#2-4-1-Softmax-Regression中的参数特点" class="headerlink" title="2.4.1 Softmax Regression中的参数特点"></a>2.4.1 Softmax Regression中的参数特点</h4><p>在Softmax Regression中存在着参数冗余的问题。简单来讲就是参数中有些参数是没有任何用的，为了证明这点，假设从参数向量 _θ j _ 中减去向量 _ψ_ ，假设函数为：</p>
<p><img src="Image00102.jpg" alt></p>
<p>从上面可以看出从参数向量 _θ j _ 中减去向量 _ψ_ 对预测结果并没有任何影响，也就是说在模型中，存在着多组的最优解。</p>
<h4 id="2-4-2-由Softmax-Regression到Logistic-Regression"><a href="#2-4-2-由Softmax-Regression到Logistic-Regression" class="headerlink" title="2.4.2 由Softmax Regression到Logistic Regression"></a>2.4.2 由Softmax Regression到Logistic Regression</h4><p>Logistic Regression算法是Softmax Regression的特征情况，即k=2时的情况，当k=2时，Softmax Regression算法的假设函数为：</p>
<p><img src="Image00105.jpg" alt></p>
<p>利用Softmax Regression参数冗余的特点，令 _ψ_ = _θ 1 _ ，从两个向量中都减去这个向量，得到：</p>
<p><img src="Image00107.jpg" alt></p>
<p>在Logistic Regression算法中，假设函数为：</p>
<p><img src="Image00111.jpg" alt></p>
<p>由上述的k=2时的Softmax Regression的假设函数和Logistic Regression的假设函数可知，两者是等价的。</p>
<h3 id="2-5-Softmax-Regression算法实践"><a href="#2-5-Softmax-Regression算法实践" class="headerlink" title="2.5 Softmax Regression算法实践"></a>2.5 Softmax Regression算法实践</h3><p>有了以上的理论储备，我们利用上述实现好的函数，构建Softmax Regression分类器。在训练分类器的过程中，我们使用如图2.2所示的多分类数据作为训练数据：</p>
<p><img src="Image00114.jpg" alt></p>
<p>图2.2 多分类数据</p>
<p>利用Softmax Regression算法对其进行分类的过程中，主要有两个部分：①利用训练数据对模型进行训练；②对新的数据进行预测。</p>
<h4 id="2-5-1-对Softmax-Regression算法的模型进行训练"><a href="#2-5-1-对Softmax-Regression算法的模型进行训练" class="headerlink" title="2.5.1 对Softmax Regression算法的模型进行训练"></a>2.5.1 对Softmax Regression算法的模型进行训练</h4><p>首先，我们利用训练样本训练模型，为了使得Python能够支持中文的注释和利用numpy，我们需要在训练文件“softmax_regression_train.py”的开始加入：</p>
<p><img src="Image00568.jpg" alt></p>
<p>模型训练的主函数如程序清单2-3所示。</p>
<p><strong>程序清单2-3 模型训练的主函数</strong></p>
<p><img src="Image00083.jpg" alt></p>
<p>在模型训练的主函数中，首先需要导入训练数据，如程序代码中的①所示，导入训练数据的load_data函数如程序清单2-4所示。在导入了训练数据之后，便可以利用梯度下降法对模型进行训练，如程序代码中的②所示。当模型训练结束后，将最终的模型参数保存到文件weights中，如程序代码中的③所示，保存模型的save_model函数的实现如程序清单2-5所示。</p>
<p><strong>程序清单2-4 导入训练数据的load_data函数</strong></p>
<p><img src="Image00741.jpg" alt></p>
<p><img src="Image00309.jpg" alt></p>
<p>在程序清单2-4中，导入训练数据的函数load_data的输入为训练样本的文件名，通过解析输出训练数据的特征feature_data、标签label_data和训练样本的类别个数k。</p>
<h4 id="2-5-2-最终的模型"><a href="#2-5-2-最终的模型" class="headerlink" title="2.5.2 最终的模型"></a>2.5.2 最终的模型</h4><p>训练的具体为：</p>
<p><img src="Image00417.jpg" alt></p>
<p>通过训练，得到了最终的模型的参数，模型的参数保存在文件 weights 中，其中参数为：</p>
<p><img src="Image00585.jpg" alt></p>
<p>保存Softmax模型的save_model函数的具体实现如程序清单2-5所示。</p>
<p><strong>程序清单2-5 保存训练模型的save_model函数</strong></p>
<p><img src="Image00816.jpg" alt></p>
<p>在程序清单2-5中，函数save_model将训练好的Softmax模型保存到对应的文件中，其中，save_model函数的输入是保存的文件名file_name和对应的Softmax模型weights。</p>
<h4 id="2-5-3-对新的数据的预测"><a href="#2-5-3-对新的数据的预测" class="headerlink" title="2.5.3 对新的数据的预测"></a>2.5.3 对新的数据的预测</h4><p>对于分类算法而言，训练好的模型需要能够对新的数据集进行划分。利用上述步骤，我们训练好Softmax Regression模型，并将其保存在“weights”文件中，此时，我们需要利用训练好的Softmax Regression模型对新数据进行预测，同样，为了能够使用numpy中的函数和对中文注释的支持，在文件“softmax_regression_test.py”文件开始，我们加入：</p>
<p><img src="Image00724.jpg" alt></p>
<p>测试的主程序如程序清单2-6所示。</p>
<p><strong>程序清单2-6 测试模型的主程序</strong></p>
<p><img src="Image00861.jpg" alt></p>
<p><img src="Image00023.jpg" alt></p>
<p>在程序清单2-6中，首先是导入模型的参数，如程序代码中的①所示，load_weights函数的具体实现如程序清单2-7所示。同时需要导入测试的数据，如程序代码中的②所示，load_data函数的具体实现如程序清单2-8所示。然后利用训练好的Softmax模型对测试数据进行预测，如程序代码中的③所示，predict函数的具体实现如程序清单2-9所示。最后得到最终的预测结果，并将其保存到文件result中，如程序代码中的④所示，save_result函数的具体实现如程序清单2-10所示。</p>
<p><strong>程序清单2-7 导入模型参数的load_weights函数</strong></p>
<p><img src="Image00910.jpg" alt></p>
<p>在程序清单 2-7 中，需要导入 random 模块，这个模块的主要功能是为了生成下面的测试数据。在 load_weights 函数中，其输入为模型权重所在文件 weights_path，其输出为权重所在的矩阵，以及权重矩阵的行数m和列数n。</p>
<p><strong>程序清单2-8 导入测试数据的load_data函数</strong></p>
<p><img src="Image00256.jpg" alt></p>
<p>在程序清单2-8中，load_data函数用于生成测试样本，其中，函数的输入为样本的个数num和样本的维数m。其输出为生成的测试样本testDataSet。在生成样本的过程中使用到了 random 模块中的 random（）方法，该方法主要是生成（0.0，1.0）之间的随机数。因此，需要在“softmax_regression_test.py”文件中导入random模块：</p>
<p><img src="Image00045.jpg" alt></p>
<p><strong>程序清单2-9 生成预测结果</strong></p>
<p><img src="Image00468.jpg" alt></p>
<p>在程序清单2-9中，predict函数对测试数据进行了预测，并将最终的预测结果存到h中，predict函数的输入为测试数据test_data和模型的权重weights，函数的输出是每个测试样本对应的类别。在函数中①处，得到了每个样本属于每一个类别的概率，最终在②处返回概率值最大的index作为最终的类别标签。</p>
<p>在本次测试中随机生成了4 000个样本，目的是为了能够更好地刻画出分类的边界，最终的分类效果如图2.3所示。</p>
<p><img src="Image00439.jpg" alt></p>
<p>图2.3 最终的分类边界</p>
<p>在图2.3中，通过点的不同深浅区分出4个类别之间的边界。最终利用save_result函数将预测的结果保存到指定的文件中，save_result函数的具体实现如程序清单2-10所示。</p>
<p><strong>程序清单2-10 保存最终预测结果的save_result函数</strong></p>
<p><img src="Image00669.jpg" alt></p>
<p>在程序清单2-10中，函数save_result将最终的预测结果result保存到指定的文件file_name 中。save_result 函数的输入分别为最终的预测结果 result 和保存的文件名file_name。</p>
<p><strong>参考文献</strong></p>
<p>[1] Wikipedia.MNIST database[DB/OL].<a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/MNIST_database</a></p>
<p>[2] Yann LeCun.THE MNIST DATABASE of handwritten digits[DB/OL].<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a></p>
<p>[3] 李航.统计学习方法[M].北京：清华大学出版社.2012</p>
<p>[4] UFLDL.UFLDL Tutorial[DB/OL].<a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank" rel="noopener">http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial</a></p>
<p>[5] UFLDL.UFLDL 教程[DB/OL].http：//deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B</p>
<h2 id="3-Factorization-Machine"><a href="#3-Factorization-Machine" class="headerlink" title="3 Factorization Machine"></a>3 Factorization Machine</h2><p>在Logistic Regression算法的模型中使用的是特征的线性组合，最终得到的分隔超平面属于线性模型，其只能处理线性可分的二分类问题。现实生活中的分类问题是多种多样的，存在大量的非线性可分的分类问题。</p>
<p>为了使得Logistic Regression算法能够处理更多的复杂问题，对Logistic Regression算法的优化主要有两种：①对特征进行处理，如核函数的方法，将非线性可分的问题转换成近似线性可分的问题；②对 Logistic Regression 算法进行扩展，因子分解机（Factorization Machine，FM）是对基本Logistic Regression算法的扩展，是由Steffen Rendle提出的一种基于矩阵分解的机器学习算法。</p>
<h3 id="3-1-Logistic-Regression算法的不足"><a href="#3-1-Logistic-Regression算法的不足" class="headerlink" title="3.1 Logistic Regression算法的不足"></a>3.1 Logistic Regression算法的不足</h3><p>由于Logistic Regression算法简单、易于实现等特点，在工业界中得到了广泛使用，但是基本的Logistic Regression算法只能处理线性可分的二分类问题，对于如图3.1所示的非线性可分的二分类问题，基本的Logistic Regression算法却不能很好地进行分类。</p>
<p><img src="Image00130.jpg" alt></p>
<p>图3.1 非线性可分的数据</p>
<p>处理类似图3.1所示的非线性可分的问题，基本的Logistic Regression算法并不能很好地将上述的数据分开，为了能够利用Logistic Regression算法处理图3.1中所示的非线性可分的数据，通常有两种方法：①利用人工对特征进行处理，如使用核函数对特征进行处理。对于图3.1中所示的数据，利用函数 f （x）=x  2 进行特征处理，处理后的数据如图3.2所示。从图3.2中可以看出，非线性可分的数据经过人工特征处理后，变成了线性可分，此时，可以使用基本的Logistic Regression算法进行处理。但是，人工的特征处理需要有一些领域的知识，对初学者的难度比较大；②对基本的Logistic Regression算法扩展，以适应更难的分类问题。</p>
<p><img src="Image00157.jpg" alt></p>
<p>图3.2 人工处理后的特征</p>
<p>因子分解机（Factorization Machine，FM）算法是对Logistic Regression算法的扩展，在因子分解机FM模型中，不仅包含了Logistic Regression模型中的线性项，还包含了非线性的交叉项，利用矩阵分解的方法对模型中交叉项的系数学习，得到每一项的系数，而无需人工参与。</p>
<h3 id="3-2-因子分解机FM的模型"><a href="#3-2-因子分解机FM的模型" class="headerlink" title="3.2 因子分解机FM的模型"></a>3.2 因子分解机FM的模型</h3><h4 id="3-2-1-因子分解机FM模型"><a href="#3-2-1-因子分解机FM模型" class="headerlink" title="3.2.1 因子分解机FM模型"></a>3.2.1 因子分解机FM模型</h4><p>对于因子分解机FM模型，引入度的概念。对于度为2的因子分解机FM的模型为：</p>
<p><img src="Image00077.jpg" alt></p>
<p>其中，参数w 0 ∈R，W∈R  n ，V∈R  n×k 。<img src="Image00194.jpg" alt> 表示的是两个大小为k的向量V i 和向量V j 的点积：</p>
<p><img src="Image00305.jpg" alt></p>
<p>其中，V i 表示的是系数矩阵V 的第i维向量，且V i =（v i，1 ，v i，2 ，…，v i，k ），k∈N + 称为超参数，且k的大小称为因子分解机FM算法的度。在因子分解机FM模型中，前面两部分是传统的线性模型，最后一部分将两个互异特征分量之间的相互关系考虑进来。</p>
<h4 id="3-2-2-因子分解机FM可以处理的问题"><a href="#3-2-2-因子分解机FM可以处理的问题" class="headerlink" title="3.2.2 因子分解机FM可以处理的问题"></a>3.2.2 因子分解机FM可以处理的问题</h4><p>因子分解机FM算法可以处理如下三类问题：</p>
<p>1.回归问题（Regression）</p>
<p>2.二分类问题（Binary Classification）</p>
<p>3.排序（Ranking）</p>
<p>上述的FM模型可以直接处理回归问题，对于二分类问题，其最终的形式为：</p>
<p><img src="Image00650.jpg" alt></p>
<p>其中， _σ_ 是阈值函数，通常取为Sigmoid函数：</p>
<p><img src="Image00653.jpg" alt></p>
<p>在本章中，主要是利用因子分解机FM算法处理二分类问题。</p>
<h4 id="3-2-3-二分类因子分解机FM算法的损失函数"><a href="#3-2-3-二分类因子分解机FM算法的损失函数" class="headerlink" title="3.2.3 二分类因子分解机FM算法的损失函数"></a>3.2.3 二分类因子分解机FM算法的损失函数</h4><p>在二分类问题中使用logit loss作为优化的标准，即：</p>
<p><img src="Image00656.jpg" alt></p>
<h3 id="3-3-FM算法中交叉项的处理"><a href="#3-3-FM算法中交叉项的处理" class="headerlink" title="3.3 FM算法中交叉项的处理"></a>3.3 FM算法中交叉项的处理</h3><h4 id="3-3-1-交叉项系数"><a href="#3-3-1-交叉项系数" class="headerlink" title="3.3.1 交叉项系数"></a>3.3.1 交叉项系数</h4><p>在基本线性回归模型的基础上引入交叉项，如下：</p>
<p><img src="Image00316.jpg" alt></p>
<p>这种直接在交叉项x i x  j 的前面加上交叉项系数w i，j 的方式，在稀疏数据的情况下存在一个很大的缺陷，即在对于观察样本中未出现交互的特征分量时，不能对相应的参数进行估计。</p>
<p>对每一个特征分量x i 引入辅助向量V i =（v i，1 ，v i，2 ，…，v i，k ），利用<img src="Image00665.jpg" alt> 对交叉项的系数w i，j 进行估计，即</p>
<p><img src="Image00018.jpg" alt></p>
<p>令</p>
<p><img src="Image00382.jpg" alt></p>
<p>则</p>
<p><img src="Image00251.jpg" alt></p>
<p>这就对应了一种矩阵的分解。对k值的限定、FM的表达能力均有一定的影响。</p>
<h4 id="3-3-2-模型的求解"><a href="#3-3-2-模型的求解" class="headerlink" title="3.3.2 模型的求解"></a>3.3.2 模型的求解</h4><p>对于交叉项<img src="Image00355.jpg" alt> 的求解，可以采用公式<img src="Image00441.jpg" alt> ，其具体过程如下所示：</p>
<p><img src="Image00466.jpg" alt></p>
<h3 id="3-4-FM算法的求解"><a href="#3-4-FM算法的求解" class="headerlink" title="3.4 FM算法的求解"></a>3.4 FM算法的求解</h3><p>对于FM算法的求解，在前几章中，主要是利用了梯度下降法，在梯度下降法中，在每一个的迭代过程中，利用全部的数据进行模型参数的学习，对于数据量特别大的情况，每次迭代求解所有样本需要花费大量的计算成本。</p>
<h4 id="3-4-1-随机梯度下降（Stochastic-Gradient-Descent）"><a href="#3-4-1-随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="3.4.1 随机梯度下降（Stochastic Gradient Descent）"></a>3.4.1 随机梯度下降（Stochastic Gradient Descent）</h4><p>随机梯度下降算法（Stochastic Gradient Descent）在每次迭代的过程中，仅根据一个样本对模型中的参数进行调整。</p>
<p>随机梯度下降法的优化过程为：</p>
<p><img src="Image00295.jpg" alt></p>
<h4 id="3-4-2-基于随机梯度的方式求解"><a href="#3-4-2-基于随机梯度的方式求解" class="headerlink" title="3.4.2 基于随机梯度的方式求解"></a>3.4.2 基于随机梯度的方式求解</h4><p>假设数据集中有m个训练样本，即{X  （1） ，X  （2） ，…，X （m） }，每一个样本X  （i） 有n个特征，即<img src="Image00081.jpg" alt> 。对于度为2的因子分解机FM模型，其主要的参数有一次项和常数项的参数w 0 ，w 1 ，…，w n 以及交叉项的系数矩阵V。在利用随机梯度对模型的参数进行学习的过程中，主要是对损失函数<img src="Image00071.jpg" alt> 求导数，即：</p>
<p><img src="Image00084.jpg" alt></p>
<p>而<img src="Image00085.jpg" alt> 为：</p>
<p><img src="Image00087.jpg" alt></p>
<h4 id="3-4-3-FM算法流程"><a href="#3-4-3-FM算法流程" class="headerlink" title="3.4.3 FM算法流程"></a>3.4.3 FM算法流程</h4><p>利用随机梯度下降法对因子分解机FM模型中的参数进行学习的基本步骤如下：</p>
<p>1.初始化权重w 0 ，w 1 ，…，w n 和V</p>
<p>2.对每一个样本：</p>
<p><img src="Image00090.jpg" alt></p>
<p>对特征i∈{1，…，n}：</p>
<p><img src="Image00092.jpg" alt></p>
<p>对 f ∈{1，…，k}：</p>
<p><img src="Image00095.jpg" alt></p>
<p>3.重复步骤2，直到满足终止条件</p>
<p>现在，让我们一起利用Python实现上述因子分解机FM的更新过程，首先，我们需要导入numpy：</p>
<p><img src="Image00096.jpg" alt></p>
<p>利用随机梯度下降法训练因子分解机 FM 模型的参数的具体过程如程序清单 3-1所示。</p>
<p><strong>程序清单3-1 利用随机梯度下降法训练FM模型</strong></p>
<p><img src="Image00556.jpg" alt></p>
<p><img src="Image00100.jpg" alt></p>
<p>程序清单 3-1 中的 stocGradAscent 函数实现了对因子分解机 FM 模型的学习，stocGradAscent函数的输入dataMatrix为特征，classLabels为样本的标签，k为FM模型的度，max_iter为随机梯度下降法的最大迭代次数，alpha为随机梯度下降法的学习率。stocGradAscent 函数的输出为模型中的权重，包括一次项和常数项的权重w 0 ，w 1 ，…，w n 以及交叉项的系数矩阵V。</p>
<p>在程序清单3-1中的stocGradAscent函数中，首先是初始化权重，其中在一次项和常数项的权重w 0 ，w 1 ，…，w n 的初始化过程中，使用了比较简单的策略，即全部初始化为 0。对交叉项系数矩阵的初始化过程中，使用的正态分布对其进行初始化，如程序代码中的①所示，函数initialize_v的具体实现如程序清单3-2所示。在初始化完所有模型参数后，利用每一个样本对其参数进行学习，其中对常数项权重w 0 的修正如程序代码中的②所示，对一次项权重w i 的修正如程序代码中的③所示，对交叉项的系数矩阵的修正如程序代码中的④所示。sigmoid函数的具体实现如程序清单3-3所示。在每完成 1000 次迭代后，计算当前的损失函数的值，如程序代码中的⑤所示，函数getCost的具体实现如程序清单3-4所示。</p>
<p><strong>程序清单3-2 初始化交叉项的权重</strong></p>
<p><img src="Image00575.jpg" alt></p>
<p>程序清单3-2中的initialize_v函数实现了对交叉项的权重进行初始化，initialize_v函数的输入为特征的个数n和 FM 模型的度k，其输出为交叉项的权重v。在初始化的过程中，使用了正态分布对权重矩阵V 中的每一个值生成随机数，如程序清单中的①所示，为了能够使用正态分布对权重进行初始化，我们需要导入normalvariate函数：</p>
<p><img src="Image00103.jpg" alt></p>
<p><strong>程序清单3-3 s igmoid函数</strong></p>
<p><img src="Image00603.jpg" alt></p>
<p><strong>程序清单3-4 getCost函数</strong></p>
<p><img src="Image00106.jpg" alt></p>
<p>在程序清单 3-4 中，函数 getCost 用于计算当前的损失函数的值，函数的输入是利用当前的FM模型对数据集的预测结果predict和样本的标签classLabels，最终得到当前的损失函数值error。</p>
<h3 id="3-5-因子分解机FM算法实践"><a href="#3-5-因子分解机FM算法实践" class="headerlink" title="3.5 因子分解机FM算法实践"></a>3.5 因子分解机FM算法实践</h3><p>有了以上的理论储备，我们利用上述实现好的函数，构建因子分解机FM分类器。在训练分类器的过程中，我们使用图3.3所示的非线性可分的数据集作为FM模型的训练数据集：</p>
<p><img src="Image00627.jpg" alt></p>
<p>图3.3 非线性可分的数据</p>
<p>利用因子分解机FM算法对其进行分类的过程中，主要有两个部分，①利用训练数据对模型进行训练；②对新的数据进行预测。</p>
<h4 id="3-5-1-训练FM模型"><a href="#3-5-1-训练FM模型" class="headerlink" title="3.5.1 训练FM模型"></a>3.5.1 训练FM模型</h4><p>首先，我们利用训练样本训练模型，为了使得Python能够支持中文的注释和利用numpy，我们需要在训练文件“FM_train.py”的开始加入：</p>
<p><img src="Image00109.jpg" alt></p>
<p>因子分解机FM模型训练的主函数如程序清单3-5所示。</p>
<p><strong>程序清单3-5 FM训练的主函数</strong></p>
<p><img src="Image00647.jpg" alt></p>
<p><img src="Image00112.jpg" alt></p>
<p>在程序清单3-5中，即在因子分解机FM算法的训练过程中，首先需要导入训练数据，如程序代码中的①所示，loadDataSet函数的具体实现如程序清单3-6所示。在完成训练数据的导入后，利用训练数据对模型进行训练，如程序代码中的②所示，函数stocGradAscent的具体实现如程序清单3-1所示。完成训练后，计算训练的准确性，在计算训练准确性时，首先需要利用训练好的因子分解机FM模型对训练数据进行预测，如程序代码中的③所示，函数getPrediction的具体实现如程序清单3-7所示，在完成对训练数据的预测后，需要将预测值与样本的标签进行比较，以求得训练好的因子分解机FM模型的训练准确性，如程序代码中的④所示，函数getAccuracy的具体实现如程序清单 3-8 所示。最终，需要将因子分解机 FM 模型中的参数保存到文件“weights”中，模型的参数包括一次项和常数项的权重w 0 ，w 1 ，…，w n 以及交叉项的权重V，保存模型的save_model函数的具体实现如程序清单3-9所示。</p>
<p><strong>程序清单3-6 导入训练数据的loadDataSet函数</strong></p>
<p><img src="Image00814.jpg" alt></p>
<p>在程序清单3-6中，主要实现了对训练数据的导入。函数loadDataSet的输入为训练数据所在的位置，输出为训练数据的特征dataMat和训练数据的标签labelMat。由于在因子分解机FM模型中使用的是log损失函数，因此在导入标签的过程中，需要将原始数据中的{0，1}转换成{-1，1}，如程序代码中的①所示。</p>
<p><strong>程序清单3-7 对训练样本的预测的getPrediction函数</strong></p>
<p><img src="Image00115.jpg" alt></p>
<p>在程序清单3-7中，函数getPrediction主要利用训练好的因子分解机FM模型对样本进行预测。函数 getPrediction 的输入为训练数据的特征 dataMatrix，因子分解机FM模型的常数项权重w 0 ，一次项权重w和交叉项权重矩阵V，函数getPrediction的输出为预测值 result。在预测的过程中，通过因子分解机 FM 模型的参数，实现对样本的值的预测，如程序代码中的①所示。</p>
<p><strong>程序清单3-8 计算训练准确性的getAccuracy函数</strong></p>
<p><img src="Image00075.jpg" alt></p>
<p><img src="Image00446.jpg" alt></p>
<p>在程序清单 3-8 中，函数 getAccuracy 主要实现了对模型预测效果的评价。函数getAccuracy的输入为对训练数据的预测 predict 和训练数据的标签 classLabels，函数getAccuracy的输出为错误率。通过比较预测 predict 和训练数据的标签 classLabels，统计预测的准确性。</p>
<h4 id="3-5-2-最终的训练效果"><a href="#3-5-2-最终的训练效果" class="headerlink" title="3.5.2 最终的训练效果"></a>3.5.2 最终的训练效果</h4><p>FM模型的训练过程为：</p>
<p><img src="Image00450.jpg" alt></p>
<p>在上述的训练过程中，得到的因子分解机FM模型的参数如表3-1所示。</p>
<p>表3-1 因子分解机FM的权重值</p>
<p><img src="Image00079.jpg" alt></p>
<p>在训练过程的最后，需要保存上述训练好的FM模型，save_model函数实现了将训练好的FM模型保存到指定的文件中的功能，save_model函数的具体实现如程序清单3-9所示。</p>
<p><strong>程序清单3-9 保存FM模型的save_model函数</strong></p>
<p><img src="Image00425.jpg" alt></p>
<p>在程序清单 3-9 中，save_model 函数将训练好的 FM 模型保存到文件 file_name中，FM模型中的参数包括：①偏置项w 0 ，如程序代码中的①所示；②一次项的权重w，如程序代码中的②所示；③交叉项的权重V，如程序代码中的③所示。</p>
<p>在对训练样本进行预测时，最终的训练准确性为0.99，最终的分隔超平面如图3.4所示。</p>
<p><img src="Image00703.jpg" alt></p>
<p>图3.4 分隔超平面</p>
<h4 id="3-5-3-对新的数据进行预测"><a href="#3-5-3-对新的数据进行预测" class="headerlink" title="3.5.3 对新的数据进行预测"></a>3.5.3 对新的数据进行预测</h4><p>对于分类算法而言，训练好的模型需要能够对新的数据集进行划分。利用上述步骤，我们训练好因子分解机FM模型，并将其保存在“weights”文件中，此时，我们需要利用训练好的FM模型对新数据进行预测，同样，为了能够使用numpy中的函数和对中文注释的支持，在文件“FM_test.py”的开始，我们加入：</p>
<p><img src="Image00571.jpg" alt></p>
<p>对新数据的预测的主函数如程序清单3-10所示。</p>
<p><strong>程序清单3-10 对新数据的预测的主函数</strong></p>
<p><img src="Image00574.jpg" alt></p>
<p>在程序清单3-10中，对新数据的预测的主要步骤有：①导入测试数据，如程序代码中的①所示；②导入因子分解机FM模型，如程序代码中的②所示；③计算得到预测值，如程序代码中的③所示，其中函数getPrediction如程序清单3-7所示；④保存最终的预测结果，如程序代码中的④所示。对于导入测试数据load_DataSet函数的具体实现如程序清单3-11所示。导入因子分解机FM模型的load_model函数如程序清单3-12所示。保存最终预测结果的save_result函数的具体实现如程序清单3-13所示。</p>
<p><strong>程序清单3-11 导入测试数据的loadDataSet函数</strong></p>
<p><img src="Image00756.jpg" alt></p>
<p>在程序清单 3-11 中，其头文件部分，导入了训练因子分解机 FM 模型中的getPrediction函数，getPrediction函数的具体实现如程序清单3-7所示。函数loadDataSet的输入为测试数据的位置，输出为测试数据的特征。</p>
<p><strong>程序清单3-12 导入测试数据的loadModel函数</strong></p>
<p><img src="Image00582.jpg" alt></p>
<p><img src="Image00563.jpg" alt></p>
<p>在程序清单3-12中，函数loadModel的输入为FM模型保存的文件，输出为FM模型中的参数。FM模型中的参数包括偏置项w  0 ，一次项的权重w和交叉项的权重V。偏置项w 0 的导入如程序代码中的①所示，一次项的权重w的导入如程序代码中的②所示，交叉项的权重V 的导入如程序代码中的③所示。</p>
<p><strong>程序清单3-13 保存最终预测结果的save_result函数</strong></p>
<p><img src="Image00591.jpg" alt></p>
<p>在程序清单3-13中，函数save_result将FM模型对测试数据的预测结果保存到文件file_name中。</p>
<p><strong>参考文献</strong></p>
<p>[1] Rendle S.Factorization Machines[C]//IEEE International Conference on Data Mining.IEEE Computer Society,2010:995-1000.</p>
<p>[2] Rendle S.Factorization Machines with libFM[J].Acm Transactions on Intelligent Systems&amp;Technology,2012,3(3):219-224.</p>
<p>[3] Steffen Rendle.libFM:Factorization Machine Library[DB/OL].<a href="http://www.libfm.org/" target="_blank" rel="noopener">http://www.libfm.org/</a>.</p>
<h2 id="4-支持向量机"><a href="#4-支持向量机" class="headerlink" title="4 支持向量机"></a>4 支持向量机</h2><p>支持向量机（Support Vector Machine）是由Vapnik等人于1995年提出来的，之后随着统计理论的发展，支持向量机SVM也逐渐受到了各领域研究者的关注，在很短的时间就得到了很广泛的应用。支持向量机SVM是被公认的比较优秀的分类模型，同时，在支持向量机的发展过程中，其理论方面的研究得到了同步的发展，为支持向量机的研究提供了强有力的理论支撑。</p>
<h3 id="4-1-二分类问题"><a href="#4-1-二分类问题" class="headerlink" title="4.1 二分类问题"></a>4.1 二分类问题</h3><h4 id="4-1-1-二分类的分隔超平面"><a href="#4-1-1-二分类的分隔超平面" class="headerlink" title="4.1.1 二分类的分隔超平面"></a>4.1.1 二分类的分隔超平面</h4><p>在前面的章节中，我们介绍了用于处理二分类问题的Logistic Regression算法和用于处理多分类问题的Softmax Regression算法。典型的二分类问题，如图4.1所示。</p>
<p><img src="Image00842.jpg" alt></p>
<p>图4.1 典型的二分类问题</p>
<p>对于如图 4.1 所示的二分类问题中，“.”表示正类，“。”表示负类。我们试图寻找到图中的分隔超平面，能够分隔图中的正负样本，其中，分隔超平面为：</p>
<p><img src="Image00882.jpg" alt></p>
<p>最终得到如下所示的分类决策函数：</p>
<p><img src="Image00888.jpg" alt></p>
<p>其中，函数sign （x）为符号函数：</p>
<p><img src="Image00203.jpg" alt></p>
<p>其中，当W ∗ ·X+b ∗ ＞0时，为正类，当W ∗ ·X+b ∗ ＜0时，为负类。</p>
<h4 id="4-1-2-感知机算法"><a href="#4-1-2-感知机算法" class="headerlink" title="4.1.2 感知机算法"></a>4.1.2 感知机算法</h4><p>对于二分类问题，假设有m个训练样本{（X  （1） ，y （1） ），（X  （2） ，y （2） ），…，（X  （m） ，y （m）<br>）}，其中，y∈{-1，1}。那么，应该如何从训练样本中得到分隔超平面W ∗ · X+b ∗ =0呢？</p>
<p>对于如图4.1所示的二分类问题，我们希望构建好的分隔超平面能够将正类（图4.1中的“.”）和负类（图4.1中的“。”）全部正确区分开。1957年由Rosenblatt提出了感知机算法，在感知机算法中直接使用通误分类的样本到分隔超平面之间的距离S作为其损失函数，并利用梯度下降法求得误分类的损失函数的极小值，得到最终的分隔超平面。</p>
<p>对于训练样本点X  （i） ，其到分隔超平面的距离S为：</p>
<p><img src="Image00020.jpg" alt></p>
<p>其中，<img src="Image00418.jpg" alt> 为W 的L 2 范数。对于图4.1中，样本点X  （i） 到分隔超平面的距离S如图4.2所示。</p>
<p><img src="Image00621.jpg" alt></p>
<p>图4.2 样本点到分隔超平面的距离</p>
<p>在训练样本中，对于误分类的样本（X （i） ，y （i） ），即预测值W ·X （i） +b与真实值y （i） 异号，即：</p>
<p><img src="Image00089.jpg" alt></p>
<p>则误分类样本到分隔超平面之间的距离为：</p>
<p><img src="Image00110.jpg" alt></p>
<p>若不考虑<img src="Image00820.jpg" alt> ，即为感知机算法的损失函数。感知机算法的损失函数为：</p>
<p><img src="Image00634.jpg" alt></p>
<p>通过求解损失函数的最小值<img src="Image00180.jpg" alt> 求得最终的分隔超平面。</p>
<h4 id="4-1-3-感知机算法存在的问题"><a href="#4-1-3-感知机算法存在的问题" class="headerlink" title="4.1.3 感知机算法存在的问题"></a>4.1.3 感知机算法存在的问题</h4><p>在感知机算法中，通过最小化误分类样本到分隔超平面的距离，求得最终的分隔超平面，但是对于感知机算法来说，分隔超平面参数W 和b的初始值和选择误分类样本的顺序对最终的分隔超平面的计算都有影响，采用不同的初始值或者不同的误分类点，最终的分隔超平面是不同的。对于如图4.1所示的训练数据，最终的分隔超平面如图4.3所示。</p>
<p><img src="Image00641.jpg" alt></p>
<p>图4.3 不同的分隔超平面</p>
<p>在图4.3中的两个分隔超平面都能将正负样本区分开。对于感知机算法，采用不同的初始值或者不同的误分类点，最终的分隔超平面是不同的。在这些分隔超平面中是否存在一个最好的分隔超平面呢？</p>
<h3 id="4-2-函数间隔和几何间隔"><a href="#4-2-函数间隔和几何间隔" class="headerlink" title="4.2 函数间隔和几何间隔"></a>4.2 函数间隔和几何间隔</h3><p>一般来讲，一个样本点距离分隔超平面的远近可以表示分类预测的确信程度。在图4.1中，样本点A离分隔超平面最远，若预测其为正类，就比较确信该预测是正确的；而样本点C离分隔超平面较近，若预测其为正类，就不是那么确信。为了能够表示分类预测的确信程度，我们分别定义函数间隔（Functional Margin）和几何间隔（Geometric Margin）。</p>
<h4 id="4-2-1-函数间隔"><a href="#4-2-1-函数间隔" class="headerlink" title="4.2.1 函数间隔"></a>4.2.1 函数间隔</h4><p>在感知机算法中，我们注意到分隔超平面W ·X+b确定的情况下，<img src="Image00231.jpg" alt> 可以相对地表示样本点X  （i） 距离分隔超平面的远近。而当预测值W · X  （i） +b和样本标签y （i） 同号时，表明最终的分类是正确的，因此，可以使用y （i） （W · X<br>（i） +b）来表示分类的正确性和确信度，这便是函数间隔的定义。</p>
<p>对于给定的训练数据集{（X  （1） ，y （1） ），（X  （2） ，y （2） ），…，（X  （m） ，y （m）<br>）}和分隔超平面，定义分隔超平面关于样本点（X  （i） ，y （i） ）的函数间隔为：</p>
<p><img src="Image00469.jpg" alt></p>
<p>同时，定义分隔超平面关于训练数据集的函数间隔为分隔超平面关于训练数据集中所有样本点的函数间隔的最小值：</p>
<p><img src="Image00557.jpg" alt></p>
<p>函数间隔可以表示分类预测的正确性和确定性。但是，在分隔超平面中，如果其参数W 和b同时扩大为原来的2倍，这对于分隔超平面来说，并没有任何改变，但是对于函数间隔<img src="Image00447.jpg" alt> 来说，即扩大为原来的2倍。为了解决这样的问题，我们引入几何间隔。</p>
<h4 id="4-2-2-几何间隔"><a href="#4-2-2-几何间隔" class="headerlink" title="4.2.2 几何间隔"></a>4.2.2 几何间隔</h4><p>为了能够使得间隔是一个确定的值，可以对分隔超平面的参数W 加上某些约束，</p>
<p>如归一化<img src="Image00765.jpg" alt> 。在图4.2中，对于样本X  （i） ，其到分隔超平面之间的距离S为：</p>
<p><img src="Image00876.jpg" alt></p>
<p>而当W · X  （i） +b与y （i） 同号时，表示预测正确，则样本X  （i） 到分隔超平面之间的距离S可以表示为：</p>
<p><img src="Image00357.jpg" alt></p>
<p>这便是几何间隔的定义。</p>
<p>对于给定的训练数据集{（X  （1） ，y （1） ），（X  （2） ，y （2） ），…，（X  （m） ，y （m）<br>）}和分隔超平面，定义分隔超平面关于样本点（X  （i） ，y （i） ）的几何间隔为：</p>
<p><img src="Image00196.jpg" alt></p>
<p>同时，定义分隔超平面关于训练数据集的几何间隔为分隔超平面关于训练数据集中所有样本点的几何间隔的最小值：</p>
<p><img src="Image00404.jpg" alt></p>
<p>从上面的定义不难发现，几何间隔其实就是样本到分隔超平面的距离。对于几何间隔和函数间隔，有如下的关系：</p>
<p><img src="Image00422.jpg" alt></p>
<h3 id="4-3-支持向量机"><a href="#4-3-支持向量机" class="headerlink" title="4.3 支持向量机"></a>4.3 支持向量机</h3><p>与感知机算法不同，在支持向量机（Support Vector Machines，SVM）中，求解出的分隔超平面不仅能够正确划分训练数据集，而且几何间隔最大。</p>
<h4 id="4-3-1-间隔最大化"><a href="#4-3-1-间隔最大化" class="headerlink" title="4.3.1 间隔最大化"></a>4.3.1 间隔最大化</h4><p>对于几何间隔最大的分隔超平面：</p>
<p><img src="Image00688.jpg" alt></p>
<p>同时，对于每一个样本，需要满足：</p>
<p><img src="Image00444.jpg" alt></p>
<p>考虑到几何间隔和函数间隔之间的关系，则上述的几何间隔最大的分隔超平面可以等价为：</p>
<p><img src="Image00723.jpg" alt></p>
<p>同时需要满足：</p>
<p><img src="Image00645.jpg" alt></p>
<p>在函数间隔中，函数间隔<img src="Image00577.jpg" alt> 的取值并不影响到最优问题的解，如上所述，当参数W 和b同时扩大为原来的2倍，函数间隔<img src="Image00773.jpg" alt> 也会同时扩大为原来的2倍，这对于上述的优化问题和约束条件并没有影响，因此，可以取<img src="Image00791.jpg" alt> ，则上述的优化问题变成：</p>
<p><img src="Image00167.jpg" alt></p>
<h4 id="4-3-2-支持向量和间隔边界"><a href="#4-3-2-支持向量和间隔边界" class="headerlink" title="4.3.2 支持向量和间隔边界"></a>4.3.2 支持向量和间隔边界</h4><p>对于如图4.1所示的线性可分的二分类问题，在m个训练样本中，与分隔超平面距离最近的样本称为支持向量（Support Vector）。支持向量X  （i） 对应着约束条件为：</p>
<p><img src="Image00595.jpg" alt></p>
<p>当y （i） =+1时，支持向量所在的超平面为：</p>
<p><img src="Image00602.jpg" alt></p>
<p>当y （i） =-1时，支持向量所在的超平面为：</p>
<p><img src="Image00606.jpg" alt></p>
<p>对于支持向量所在的超平面H 1 和H  2 ，如图4.4所示。</p>
<p><img src="Image00611.jpg" alt></p>
<p>图4.4 支持向量以及支持向量所在的超平面</p>
<p>在图4.4中，超平面H 1 和超平面H  2 之间的距离成为间隔，超平面H 1 和超平面H 2 又称为间隔边界。在确定最终的分隔超平面时，只有支持向量起作用，其他的样本点并不起作用，由于支持向量在确定分割超平面中起着重要的作用，因此，这种分类模型被称为支持向量机。</p>
<h4 id="4-3-3-线性支持向量机"><a href="#4-3-3-线性支持向量机" class="headerlink" title="4.3.3 线性支持向量机"></a>4.3.3 线性支持向量机</h4><p>对于如图4.1所示的数据集，其条件极为苛刻，要求所有的样本都是线性可分的，即存在分隔超平面，能够将所有的正样本和负样本正确区分开，但是在实际情况中，数据集很难满足这样的条件，对于一个数据集，其中存在部分的特异点，但是将这些特异点除去后，剩下的大部分的样本点组成的集合是线性可分的。</p>
<p>对于线性不可分的某些样本点（X  （i） ，y （i） ）意味着其不能满足函数间隔大于或等于1 的约束条件，为了解决这个问题，可以对每个样本点（X  （i） ，y （i） ）引进一个松弛变量 _ξ i _ ≥0，使得函数间隔加上松弛变量大于或等于1，这样，约束条件变为：</p>
<p><img src="Image00614.jpg" alt></p>
<p>同时，对每个松弛变量  i _ξ_ ，支付一个代价C，此时，目标函数变为：</p>
<p><img src="Image00618.jpg" alt></p>
<p>此时的优化目标为：</p>
<p><img src="Image00065.jpg" alt></p>
<h3 id="4-4-支持向量机的训练"><a href="#4-4-支持向量机的训练" class="headerlink" title="4.4 支持向量机的训练"></a>4.4 支持向量机的训练</h3><h4 id="4-4-1-学习的对偶算法"><a href="#4-4-1-学习的对偶算法" class="headerlink" title="4.4.1 学习的对偶算法"></a>4.4.1 学习的对偶算法</h4><p>通过以上的分析，我们可以知道，在SVM中，对分隔超平面的求解转化为对如下带约束的最小优化问题的求解：</p>
<p><img src="Image00626.jpg" alt></p>
<p>对于带约束的优化问题的求解，可以使用拉格朗日乘数法，将其转化为无约束优化问题的求解。对于上述的带约束的优化问题，可以转换成如下的拉格朗日函数：</p>
<p><img src="Image00629.jpg" alt></p>
<p>其中， _α_ =（ _α 1 _ ， _α 2 _ ，…， _α m _ ）， _β_ =（ _β 1 _ ， _β 2 _ ，…， _β m _ ），且 _α i _ ≥0， _β i _ ≥0。向量 _α_ 和向量 _β_ 称为拉格朗日乘子向量。上述的最小优化问题即为：</p>
<p><img src="Image00632.jpg" alt></p>
<p>根据拉格朗日对偶性，原始问题的对偶问题为：</p>
<p><img src="Image00182.jpg" alt></p>
<p>先求<img src="Image00637.jpg" alt> ，再对拉格朗日函数L（W，b， _ξ_ ， _α_ ， _β_ ）中的W，b和 _ξ_ 求偏导，并令其为0。</p>
<p><img src="Image00207.jpg" alt></p>
<p>化简得：</p>
<p><img src="Image00646.jpg" alt></p>
<p>将上述代入到<img src="Image00253.jpg" alt> 中，得到：</p>
<p><img src="Image00274.jpg" alt></p>
<p>再对<img src="Image00293.jpg" alt> 求 _α_ 的极大，即得对偶问题：</p>
<p><img src="Image00660.jpg" alt></p>
<p>由C _-α i -β i _ =0， _α i _ ≥0和 _β i _ ≥0可得：</p>
<p><img src="Image00334.jpg" alt></p>
<p>同时，将求解最大化问题转换为求解最小化问题，则上述的优化问题转换成：</p>
<p><img src="Image00668.jpg" alt></p>
<p>当 _α ∗ _ 为上述对偶问题的最优解时，根据<img src="Image00674.jpg" alt> 可以求得原始问题的最优解：</p>
<p><img src="Image00678.jpg" alt></p>
<p>对于b的最优解b ∗ ，选择 _α ∗ _ 的一个分量<img src="Image00682.jpg" alt> ，其中<img src="Image00902.jpg" alt> 满足：<img src="Image00692.jpg" alt> ，b ∗ 为：</p>
<p><img src="Image00713.jpg" alt></p>
<h4 id="4-4-2-由线性支持向量机到非线性支持向量机"><a href="#4-4-2-由线性支持向量机到非线性支持向量机" class="headerlink" title="4.4.2 由线性支持向量机到非线性支持向量机"></a>4.4.2 由线性支持向量机到非线性支持向量机</h4><p>对于一个非线性可分的问题，如在第3章中图3.1所示的非线性可分的数据集，可以采用核函数的方式将非线性问题转换成线性问题，在本章中主要用到的函数是高斯核函数：</p>
<p><img src="Image00573.jpg" alt></p>
<p>对于核函数的更多知识，请参见第11章的11.2节核函数。对于非线性支持向量机，此时的优化目标为：</p>
<p><img src="Image00748.jpg" alt></p>
<h4 id="4-4-3-序列最小最优化算法SMO"><a href="#4-4-3-序列最小最优化算法SMO" class="headerlink" title="4.4.3 序列最小最优化算法SMO"></a>4.4.3 序列最小最优化算法SMO</h4><p>通过拉格朗日的对偶性，我们将原始的带约束的优化问题转换成其对偶问题，并通过对对偶问题的求解，得到对偶问题的最优解 _α ∗ _ ，最终得到原始问题的最优解W<br>∗ 和b ∗ 。对于如上的带约束的优化问题，我们应该如何求解呢？</p>
<p>对于如上的带约束的优化问题，可以使用二次规划的方法进行求解，在SVM的发展过程中，围绕着如何高效地求解上述带约束的优化问题，许多的求解方法被提出来，其中，在 1998 年，由 Platt 提出的序列最小最优化算法（Sequential Minimal Optimization，SMO）被广泛应用。</p>
<p>序列最小最优化算法SMO的思想是将一个大的问题划分成一系列小的问题，通过对这些子问题的求解，达到对对偶问题的求解过程。在SMO算法中，不断将对偶问题的二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行求解。对于SMO算法，每次取两个变量进行更新，假设取得的变量为 _α 1 _ 和 _α 2 _ ，使其他的变量为固定的值，则由约束条件<img src="Image00580.jpg" alt> 可知：</p>
<p><img src="Image00783.jpg" alt></p>
<p>如果此时 _α 2 _ 被确定了，那么 _α 1 _ 可由上式确定。那么，接下来的问题是：①每次应该如何选择需要更新的两个变量 _α 1 _ 和 _α 2 _ ？②选择出的两个变量 _α 1 _ 和 _α 2 _ 应该如何更新？</p>
<p>在此，我们先求解两个变量 _α 1 _ 和 _α 2 _ 的更新方法。对于选择出的两个变量 _α 1 _ 和 _α 2 _ ，此时的优化问题为：</p>
<p><img src="Image00804.jpg" alt></p>
<p>其中，K ij =K（X （i） ，X （j） ），M 1 和M 2 表示的是与 _α 1 _ 和 _α 2 _ 无关的常数项。对于需要求解的优化函数，令：</p>
<p><img src="Image00593.jpg" alt></p>
<p>需要求解的最优化问题为：</p>
<p><img src="Image00396.jpg" alt></p>
<p>由<img src="Image00874.jpg" alt> 可知：</p>
<p><img src="Image00900.jpg" alt></p>
<p>由于（y （1） ） 2 =1，所以上式可以转换成：</p>
<p><img src="Image00612.jpg" alt></p>
<p>将其代入到优化目标函数W （ _α 1 _ ， _α 2 _ ）中，得到：</p>
<p><img src="Image00272.jpg" alt></p>
<p>此时，变成只对 _α 2 _ 求解最优化的问题。为了求得<img src="Image00777.jpg" alt> ，即为：</p>
<p><img src="Image00426.jpg" alt></p>
<p>令其为0，得到：</p>
<p><img src="Image00101.jpg" alt></p>
<p>在此，令：</p>
<p><img src="Image00119.jpg" alt></p>
<p>则：</p>
<p><img src="Image00442.jpg" alt></p>
<p>然而，假设第i代时，<img src="Image00534.jpg" alt> ，则对于第i+1代时，</p>
<p><img src="Image00643.jpg" alt></p>
<p>则：</p>
<p><img src="Image00221.jpg" alt></p>
<p>其中， _η_ =K 11 +K 22 -2K 12 。然而，对于新求出的<img src="Image00849.jpg" alt> ，需要满足上述的约束条件， _α 1 _ 和 _α 2 _ 所满足的约束条件如图4.5所示。</p>
<p><img src="Image00132.jpg" alt></p>
<p>图4.5 变量的约束条件</p>
<p>由于y （i） ∈{-1，1}，因此，当y （1） ≠ y （2） 时，即y （1） 与y （2） 异号，有 _α 1 -α 2 _ =k 1 ，其中k 1 为常数，如图4.5中的左图所示。同理，当y （1） =y （2） 时，即y （1） 与y （2） 同号，有 _α 1 _ + _α 2 _ =k 2 ，其中k 2 为常数，如图4.5中的右图所示。对于<img src="Image00164.jpg" alt> ，其需要满足：</p>
<p><img src="Image00303.jpg" alt></p>
<p>其中，当y （1） ≠ y （2） 时：</p>
<p><img src="Image00327.jpg" alt></p>
<p>当y （1） =y （2） 时：</p>
<p><img src="Image00346.jpg" alt></p>
<p>因此，新求解出的<img src="Image00671.jpg" alt> 值为：</p>
<p><img src="Image00393.jpg" alt></p>
<p>当求解出<img src="Image00412.jpg" alt> 后，由<img src="Image00433.jpg" alt> 可知：</p>
<p><img src="Image00690.jpg" alt></p>
<p>我们对两个变量 _α 1 _ 和 _α 2 _ 的求解方法进行了探讨，那么，我们如何选择两个变量 _α 1 _ 和 _α 2 _ 呢？当所有的变量都满足KKT条件时，那么便是最优化问题的解。那么对于第一个变量，我们可以选择那些不满足KKT条件的变量，假设第一个变量为 _α 1 _ 。当 _α 1 _ 满足KKT条件时，即：</p>
<p><img src="Image00249.jpg" alt></p>
<p>由于<img src="Image00252.jpg" alt> ，则：</p>
<p><img src="Image00255.jpg" alt></p>
<p>因此，不满足KKT条件的情况如下所示：</p>
<p>• 如果y  （1） E 1 ＜0，即y  （1） g （X （1） ）＜1时，此时，若 _α 1 _ ＜C则违反KKT条件；</p>
<p>• 如果y  （1） E 1 ＞0，即y  （1） g （X （1） ）＞1时，此时，若 _α 1 _ ＞0则违反KKT条件；</p>
<p>• 如果y （1） E 1 =0，即y （1） g（X （1） ）=1时，表明是支持向量，此时无需优化。</p>
<p>通过检查每一个样本点是否符合上述不满足KKT条件，选择出第一个变量 _α_ 。 1</p>
<p>假设我们选择出的第一个变量为 _α 1 _ ，此时，我们需要选择出第二个变量 _α 2 _ ，选择第二个变量的原则是要使得 _α 2 _ 能够发生足够大的变化。其中， _α 2 _ 的更新公式为：</p>
<p><img src="Image00259.jpg" alt></p>
<p><img src="Image00263.jpg" alt> 依赖于E 1 -E  2 ，我们选择<img src="Image00266.jpg" alt> ，以使得E 1 -E  2 最大。</p>
<p>当更新完成 _α 1 _ 和 _α 2 _ 后，需要重新计算阈值b，由：</p>
<p><img src="Image00270.jpg" alt></p>
<p>可知：</p>
<p><img src="Image00273.jpg" alt></p>
<p>而E 1 为：</p>
<p><img src="Image00275.jpg" alt></p>
<p>因此，<img src="Image00279.jpg" alt> 可以表示为：</p>
<p><img src="Image00282.jpg" alt></p>
<p>同理，<img src="Image00284.jpg" alt> 可以表示为：</p>
<p><img src="Image00288.jpg" alt></p>
<p>当更新完阈值b后，需要重新计算误差E i ：</p>
<p><img src="Image00291.jpg" alt></p>
<h3 id="4-5-支持向量机SVM算法实践"><a href="#4-5-支持向量机SVM算法实践" class="headerlink" title="4.5 支持向量机SVM算法实践"></a>4.5 支持向量机SVM算法实践</h3><p>接下来我们利用Python构建一个完整的SVM分类器。在构建SVM分类器的过程中，包含了SVM分类器的训练和利用SVM分类器对未知数据的分类。在实践的过程中，我们首先建立“svm.py”文件。“svm.py”文件中包含了SVM模型训练以及利用SVM模型对未知数据预测的函数。</p>
<h4 id="4-5-1-训练SVM模型"><a href="#4-5-1-训练SVM模型" class="headerlink" title="4.5.1 训练SVM模型"></a>4.5.1 训练SVM模型</h4><p>在SVM模型的训练过程中，主要使用到的文件包括“svm.py”和“svm_train.py”。首先我们为SVM模型声明一个类，打开“svm.py”文件，为使Python文件支持中文的注释和在SVM中使用矩阵的相关计算，需要在“svm.py”文件的开始加入：</p>
<p><img src="Image00761.jpg" alt></p>
<p>同时，在训练好SVM分类器后，需要将SVM模型保存到本地，此时，需要使用到cPickle模块，我们需要导入该模块：</p>
<p><img src="Image00299.jpg" alt></p>
<p>首先，我们需要为SVM模型构建相应的类，SVM模型的类如程序清单4-1所示。</p>
<p><strong>程序清单4-1 SVM模型对应的类</strong></p>
<p><img src="Image00302.jpg" alt></p>
<p><img src="Image00306.jpg" alt></p>
<p>在程序清单4-1中，实现了SVM模型的类，在SVM模型的类中，包含了SVM模型的训练数据，SVM模型中的参数等。其中，calc_kernel函数用于根据指定的核函数kernel_opt计算样本的核函数矩阵，如程序代码中的①所示。calc_kernel函数的具体实现如程序清单4-2所示。</p>
<p><strong>程序清单4-2 样本的核函数矩阵</strong></p>
<p><img src="Image00310.jpg" alt></p>
<p>在程序清单 4-2 中，calc_kernel 函数用于根据指定的核函数类型以及参数kernel_option计算最终的样本核函数矩阵，样本核函数矩阵为：</p>
<p><img src="Image00314.jpg" alt></p>
<p>其中，K i，j 表示的是第i个样本和第 j个样本之间的核函数的值，在计算的过程中，利用cal_kernel_value函数计算每一个样本与其他样本的核函数的值，如程序代码中的①所示，函数cal_kernel_value的具体实现如程序清单4-3所示。</p>
<p><strong>程序清单4-3 样本之间的核函数的值</strong></p>
<p><img src="Image00320.jpg" alt></p>
<p><img src="Image00322.jpg" alt></p>
<p>在程序清单 4-3 中，cal_kernel_value 函数用于根据指定的核函数类型以及参数kernel_option计算样本train_x_i与其他所有样本之间的核函数的值。在实现的过程中，只实现了高斯核函数，如程序代码中的①所示，高斯核函数的具体形式如 4.3.4 节所示。若没有指定核函数的类型，则默认不使用核函数，如程序代码中的②所示。</p>
<p>当定义好SVM模型后，我们需要完成SVM模型的最重要的功能，即利用SMO算法对SVM模型进行训练，训练SVM模型的具体过程如程序清单4-4所示。</p>
<p><strong>程序清单4-4 SVM 模型的训练</strong></p>
<p><img src="Image00326.jpg" alt></p>
<p><img src="Image00328.jpg" alt></p>
<p>在程序清单4-4中，函数SVM_training通过在非边界样本或所有样本中交替遍历，选择出第一个需要优化的  i _α_ ，优先选择遍历非边界样本，因为非边界样本更有可能需要调整，而边界样本常常不能得到进一步调整而留在边界上。循环遍历非边界样本并选出它们当中违反KKT条件的样本进行调整，直到非边界样本全部满足KKT条件为止。当某一次遍历发现没有非边界样本得到调整时，就遍历所有样本，以检验是否整个集合都满足 KKT 条件。如果在整个集合的检验中又有样本被进一步优化，就有必要再遍历非边界样本。这样，不停地在“遍历所有样本”和“遍历非边界样本”之间切换，直到整个训练集都满足KKT条件为止。在选择出第一个变量 i _α_ 后，需要判断其是否满足条件，同时需要选择第二个变量 _α j _ ，如程序代码中的①和②所示，函数choose_and_update的具体实现如程序清单4-5所示。</p>
<p><strong>程序清单4-5 选择并更新参数</strong></p>
<p><img src="Image00331.jpg" alt></p>
<p><img src="Image00333.jpg" alt></p>
<p>在程序清单4-5中，函数choose_and_update实现了SMO中最核心的部分，在函数 choose_and_update 中，首先，判断选择出的第一个变量  i _α_ 是否满足要求，在判断的过程中需要计算第一个变量的误差值E i ，如程序代码中的①所示，函数 cal_error的具体实现如程序清单4-6所示；当检查完第一个变量  i _α_ 满足条件后，需要选择第二个变量 _α j _ ，对于第二个变量，选择的标准是使得其改变最大，选择的具体过程如程序代码中的②所示，函数select_second_sample_j的具体实现如程序清单4-7所示。当两个变量 i _α_ 和 _α j _ 都更新完成后，此时需要重新计算b的值，如程序代码中的③所示。最终，需要重新计算两个变量  i _α_ 和 _α j _ 对应的误差值E i 和E  j ，如程序代码中的④和⑤所示，函数update_error_tmp的具体实现如程序清单4-8所示。</p>
<p><strong>程序清单4-6 计算误差</strong></p>
<p><img src="Image00336.jpg" alt></p>
<p>在程序清单4-6中，函数cal_error用于计算变量alpha_k对应的误差error_k。</p>
<p><strong>程序清单4-7 选择第二个变量</strong></p>
<p><img src="Image00339.jpg" alt></p>
<p><img src="Image00345.jpg" alt></p>
<p>在程序清单 4-7 中，函数 select_second_sample_j 用于选择出第二个变量 _α j _ ，对于第二个变量的选择，选择的标准是误差值改变最大的，如程序代码中的①所示。若此时，候选集的长度为0，则随机选择 _α j _ ，如程序代码中的②所示。</p>
<p><strong>程序清单4-8 重新计算误差值</strong></p>
<p><img src="Image00349.jpg" alt></p>
<p>在程序清单4-8中，函数update_error_tmp用于重新计算变量alpha_k对应的误差，通过函数cal_error计算对应的误差，函数cal_error的具体计算过程如程序清单4-6所示。</p>
<h4 id="4-5-2-利用训练样本训练SVM模型"><a href="#4-5-2-利用训练样本训练SVM模型" class="headerlink" title="4.5.2 利用训练样本训练SVM模型"></a>4.5.2 利用训练样本训练SVM模型</h4><p>在训练 SVM 模型的过程中，我们建立“svm_train.py”文件。首先我们需要在“svm_train.py”文件中增加如下的代码，以实现对中文注释的支持，同时，为了能够利用文件“svm.py”中的函数，我们需要将其导入：</p>
<p><img src="Image00353.jpg" alt></p>
<p>在“svm_train.py”文件中增加主函数，主函数的具体实现如程序清单4-9所示。</p>
<p><strong>程序清单4-9 SVM 模型训练的主函数</strong></p>
<p><img src="Image00356.jpg" alt></p>
<p>在利用训练数据对 SVM 模型进行训练的过程中，主要分为：①利用函数load_data_libsvm导入训练数据，如程序代码中的①所示；②调用“svm.py”文件中的SVM_training方法对SVM模型进行训练，如程序代码中的②所示；③利用“svm.py”文件中的cal_accuracy函数对模型准确性进行评测，如程序代码中的③所示；④最后，利用“svm.py”文件中的 save_model 函数将最终的 SVM 模型保存到指定的文件中，如程序代码中的④所示。</p>
<p>首先，我们需要导入准备好的训练数据，在该实验中，我们使用到了训练数据“heart_scale”，该数据的样本格式如下所示：</p>
<p><img src="Image00413.jpg" alt></p>
<p>具体的样本形式如下所示：</p>
<p><img src="Image00733.jpg" alt></p>
<p>其中，第一列“+1”为样本标签，其余为样本的特征，在样本特征中，以“索引：值”的形式存储每一维特征。我们需要导入训练数据，导入训练数据的具体过程如程序清单4-10所示。</p>
<p><strong>程序清单4-10 导入训练数据</strong></p>
<p><img src="Image00613.jpg" alt></p>
<p>在程序清单 4-10 中，通过函数 load_data_libsvm将训练数据“heart_scale”中的样本特征与样本标签分开，转换后分别放入到矩阵data和矩阵label中。</p>
<p>在导入完成训练数据后，通过在“svm.py”文件中构建好函数SVM_training，实现对SVM模型的训练，函数SVM_training的具体实现如程序清单4-4所示；当训练完成后，我们需要对训练好的SVM模型进行评估，此时，我们需要在svm.py文件中构建函数cal_accuracy，函数cal_accuracy的具体形式如程序清单4-11所示。</p>
<p><strong>程序清单4-11 计算SVM模型的准确性</strong></p>
<p><img src="Image00764.jpg" alt></p>
<p>在程序清单4-11中，函数cal_accuracy利用训练好的SVM模型，对训练样本进行预测，如程序代码中的①所示；在得到预测值后，与其标签进行比较，如果预测值的符号与真实值的符号一致，则说明预测准确，否则预测不准确，比较的过程如程序代码中的②所示。对每一个训练样本预测函数svm_predict的具体过程如程序清单4-12所示。</p>
<p><strong>程序清单4-12 对每一个样本预测</strong></p>
<p><img src="Image00583.jpg" alt></p>
<p>在程序清单 4-12 中，svm_predict 函数用于对每一个样本进行预测。在预测的过程中，主要分为：①利用函数cal_kernel_value计算核函数的值，如程序代码中的①所示，函数cal_kernel_value的具体实现如程序清单4-3所示；②计算预测值，如程序代码中的②所示。计算预测值的方法为：</p>
<p><img src="Image00588.jpg" alt></p>
<p>当训练完SVM模型后，需要保存最终的SVM模型，保存SVM模型的具体过程如程序清单4-13所示。</p>
<p><strong>程序清单4-13 保存SVM模型</strong></p>
<p><img src="Image00129.jpg" alt></p>
<p>在程序清单4-13中，save_svm_model函数将训练好的SVM模型svm_model保存到model_file指定的文件中，在保存SVM模型的过程中使用到了cPickle模块中的dump方法。</p>
<p>SVM模型的训练过程为：</p>
<p><img src="Image00599.jpg" alt></p>
<p>最终，SVM训练的准确性为97.037%。</p>
<h4 id="4-5-3-利用训练好的SVM模型对新数据进行预测"><a href="#4-5-3-利用训练好的SVM模型对新数据进行预测" class="headerlink" title="4.5.3 利用训练好的SVM模型对新数据进行预测"></a>4.5.3 利用训练好的SVM模型对新数据进行预测</h4><p>对于分类算法而言，训练好的模型需要能够对新的数据集进行划分。利用上述步骤，我们训练好支持向量机 SVM 模型，并将其保存在“model_file”文件中，此时，我们需要利用训练好的SVM模型对新数据进行预测，同样，为了能够使用numpy中的函数和对中文注释的支持，在文件“svm_test.py”开始，我们加入：</p>
<p><img src="Image00604.jpg" alt></p>
<p>同时，在对新数据进行预测的过程中，需要使用到cPickle模块进行模型的导入，需要使用到“svm.py”文件中的svm_predict函数，因此，需要在文件“svm_test.py”文件中导入这些模块：</p>
<p><img src="Image00465.jpg" alt></p>
<p>对新数据的预测的主函数如程序清单4-14所示。</p>
<p><strong>程序清单4-14 对新数据的预测的主函数</strong></p>
<p><img src="Image00553.jpg" alt></p>
<p>在程序清单4-14中，对新数据的预测的主要步骤有：①导入测试数据，如程序代码中的①所示，其中，函数load_test_data的具体形式如程序清单4-15所示；②导入支持向量机SVM模型，如程序代码中的②所示，函数load_svm_model的具体形式如程序清单 4-16 所示；③计算得到预测值，如程序代码中的③所示，其中函数get_prediction 如程序清单 4-17 所示；④保存最终的预测结果，如程序代码中的④所示，函数save_prediction的具体形式如程序清单4-18所示。</p>
<p><strong>程序清单4-15 导入测试数据集</strong></p>
<p><img src="Image00032.jpg" alt></p>
<p><img src="Image00054.jpg" alt></p>
<p>在程序清单4-15中，函数load_test_data用于导入测试数据，测试数据与训练数据的主要区别是：在测试数据中不包含样本标签，因此在导入测试数据的过程中只需要导入样本的特征。</p>
<p><strong>程序清单4-16 导入SVM模型</strong></p>
<p><img src="Image00624.jpg" alt></p>
<p>在程序清单4-16中，函数load_svm_model用于导入训练好的SVM模型，在导入SVM模型的过程中使用到了cPickle模块中的load函数，如程序代码中的①所示。</p>
<p><strong>程序清单4-17 对新数据的预测</strong></p>
<p><img src="Image00072.jpg" alt></p>
<p><img src="Image00189.jpg" alt></p>
<p>在程序清单4-17中，函数get_prediction利用训练好的SVM模型对测试样本进行预测，在预测的过程中，利用“svm.py”中的 svm_predict 函数分别对每一个样本进行预测，如程序代码中的①所示，当预测完成后，利用sign函数将其转换成对应的类别，如程序代码中的②所示。</p>
<p><strong>程序清单4-18 保存最终的预测结果</strong></p>
<p><img src="Image00633.jpg" alt></p>
<p>在程序清单4-18中，函数save_prediction将预测的结果prediction保存到指定的文件result_file中。</p>
<p><strong>参考文献</strong></p>
<p>[1] 李航.统计学习方法[M].北京：清华大学出版社.2012.</p>
<p>[2] Platt J C.Sequential Minimal Optimization:A Fast Algorithm for Training Support Vector Machines[C].//Advances in Kernel Methods-support Vector Learning.1998:212-223.</p>
<p>[3] Peter Harrington.机器学习实战[M].王斌，译.人民邮电出版社.2013.</p>
<h2 id="5-随机森林"><a href="#5-随机森林" class="headerlink" title="5 随机森林"></a>5 随机森林</h2><p>对于一个复杂的分类问题来说，训练一个复杂的分类模型通常比较耗费时间，同时，为了能够提高对分类问题的预测准确性，通常可以选择训练多个分类模型，并将各自的预测结果组合起来，得到最终的预测。集成学习（Ensemble Learning）便是这样一种学习方法，集成学习是指将多种学习算法，通过适当的形式组合起来完成同一个任务。在集成学习中，主要分为bagging算法和boosting算法。</p>
<p>随机森林（Random Forest）是bagging算法中最重要的一种算法，通过对数据集的采样生成多个不同的数据集，并在每一个数据集上训练一棵分类树，最终结合每一棵分类树的预测结果作为随机森林的预测结果。</p>
<h3 id="5-1-决策树分类器"><a href="#5-1-决策树分类器" class="headerlink" title="5.1 决策树分类器"></a>5.1 决策树分类器</h3><h4 id="5-1-1-决策树的基本概念"><a href="#5-1-1-决策树的基本概念" class="headerlink" title="5.1.1 决策树的基本概念"></a>5.1.1 决策树的基本概念</h4><p>决策树（Decision Tree）算法是一类常用的机器学习算法，在分类问题中，决策树算法通过样本中某一维属性的值，将样本划分到不同的类别中。以二分类为例，二分类的数据集如表5-1所示。</p>
<p>表5-1 数据集</p>
<p><img src="Image00168.jpg" alt></p>
<p>在表5-1中，有5个样本，样本中的属性为“是否用鳃呼吸”和“有无鱼鳍”，通过对样本的学习，如“鲸鲨”，可以利用学习到的决策树模型对于一个新的样本，正确地做出决策，即判断其是否为鱼。</p>
<p>决策树算法是基于树形结构来进行决策的。如对于表5-1所示的数据，首先通过属性“是否用鳃呼吸”判断样本是否为鱼，如图5.1所示。</p>
<p><img src="Image00639.jpg" alt></p>
<p>图5.1 通过属性“是否用鳃呼吸”划分数据</p>
<p>从图5.1中可以看出，通过属性“是否用鳃呼吸”，已经将一部分样本区分开，即不用鳃呼吸的不是鱼，接下来对剩下的样本利用第二维属性“有无鱼鳍”进行划分，如图5.2所示。</p>
<p><img src="Image00644.jpg" alt></p>
<p>图5.2 通过属性“有无鱼鳍”继续划分数据</p>
<p>在图5.2中，通过属性“有无鱼鳍”对剩余的样本继续划分，得到了最终的决策，从图5.2中可以看出，不用鳃呼吸的不是鱼，用鳃呼吸但是没有鱼鳍的也不是鱼，用鳃呼吸同时有鱼鳍的是鱼。对于一个新的样本“鲸鲨”，其样本属性为{用鳃呼吸，有鱼鳍}，符合上述对鱼的判断，因此，认为鲸鲨为鱼。</p>
<h4 id="5-1-2-选择最佳划分的标准"><a href="#5-1-2-选择最佳划分的标准" class="headerlink" title="5.1.2 选择最佳划分的标准"></a>5.1.2 选择最佳划分的标准</h4><p>对于表5-1中所示的数据，其中每个样本包含了两个特征，分别为是否用鳃呼吸和有无鱼鳍，对于这两维特征，选择划分数据集的特征的时候存在一定的顺序，如图5.1中，首先选择的是“是否用鳃呼吸”，选择的依据是这一维特征对数据的划分更具有区分性，在决策树算法中，通常有这些标准：信息增益（Information Gain）、增益率（Gain Ratio）和基尼指数（Gini Index）。</p>
<p>熵（Entropy）是度量样本集合纯度最常用的一种指标，对于包含m个训练样本的数据集D：{（X  （1） ，y （1） ），…，（X  （m） ，y （m）<br>）}，在数据集D中，第k类的样本所占的比例为p k ，则数据集D的信息熵为：</p>
<p><img src="Image00648.jpg" alt></p>
<p>其中，K表示的是数据集D中类别的个数。对于表5.1所示的数据集，其信息熵为：</p>
<p><img src="Image00264.jpg" alt></p>
<p>当把样本按照特征A的值a划分成两个独立的子数据集D 1 和D 2 时，此时整个数据集D的熵为两个独立数据集D 1 的熵和D 2 的熵的加权和，即：</p>
<p><img src="Image00283.jpg" alt></p>
<p>其中，D 1 表示的是数据集D 1 中的样本的个数，D 2 表示的是数据集D 2 中的样本的个数。对于表5-1所示的数据集，将样本按照特征“是否用鳃呼吸”划分成两个独立的子数据集，如图5.1所示，此时，数据集D的信息熵为：</p>
<p><img src="Image00659.jpg" alt></p>
<p>由上述的划分可以看出，在划分后数据集D的信息熵减小了，对于给定的数据集，划分前后信息熵的减少量称为信息增益（Information Gain），即：</p>
<p><img src="Image00663.jpg" alt></p>
<p>其中<img src="Image00667.jpg" alt> 表示的是属于第p类的样本的个数。信息熵表示的数据集中的不纯度，信息熵较小表明数据集纯度提升了。在选择数据集划分的标准时，通常选择能够使得信息增益最大的划分。ID3决策树算法就是利用信息增益作为划分数据集的一种方法。</p>
<p>增益率（Gain Ratio）是可以作为选择最优划分属性的方法，增益率的计算方法为：</p>
<p><img src="Image00368.jpg" alt></p>
<p>其中，IV （A）被称为特征A的“固有值（Intrinsic Value）”，即：</p>
<p><img src="Image00676.jpg" alt></p>
<p>在著名的C4.5决策树算法中就是利用增益率作为划分数据集的方法。</p>
<p>基尼指数（Gini Index）也可以选择最优的划分属性，对于数据集D，假设有K个分类，则样本属于第k个类的概率为p k ，则此概率分布的基尼指数为：</p>
<p><img src="Image00680.jpg" alt></p>
<p>对于数据集D，其基尼指数为：</p>
<p><img src="Image00686.jpg" alt></p>
<p>其中，<img src="Image00455.jpg" alt> 表示数据集D中，属于类别k的样本的个数。若此时根据特征A将数据集D划分成独立的两个数据集D 1 和D 2 ，此时的基尼指数为：</p>
<p><img src="Image00788.jpg" alt></p>
<p>在如表5-1所示的数据集D中，其基尼指数为：</p>
<p><img src="Image00790.jpg" alt></p>
<p>利用特征“是否用鳃呼吸”将数据集D划分成独立的两个数据集D 1 和D 2 后，其基尼指数为：</p>
<p><img src="Image00794.jpg" alt></p>
<p>在CART决策树算法中利用Gini指数作为划分数据集的方法。</p>
<p>现在，让我们一起利用Python实现上述的Gini指数的计算过程，Gini指数的具体计算方法如程序清单5-1所示。</p>
<p><strong>程序清单5-1 Gini指数的计算</strong></p>
<p><img src="Image00798.jpg" alt></p>
<p>在程序清单5-1中，函数cal_gini_index用于计算数据集data的Gini指数，在计算 Gini 指数的过程中，需要判断数据集中类别标签的个数，label_uniq_cnt 函数用于计算数据集 data 中不同的类别标签的个数，如程序代码中的①所示，函数label_uniq_cnt的具体实现如程序清单5-2所示。通过统计不同类别标签的个数，并根据上述的计算方法计算当前的数据集 data 中的Gini指数，其具体的计算方法如程序代码中的②和③所示，在计算Gini指数的过程中，需要用到pow函数，因此，在程序开始前，我们需要导入pow：</p>
<p><img src="Image00803.jpg" alt></p>
<p><strong>程序清单5-2 统计数据集中不同标签的个数</strong></p>
<p><img src="Image00808.jpg" alt></p>
<p>在程序清单5-2中，函数label_uniq_cnt用于统计数据集data中不同标签的个数，并将统计结果存储到字典label_uniq_cnt中。</p>
<h4 id="5-1-3-停止划分的标准"><a href="#5-1-3-停止划分的标准" class="headerlink" title="5.1.3 停止划分的标准"></a>5.1.3 停止划分的标准</h4><p>在按照特征对上述的数据进行划分的过程中，需要设置划分的终止条件，通常在算法的过程中，设置划分终止条件的方法主要有：①结点中的样本数小于给定阀值；②样本集的基尼指数小于给定阀值（样本基本属于同一类）；③没有更多特征。</p>
<p>在图5.2所示的最终的划分中，当叶子节点中的所有样本属于同一个类别时，停止划分。</p>
<h3 id="5-2-CART分类树算法"><a href="#5-2-CART分类树算法" class="headerlink" title="5.2 CART分类树算法"></a>5.2 CART分类树算法</h3><h4 id="5-2-1-CART分类树算法的基本原理"><a href="#5-2-1-CART分类树算法的基本原理" class="headerlink" title="5.2.1 CART分类树算法的基本原理"></a>5.2.1 CART分类树算法的基本原理</h4><p>CART算法（Classification And Regression Tree）是决策树的一种，如上所述，主要的决策树模型有ID3算法、C4.5算法和CART算法，与ID3算法和C4.5算法不同的是，CART算法既能处理分类问题也可以处理回归问题。</p>
<p>CART 算法既可以用于创建分类树（Classification Tree），也可以用于创建回归树（Regression Tree），在本章中，主要是利用CART算法创建分类树。</p>
<h4 id="5-2-2-CART分类树的构建"><a href="#5-2-2-CART分类树的构建" class="headerlink" title="5.2.2 CART分类树的构建"></a>5.2.2 CART分类树的构建</h4><p>在CART分类树算法中，利用Gini指数作为划分数的指标，通过样本中的特征，对样本进行划分，直到所有的叶节点中的所有样本都为同一个类别为止，CART分类树的构建过程如下所示：</p>
<p>• 对于当前训练数据集，遍历所有属性及其所有可能的切分点，寻找最佳切分属性及其最佳切分点，使得切分之后的基尼指数最小，利用该最佳属性及其最佳切分点将训练数据集切分成两个子集，分别对应判别结果为左子树和判别结果为右子树。</p>
<p>• 重复以下的步骤直至满足停止条件：为每一个叶子节点寻找最佳切分属性及其最佳切分点，将其划分为左右子树。</p>
<p>• 生成CART决策树。</p>
<p>现在，让我们一起利用Python实现CART决策树。为了能构建CART分类树算法，首先，需要为CART分类树中节点设置一个结构，并将其保存到CART树的文件“tree.py”中，其具体的实现如程序清单5-3所示。</p>
<p><strong>程序清单5-3 树中节点的结构</strong></p>
<p><img src="Image00811.jpg" alt></p>
<p><img src="Image00815.jpg" alt></p>
<p>在程序清单5-3中，为树中节点设置node类，在node类中，属性fea表示的是待切分的特征的索引值，属性value表示的是待切分的特征的索引处的具体的值，当node为叶子节点时，属性results表示的是该叶子节点所属的类别，属性right表示的是树中节点node的右子树，属性left表示的是树中节点的左子树。</p>
<p>当定义好树的节点后，利用训练数据训练CART分类树模型，其具体实现如程序清单5-4所示。</p>
<p><strong>程序清单5-4 构建CART分类树</strong></p>
<p><img src="Image00818.jpg" alt></p>
<p><img src="Image00823.jpg" alt></p>
<p>在程序清单5-4中，函数build_tree用于构建CART分类树，在构建分类树的过程中，主要有如下的几步：①计算当前的Gini指数；②尝试按照数据集中的每一个特征将树划分成左右子树，计算出最好的划分，通过迭代的方式继续对左右子树进行划分；③判断当前是否还可以继续划分，若不能继续划分则退出。</p>
<p>在构建 CART 分类树的过程中，首先是计算当前的 Gini 指数，如程序代码中的①所示，函数cal_gini_index的具体实现如程序清单5-1所示。</p>
<p>在划分的过程中，需要按照Gini指数找到最好的划分。寻找最好的划分的方法是遍历所有的样本的特征，取得能够使得划分前后Gini指数的变化最大的特征，按照该特征的值将树划分成左右子树。在寻找最好划分的过程中，首先取得所有样本在 fea特征处的可能的取值，并将其存储到字典feature_values中，如程序代码中的②所示。对特征fea处的每一种可能取值，利用函数split_tree尝试将数据集data划分成左右子树set_1和set_2，如程序代码中的③所示。函数split_tree按照指定的特征fea处的值value将数据集划分成左右子树，函数split_tree的具体实现如程序清单5-5所示。划分后，计算此时的Gini指数，此时的Gini指数为左右子树的Gini指数之和，如程序代码中的④所示。判断当前的Gini指数与划分前Gini指数的变化，找到能够使得Gini指数变化最大的特征作为最终的划分标准，如程序代码中的⑤所示。</p>
<p>待找到了当前的最好划分后，将数据集data划分成左右子树set_1和set_2，判断此时的划分中Gini指数是否为0，若不为0，则对左右子树重复上述的划分过程，如程序代码中的⑥和⑦所示；若此时Gini指数为0，则停止划分，返回叶子节点的标签，如程序代码中的⑧所示。</p>
<p><strong>程序清单5-5 划分左右子树的split_tree函数</strong></p>
<p><img src="Image00828.jpg" alt></p>
<p>在程序清单5-5中，函数split_tree主要用于特征的值是连续的值时的划分，当特征 fea 处的值是一些连续值的时候，当该处的值大于或等于待划分的值 value 时，将该样本划分到set_1中，如程序代码中的①所示，否则，划分到set_2中，如程序代码中的②所示。</p>
<h4 id="5-2-3-利用构建好的分类树进行预测"><a href="#5-2-3-利用构建好的分类树进行预测" class="headerlink" title="5.2.3 利用构建好的分类树进行预测"></a>5.2.3 利用构建好的分类树进行预测</h4><p>当整个CART分类树构建完成后，利用训练样本对分类树进行训练，最终得到分类树的模型，对于未知的样本，需要用训练好的分类树的模型对其进行预测，对样本进行预测的过程如程序清单5-6所示。</p>
<p><strong>程序清单5-6 利用训练好的分类树对新样本进行预测</strong></p>
<p><img src="Image00833.jpg" alt></p>
<p><img src="Image00839.jpg" alt></p>
<p>在程序清单5-6中，函数predict利用训练好的CART分类树模型tree对样本sample进行预测，当只有树根时，直接返回树根的类标签，如程序代码中的①所示，若此时有左右子树，则根据指定的特征fea处的值进行比较，选择左右子树，直到找到最终的标签。</p>
<h3 id="5-3-集成学习（Ensemble-Learning）"><a href="#5-3-集成学习（Ensemble-Learning）" class="headerlink" title="5.3 集成学习（Ensemble Learning）"></a>5.3 集成学习（Ensemble Learning）</h3><h4 id="5-3-1-集成学习的思想"><a href="#5-3-1-集成学习的思想" class="headerlink" title="5.3.1 集成学习的思想"></a>5.3.1 集成学习的思想</h4><p>在前面章节中，面对一个复杂的分类问题，我们试图寻找到一种高效的算法处理这类复杂的分类问题，通过对训练数据的学习，构建出分类模型。然而，面对一个较为复杂的分类问题，训练一个高效的分类算法通常需要花费很多的资源，同时，训练好的模型在面对复杂的分类问题时，有时会显得不足。</p>
<p>集成学习（Ensemble Learning）是一种新的学习策略，对于一个复杂的分类问题，通过训练多个分类器，利用这些分类器来解决同一个问题。这样的思想有点类似于“三个臭皮匠赛过诸葛亮”，例如，在医学方面，面对一个新型的或者罕见的疾病时，通常会组织多个医学“专家”会诊，通过结合这些“专家”的意见，最终给出治疗的方法。在集成学习中，通过学习多个分类器，通过结合这些分类器对于同一个样本的预测结果，给出最终的预测结果。</p>
<h4 id="5-3-2-集成学习中的典型方法"><a href="#5-3-2-集成学习中的典型方法" class="headerlink" title="5.3.2 集成学习中的典型方法"></a>5.3.2 集成学习中的典型方法</h4><p>在集成学习方法中，其泛化能力比单个学习算法的泛化能力强很多。在集成学习方法中，根据多个分类器学习方式的不同，可以分为：Bagging算法和Boosting算法。Bagging（Bootstrap Aggregating）算法通过对训练样本有放回的抽取，由此产生多个训练数据的子集，并在每一个训练集的子集上训练一个分类器，最终分类结果是由多个分类器的分类结果投票而产生的。Bagging算法的整个过程如图5.3所示。</p>
<p><img src="Image00841.jpg" alt></p>
<p>图5.3 Bagging算法过程</p>
<p>在图5.3 中，对于一个分类问题而言，假设有n个分类器，每次通过有放回的从原始数据集中抽取训练样本，分别训练这n个分类器{ _φ 1 _ ， _φ 2 _ ，…， _φ n _ }，最终，通过组合n个分类器的结果作为最终的预测结果。</p>
<p>与Bagging算法不同，Boosting算法通过顺序地给训练集中的数据项重新加权创造不同的基础学习器。Boosting算法的核心思想是重复应用一个基础学习器来修改训练数据集，这样在预定数量的迭代下可以产生一系列的基础学习器。在训练开始，所有的数据项都被初始化为同一个权重，在这次初始化之后，每次增强的迭代都会生成一个适应加权之后的训练数据集的基础学习器。每一次迭代的错误率都会计算出来，而且正确划分的数据项的权重会被降低，然后错误划分的数据项权重将会增大。Boosting算法的最终模型是一系列基础学习器的线性组合，而且系数依赖于各个基础学习器的表现。Boosting 算法有很多版本，但是目前使用最广泛的是 AdaBoost 算法和GBDT算法。Boosting算法的整个过程如图5.4所示。</p>
<p><img src="Image00844.jpg" alt></p>
<p>图5.4 Boosting算法的整个过程</p>
<p>在图5.4中，对于包含n个分类器的Boosting算法，依次利用训练样本对其进行学习，在每个分类器中，其样本的权重是不一样的，如对于第i+1个分类器来讲，第i个分类器会对每个样本进行评估，预测错误的样本，其权重会增加，反之，则减小。训练好每一个分类器后，对每一个分类器的结果线性加权得到最终的预测结果。</p>
<h3 id="5-4-随机森林（Random-Forests）"><a href="#5-4-随机森林（Random-Forests）" class="headerlink" title="5.4 随机森林（Random Forests）"></a>5.4 随机森林（Random Forests）</h3><h4 id="5-4-1-随机森林算法模型"><a href="#5-4-1-随机森林算法模型" class="headerlink" title="5.4.1 随机森林算法模型"></a>5.4.1 随机森林算法模型</h4><p>随机森林（Random Forest，RF）算法是一种重要的基于Bagging的集成学习方法，加州大学伯克利分校的Breiman Leo和Adele Cutler在2001年发表的论文中提到了Random Forest 算法，随机森林算法可以用来做分类、回归等问题，在本章节中主要介绍随机森林在分类问题中的应用。</p>
<p>随机森林算法是由一系列的决策树组成，它通过自助法（Bootstrap）重采样技术，从原始训练样本集中有放回地重复随机抽取m个样本，生成新的训练样本集合，然后根据自助样本集生成k个分类树组成随机森林，新数据的分类结果按分类树投票多少形成的分数而定。其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和它们之间的相关性。特征选择采用随机的方法去分裂每一个节点，然后比较不同情况下产生的误差。能够检测到的内在估计误差、分类能力和相关性决定选择特征的数目。单棵树的分类能力可能很小，但在随机产生大量的决策树后，一个测试样品可以通过统计每一棵树的分类结果，从面选择最可能的分类。</p>
<h4 id="5-4-2-随机森林算法流程"><a href="#5-4-2-随机森林算法流程" class="headerlink" title="5.4.2 随机森林算法流程"></a>5.4.2 随机森林算法流程</h4><p>随机森林算法是通过训练多个决策树，生成模型，然后综合利用多个决策树进行分类。随机森林算法只需要两个参数：构建的决策树的个数n tree ，在决策树的每个节点进行分裂时需要考虑的输入特征的个数k，通常k可以取为log  2 n，其中n表示的是原数据集中特征的个数。对于单棵决策树的构建，可以分为如下的步骤：</p>
<p>• 假设训练样本的个数为m，则对于每一棵决策树的输入样本的个数都为m，且这m个样本是通过从训练集中有放回地随机抽取得到的。</p>
<p>• 假设训练样本特征的个数为n，对于每一棵决策树的样本特征是从该n个特征中随机挑选k个，然后从这k个输入特征里选择一个最好的进行分裂。</p>
<p>• 每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树分裂过程中不需要剪枝。</p>
<p>根据上述的过程，我们利用Python实现随机森林的训练过程，在实现随机森林的训练过程时，需要使用到numpy中的函数，因此需要导入numpy模块：</p>
<p><img src="Image00847.jpg" alt></p>
<p>随机森林的构建过程如程序清单5-7所示。</p>
<p><strong>程序清单5-7 构建随机森林</strong></p>
<p><img src="Image00850.jpg" alt></p>
<p><img src="Image00854.jpg" alt></p>
<p>在程序清单5-7中，函数random_forest_training用于构建具有多棵树的随机森林，其中函数的输入data_train表示的是训练数据，trees_num表示的是在随机森林中分类树的数量。在随机森林算法中，随机选择的特征的个数通常为k=log 2 n，其中n表示的是原数据集中特征的个数，如程序代码中的①所示；在程序代码中，使用到了math模块中的log函数，因此，需要导入log函数：</p>
<p><img src="Image00857.jpg" alt></p>
<p>当随机森林中分类树的数量 trees_num 和每一棵树的特征的个数k设置完成后，便可以利用训练样本训练随机森林中的每一棵树。在训练每一棵树的过程中，主要有如下几步：①从样本集中随机选择m个样本中的k个特征，其中，m为原始数据集中的样本个数，如程序代码中的②所示，函数choose_sample的具体实现如程序清单5-8所示；②利用选择好的只包含部分特征的数据集 data_sample 构建分类树模型，如程序代码中的③所示，函数 build_tree 的具体实现如程序清单 5-4 所示，在此，为了使用build_tree函数，需要从“tree.py”文件中导入build_tree函数：</p>
<p><img src="Image00859.jpg" alt></p>
<p>③当训练好CART树后，保存训练好的分类树模型，如程序代码中的④所示；④保存在该分类树下选择的特征feature，这一步主要是保证对新的数据集进行预测时，能够从中选择出特征。</p>
<p><strong>程序清单5-8 随机选择样本及特征</strong></p>
<p><img src="Image00864.jpg" alt></p>
<p><img src="Image00869.jpg" alt></p>
<p>在程序清单5-8中，choose_samples函数的功能是从原始的训练样本data中随机选择出m个样本，这里的随机选择是指有放回地选择，样本之间是可以重复的，同时这m个样本中只保留k维特征，用来组成新的样本data_sample，同时为了能够还原选择出的样本特征，需要保存选择出的特征feature。在随机选择的过程中，使用到了random模块中的randint函数，因此需要导入random模块：</p>
<p><img src="Image00873.jpg" alt></p>
<h3 id="5-5-随机森林RF算法实践"><a href="#5-5-随机森林RF算法实践" class="headerlink" title="5.5 随机森林RF算法实践"></a>5.5 随机森林RF算法实践</h3><p>在如上的几节中，我们介绍了随机森林RF算法的基本概念和具体的构建过程，介绍了CART树的基本概念和如何构建一棵CART分类树。接下来，我们利用图5.5所示的非线性可分的分类数据，并结合之前完成的函数，训练完整的随机森林RF模型，训练数据如图5.5所示。</p>
<p><img src="Image00877.jpg" alt></p>
<p>图5.5 非线性可分的数据集</p>
<p>利用随机森林算法对其进行分类的过程中，主要有两个部分：①利用训练数据对模型进行训练；②对新的数据进行预测。</p>
<h4 id="5-5-1-训练随机森林模型"><a href="#5-5-1-训练随机森林模型" class="headerlink" title="5.5.1 训练随机森林模型"></a>5.5.1 训练随机森林模型</h4><p>首先，我们利用训练样本训练模型，为了使得 Python 能够支持中文注释和利用numpy，我们需要在训练文件“random_forests_train.py”的开始加入：</p>
<p><img src="Image00881.jpg" alt></p>
<p>同时，在训练随机森林模型时，还需要使用到如下的一些函数：</p>
<p><img src="Image00886.jpg" alt></p>
<p>其中，random模块用于随机选择样本和特征，math模块中的log函数用于计算选择的特征个数，tree模块包含构建CART分类树的主要过程，tree模块中的build_tree用于构建CART分类树模型，predict函数利用构建好的CART树模型对样本进行预测，cPickle模块用于保存和导入训练好的随机森林RF模型。</p>
<p>随机森林模型训练的主函数如程序清单5-9所示。</p>
<p><strong>程序清单5-9 随机森林训练的主函数</strong></p>
<p><img src="Image00891.jpg" alt></p>
<p>程序清单5-9是训练随机森林模型的主函数，在训练随机森林模型的过程中，主要包括：①导入训练数据，如程序代码中的①所示，函数 load_data 的具体实现如程序清单5-10所示；②利用训练数据data_train训练随机森林模型，如程序代码中的②所示，函数random_forest_training的具体实现如程序清单5-7所示；③评估训练好的随机森林模型，首先是利用训练好的随机森林模型对训练样本进行预测，如程序代码中的③所示，函数get_predict的具体实现如程序清单5-11所示，在得到预测值后，比较预测值与训练样本中的标签之间的差异，如程序代码中的④所示，函数cal_correct_rate的具体实现如程序清单5-12所示；④保存最终的随机森林模型，如程序代码中的⑤所示，函数save_model的具体实现如程序清单5-13所示。</p>
<p><strong>程序清单5-10 导入训练集</strong></p>
<p><img src="Image00896.jpg" alt></p>
<p><img src="Image00899.jpg" alt></p>
<p>在程序清单5-10中，首先需要导入一些模块，其中cPickle模块用于保存训练好的随机森林模型到本地，如程序代码中的①所示。函数 load_data 用于导入保存训练数据的文件“file_name”。</p>
<p><strong>程序清单5-11 get_predict对样本预测</strong></p>
<p><img src="Image00903.jpg" alt></p>
<p>在程序清单5-11中，函数get_predict利用训练好的随机森林模型对训练数据进行预测，其中，trees_feature 中保存了每一棵分类树中随机选择的特征，在利用每一棵树对样本进行预测的过程中，根据trees_feature中选择好的特征对原始数据集采样，并利用对应的分类树对采样后的数据进行预测，采样的过程如程序代码中的①所示，函数split_data的具体过程如程序清单5-14所示，预测的过程如程序代码中的②所示，函数predict的具体过程如程序清单5-6所示。当所有的分类树对样本都预测完成后，结合所有的预测结果作为随机森林模型的预测结果，如程序代码中的③所示。</p>
<p><strong>程序清单5-12 计算模型的预测准确性</strong></p>
<p><img src="Image00907.jpg" alt></p>
<p>在程序清单5-12中，函数cal_correct_rate通过比较预测结果final_predict与原始样本中的标签，若两者同号，则表明预测正确，如程序代码中的①所示。最终返回正确率，如程序代码中的②所示。</p>
<p><strong>程序清单5-13 保存最终的模型</strong></p>
<p><img src="Image00909.jpg" alt></p>
<p>在程序清单5-13中，函数save_model用于将最终的随机森林模型trees_result保存到result_file文件中，同时将每棵树选择的特征trees_feature保存到feature_file文件中。</p>
<p><strong>程序清单5-14 划分数据的split_data函数</strong></p>
<p><img src="Image00570.jpg" alt></p>
<p>在程序清单5-14中，函数split_data按照feature中的特征从原始数据集data_train中选择出指定的特征，并将其保存到data中。</p>
<h4 id="5-5-2-最终的训练结果"><a href="#5-5-2-最终的训练结果" class="headerlink" title="5.5.2 最终的训练结果"></a>5.5.2 最终的训练结果</h4><p>随机森林的训练过程为：</p>
<p><img src="Image00380.jpg" alt></p>
<p>最终，随机森林的训练准确率为100%。</p>
<p>当设置的分类树的数量为50棵时，其结果如图5.6所示。</p>
<p><img src="Image00576.jpg" alt></p>
<p>图5.6 当分类树为50棵时的分类结果</p>
<h4 id="5-5-3-对新数据的预测"><a href="#5-5-3-对新数据的预测" class="headerlink" title="5.5.3 对新数据的预测"></a>5.5.3 对新数据的预测</h4><p>利用上述内容，我们训练好随机森林RF模型，并将RF模型保存在“result_file”文件中，将每一棵CART分类树中选择的特征编号保存到“feature_file”文件中。此时，我们需要利用训练好的RF模型对新数据进行预测，而为了能够导入训练好的RF模型和利用 RF 模型对新数据进行预测，同时实现对中文注释的支持，我们在文件“random_forests_test.py”的开始，加入：</p>
<p><img src="Image00895.jpg" alt></p>
<p>对新数据的预测的主函数如程序清单5-15所示。</p>
<p><strong>程序清单5-15 对新数据的预测的主函数</strong></p>
<p><img src="Image00525.jpg" alt></p>
<p><img src="Image00778.jpg" alt></p>
<p>程序清单5-15是利用训练好的随机森林模型对新数据预测的主函数，在程序清单5-15中，对新数据的预测主要包括：①导入需要预测的数据，如程序代码中的①所示，函数load_data的具体实现如程序清单5-16所示；②导入训练好的随机森林模型和每一个分类中选择的特征，如程序代码中的②所示，函数load_model的具体实现如程序清单5-17所示；③利用训练好的随机森林模型对测试数据进行预测，如程序代码中的③所示，函数get_predict的具体实现如程序清单5-11所示；④最终，将预测的结果保存到文件final_result中，如程序代码中的④所示，函数save_reuslt的具体实现如程序清单5-18所示。</p>
<p><strong>程序清单5-16 导入待分类的数据集</strong></p>
<p><img src="Image00829.jpg" alt></p>
<p>在程序清单5-16中，在load_data函数中，需要将存储在文件file_name中的数据导入到test_data中，为了与训练数据格式一致，在每一个样本中，加入一个值为0的标签，如程序代码中的①所示。</p>
<p><strong>程序清单5-17 导入随机森林模型</strong></p>
<p><img src="Image00851.jpg" alt></p>
<p><img src="Image00215.jpg" alt></p>
<p>在程序清单 5-17 中，load_model 函数主要用于导入存储在文件“result_file”中的随机森林模型和存储在文件“feature_file”中的每一棵分类树选择的特征。在导入随机森林模型的过程中，使用到了 cPickle 模型中的 load 函数，如程序代码中的①所示。</p>
<p><strong>程序清单5-18 保存最终的预测结果</strong></p>
<p><img src="Image00610.jpg" alt></p>
<p>在程序清单5-18中，save_result函数将预测结果prediction保存到文件“result_file”中。</p>
<p><strong>参考文献</strong></p>
<p>[1] 周志华.机器学习[M].北京：清华大学出版社.2016.</p>
<p>[2] GJS Blog.Ensemble learning（集成学习）[DB/OL].http：//www.cnblogs.com/GuoJiaSheng/p/4033584.html</p>
<p>[3] Wikipedia.Ensemble learning[DB/OL].<a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Ensemble_learning</a></p>
<h2 id="6-BP神经网络"><a href="#6-BP神经网络" class="headerlink" title="6 BP神经网络"></a>6 BP神经网络</h2><p>人工神经网络（Artificial Neural Network，ANN）作为对人脑最简单的一种抽象和模拟，是人们模仿人的大脑神经系统信息处理功能的一个智能化系统，是 20 世纪80年代以来人工智能领域兴起的研究热点。在神经网络发展的不同阶段，相继出现了不同的神经网络模型，从最初的浅层神经网络到现在如火如荼的深度神经网络。</p>
<p>在神经网络技术的发展过程中，BP（Back Propagation）神经网络的出现和发展对整个神经网络技术的发展起着重要的作用，BP 神经网络通常指的是具有三层网络结构的浅层神经网络。</p>
<h3 id="6-1-神经元概述"><a href="#6-1-神经元概述" class="headerlink" title="6.1 神经元概述"></a>6.1 神经元概述</h3><h4 id="6-1-1-神经元的基本结构"><a href="#6-1-1-神经元的基本结构" class="headerlink" title="6.1.1 神经元的基本结构"></a>6.1.1 神经元的基本结构</h4><p>神经网络是由一个个被称为“神经元”的基本单元构成，神经元结构由输入、计算单元和输出组成，单个神经元的结构如图6.1所示。</p>
<p><img src="Image00007.jpg" alt></p>
<p>图6.1 神经元的结构</p>
<h4 id="6-1-2-激活函数"><a href="#6-1-2-激活函数" class="headerlink" title="6.1.2 激活函数"></a>6.1.2 激活函数</h4><p>对于图6.1所示的神经元结构，其输入为x 1 、x 2 、x 3 和截距+1，其输出为：</p>
<p><img src="Image00616.jpg" alt></p>
<p>其中，W 表示的是权重向量，函数 f：R→R称为激活函数，通常激活函数可以选择为Sigmoid函数，或者tanh双曲正切函数，其中，Sigmoid函数的形式为：</p>
<p><img src="Image00620.jpg" alt></p>
<p>双曲正切函数的形式为：</p>
<p><img src="Image00076.jpg" alt></p>
<p>以下分别是Sigmoid函数和tanh函数的图像，左边为Sigmoid函数的图像，右边为tanh函数的图像：</p>
<p><img src="Image00628.jpg" alt></p>
<p>图6.2 Sigmoid函数和tanh函数的图像</p>
<p>Sigmoid函数的区间为[0，1]，而tanh函数的区间为[-1，1]。若是使用sigmoid作为神经元的激活函数，则当神经元的输出为1时，表示该神经元被激活，否则称为未被激活。同样，对于激活函数是tanh时，神经元的输出为1时，表示该神经元被激活，否则称为未被激活。</p>
<p>近年来，一些新的激活函数被提出，如ReLu、SoftPlus等。激活函数ReLu的具体形式为：</p>
<p><img src="Image00631.jpg" alt></p>
<p>激活函数SoftPlus的具体形式为：</p>
<p><img src="Image00143.jpg" alt></p>
<p>激活函数ReLu和SoftPlus的图像如图6.3所示。</p>
<p><img src="Image00636.jpg" alt></p>
<p>图6.3 ReLu函数和SoftPlus函数</p>
<h3 id="6-2-神经网络模型"><a href="#6-2-神经网络模型" class="headerlink" title="6.2 神经网络模型"></a>6.2 神经网络模型</h3><h4 id="6-2-1-神经网络的结构"><a href="#6-2-1-神经网络的结构" class="headerlink" title="6.2.1 神经网络的结构"></a>6.2.1 神经网络的结构</h4><p>神经网络是由很多的神经元联结而成的，一个三层的神经网络的结构如图6.4所示。</p>
<p><img src="Image00193.jpg" alt></p>
<p>图6.4 三层的神经网络结构</p>
<p>在神经网络中，一个神经元的输出是另一个神经元的输入，+1项表示的是偏置项。在图6.4中，三层的神经网络结构含有一个隐含层的神经网络模型，其中L 1 层为输入层，L 2 层为隐含层，L 3 层为输出层。</p>
<h4 id="6-2-2-神经网络中的参数说明"><a href="#6-2-2-神经网络中的参数说明" class="headerlink" title="6.2.2 神经网络中的参数说明"></a>6.2.2 神经网络中的参数说明</h4><p>在神经网络中，主要有如下的一些参数：</p>
<p>• 网络的层数n l 。在图6.4 所示的神经网络的层数n l =3，将第 l 层记为L l ，则对于上述的神经网络，输入层为L 1 ，输出层为L 3 。</p>
<p>• 网络权重和偏置（W，b）=（W （1） ，b （1） ，W （2） ，b （2） ），其中<img src="Image00478.jpg" alt> 表示的是第l层的第 j个神经元和第l+1层的第i个神经元之间的连接参数，<img src="Image00242.jpg" alt> 标识的是第l+1层的第i个神经元的偏置项。在图6.4所示的神经网络中，W  （1） ∈R 3×3 ，W  （2） ∈R 1×3 。</p>
<h4 id="6-2-3-神经网络的计算"><a href="#6-2-3-神经网络的计算" class="headerlink" title="6.2.3 神经网络的计算"></a>6.2.3 神经网络的计算</h4><p>在神经网络中，一个神经元的输出是另一个神经元的输入。假设<img src="Image00652.jpg" alt> 表示的是第l层的第i个神经元的输入，假设<img src="Image00654.jpg" alt> 表示的是第l层的第i个神经元的输出，其中，当l=1时，<img src="Image00890.jpg" alt> 。根据上述的神经网络中的权重和偏置，就可以计算神经网络中每一个神经元的输出，从而计算出神经网络的最终的输出h W，b 。</p>
<p>对于上述的神经网络结构，有下述的计算：</p>
<p><img src="Image00091.jpg" alt></p>
<p>从而，上述神经网络结构的最终输出结果为：</p>
<p><img src="Image00209.jpg" alt></p>
<p>上述的步骤称为前向传播，指的是信号从输入层，经过每一个神经元，直到输出神经元的传播过程。</p>
<h3 id="6-3-神经网络中参数的求解"><a href="#6-3-神经网络中参数的求解" class="headerlink" title="6.3 神经网络中参数的求解"></a>6.3 神经网络中参数的求解</h3><h4 id="6-3-1-神经网络损失函数"><a href="#6-3-1-神经网络损失函数" class="headerlink" title="6.3.1 神经网络损失函数"></a>6.3.1 神经网络损失函数</h4><p>对于上述神经网络模型，假设有m个训练样本{（X  （1） ，y （1） ），…，（X  （m） ，y （m）<br>）}，对于一个训练样本（X，y），其损失函数为：</p>
<p><img src="Image00317.jpg" alt></p>
<p>为了防止模型的过拟合，在损失函数中会加入正则项，即：</p>
<p><img src="Image00423.jpg" alt></p>
<p>其中，loss表示的是损失函数，R表示的是正则项。则对于上述的含有m个样本的训练集，其损失函数为：</p>
<p><img src="Image00521.jpg" alt></p>
<p>通常，偏置项并不放在正则化中，因为在正则化中放入偏置项只会对神经网络产生很小的影响。</p>
<h4 id="6-3-2-损失函数的求解"><a href="#6-3-2-损失函数的求解" class="headerlink" title="6.3.2 损失函数的求解"></a>6.3.2 损失函数的求解</h4><p>我们的目标是求得参数W 和参数b以使得损失函数J （W，b）达到最小值。首先需要对参数进行随机初始化，即将参数初始化为一个很小的接近0的随机值。</p>
<p>参数的初始化有很多不同的策略，基本的是要在0附近的很小的邻域内取得随机值。</p>
<p>在随机初始化参数后，利用前向传播得到预测值h W，b ，进而可以得到损失函数，此时需要利用损失函数对其参数进行调整，可以使用梯度下降的方法，梯度下降对参数的调整如下：</p>
<p><img src="Image00625.jpg" alt></p>
<p>其中， _α_ 称为学习率，在计算参数的更新公式中，需要使用到反向传播算法。而<img src="Image00731.jpg" alt> 的具体形式如下：</p>
<p><img src="Image00248.jpg" alt></p>
<p>反向传播算法的思路如下：对于给定的训练数据（X，y），通过前向传播算法计算出每一个神经元的输出值，当所有神经元的输出都计算完成后，对每一个神经元计算其“残差”，如第l层的神经元i的残差可以表示为<img src="Image00352.jpg" alt> 。该残差表示的是该神经元对最终的残差产生的影响。这里主要分为两种情况：一是神经元为输出神经元，二是神经元为非输出神经元。这里假设<img src="Image00461.jpg" alt> 表示第l层上的第i个神经元的输入加权和，假设<img src="Image00549.jpg" alt> 表示的是第l层上的第i个神经元的输出，即<img src="Image00058.jpg" alt> 。</p>
<p>• 对于输出层 nl 上的神经元i，其残差为：</p>
<p><img src="Image00757.jpg" alt></p>
<p>• 对于非输出层，即对于l=n l-1 ，n l-2 ，…，2各层，第l层的残差的计算方法如下（以第n l-1 层为例）：</p>
<p><img src="Image00550.jpg" alt></p>
<p>因此有：</p>
<p><img src="Image00067.jpg" alt></p>
<p>对于神经网络中的权重和偏置的更新公式为：</p>
<p><img src="Image00184.jpg" alt></p>
<h4 id="6-3-3-BP神经网络的学习过程"><a href="#6-3-3-BP神经网络的学习过程" class="headerlink" title="6.3.3 BP神经网络的学习过程"></a>6.3.3 BP神经网络的学习过程</h4><p>对于神经网络的学习过程，大致分为如下的几步：</p>
<p>• 初始化参数，包括权重、偏置、网络层结构、激活函数等</p>
<p>• 循环计算</p>
<p>• 正向传播，计算误差</p>
<p>• 反向传播，调整参数</p>
<p>• 返回最终的神经网络模型</p>
<p>现在，让我们一起利用Python实现上述的BP神经网络的更新过程，首先，我们需要导入在训练过程中需要用到函数：</p>
<p><img src="Image00296.jpg" alt></p>
<p>BP神经网络模型的训练过程如程序清单6-1所示。</p>
<p><strong>程序清单6-1 BP神经网络模型的训练</strong></p>
<p><img src="Image00406.jpg" alt></p>
<p><img src="Image00505.jpg" alt></p>
<p>在程序清单6-1中，函数bp_train实现了对BP神经网络的训练，其输入为训练数据的特征feature，训练数据的标签label，隐含层节点个数n_hidden，最大的迭代次数maxCycle，梯度下降过程中的学习率alpha和最终的输出节点个数n_output。输出为BP神经网络的模型，包括输入层到隐含层的权重w 0 和偏置b 0 ，隐含层到输出层的权重w 1 和偏置b 1 。在模型训练之前，首先是对输入层到隐含层的权重w 0 和偏置b 0 ，隐含层到输出层的权重w 1 和偏置b 1 进行初始化，初始化的过程如程序代码中的①、②、③和④所示，从指定的区间中生成随机数，程序代码中使用的区间为：</p>
<p><img src="Image00116.jpg" alt></p>
<p>其中，fan in 为i-1层节点的个数，fan out 为第i层节点的个数。在对BP神经网络初始化完成后，利用训练数据对BP神经网络模型进行训练，训练的过程包括以下几个方面：①信号的正向传播，如程序代码中的⑤所示；②误差的反向传播，如程序代码中的⑥所示；③利用反向传播的误差修正BP神经网络模型中的参数，如程序代码中的⑦所示；④在每100代后计算当前的损失函数的值，如程序代码中的⑧所示。</p>
<p>在信号的正向传播过程中，对于BP神经网络，主要分为：①计算隐含层的输入，如程序代码中的hidden_in函数，hidden_in函数的具体实现如程序清单6-2所示；②计算隐含层的输出，如程序代码中的hidden_out函数，hidden_out函数的具体实现如程序清单6-3所示；③计算输出层的输入，如程序代码中的predict_in函数，predict_in函数的具体实现如程序清单 6-4 所示；④计算输出层的输出，如程序代码中的predict_out函数，predict_out函数的具体实现如程序清单6-5所示。</p>
<p><strong>程序清单6-2 计算隐含层的输入的hidden_in函数</strong></p>
<p><img src="Image00651.jpg" alt></p>
<p>在程序清单6-2中，函数hidden_in对隐含层的输入进行计算。在函数hidden_in中，其输入为训练数据的特征 feature，输入层到隐含层的权重 w 0 和输入层到隐含层的偏置b 0 ，其输出为隐含层的输入hidden_in。计算的方法如上所述。</p>
<p><strong>程序清单6-3 计算隐含层的输出的hidden_out函数</strong></p>
<p><img src="Image00286.jpg" alt></p>
<p><img src="Image00301.jpg" alt></p>
<p>在程序清单6-3中，函数hidden_out对隐含层的输出进行计算。在函数hidden_out中，其输入为隐含层的输入hidden_in，其输出为隐含层的输出hidden_output。计算的方法是对隐含层的输入 hidden_in 中的每一个值计算其 Sigmoid 值，如程序代码中的①所示，sig函数的具体实现如程序清单6-6所示。</p>
<p><strong>程序清单6-4 计算输出层的输入的predict_in函数</strong></p>
<p><img src="Image00120.jpg" alt></p>
<p>在程序清单6-4中，函数predict_in对输出层的输入进行计算。在函数predict_in中，其输入为隐含层的输出hidden_out，隐含层到输出层的权重w 1 和隐含层到输出层的偏置b 1 ，其输出为隐含层的输入predict_in。</p>
<p><strong>程序清单6-5 计算输出层的输出的predict_out函数</strong></p>
<p><img src="Image00243.jpg" alt></p>
<p>在程序清单6-5中，函数predict_out对输出层的输出进行计算。在函数predict_out中，其输入为输出层的输入predict_in，其输出为输出层的输出result。计算的方法是对输出层的输入predict_in中的每一个值计算其Sigmoid值。</p>
<p><strong>程序清单6-6 求Sigmoid值的sig函数</strong></p>
<p><img src="Image00347.jpg" alt></p>
<p>在程序清单6-6中，sig函数实现了对数值或者矩阵的Sigmoid值的计算。</p>
<p>在误差的反向传播的过程中，对于BP神经网络，主要分为：①计算隐含层到输出层之间的残差；②计算输入层到隐含层之间的残差。在残差的计算过程中使用到了partial_sig函数，partial_sig函数的具体实现如程序清单6-7所示。</p>
<p><strong>程序清单6-7 partial_sig函数</strong></p>
<p><img src="Image00456.jpg" alt></p>
<p>在程序清单6-7中，函数partial_sig计算输入Sigmoid函数在输入为x时的导函数的值，具体的计算方法如程序代码中的①所示。假设Sigmoid<br>（x）= _σ_ （x），则其导函数为：</p>
<p><img src="Image00544.jpg" alt></p>
<p>当BP神经网络中的权重更新完成后，每100次迭代后，需要计算当前的损失函数的值，get_cost函数用于计算当前的损失函数的值，get_cost函数的具体实现如程序清单6-8所示。</p>
<p><strong>程序清单6-8 get_cost函数</strong></p>
<p><img src="Image00827.jpg" alt></p>
<p><img src="Image00753.jpg" alt></p>
<p>在程序清单6-8中，get_cost函数的输入为利用当前的BP神经网络模型得到的预测值与样本标签之间的差值cost，输出为当前的损失函数的值。</p>
<h3 id="6-4-BP神经网络中参数的设置"><a href="#6-4-BP神经网络中参数的设置" class="headerlink" title="6.4 BP神经网络中参数的设置"></a>6.4 BP神经网络中参数的设置</h3><p>在BP神经网络中存在很多的参数，有些参数的选择是不能通过梯度下降法得到的，这些参数称为超参数。一般无法得到超参数的最优解。首先，我们不能单独优化每一个超参数。其次，我们不能直接使用梯度下降法，因为有些超参数是离散的，有些超参数是连续的。最后，这是非凸优化问题，找到一个局部最优解需要花费很大的功夫。在多年的研究中，研究者们已经设计出大量的经验法则用于在一个神经网络中选择超参数。除了超参数的选择外，在BP神经网络中，非线性变换的选择也同样重要，不同的非线性变换具有不同的性质。</p>
<h4 id="6-4-1-非线性变换"><a href="#6-4-1-非线性变换" class="headerlink" title="6.4.1 非线性变换"></a>6.4.1 非线性变换</h4><p>两个最常见的非线性函数是sigmoid函数和tanh函数。其中sigmoid函数的输出均值不为0，这会导致后一层的神经元得到上一层输出的非0均值的信号作为输入。与sigmoid函数不一样的是，tanh函数的输出均值为0，因此，tanh函数通常具有更好的收敛性。</p>
<p>在本文的实验中，我们依旧选择sigmoid函数作为激活函数。</p>
<h4 id="6-4-2-权重向量的初始化"><a href="#6-4-2-权重向量的初始化" class="headerlink" title="6.4.2 权重向量的初始化"></a>6.4.2 权重向量的初始化</h4><p>在初始化阶段，权重应该设置在原点的附近，而且应尽可能的小，这样，激活函数对其进行操作就像是线性函数，此处的梯度也是最大的。</p>
<p>对于tanh激活函数，在区间：</p>
<p><img src="Image00858.jpg" alt></p>
<p>上以均匀分布的方式产生随机数。而对于sigmoid激活函数，则是在区间：</p>
<p><img src="Image00063.jpg" alt></p>
<p>上以均匀分布的方式产生随机数。其中，fan in 是i-1层节点的个数，而 fan out 是i层节点的个数。</p>
<h4 id="6-4-3-学习率"><a href="#6-4-3-学习率" class="headerlink" title="6.4.3 学习率"></a>6.4.3 学习率</h4><p>对于学习率的选择，最简单的办法是选择一个固定的学习率，即常数，如10 -2 ，10 -3 ，……除了设置固定的学习率外，同样可以设置动态的学习率，如随着迭代的代数t动态变化的学习率：</p>
<p><img src="Image00178.jpg" alt></p>
<p>其中， _α_ 是初始的学习率，t是迭代的次数。</p>
<h4 id="6-4-4-隐含层节点的个数"><a href="#6-4-4-隐含层节点的个数" class="headerlink" title="6.4.4 隐含层节点的个数"></a>6.4.4 隐含层节点的个数</h4><p>隐含层节点个数的选择取决于具体的数据集，对于越复杂的数据分布，神经网络需要越强的能力去对这批数据建模，因此，需要越多的隐含层节点个数。</p>
<h3 id="6-5-BP神经网络算法实践"><a href="#6-5-BP神经网络算法实践" class="headerlink" title="6.5 BP神经网络算法实践"></a>6.5 BP神经网络算法实践</h3><p>有了以上的理论储备，我们利用上述实现好的函数，构建BP神经网络分类器。在训练分类器的过程中，我们使用如图6.5所示的非线性可分的数据集作为训练数据集：</p>
<p><img src="Image00290.jpg" alt></p>
<p>图6.5 非线性可分数据集</p>
<p>在利用BP神经网络算法对其进行分类的过程中，主要有两个部分：①利用训练数据对模型进行训练；②对新的数据进行预测。</p>
<h4 id="6-5-1-训练BP神经网络模型"><a href="#6-5-1-训练BP神经网络模型" class="headerlink" title="6.5.1 训练BP神经网络模型"></a>6.5.1 训练BP神经网络模型</h4><p>首先，我们利用训练样本训练模型，为了使得Python能够支持中文的注释和利用numpy，我们需要在训练文件“bp_train.py”的开始加入：</p>
<p><img src="Image00401.jpg" alt></p>
<p>同时，在训练BP神经网络模型的过程中，需要用到sqrt函数，因此，我们需要在文件“bp_train.py”中加入：</p>
<p><img src="Image00499.jpg" alt></p>
<p>BP神经网络模型的训练的主函数如程序清单6-9所示。</p>
<p><strong>程序清单6-9 BP神经网络模型的训练的主函数</strong></p>
<p><img src="Image00594.jpg" alt></p>
<p><img src="Image00708.jpg" alt></p>
<p>在程序清单6-9中，训练BP神经网络模型主要包括：①导入训练数据，如程序代码中的①所示，导入训练数据的load_data函数的具体实现如程序清单6-10所示；②利用训练数据对 BP神经网络模型进行训练，如程序代码中的②所示，训练 BP神经网络的bp_train函数如程序清单6-1所示；③在训练完成后，保存训练好的BP神经网络模型，如程序代码中的③所示，函数 save_model 的具体实现如程序清单 6-11所示；④计算训练好的模型在训练数据上的准确性，此时需要先利用训练好的BP神经网络模型对训练数据进行预测，如程序代码中的④所示，再计算预测结果与真实结果之间的差异，得到模型在训练数据集上的准确性，如程序代码中的⑤所示，预测函数get_predict的具体实现如程序清单6-12所示，计算错误率的函数err_rate如程序清单6-13所示。</p>
<p><strong>程序清单6-10 导入训练数据的load_data函数</strong></p>
<p><img src="Image00797.jpg" alt></p>
<p><img src="Image00121.jpg" alt></p>
<p>在程序清单6-10中，函数load_data将训练数据分成特征和标签分别导入到特征数组feature_data和标签数组label_data中，在获取标签的过程中，需要计算训练数据中类别的个数，如程序代码中的①所示，对于标签，如二分类的输入标签为{0，1}，在转换的过程中，需要将0转换成[1，0]，将1转换成[0，1]，如程序代码中的②所示。</p>
<p><strong>程序清单6-11 保存BP神经网络模型的save_model函数</strong></p>
<p><img src="Image00125.jpg" alt></p>
<p>在程序清单6-11中，函数save_model将训练好的BP神经网络模型保存对应的文件中，在三层的网络结构中，需要保存的参数包括输入层到隐含层之间的权重w 0 ，输入层到隐含层之间的偏置b0，隐含层到输出层之间的权重w 1 和隐含层到输出层之间的偏置b 1 。在save_model函数中定义了write_file函数，用于将source中的值写入到file_name对应的文件中，保存w 0 的过程如程序代码中的①所示，保存w 1 的过程如程序代码中的②所示，保存 b 0 的过程如程序代码中的③所示，保存 b 1 的过程如程序代码中的④所示。</p>
<p><strong>程序清单6-12 对样本进行预测的get_predict函数</strong></p>
<p><img src="Image00128.jpg" alt></p>
<p>在程序清单6-12中，get_predict函数对训练数据进行预测，get_predict函数的输入为训练数据的特征和BP神经网络模型的参数。计算的方法与程序清单6-1中的信息的正向传播一致，具体过程如程序清单中的①所示。</p>
<p><strong>程序清单6-13 计算错误率的err_rate函数</strong></p>
<p><img src="Image00131.jpg" alt></p>
<p>在程序清单6-13中，函数err_rate将训练的结果pre与样本中的标签label进行对比，最终计算出错误率。</p>
<h4 id="6-5-2-最终的训练效果"><a href="#6-5-2-最终的训练效果" class="headerlink" title="6.5.2 最终的训练效果"></a>6.5.2 最终的训练效果</h4><p>BP神经网络的训练过程为：</p>
<p><img src="Image00135.jpg" alt></p>
<p>最终在训练数据上的准确率为0.99，为了能够清晰看到分隔超平面，我们在区间[-4.5，4.5]上随机生成20000个样本，生成样本点的具体过程如代码清单6-14所示，最终的分隔超平面如图6.6所示。</p>
<p><img src="Image00141.jpg" alt></p>
<p>图6.6 最终的分隔超平面</p>
<p><strong>程序清单6-14 随机生成20000个样本的generate_data函数</strong></p>
<p><img src="Image00145.jpg" alt></p>
<p>在程序清单6-14中，函数generate_data在区间[-4.5，4.5]上生成了20000个样本，并将这些样本保存到文件test_data中。</p>
<h4 id="6-5-3-对新数据的预测"><a href="#6-5-3-对新数据的预测" class="headerlink" title="6.5.3 对新数据的预测"></a>6.5.3 对新数据的预测</h4><p>在训练完 BP神经网络后，需要利用训练好的 BP神经网络模型对新的数据进行预测。利用上述步骤，我们训练好 BP 神经网络模型，并将其保存在“weight_w0”、“weight_w1”、“weight_b0”和“weight_b1”文件中，此时，我们需要利用训练好的BP神经网络模型对新数据进行预测，同样，为了能够使用numpy中的函数和对中文注释的支持，在文件“bp_test.py”开始，我们加入：</p>
<p><img src="Image00150.jpg" alt></p>
<p>同时，为了使用文件“bp_train.py”，需要在文件“bp_test.py”中加入：</p>
<p><img src="Image00155.jpg" alt></p>
<p>如上述generate_data函数生成的20000个样本，对新数据进行预测的主函数如程序清单6-15所示。</p>
<p><strong>程序清单6-15 对新数据的预测的主函数</strong></p>
<p><img src="Image00158.jpg" alt></p>
<p>在程序清单6-15中，利用训练好的BP神经网络模型对新数据进行预测，主要的步骤为：①导入新的数据集，如程序代码中的所示，导入新数据集的 load_data 函数如程序清单6-16所示；②导入训练好的BP神经网络模型，即导入BP神经网络中的四个参数，如程序代码中的②所示，导入BP神经网络模型的load_model函数如程序清单6-17所示；③在BP神经网络模型和测试数据都导入后，利用BP神经网络对这些数据进行预测，如程序代码中的③所示，函数get_predict的具体实现如程序清单6-12所示；④最终，将预测的结果保存到指定的文件中，如程序代码中的④所示，保存预测结果的save_predict函数的具体实现如程序清单6-18所示。</p>
<p><strong>程序清单6-16 导入测试数据的load_data函数</strong></p>
<p><img src="Image00161.jpg" alt></p>
<p><img src="Image00736.jpg" alt></p>
<p>在程序清单6-16中，在对新数据的预测中，要使用到bp_train文件中的get_predict函数，因此，首先需要从bp_train文件中导入get_predict函数。在函数load_data中，需要将file_name指定的文件中的测试数据导入到数组feature_data中。</p>
<p><strong>程序清单6-17 导入BP神经网络模型的load_model函数</strong></p>
<p><img src="Image00169.jpg" alt></p>
<p>在程序清单6-17中，load_model函数需要将训练好的BP神经网络模型导入，BP神经网络模型分别保存在4个文件中，即file_w0、file_w1、file_b0、file_b1。需要分别导入4个文件，如程序代码中的①、②、③和④所示。</p>
<p><strong>程序清单6-18 保存最终预测结果的save_predict函数</strong></p>
<p><img src="Image00172.jpg" alt></p>
<p><img src="Image00177.jpg" alt></p>
<p>在程序清单6-18中，函数save_predict将预测的结果pre保存到file_name对应的文件中。</p>
<p><strong>参考文献</strong></p>
<p>[1] 周志华.机器学习[M].北京：清华大学出版社.2016.</p>
<p>[2] 邱希鹏.《神经网络与深度学习》讲义[DB/OL].http：//nlp.fudan.edu.cn/dl-book/</p>
<p>[3] 仙道菜.神经网络之激活函数（Activation Function）[DB/OL].http：//blog.csdn.net/cyh_24/article/details/5 0593400</p>
<p>[4] Mhaskar H N,Micchelli C A.How to Choose an Activation Function.[C]//Advances in Neural Information Processing Systems.1993:319-326.</p>
<h1 id="第二部分-回归算法"><a href="#第二部分-回归算法" class="headerlink" title="第二部分 回归算法"></a>第二部分 回归算法</h1><p>回归算法与分类算法都属于监督学习算法，不同的是，在分类算法中标签是一些离散的值，代表着不同的类别，而在回归算法中，标签是一些连续的值，回归算法需要训练得到样本特征到这些连续标签之间的映射。</p>
<p>第7章介绍最基本的线性回归算法。线性回归算法是很多算法的基础，然而，基本的线性回归算法对处理复杂的数据表现出很多的不足，因此利用局部信息的局部加权线性回归算法被提出。在基本线性回归中，使用了牛顿法对其进行训练。如果在训练数据中，样本之间存在很高的相关性，利用基本线性回归算法很难得到泛化能力较高的模型，因此在第8章中介绍基于L 2 正则的岭回归（Ridge Regression）算法和基于L 1 正则的Lasso算法。在岭回归的训练中，使用了拟牛顿法L- BFGS算法对其进行训练。线性回归算法是一种全局的回归算法，对于局部的拟合效果并不好，第9章中介绍CART树回归算法，CART树回归算法能有效利用局部信息对数据进行拟合。</p>
<h2 id="7-线性回归"><a href="#7-线性回归" class="headerlink" title="7 线性回归"></a>7 线性回归</h2><p>回归（Regression）是另一类重要的监督学习算法。与分类问题不同的是，在回归问题中，其目标是通过对训练样本的学习，得到从样本特征到样本标签之间的映射，其中，在回归问题中，样本标签是连续值。典型的回归问题有：①根据人的身高、性别和体重等信息预测其鞋子的大小；②根据房屋的面积、卧室的数量预估房屋的价格；③根据博文的历史阅读数量预测该用户的博文阅读数等。</p>
<p>线性回归（Linear Regression）是一类重要的回归问题。在线性回归中，目标值与特征之间存在线性相关的关系。</p>
<h3 id="7-1-基本线性回归"><a href="#7-1-基本线性回归" class="headerlink" title="7.1 基本线性回归"></a>7.1 基本线性回归</h3><h4 id="7-1-1-线性回归的模型"><a href="#7-1-1-线性回归的模型" class="headerlink" title="7.1.1 线性回归的模型"></a>7.1.1 线性回归的模型</h4><p>对于线性回归算法，我们希望从训练数据中学习到线性回归方程，即</p>
<p><img src="Image00181.jpg" alt></p>
<p>其中，b称为偏置，w i 为回归系数。对于上式，令x 0 =1，则上式可以表示为</p>
<p><img src="Image00186.jpg" alt></p>
<p>假设小麦的产量y与施肥量x之间的关系如表7-1所示。</p>
<p>表7-1 小麦产量y与施肥量x之间的关系</p>
<p><img src="Image00190.jpg" alt></p>
<p>对于回归方程，需要从数据中学习到相应的回归系数w i ，表7-1中小麦产量y与施肥量x之间的关系如图7.1所示。</p>
<p><img src="Image00195.jpg" alt></p>
<p>图7.1 产量与施肥量之间的关系</p>
<h4 id="7-1-2-线性回归模型的损失函数"><a href="#7-1-2-线性回归模型的损失函数" class="headerlink" title="7.1.2 线性回归模型的损失函数"></a>7.1.2 线性回归模型的损失函数</h4><p>在线性回归模型中，其目标是求出线性回归方程，即求出线性回归方程中的回归系数w i 。线性回归的评价是指如何度量预测值（Prediction）与标签（Label）之间的接近程序，线性回归模型的损失函数可以是绝对损失（Absolute Loss）或者平方损失（Squared Loss）。其中，绝对损失函数为：</p>
<p><img src="Image00200.jpg" alt></p>
<p>其中，<img src="Image00204.jpg" alt> 为预测值，且<img src="Image00208.jpg" alt> 。</p>
<p>平方损失函数为：</p>
<p><img src="Image00213.jpg" alt></p>
<p>由于平方损失处处可导，通常使用平方误差作为线性回归模型的损失函数。假设有m个训练样本，每个样本中有n-1个特征，则平方误差可以表示为：</p>
<p><img src="Image00218.jpg" alt></p>
<p>对于如上的损失函数，线性回归的求解是希望求得平方误差的最小值。</p>
<h3 id="7-2-线性回归的最小二乘解法"><a href="#7-2-线性回归的最小二乘解法" class="headerlink" title="7.2 线性回归的最小二乘解法"></a>7.2 线性回归的最小二乘解法</h3><h4 id="7-2-1-线性回归的最小二乘解法"><a href="#7-2-1-线性回归的最小二乘解法" class="headerlink" title="7.2.1 线性回归的最小二乘解法"></a>7.2.1 线性回归的最小二乘解法</h4><p>对于线性回归模型，假设训练集中有m个训练样本，每个训练样本中有n-1个特征，可以使用矩阵的表示方法，预测函数可以表示为：</p>
<p><img src="Image00222.jpg" alt></p>
<p>其损失函数可以表示为</p>
<p><img src="Image00226.jpg" alt></p>
<p>其中，标签Y为m×1的矩阵，训练特征X为m×n的矩阵，回归系数W 为n×1的矩阵。在最小二乘法中，对W 求导，即</p>
<p><img src="Image00228.jpg" alt></p>
<p>令其为0，得到</p>
<p><img src="Image00233.jpg" alt></p>
<p>现在让我们一起利用Python实现最小二乘的解法，在最小二乘法的求解过程中，需要用到矩阵的计算，因此，我们需要导入Python的矩阵计算模块：</p>
<p><img src="Image00236.jpg" alt></p>
<p>最小二乘的具体实现如程序清单7-1所示。</p>
<p><strong>程序清单7-1 最小二乘求解</strong></p>
<p><img src="Image00240.jpg" alt></p>
<p><img src="Image00244.jpg" alt></p>
<p>在程序清单7-1中，函数least_square实现了线性回归模型的最小二乘解法，函数的输入是训练数据的特征和标签，其输出是线性回归模型的回归系数。具体的回归系数的求解如程序清单中的①所示，其与上述的公式一致。</p>
<h4 id="7-2-2-广义逆的概念"><a href="#7-2-2-广义逆的概念" class="headerlink" title="7.2.2 广义逆的概念"></a>7.2.2 广义逆的概念</h4><p>对于线性回归的模型，其预测函数的矩阵表示为</p>
<p><img src="Image00118.jpg" alt></p>
<p>若矩阵X是一个方阵，且矩阵X的行列式<img src="Image00123.jpg" alt> ，则矩阵X的逆X -1 存在，即对于满秩矩阵 X，其逆矩阵存在。如果矩阵 X 不是方阵，可以求矩阵 X 的Moore-Penrose广义逆X † 。Moore-Penrose广义逆具有很好的性质，如Moore- Penrose广义逆存在而且唯一，则回归系数可以表示为</p>
<p><img src="Image00126.jpg" alt></p>
<h3 id="7-3-牛顿法"><a href="#7-3-牛顿法" class="headerlink" title="7.3 牛顿法"></a>7.3 牛顿法</h3><p>除了前面说的梯度下降法，牛顿法也是机器学习中用的比较多的一种优化算法。牛顿法的基本思想是利用迭代点x k 处的一阶导数（梯度）和二阶导数（Hessen矩阵）对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值。牛顿法下降的速度比梯度下降的快，而且能高度逼近最优值。牛顿法分为基本的牛顿法和全局牛顿法。</p>
<h4 id="7-3-1-基本牛顿法的原理"><a href="#7-3-1-基本牛顿法的原理" class="headerlink" title="7.3.1 基本牛顿法的原理"></a>7.3.1 基本牛顿法的原理</h4><p>基本牛顿法是一种基于导数的算法，它每一步的迭代方向都是沿着当前点函数值下降的方向。对于一维的情形，对于一个需要求解的优化函数 f<br>（x），求函数的极值的问题可以转化为求导函数 f′（x）=0。对函数 f （x）进行泰勒展开到二阶，得到</p>
<p><img src="Image00590.jpg" alt></p>
<p>对上式求导并令其为0，则为</p>
<p><img src="Image00133.jpg" alt></p>
<p>即得到</p>
<p><img src="Image00139.jpg" alt></p>
<p>这就是牛顿法的更新公式。</p>
<h4 id="7-3-2-基本牛顿法的流程"><a href="#7-3-2-基本牛顿法的流程" class="headerlink" title="7.3.2 基本牛顿法的流程"></a>7.3.2 基本牛顿法的流程</h4><p>1.给定终止误差值0≤ _ε_ ≪1，初始点x 0 ∈R  n ，令k=0；</p>
<p>2.计算g  k =∇f （x k ），若<img src="Image00142.jpg" alt> ，则停止，输出x ≈x k ；</p>
<p>3.计算G k =∇ 2 f （x k ），并求解线性方程组G k d=-g  k 得解d  k ；</p>
<p>4.令x k+1 =x k +d  k ，k=k+1，并转2。</p>
<h4 id="7-3-3-全局牛顿法"><a href="#7-3-3-全局牛顿法" class="headerlink" title="7.3.3 全局牛顿法"></a>7.3.3 全局牛顿法</h4><p>牛顿法最突出的优点是收敛速度快，具有局部二阶收敛性，但是，基本牛顿法初始点需要足够“靠近”极小点，否则，有可能导致算法不收敛，此时就引入了全局牛顿法。全局牛顿法的流程为：</p>
<p>1.给定终止误差值0≤ _ε_ ≪1， _δ_ ∈（0，1）， _σ_ ∈（0，0.5），初始点x 0 ∈R n ，令k=0；</p>
<p>2.计算g  k =∇f （x k ），若<img src="Image00148.jpg" alt> ，则停止，输出x ∗ ≈x k ；</p>
<p>3.计算G k =∇ 2 f （x k ），并求解线性方程组G k d=-g  k 得解d  k ；</p>
<p>4.设m k 是不满足下列不等式的最小非负整数m：</p>
<p><img src="Image00153.jpg" alt></p>
<p>5.令 _α k _ = _δ m k _ ，x k+1 =x k + _α k _ d  k ，k=k+1，并转2。</p>
<p>全局牛顿法的具体实现如程序清单7-2所示。</p>
<p><strong>程序清单7-2 全局牛顿法</strong></p>
<p><img src="Image00156.jpg" alt></p>
<p>在程序清单7-2中，函数newton利用全局牛顿法对线性回归模型中的参数进行学习，函数newton的输入为训练特征feature、训练的目标值label、全局牛顿法的最大迭代次数iterMax以及全局牛顿法的两个参数sigma和delta。函数newton的输出是线性回归模型的参数 w。在函数 newton 中需要计算损失函数的一阶导数，如程序代码中的①所示，计算损失函数的二阶导数，如程序代码中的②所示，同时需要计算最小的m值，如程序代码中的③所示，最终根据上述的值更新权重，如程序代码中的④所示。求最小m值的函数 get_min_m 如程序清单 7-3 所示。求一阶导数的函数first_derivativ 的具体实现如程序清单 7-5 所示。求二阶导数的函数 second_derivative如程序清单7-6所示。</p>
<p><strong>程序清单7-3 最小m值的计算</strong></p>
<p><img src="Image00159.jpg" alt></p>
<p><img src="Image00162.jpg" alt></p>
<p>程序清单7-3中实现了全局牛顿法中最小m值的确定，在函数get_min_m中，其输入为训练数据的特征 feature，训练数据的目标值 label，全局牛顿法的参数 sigma、delta、d以及损失函数的一阶导数值g。其输出是最小的m值m。在计算的过程中，计算损失函数值时使用到了get_error函数，其具体实现如程序清单7-4所示。</p>
<p><strong>程序清单7-4 损失函数的计算</strong></p>
<p><img src="Image00343.jpg" alt></p>
<p>程序清单7-4中的get_error函数实现的是对于不同的线性回归的模型的损失函数值。函数get_error的输入为训练数据的特征feature，训练数据的目标值label和线性回归模型的参数，其输出为损失函数值。</p>
<h4 id="7-3-4-Armijo搜索"><a href="#7-3-4-Armijo搜索" class="headerlink" title="7.3.4 Armijo搜索"></a>7.3.4 Armijo搜索</h4><p>全局牛顿法是基于Armijo的搜索，满足Armijo准则：</p>
<p>给定 _β_ ∈（0，1）， _σ_ ∈（0，0.5），令步长因子 _α k _ = _β m k _ ，其中m k 是满足下列不等式的最小非负整数：</p>
<p><img src="Image00170.jpg" alt></p>
<h4 id="7-3-5-利用全局牛顿法求解线性回归模型"><a href="#7-3-5-利用全局牛顿法求解线性回归模型" class="headerlink" title="7.3.5 利用全局牛顿法求解线性回归模型"></a>7.3.5 利用全局牛顿法求解线性回归模型</h4><p>假设有m个训练样本，其中，每个样本有n-1个特征，则线性回归模型的损失函数为：</p>
<p><img src="Image00174.jpg" alt></p>
<p>若是利用全局牛顿法求解线性回归模型，需要计算线性回归模型损失函数的一阶导数和二阶导数，其一阶导数为：</p>
<p><img src="Image00179.jpg" alt></p>
<p>其实现如程序清单7-5所示。</p>
<p><strong>程序清单7-5 一阶导数</strong></p>
<p><img src="Image00183.jpg" alt></p>
<p>程序清单 7-5 中的 first_derivativ 实现了损失函数一阶导数值的求解。在函数first_derivativ 中，其输入为训练数据的特征 feature 和训练数据的目标值 label，其输出为损失函数的一阶导数g，其中g是一个n×1的向量。</p>
<p>损失函数的二阶导数为</p>
<p><img src="Image00187.jpg" alt></p>
<p>其具体实现如程序清单7-6所示。</p>
<p><strong>程序清单7-6 二阶导数</strong></p>
<p><img src="Image00192.jpg" alt></p>
<p>程序清单7-6中的second_derivative函数实现了损失函数二阶导数值的计算。在函数second_derivative中，其输入为训练数据的特征feature，输出为损失函数的二阶导数G，其中G是一个n×n的矩阵。</p>
<h3 id="7-4-利用线性回归进行预测"><a href="#7-4-利用线性回归进行预测" class="headerlink" title="7.4 利用线性回归进行预测"></a>7.4 利用线性回归进行预测</h3><p>有了以上的理论准备，我们利用上述实现好的函数，构建线性回归模型。在训练线性回归模型的过程中，我们使用如图7.2所示的数据集作为训练数据集。</p>
<p><img src="Image00198.jpg" alt></p>
<p>图7.2 原始数据</p>
<p>在求解模型的过程中，我们分别利用最小二乘法和全局牛顿法对其回归系数进行求解，求解的过程分为：①训练线性回归模型；②利用训练好的线性回归模型预测新的数据。</p>
<h4 id="7-4-1-训练线性回归模型"><a href="#7-4-1-训练线性回归模型" class="headerlink" title="7.4.1 训练线性回归模型"></a>7.4.1 训练线性回归模型</h4><p>首先，我们利用训练样本训练模型，为了使得Python能够支持中文的注释和利用numpy，我们需要在“linear_regression_train.py”文件的开始加入：</p>
<p><img src="Image00202.jpg" alt></p>
<p>同时，在计算最小 m 值的过程中，需要使用 pow 函数，因此在“linear_regression_train.py”文件中加入：</p>
<p><img src="Image00206.jpg" alt></p>
<p>线性回归模型的训练的主函数如程序清单7-7所示。</p>
<p><strong>程序清单7-7 线性回归模型训练的主函数</strong></p>
<p><img src="Image00210.jpg" alt></p>
<p>程序清单7-7是线性回归模型训练的主函数，在线性回归模型的训练过程中，首先是导入训练数据，如程序代码中的①所示，函数 load_data 的具体实现如程序清单7-8 所示。导入完训练数据后，可以利用最小二乘法对其参数进行训练，如程序代码中的②所示，最小二乘法的具体实现如程序清单7-1所示，也可以利用全局牛顿法对其参数进行训练，如程序代码中的③所示，全局牛顿法的具体实现如程序清单7-2所示。训练完成后，将最终的线性回归的模型参数保存在文件“weights”中，如程序代码中的④所示，保存模型的save_model函数的具体实现如程序清单7-9所示。</p>
<p><strong>程序清单7-8 导入训练数据</strong></p>
<p><img src="Image00214.jpg" alt></p>
<p>在程序清单7-8中，load_data函数将训练数据集中的特征导入到矩阵feature中，将样本标签导入到矩阵label中。</p>
<p><strong>程序清单7-9 保存模型的save_model函数</strong></p>
<p><img src="Image00219.jpg" alt></p>
<p>在程序清单7-9中，函数save_model将训练好的线性回归模型w保存到file_name指定的文件中。</p>
<h4 id="7-4-2-最终的训练结果"><a href="#7-4-2-最终的训练结果" class="headerlink" title="7.4.2 最终的训练结果"></a>7.4.2 最终的训练结果</h4><p>若使用最小二乘法进行训练，则训练过程为：</p>
<p><img src="Image00223.jpg" alt></p>
<p>若使用全局牛顿法进行训练，则训练过程为：</p>
<p><img src="Image00227.jpg" alt></p>
<p>对于使用最小二乘法和全局牛顿法，线性回归模型最终得到了相同的参数值，最终的参数值为：</p>
<p><img src="Image00230.jpg" alt></p>
<p>最终的数据的拟合效果如图7.3所示。</p>
<p><img src="Image00234.jpg" alt></p>
<p>图7.3 最终的数据拟合效果</p>
<h4 id="7-4-3-对新数据的预测"><a href="#7-4-3-对新数据的预测" class="headerlink" title="7.4.3 对新数据的预测"></a>7.4.3 对新数据的预测</h4><p>对于回归算法而言，训练好的模型需要能够对新的数据集进行预测。利用上述步骤，我们训练好线性回归模型，并将其保存在“weights”文件中，此时，我们需要利用训练好的线性回归模型对新数据进行预测，同样，为了能够使用numpy中的函数和对中文注释的支持，在文件“linear_regression_test.py”的开始，我们加入：</p>
<p><img src="Image00237.jpg" alt></p>
<p>在对新数据的预测中，其主函数如程序清单7-10所示。</p>
<p><strong>程序清单7-10 对新数据的预测的主函数</strong></p>
<p><img src="Image00241.jpg" alt></p>
<p>在程序清单 7-10 中，对新数据的预测主要有如下的步骤：①利用函数 load_data导入测试数据集，如程序代码中的①所示，load_data函数的具体实现如程序清单7-11所示；②利用函数load_model导入训练好的线性回归的模型，如程序代码中的②所示，函数load_model的具体实现如程序清单7-12所示；③利用函数get_prediction对新数据进行预测，如程序代码中的③所示，函数get_prediction的具体实现如程序清单7-13所示；④最终将预测的结果保存到文件“predict_result”中，如程序代码中的④所示，save_model函数的具体实现如程序清单7-14所示。</p>
<p><strong>程序清单7-11 导入测试数据集</strong></p>
<p><img src="Image00246.jpg" alt></p>
<p><img src="Image00458.jpg" alt></p>
<p>在程序清单7-11中，函数load_data实现了导入测试数据集的功能，函数load_data的输入为测试数据集的位置，输出为测试数据集。</p>
<p><strong>程序清单7-12 导入线性回归模型</strong></p>
<p><img src="Image00545.jpg" alt></p>
<p>在程序清单 7-12 中，函数 load_model 将训练好的线性回归模型导入，函数load_model的输入为线性回归的参数所在的文件，其输出为权重值。</p>
<p><strong>程序清单7-13 对新数据的预测</strong></p>
<p><img src="Image00655.jpg" alt></p>
<p>在程序清单7-13中，函数get_prediction利用训练好的线性回归模型对新数据进行预测，函数 get_prediction 的输入为测试数据 data 和线性回归模型w，其输出为最终的预测值。</p>
<p><strong>程序清单7-14 保存最终的预测结果的save_predict函数</strong></p>
<p><img src="Image00754.jpg" alt></p>
<p><img src="Image00537.jpg" alt></p>
<p>在程序清单7-14中，save_predict函数将预测的结果predict保存到file_name指定的文件中。</p>
<h3 id="7-5-局部加权线性回归"><a href="#7-5-局部加权线性回归" class="headerlink" title="7.5 局部加权线性回归"></a>7.5 局部加权线性回归</h3><h4 id="7-5-1-局部加权线性回归模型"><a href="#7-5-1-局部加权线性回归模型" class="headerlink" title="7.5.1 局部加权线性回归模型"></a>7.5.1 局部加权线性回归模型</h4><p>在线性回归中会出现欠拟合的情况，有些方法可以用来解决这样的问题。局部加权线性回归（LWLR）就是这样的一种方法。局部加权线性回归采用的是给预测点附近的每个点赋予一定的权重，此时的回归系数可以表示为：</p>
<p><img src="Image00064.jpg" alt></p>
<p>M为给每个点的权重。</p>
<p>LWLR使用核函数来对附近的点赋予更高的权重，常用的有高斯核，对应的权重为：</p>
<p><img src="Image00701.jpg" alt></p>
<p>这样的权重矩阵只含对角元素。</p>
<p>局部加权线性回归的具体实现如程序清单7-15所示。</p>
<p><strong>程序清单7-15 局部加权线性回归</strong></p>
<p><img src="Image00292.jpg" alt></p>
<p><img src="Image00403.jpg" alt></p>
<p>在程序清单7-15中，函数lwlr实现了局部加权线性回归模型的训练，在lwlr函数中，输入为训练数据的特征 feature，训练数据的预测值 label 以及高斯核函数的参数k，函数lwlr的输出为训练样本的最终的预测值。对于每一个样本，需要计算其与其他所有样本之间的权重，如程序代码中的①所示，再利用如上的加权线性回归模型的计算方法，求得权重，如程序代码中的②所示。</p>
<h4 id="7-5-2-局部加权线性回归的最终结果"><a href="#7-5-2-局部加权线性回归的最终结果" class="headerlink" title="7.5.2 局部加权线性回归的最终结果"></a>7.5.2 局部加权线性回归的最终结果</h4><p>当k=1时，最终的结果如图7.4所示，当k=0.01时，最终的结果如图7.5所示，当k=0.002时，最终的结果如图7.6所示。</p>
<p><img src="Image00500.jpg" alt></p>
<p>图7.4 k=1</p>
<p><img src="Image00596.jpg" alt></p>
<p>图7.5 k=0.01</p>
<p><img src="Image00710.jpg" alt></p>
<p>图7.6 k=0.002</p>
<p>当k的值逐渐变小，其拟合数据的能力也在变强。当k取得较大值时，如图 7.4所示，出现了欠拟合，不能很好地反映数据的真实情况；当k值取得较小时，如图7.6所示，出现了过拟合。</p>
<p><strong>参考文献</strong></p>
<p>[1] 周志华.机器学习[M].北京：清华大学出版社.2016.</p>
<p>[2] 陈宝林.最优化理论与算法[M].北京：清华大学出版社.2005.</p>
<p>[3] Peter Harrington.机器学习实战[M].王斌，译.北京：人民邮电出版社.2013.</p>
<h2 id="8-岭回归和Lasso回归"><a href="#8-岭回归和Lasso回归" class="headerlink" title="8 岭回归和Lasso回归"></a>8 岭回归和Lasso回归</h2><p>在处理较为复杂的数据的回归问题时，普通的线性回归算法通常会出现预测精度不够，如果模型中的特征之间有相关关系，就会增加模型的复杂程度，并且对整个模型的解释能力并没有提高，这时，就需要对数据中的特征进行选择。对于回归算法，特征选择的方法有岭回归（Ridge Regression）和Lasso回归。</p>
<p>岭回归和Lasso回归都属于正则化的特征选择方法，对于处理较为复杂的数据回归问题通常选用这两种方法。</p>
<h3 id="8-1-线性回归存在的问题"><a href="#8-1-线性回归存在的问题" class="headerlink" title="8.1 线性回归存在的问题"></a>8.1 线性回归存在的问题</h3><p>如果模型中的特征之间有相关关系，就会增加模型的复杂程度。当数据集中的特征之间有较强的线性相关性时，即特征之间出现严重的多重共线性时，用普通最小二乘法估计模型参数，往往参数估计的方差太大，此时，求解出来的模型就很不稳定。在具体取值上与真值有较大的偏差，有时会出现与实际经济意义不符的正负号。</p>
<p>假设已知线性回归模型为：</p>
<p><img src="Image00799.jpg" alt></p>
<p>其中，x 1 ∈（0，10），x 2 ∈（10，25）。其中部分训练数据如表8-1所示。</p>
<p>表8-1 部分训练数据</p>
<p><img src="Image00004.jpg" alt></p>
<p>利用普通最小二乘法求回归系数的估计得：</p>
<p><img src="Image00431.jpg" alt></p>
<p>这与实际模型中的参数有很大的差别。计算x 1 、x 2 的样本相关系数得r 12 =0.9854，表明x 1 与x 2 之间高度相关。通过这个例子可以看到解释变量之间高度相关时，普通最小二乘估计明显变坏。</p>
<h3 id="8-2-岭回归模型"><a href="#8-2-岭回归模型" class="headerlink" title="8.2 岭回归模型"></a>8.2 岭回归模型</h3><h4 id="8-2-1-岭回归模型"><a href="#8-2-1-岭回归模型" class="headerlink" title="8.2.1 岭回归模型"></a>8.2.1 岭回归模型</h4><p>岭回归（Ridge Regression）是在平方误差的基础上增加正则项：</p>
<p><img src="Image00239.jpg" alt></p>
<p>其中， _λ_ ＞0。通过确定 _λ_ 的值可以使得在方差和偏差之间达到平衡：随着 _λ_ 的增大，模型方差减小而偏差增大。</p>
<h4 id="8-2-2-岭回归模型的求解"><a href="#8-2-2-岭回归模型的求解" class="headerlink" title="8.2.2 岭回归模型的求解"></a>8.2.2 岭回归模型的求解</h4><p>与线性回归一样，在利用最小二乘法求解岭回归模型的参数时，首先对W 求导，结果为：</p>
<p><img src="Image00341.jpg" alt></p>
<p>令其为0，可求得W 的值为：</p>
<p><img src="Image00452.jpg" alt></p>
<p>其中，I是单位对角矩阵。</p>
<p>现在让我们一起利用Python实现最小二乘的解法，在最小二乘法的求解过程中，需要用到矩阵的计算，因此，我们需要导入Python的矩阵计算模块：</p>
<p><img src="Image00541.jpg" alt></p>
<p>岭回归的最小二乘解法的具体实现如程序清单8-1所示。</p>
<p><strong>程序清单8-1 岭回归的最小二乘解法</strong></p>
<p><img src="Image00371.jpg" alt></p>
<p>程序清单8-1中的ridge_regression函数实现了岭回归模型的最小二乘解法。函数ridge_regression 的输入为训练数据的特征 feature、训练数据的目标值 label 以及参数lam，ridge_regression 函数的输出为权重 w。在岭回归的求解过程中，最关键的是上述权重的求解过程，如程序代码中的①所示。</p>
<h3 id="8-3-Lasso回归模型"><a href="#8-3-Lasso回归模型" class="headerlink" title="8.3 Lasso回归模型"></a>8.3 Lasso回归模型</h3><p>Lasso采用的则是L 1 正则，即Lasso是在平方误差的基础上增加L 1 正则：</p>
<p><img src="Image00751.jpg" alt></p>
<p>其中， _λ_ ＞0。通过确定 _λ_ 的值可以使得在方差和偏差之间达到平衡：随着 _λ_ 的增大，模型方差减小而偏差增大。与基于L 2 正则的岭回归不同的是，上述的损失函数在w j =0处是不可导的，因此传统的基于梯度的方法不能直接应用在上述的损失函数的求解上。为了求解这样的问题，一些近似的优化算法被采用，或者可以采用一些简单的方法来近似这样的优化过程。</p>
<h3 id="8-4-拟牛顿法"><a href="#8-4-拟牛顿法" class="headerlink" title="8.4 拟牛顿法"></a>8.4 拟牛顿法</h3><h4 id="8-4-1-拟牛顿法"><a href="#8-4-1-拟牛顿法" class="headerlink" title="8.4.1 拟牛顿法"></a>8.4.1 拟牛顿法</h4><p>BFGS算法是使用较多的一种拟牛顿方法，是由Broyden、Fletcher、Goldfarb和Shanno四个人分别提出的，故称为BFGS校正。</p>
<p>对于拟牛顿方程：</p>
<p><img src="Image00856.jpg" alt></p>
<p>可以化简为：</p>
<p><img src="Image00138.jpg" alt></p>
<p>令B k+1 ≜G k+1 ，则可得：</p>
<p><img src="Image00173.jpg" alt></p>
<p>在BFGS校正方法中，假设：</p>
<p><img src="Image00287.jpg" alt></p>
<h4 id="8-4-2-BFGS校正公式的推导"><a href="#8-4-2-BFGS校正公式的推导" class="headerlink" title="8.4.2 BFGS校正公式的推导"></a>8.4.2 BFGS校正公式的推导</h4><p>令<img src="Image00398.jpg" alt> ，其中u k 、v k 均为n×1的向量。y k =∇f （x k+1 ）-∇f （x k ），s k =x k+1 -x k 。</p>
<p>则对于拟牛顿方程B k+1 （x k+1 -x k ）=∇f （x k+1 ）-∇f （x k ）可以化简为：</p>
<p><img src="Image00495.jpg" alt></p>
<p>将B k+1 =B k +E k 代入上式：</p>
<p><img src="Image00589.jpg" alt></p>
<p>将<img src="Image00705.jpg" alt> 代入上式：</p>
<p><img src="Image00793.jpg" alt></p>
<p><img src="Image00000.jpg" alt></p>
<p>已知<img src="Image00001.jpg" alt> 均为实数，y k -B k s k 为n×1的向量。上式中，参数 _α_ 和 _β_ 解的可能性有很多，我们取特殊的情况，假设u k =rB k s k ，v k = _θ_ y k ，则：</p>
<p><img src="Image00003.jpg" alt></p>
<p>代入上式：</p>
<p><img src="Image00006.jpg" alt></p>
<p>即</p>
<p><img src="Image00011.jpg" alt></p>
<p>令<img src="Image00014.jpg" alt> ，则：</p>
<p><img src="Image00019.jpg" alt></p>
<p>最终的BFGS校正公式为：</p>
<p><img src="Image00022.jpg" alt></p>
<h4 id="8-4-3-BFGS校正的算法流程"><a href="#8-4-3-BFGS校正的算法流程" class="headerlink" title="8.4.3 BFGS校正的算法流程"></a>8.4.3 BFGS校正的算法流程</h4><p>设B k 对称正定，B k+1 由上述的 BFGS 校正公式确定，那么B k+1 对称正定的充要条件是<img src="Image00027.jpg" alt> 。</p>
<p>在利用 Armijo 搜索准则时，并不是都满足上述的充要条件，此时可以对 BFGS校正公式做些许改变：</p>
<p><img src="Image00031.jpg" alt></p>
<p>BFGS拟牛顿法的算法流程：</p>
<p>• 初始化参数 _δ_ ∈（0，1）， _σ_ ∈（0，0.5），初始化点x 0 ，终止误差0≤ _ε_ ≪1，初始化对称正定矩阵B 0 。令k：=0。</p>
<p>• 重复以下过程：</p>
<p>① 计算g  k =∇f （x k ）。若<img src="Image00036.jpg" alt> ，退出。输出x k 作为近似极小值点。</p>
<p>② 解线性方程组得解d k ：B k d=-g k</p>
<p>③ 设m k 是满足如下不等式的最小非负整数m：</p>
<p><img src="Image00042.jpg" alt></p>
<p>令 _α k _ = _δ m k _ ，x k+1 =x k + _α k _ d  k</p>
<p>④ 由上述公式确定B k+1</p>
<p>• 令k：=k+1</p>
<p>利用Sherman-Morrison公式可对上式进行变换，得到：</p>
<p><img src="Image00044.jpg" alt></p>
<p>令<img src="Image00047.jpg" alt> ，则得到：</p>
<p><img src="Image00050.jpg" alt></p>
<p>利用BFGS求解岭回归模型的具体过程如程序清单8-2所示。</p>
<p><strong>程序清单8-2 岭回归的BFGS解法</strong></p>
<p><img src="Image00053.jpg" alt></p>
<p><img src="Image00057.jpg" alt></p>
<p>在程序清单8-2中，函数bfgs对岭回归模型中的参数进行求解。在BFGS校正公式中，最优的步长是通过Armijo线搜索的方法确定的，如程序代码中的①所示，BFGS的校正公式如程序代码中的②所示。利用上述的公式更新 Bk，如程序代码中的③所示。在 bfgs 函数中，使用到了导函数的求解函数 get_gradient，函数的具体实现如程序清单8-3所示，同时，需要计算函数值，使用到了函数get_result函数，函数的具体实现如程序清单8-4所示。</p>
<p><strong>程序清单8-3 求梯度的get_gradient函数</strong></p>
<p><img src="Image00061.jpg" alt></p>
<p><img src="Image00397.jpg" alt></p>
<p>在程序清单8-3中，get_gradient函数计算损失函数的导函数的值。</p>
<p><strong>程序清单8-4 get_result函数</strong></p>
<p><img src="Image00415.jpg" alt></p>
<p>在程序清单8-4中，get_result函数计算训练样本的损失函数的值。</p>
<h3 id="8-5-L-BFGS求解岭回归模型"><a href="#8-5-L-BFGS求解岭回归模型" class="headerlink" title="8.5 L-BFGS求解岭回归模型"></a>8.5 L-BFGS求解岭回归模型</h3><h4 id="8-5-1-BGFS算法存在的问题"><a href="#8-5-1-BGFS算法存在的问题" class="headerlink" title="8.5.1 BGFS算法存在的问题"></a>8.5.1 BGFS算法存在的问题</h4><p>在BFGS算法中，每次都要存储近似Hesse矩阵<img src="Image00435.jpg" alt> ，在高维数据时，存储<img src="Image00460.jpg" alt> 浪费很多的存储空间，而在实际的运算过程中，我们需要的是搜索方向，因此出现了L-BFGS算法，是对BFGS算法的一种改进算法。在L- BFGS算法中，只保存最近的m次迭代信息，以降低数据的存储空间。</p>
<h4 id="8-5-2-L-BFGS算法思路"><a href="#8-5-2-L-BFGS算法思路" class="headerlink" title="8.5.2 L-BFGS算法思路"></a>8.5.2 L-BFGS算法思路</h4><p>令<img src="Image00477.jpg" alt> ，则BFGS算法中的H  k+1 可以表示为：</p>
<p><img src="Image00493.jpg" alt></p>
<p>若在初始时，假定初始的矩阵H 0 =I，则我们可以得到：</p>
<p><img src="Image00512.jpg" alt></p>
<p><img src="Image00529.jpg" alt></p>
<p>则，H k+1 为：</p>
<p><img src="Image00547.jpg" alt></p>
<p>若此时，只保留最近的m步，则：</p>
<p><img src="Image00566.jpg" alt></p>
<p>这样在L-BFGS算法中，不再保存完整的H  k ，而是存储向量序列{s k }和{y k }，需要矩阵H  k 时，使用向量序列{s k }和{y k<br>}计算就可以得到，而向量序列{s k }和{y k }也不是都要保存，只要保存最新的m步向量即可。L-BFGS算法中确定新的下降方向的具体过程为：</p>
<p><img src="Image00587.jpg" alt></p>
<p>L-BFGS算法的具体实现如程序清单8-5所示。</p>
<p><strong>程序清单8-5 岭回归的拟牛顿L-BFGS解法</strong></p>
<p><img src="Image00615.jpg" alt></p>
<p><img src="Image00635.jpg" alt></p>
<p>在程序清单8-5中，函数lbfgs利用L- BFGS算法对岭回归模型进行求解。在求解的过程中，摒弃了BFGS算法中的Bk的求解，取而代之的是两个阶段的循环过程，如程序代码中的①所示。</p>
<h3 id="8-6-岭回归对数据的预测"><a href="#8-6-岭回归对数据的预测" class="headerlink" title="8.6 岭回归对数据的预测"></a>8.6 岭回归对数据的预测</h3><p>有了以上的理论准备，我们利用上述实现好的函数，构建岭回归模型。在训练岭回归模型的过程中，我们使用表8-1所示的数据集为训练数据集。我们分别利用最小二乘法，拟牛顿法BFGS和拟牛顿法L- BFGS对其回归系数进行求解，求解的过程分为：①训练线性回归模型；②利用训练好的线性回归模型预测新的数据。</p>
<h4 id="8-6-1-训练岭回归模型"><a href="#8-6-1-训练岭回归模型" class="headerlink" title="8.6.1 训练岭回归模型"></a>8.6.1 训练岭回归模型</h4><p>首先，我们利用训练样本训练模型，为了使得Python能够支持中文的注释和利用numpy，我们需要在“ridge_regression_train.py”文件的开始加入：</p>
<p><img src="Image00658.jpg" alt></p>
<p>岭回归模型的训练的主函数如程序清单8-6所示。</p>
<p><strong>程序清单8-6 岭回归模型训练的主函数</strong></p>
<p><img src="Image00684.jpg" alt></p>
<p>程序清单8-6是岭回归模型训练的主函数，在岭回归模型的训练过程中主要分为以下的步骤：①导入训练数据，如程序代码中的①所示，函数 load_data 的具体实现如程序清单8-7所示；②导入完训练数据后，可以选择不同的方法对其模型的参数进行训练，如程序代码中的②所示，函数bfgs实现了拟牛顿法BFGS的求解过程，如程序代码中的③所示，bfgs函数的具体实现如程序清单8-2所示，函数lbfgs实现了拟牛顿法L- BFGS的求解过程，如程序代码中的④所示，lbfgs函数的具体实现如程序清单8-3 所示，也可以利用最小二乘法对其参数进行训练，如程序代码中的⑤所示，最小二乘法的具体实现如程序清单8-1所示；③训练完成后，利用函数save_weights将最终的线性回归的模型参数保存在文件“weights”中，如程序代码中的⑥所示，函数save_weights的具体实现如程序清单8-8所示。</p>
<p><strong>程序清单8-7 导入训练数据的load_data函数</strong></p>
<p><img src="Image00789.jpg" alt></p>
<p>在程序清单8-7中，函数load_data将训练样本中的特征导入到矩阵feature中，同时将样本中的标签导入到矩阵label中。</p>
<p><strong>程序清单8-8 保存模型的save_model函数</strong></p>
<p><img src="Image00792.jpg" alt></p>
<p>在程序清单8-8中，函数save_model将训练好的线性回归模型w保存到file_name指定的文件中。</p>
<h4 id="8-6-2-最终的训练结果"><a href="#8-6-2-最终的训练结果" class="headerlink" title="8.6.2 最终的训练结果"></a>8.6.2 最终的训练结果</h4><p>在利用BFGS算法对模型进行求解时，其运行的过程为：</p>
<p><img src="Image00796.jpg" alt></p>
<p>在利用L-BFGS算法对模型进行求解时，其运行的过程为：</p>
<p><img src="Image00801.jpg" alt></p>
<p>最终，以上三种方法计算出来的结果一致，其结果为：</p>
<p><img src="Image00805.jpg" alt></p>
<h4 id="8-6-3-利用岭回归模型预测新的数据"><a href="#8-6-3-利用岭回归模型预测新的数据" class="headerlink" title="8.6.3 利用岭回归模型预测新的数据"></a>8.6.3 利用岭回归模型预测新的数据</h4><p>对于回归算法而言，训练好的模型需要能够对新的数据集进行预测。利用上述步骤，我们训练好线性回归模型，并将其保存在“weights”文件中，此时，我们需要利用训练好的线性回归模型对新数据进行预测，同样，为了能够使用numpy中的函数和对中文注释的支持，在文件“ridge_regression_test.py”的开始，我们加入：</p>
<p><img src="Image00810.jpg" alt></p>
<p>在对新数据的预测中，其主函数如程序清单8-9所示。</p>
<p><strong>程序清单8-9 对新数据的预测的主函数</strong></p>
<p><img src="Image00813.jpg" alt></p>
<p>在程序清单 8-9 中，对新数据的预测主要有如下的步骤：①利用函数 load_data导入测试数据集，其函数的具体实现如程序清单 8-10 所示；②利用函数 load_model导入训练好的线性回归的模型，函数load_model的具体实现如程序清单8-11所示；③利用函数get_prediction对新数据进行预测，函数get_prediction的具体实现如程序清单8-12所示；④利用函数save_result将预测的结果保存到文件“predict_result”中，函数save_result的具体实现如程序清单8-13所示。</p>
<p><strong>程序清单8-10 导入测试数据集</strong></p>
<p><img src="Image00817.jpg" alt></p>
<p><img src="Image00821.jpg" alt></p>
<p>在程序清单8-10中，函数load_data实现了导入测试数据集的功能，函数load_data的输入为测试数据集的位置，输出为测试数据集。</p>
<p><strong>程序清单8-11 导入线性回归模型</strong></p>
<p><img src="Image00826.jpg" alt></p>
<p>在程序清单 8-11 中，函数 load_model 将训练好的线性回归模型导入，函数load_model的输入为岭回归的参数所在的文件，其输出为权重值。</p>
<p><strong>程序清单8-12 对新数据的预测</strong></p>
<p><img src="Image00831.jpg" alt></p>
<p>在程序清单8-12中，函数get_prediction利用训练好的线性回归模型对新数据进行预测，函数 get_prediction 的输入为测试数据 data 和线性回归模型w，其输出为最终的预测值。</p>
<p><strong>程序清单8-13 保存最终的预测结果</strong></p>
<p><img src="Image00835.jpg" alt></p>
<p><img src="Image00840.jpg" alt></p>
<p>在程序清单8-13中，函数save_result将最终的预测结果predict保存到file_name指定的文件中。</p>
<p><strong>参考文献</strong></p>
<p>[1] 陈宝林.最优化理论与算法[M].北京：清华大学出版社.2005.</p>
<p>[2] Peter Harrington.机器学习实战[M].王斌，译.北京：人民邮电出版社.</p>
<h2 id="9-CART树回归"><a href="#9-CART树回归" class="headerlink" title="9 CART树回归"></a>9 CART树回归</h2><p>在第7章和第8章中介绍了基本的线性回归模型属于全局的模型（除局部加权线性回归外），在线性回归模型中，其前提是假设全局的数据之间是线性的，通过拟合所有的样本点，训练得到最终的模型。然而现实中的很多问题是非线性的，当处理这类复杂的数据的回归问题时，特征之间的关系并不是简单的线性关系，此时，不可能利用全局的线性回归模型拟合这类数据。</p>
<p>CART树回归算法属于一种局部的回归算法，通过将全局的数据集划分成多份容易建模的数据集，这样在每一个局部的数据集上进行局部的回归建模。</p>
<h3 id="9-1-复杂的回归问题"><a href="#9-1-复杂的回归问题" class="headerlink" title="9.1 复杂的回归问题"></a>9.1 复杂的回归问题</h3><h4 id="9-1-1-线性回归模型"><a href="#9-1-1-线性回归模型" class="headerlink" title="9.1.1 线性回归模型"></a>9.1.1 线性回归模型</h4><p>在第7章和第8章中分别介绍了线性回归的相关算法，在基本的线性回归算法中，样本的特征与样本的标签之间存在线性相关关系，但是，对于样本特征与样本标签存在非线性的关系时，如图9.1所示。</p>
<p><img src="Image00843.jpg" alt></p>
<p>图9.1 样本特征与样本标签之间是非线性关系</p>
<p>对于图9.1所示的非线性的回归问题，简单的线性回归算法无法求解，利用简单的线性回归求解的结果如图9.2所示。</p>
<p><img src="Image00846.jpg" alt></p>
<p>图9.2 简单的线性回归的求解结果</p>
<h4 id="9-1-2-局部加权线性回归"><a href="#9-1-2-局部加权线性回归" class="headerlink" title="9.1.2 局部加权线性回归"></a>9.1.2 局部加权线性回归</h4><p>为了能够实现对非线性数据的拟合，可以使用局部加权线性回归，当k=0.05时，其局部加权线性回归的求解结果如图9.3所示。</p>
<p><img src="Image00569.jpg" alt></p>
<p>图9.3 局部加权线性回归的求解结果</p>
<p>局部加权线性回归能够对非线性的数据实现较好拟合，与简单的线性回归算法相比，局部线性加权回归算法是局部的线性模型，而简单的线性回归模型是全局的模型，利用局部的模型能够较好拟合出局部的数据。</p>
<p>虽然基于局部加权线性回归模型能够较好拟合非线性数据，但是局部加权线性回归模型属于非参学习算法，在每次对数据进行预测时，需要利用数据重新训练模型的参数，当数据量较大时，这样的计算是非常耗费时间的。</p>
<p>是否存在一种基于参数的学习算法，能够实现对非线性数据的回归呢？</p>
<h4 id="9-1-3-CART算法"><a href="#9-1-3-CART算法" class="headerlink" title="9.1.3 CART算法"></a>9.1.3 CART算法</h4><p>基于树的回归算法也是一类基于局部的回归算法，通过将数据集切分成多份，在每一份数据中单独建模。与局部加权线性回归不同的是，基于树回归的算法是一种基于参数的学习算法，利用训练数据训练完模型后，参数一旦确定，无需再改变。</p>
<p>分类回归树（Classification And Regression Tree，CART）算法是使用较多的一种树模型，CART算法可以处理分类问题，也可以处理回归问题。在第5章的随机森林算法中，我们介绍了如何利用CART算法处理分类问题，在本章中，我们着重介绍如何利用CART算法处理回归问题。</p>
<p>CART算法中的树采用一种二分递归分割的技术，即将当前的样本集分为左子树和右子树两个子样本集，使得生成的每个非叶子节点都有两个分支。因此，CART算法生成的决策树是非典型的二叉树。</p>
<p>利用CART算法处理回归问题的主要步骤：①CART回归树的生成；②CART回归树的剪枝。</p>
<h3 id="9-2-CART回归树生成"><a href="#9-2-CART回归树生成" class="headerlink" title="9.2 CART回归树生成"></a>9.2 CART回归树生成</h3><h4 id="9-2-1-CART回归树的划分"><a href="#9-2-1-CART回归树的划分" class="headerlink" title="9.2.1 CART回归树的划分"></a>9.2.1 CART回归树的划分</h4><p>第5章介绍的CART分类树算法中，利用Gini指数作为划分树的指标，通过样本中的特征，对样本进行划分，直到所有的叶节点中的所有样本都为同一个类别为止。但是在 CART 回归树中，样本的标签是一系列的连续值的集合，不能再使用 Gini 指数作为划分树的指标。但是，我们注意到，Gini指数表示的是数据的混乱程度，对于连续数据，当数据分布比较分散时，各个数据与平均数的差的平方和较大，方差就较大；当数据分布比较集中时，各个数据与平均数的差的平方和较小。方差越大，数据的波动越大；方差越小，数据的波动就越小。因此，对于连续的数据，可以使用样本与平均值的差的平方和作为划分回归树的指标。假设，我们有m个训练样本{（X<br>（1） ，y （1） ），（X  （2） ，y （2） ），…，（X  （m） ，y （m） ）}，则划分CART回归树的指标为：</p>
<p><img src="Image00852.jpg" alt></p>
<p>其中，y （i） 为第i个样本的标签，y为m个样本标签值的均值。现在，让我们利用Python实现CART回归树的划分指标，在计算的过程中，需要使用numpy中的相关函数，因此，首先需要导入numpy模块：</p>
<p><img src="Image00619.jpg" alt></p>
<p>CART回归树的划分指标的具体形式如程序清单9-1所示。</p>
<p><strong>程序清单9-1 回归树的划分指标</strong></p>
<p><img src="Image00638.jpg" alt></p>
<p><img src="Image00863.jpg" alt></p>
<p>在程序清单9-1中，err_cnt函数用于计算当前节点的总方差，总方差的具体计算过程如程序代码中的①所示。</p>
<p>有了划分的标准，那么，应该如何对样本进行划分呢？与CART分类树中的方法一样，我们根据每一维特征中的每一个取值，尝试将样本划分到树节点的左右子树中，如取得样本特征中的第 j维特征中值x作为划分的值，如果一个样本在第 j维处的值大于或者等于x，则将其划分到右子树中，否则划分到左子树中，具体划分的过程如图9.4所示。</p>
<p><img src="Image00867.jpg" alt></p>
<p>图9.4 左右子树的划分</p>
<p>在图9.4中，原始节点中有m个样本，根据与值x的对比，将样本划分到左右子树中，左子树被分到m 1 个样本，剩下的m 2 个样本被划分到右子树中。具体划分的过程如程序清单9-2所示。</p>
<p><strong>程序清单9-2 左右子树的划分</strong></p>
<p><img src="Image00871.jpg" alt></p>
<p>在程序清单9-2中，split_tree函数根据fea位置处的特征，按照值value将样本划分到左右子树中，当样本在fea处的值大于或者等于value时，将其划分到右子树中，如程序代码中的①所示；否则，将其划分到左子树中，如程序代码中的②所示。</p>
<h4 id="9-2-2-CART回归树的构建"><a href="#9-2-2-CART回归树的构建" class="headerlink" title="9.2.2 CART回归树的构建"></a>9.2.2 CART回归树的构建</h4><p>CART分类树的构建过程如下所示：</p>
<p>• 对于当前训练数据集，遍历所有属性及其所有可能的切分点，寻找最佳切分属性及其最佳切分点，使得切分之后的基尼指数最小，利用该最佳属性及其最佳切分点将训练数据集切分成两个子集，分别对应着判别结果是左子树和判别结果是右子树。</p>
<p>• 对第一步中生成的两个数据子集递归地调用第一步，直至满足停止条件。</p>
<p>• 生成CART决策树</p>
<p>为了能构建CART回归树算法，首先，需要为CART回归树中节点设置一个结构，其具体的实现如程序清单9-3所示。</p>
<p><strong>程序清单9-3 CART回归树中的节点</strong></p>
<p><img src="Image00875.jpg" alt></p>
<p>在CART回归树的节点类中，属性fea表示的是待划分数据集的特征的索引，属性value表示的是划分的具体的值，属性results表示的是叶子节点的具体的值，属性right表示的是右子树，属性left表示的是左子树。</p>
<p>现在，让我们一起实现CART回归树，CART回归树的构建过程如程序清单9-4所示。</p>
<p><strong>程序清单9-4 C ART回归树的构建</strong></p>
<p><img src="Image00879.jpg" alt></p>
<p><img src="Image00883.jpg" alt></p>
<p>在程序清单 9-4 中，build_tree 函数用于构建 CART 回归树模型，在构建 CART回归树模型的过程中，如果节点中的样本的个数小于或者等于指定的最小的样本数min_sample，则该节点不再划分，如程序代码中的①所示，函数leaf用于计算当前叶子节点的值，其具体实现如程序清单9-5所示；当节点需要划分时，首先计算当前节点的error值，如程序代码中的②所示，函数err_cnt的具体实现如程序清单9-1所示；在开始构建的过程中，根据每一维特征的取值尝试将样本划分到左右子树中，如程序代码中的③所示，函数split_tree的具体实现如程序清单9-2所示，划分后产生左子树和右子树，此时，计算左右子树的error值，如程序代码中的④所示，若此时的error值小于最优的 error 值，则更新最优划分，如程序代码中的⑤所示；当该节点划分完成后，继续对其左右子树进行划分，如程序代码中的⑥所示。</p>
<p><strong>程序清单9-5 CART回归树中叶子节点的计算</strong></p>
<p><img src="Image00889.jpg" alt></p>
<p>在程序清单9-5中，函数leaf用于计算当前叶子节点的值，计算的方法是使用划分到该叶子节点的所有样本的标签的均值，如程序代码中的①所示。</p>
<h3 id="9-3-CART回归树剪枝"><a href="#9-3-CART回归树剪枝" class="headerlink" title="9.3 CART回归树剪枝"></a>9.3 CART回归树剪枝</h3><p>在CART树回归中，当树中的节点对样本一直划分下去时，会出现的最极端的情况是：每一个叶子节点中仅包含一个样本，此时，叶子节点的值即为该样本的标签的值。这种情况极易对训练样本“过拟合”，通过这样的方式训练出来的样本可以对训练样本拟合得很好，但是对于新样本的预测效果将会较差。为了防止构建好的CART树回归模型过拟合，通常需要对CART回归树进行剪枝，剪枝的目的是防止CART回归树生成过多的叶子节点。在剪枝中主要分为：前剪枝和后剪枝。</p>
<h4 id="9-3-1-前剪枝"><a href="#9-3-1-前剪枝" class="headerlink" title="9.3.1 前剪枝"></a>9.3.1 前剪枝</h4><p>前剪枝是指在生成CART回归树的过程中对树的深度进行控制，防止生成过多的叶子节点。在程序清单9-4中的build_tree函数中，我们通过参数min_sample和min_err来控制树中的节点是否需要进行更多的划分。通过不断调节这两个参数，来找到一个合适的CART树模型。</p>
<h4 id="9-3-2-后剪枝"><a href="#9-3-2-后剪枝" class="headerlink" title="9.3.2 后剪枝"></a>9.3.2 后剪枝</h4><p>后剪枝是指将训练样本分成两个部分，一部分用来训练CART树模型，这部分数据被称为训练数据，另一部分用来对生成的CART树模型进行剪枝，这部分数据被称为验证数据。</p>
<p>由上述过程可知，在后剪枝的过程中，通过验证生成好的CART树模型是否在验证数据集上发生了过拟合，如果出现过拟合的现象，则合并一些叶子节点来达到对CART树模型的剪枝。</p>
<p>在本章中，我们主要使用前剪枝的策略，通过调整参数min_sample和min_err来控制CART树模型的生成。</p>
<h3 id="9-4-CART回归树对数据预测"><a href="#9-4-CART回归树对数据预测" class="headerlink" title="9.4 CART回归树对数据预测"></a>9.4 CART回归树对数据预测</h3><p>有了以上的理论准备，我们利用上述实现好的函数，构建CART树回归模型。在训练 CART 树回归模型的过程中，我们使用图 9.1 所示的数据作为训练样本，利用CART回归树算法进行求解的过程中，主要包括：①利用训练数据训练CART回归树模型；②利用训练好的CART回归树模型对新数据进行预测。</p>
<h4 id="9-4-1-利用训练数据训练CART回归树模型"><a href="#9-4-1-利用训练数据训练CART回归树模型" class="headerlink" title="9.4.1 利用训练数据训练CART回归树模型"></a>9.4.1 利用训练数据训练CART回归树模型</h4><p>首先，我们利用训练样本训练模型，为了使得Python能够支持中文的注释和利用numpy，我们需要在“train_cart.py”文件的开始加入：</p>
<p><img src="Image00894.jpg" alt></p>
<p>同时，在训练完成 CART 回归树模型后，需要保存最终的训练模型，因此在“train_cart.py”文件中加入：</p>
<p><img src="Image00898.jpg" alt></p>
<p>CART回归树模型的训练的主程序如程序清单9-6所示。</p>
<p><strong>程序清单9-6 C ART回归树训练的主程序</strong></p>
<p><img src="Image00901.jpg" alt></p>
<p>在程序清单9-6中，CART回归树模型的训练主要包括：①导入训练数据，如程序代码中的①所示，函数 load_data 的具体实现如程序清单 9-7 所示；②训练 CART树，如程序代码中的②所示，函数buid_tree的具体实现如程序清单9-4所示；③评估训练好的CART回归树模型，如程序代码中的③所示，函数cal_error的具体实现如程序清单9-8所示；④保存训练好的CART回归树模型，如程序代码中的④所示，函数save_model的具体实现如程序清单9-9所示。</p>
<p><strong>程序清单9-7 导入训练数据</strong></p>
<p><img src="Image00905.jpg" alt></p>
<p>在程序清单9-7中，函数load_data将保存了训练数据的文件“data_file”中的数据导入到data中。</p>
<h4 id="9-4-2-最终的训练结果"><a href="#9-4-2-最终的训练结果" class="headerlink" title="9.4.2 最终的训练结果"></a>9.4.2 最终的训练结果</h4><p>当min_sample取为30，min_err取为0.3时，CART回归树的训练过程为：</p>
<p><img src="Image00908.jpg" alt></p>
<p>当训练好CART回归树，需要评估训练好的CART回归树模型时，函数cal_error用于评估训练好的CART回归树模型，其具体实现如程序清单9-8所示。</p>
<p><strong>程序清单9-8 评估训练好的CART回归树模型</strong></p>
<p><img src="Image00762.jpg" alt></p>
<p>在程序清单9-8中，函数cal_error用于评估训练好的CART回归树模型，函数的输入分别为训练数据data和训练好的CART回归树模型tree，在评估CART回归树模型的过程中，利用训练好的CART回归树模型对每一个样本进行预测，如程序代码中的①所示，函数predict的具体实现如程序清单9-10所示。当预测完成后，利用预测的值和原始的样本的标签计算残差，如程序代码中的②所示。</p>
<p>当CART回归树模型的训练和评估完成后，需要将训练好的CART回归树保存到本地，保存CART回归树的过程如程序清单9-9所示。</p>
<p><strong>程序清单9-9 利用CART回归树模型预测</strong></p>
<p><img src="Image00872.jpg" alt></p>
<p>在程序清单9-9中，函数save_model将训练好的CART回归树模型regression_tree保存到文件“result_file”中，在保存回归树的过程中，使用到了cPickle模块中的dump方法。</p>
<p><strong>程序清单9-10 利用CART回归树模型预测</strong></p>
<p><img src="Image00074.jpg" alt></p>
<p>在程序清单 9-10 中，函数 predict 利用训练好的 CART 回归树模型 tree 对样本sample进行预测。在预测的过程中，主要分为如下的情况：</p>
<p>• 若此时只有根结点，则直接返回其值作为最终的预测结果，如程序代码中的①所示；</p>
<p>• 若此时该结点有左右子树，则比较样本sample中在fea索引处的值val_sample和CART回归树模型中在划分处的值value：</p>
<p>○ 若val_sample大于或等于CART回归树模型中的值value，则选择右子树，如程序代码中的②所示；</p>
<p>○ 若val_sample小于CART回归树模型中的值value，则选择左子树，如程序代码中的③所示。</p>
<p>最终对数据的拟合效果如图9.5所示。</p>
<p><img src="Image00191.jpg" alt></p>
<p>图9.5 min_sample取为30，min_err取为0.3</p>
<p>我们对min_sample和min_err取值进行调整，当min_sample取为5，min_err取为0.1时的拟合效果如图9.6所示。</p>
<p><img src="Image00370.jpg" alt></p>
<p>图9.6 min_sample取为5，min_err取为0.1</p>
<p>从图9.5和图9.6中可以看出，相比于图9.5，图9.6中的模型对训练数据的拟合更好，容易出现过拟合现象。</p>
<h4 id="9-4-3-利用训练好的CART回归树模型对新的数据预测"><a href="#9-4-3-利用训练好的CART回归树模型对新的数据预测" class="headerlink" title="9.4.3 利用训练好的CART回归树模型对新的数据预测"></a>9.4.3 利用训练好的CART回归树模型对新的数据预测</h4><p>对于回归算法而言，训练好的模型需要能够对新的数据集进行预测。利用上述步骤，我们首先训练好CART树回归模型，并将其保存在“regression_tree”文件中，此时，我们需要利用训练好的CART树回归模型对新数据进行预测，同样，为了能够使用random中的函数，CART树模型的导入和对中文注释的支持，在“test_cart.py”文件开始，我们加入：</p>
<p><img src="Image00885.jpg" alt></p>
<p>同时，需要使用到“train_cart.py”文件中的一些函数，我们需要在“test_cart.py”文件中导入：</p>
<p><img src="Image00510.jpg" alt></p>
<p>在对新数据的预测过程中，其主函数如程序清单9-11所示。</p>
<p><strong>程序清单9-11 对新数据的预测的主函数</strong></p>
<p><img src="Image00137.jpg" alt></p>
<p>在程序清单 9-11 中，利用训练好的 CART 回归树算法对新数据进行预测时，主要包括如下几步：①导入测试数据集，如程序代码中的①所示，函数 load_data 的具体实现如程序清单 9-12 所示；②导入训练好的 CART 回归树模型，如程序代码中的②所示，函数load_model的具体实现如程序清单9-13所示；③利用训练好的CART回归树模型对测试数据进行预测，如程序代码中的③所示，函数get_prediction的具体实现如程序清单9-14所示；④保存好最终的预测结果，如程序代码中的④所示，函数save_result的具体实现如程序清单9-15所示。</p>
<p><strong>程序清单9-12 导入测试数据集</strong></p>
<p><img src="Image00721.jpg" alt></p>
<p>在程序清单9-12中，为了生成随机样本，从[0，1]之间随机生成400个样本，并将样本存储到data_test中，生成样本的过程如程序代码中的①所示。</p>
<p><strong>程序清单9-13 导入训练好的CART回归树模型</strong></p>
<p><img src="Image00812.jpg" alt></p>
<p>在程序清单9-13中，load_model函数用于从文件tree_file中导入训练好的CART回归树模型，在导入回归树模型中，使用到了cPickle模块中的load函数，如程序代码中的①所示。</p>
<p><strong>程序清单9-14 对新数据的预测</strong></p>
<p><img src="Image00015.jpg" alt></p>
<p><img src="Image00127.jpg" alt></p>
<p>在程序清单 9-14 中，函数 get_prediction 利用训练好的 CART 回归树模型regression_tree对需要预测的样本data_test进行预测，预测使用到了predict函数，如程序代码中的①所示，函数predict的具体实现如程序清单9-10所示。</p>
<p><strong>程序清单9-15 保存最终的预测结果</strong></p>
<p><img src="Image00250.jpg" alt></p>
<p>在程序清单9-15中，函数save_result将需要预测的样本data_test和最终的预测结果result存储到文件prediction_file中。</p>
<p><strong>参考文献</strong></p>
<p>[1] 李航.统计学习方法[M].北京：清华大学出版社.2012.</p>
<p>[2] Peter Harrington.机器学习实战[M].王斌，译.北京：人民邮电出版社.2013.</p>
<h1 id="第三部分-聚类算法"><a href="#第三部分-聚类算法" class="headerlink" title="第三部分 聚类算法"></a>第三部分 聚类算法</h1><p>“物以类聚”指的是事物之间通过某种相似的属性聚集到一起，聚类算法是对事物自动归类的一类算法。聚类算法是一种典型的无监督的学习算法，在聚类算法中通过定义不同的相似性的度量方法，将具有相似属性的事物聚集到同一个类中。</p>
<p>第10章将介绍最基本的聚类算法K-Means算法，K-Means聚类算法是基于距离相似性的聚类算法；第11章将介绍Mean Shift聚类算法，Mean Shift算法也是基于距离的聚类算法，与K-Means算法不同的是，在Mean Shift算法中无需指定聚类个数；第12章将介绍基于密度的聚类算法DBSCAN算法，基于距离的聚类算法的聚类簇是球状结构的，DBSCAN算法能够对任意形状的数据聚类；第13章将介绍在社交网络中用户聚类算法Label Propagation算法，Label Propagation算法利用自身的网络结构，通过节点之间的标签传播实现对网络节点的聚类。</p>
<h2 id="10-K-Means"><a href="#10-K-Means" class="headerlink" title="10 K-Means"></a>10 K-Means</h2><p>根据训练样本中是否包含标签信息，机器学习可以分为监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。聚类算法是典型的无监督学习，其训练样本中只包含样本的特征，不包含样本的标签信息。在聚类算法中，利用样本的特征，将具有相似属性的样本划分到同一个类别中。</p>
<p>K-Means算法，也被称为K-平均或K- 均值算法，是一种广泛使用的聚类算法。K-Means算法是基于相似性的无监督的算法，通过比较样本之间的相似性，将较为相似的样本划分到同一个类别中。由于K- Means算法简单、易于实现于特点，K-Means算法得到了广泛的应用，如在图像分割方面的应用。</p>
<h3 id="10-1-相似性的度量"><a href="#10-1-相似性的度量" class="headerlink" title="10.1 相似性的度量"></a>10.1 相似性的度量</h3><p>在K- Means算法中，通过某种相似性度量的方法，将较为相似的个体划分到同一个类别中。对于不同的应用场景，有着不同的相似性度量的方法，为了度量样本X和样本Y之间的相似性，一般定义一个距离函数d（X，Y），利用d（X，Y）来表示样本X和样本Y之间的相似性。通常在机器学习算法中使用到的距离函数主要有：</p>
<p>• 闵可夫斯基距离（Minkowski Distance）；</p>
<p>• 曼哈顿距离（Manhattan Distance）；</p>
<p>• 欧氏距离（Euclidean Distance）。</p>
<h4 id="10-1-1-闵可夫斯基距离"><a href="#10-1-1-闵可夫斯基距离" class="headerlink" title="10.1.1 闵可夫斯基距离"></a>10.1.1 闵可夫斯基距离</h4><p>假设有两个点，分别为点P和点Q，其对应的坐标分别为：</p>
<p><img src="Image00354.jpg" alt></p>
<p>那么，点P和点Q之间的闵可夫斯基距离可以定义为：</p>
<p><img src="Image00463.jpg" alt></p>
<h4 id="10-1-2-曼哈顿距离"><a href="#10-1-2-曼哈顿距离" class="headerlink" title="10.1.2 曼哈顿距离"></a>10.1.2 曼哈顿距离</h4><p>对于上述的点P和点Q之间的曼哈顿距离可以定义为：</p>
<p><img src="Image00551.jpg" alt></p>
<h4 id="10-1-3-欧氏距离"><a href="#10-1-3-欧氏距离" class="headerlink" title="10.1.3 欧氏距离"></a>10.1.3 欧氏距离</h4><p>对于上述的点P和点Q之间的欧氏距离可以定义为：</p>
<p><img src="Image00662.jpg" alt></p>
<p>由曼哈顿距离和欧式距离的定义可知，曼哈顿距离和欧式距离是闵可夫斯基距离的具体形式，即在闵可夫斯基距离中，当p=1时，闵可夫斯基距离即为曼哈顿距离，当p=2时，闵可夫斯基距离即为欧式距离。</p>
<p>若在样本中，特征之间的单位不一致时，利用基本的欧式距离作为相似性的度量方法会存在问题，如样本的形式为（身高，体重）。身高的度量单位是cm，范围通常为（150，190），而体重的度量单位是 kg，范围通常为（50，80）。假设此时有 3 个样本，分别为：（160，50），（170，60），（180，80）。此时可以利用标准化的欧氏距离。对于上述点P和点Q之间的标准化的欧式距离可以定义为：</p>
<p><img src="Image00759.jpg" alt></p>
<p>其中，s i 表示的是第i维的标准差。在本文的K-Means算法中使用欧氏距离作为相似性的度量，在实现的过程中使用的是欧氏距离的平方d （P，Q） 2 。</p>
<p>现在我们利用Python实现欧式距离的平方，在欧式距离的计算中，需要用到矩阵的相关计算，因此，我们需要导入numpy模块：</p>
<p><img src="Image00866.jpg" alt></p>
<p>欧式距离的平方的具体实现如程序清单10-1所示。</p>
<p><strong>程序清单10-1 欧氏距离的平方</strong></p>
<p><img src="Image00069.jpg" alt></p>
<p>在程序清单10-1中，函数distance用于计算向量vecA和向量vecB之间的欧氏距离的平方。欧氏距离的平方的具体计算过程如程序代码中的①所示。</p>
<h3 id="10-2-K-Means算法原理"><a href="#10-2-K-Means算法原理" class="headerlink" title="10.2 K-Means算法原理"></a>10.2 K-Means算法原理</h3><h4 id="10-2-1-K-Means算法的基本原理"><a href="#10-2-1-K-Means算法的基本原理" class="headerlink" title="10.2.1 K-Means算法的基本原理"></a>10.2.1 K-Means算法的基本原理</h4><p>K-Means算法是基于数据划分的无监督聚类算法，首先定义常数k，常数k表示的是最终的聚类的类别数，在确定了类别数k后，随即初始化k个类的聚类中心，通过计算每一个样本与聚类中心之间的相似度，将样本点划分到最相似的类别中。</p>
<p>对于K-Means算法，假设有m个样本{X  （1） ，X  （2） ，…，X （m） }，其中，X  （i） 表示第i个样本，每一个样本中包含n个特征<img src="Image00704.jpg" alt> 。首先随机初始化k个聚类中心，通过每个样本与k个聚类中心之间的相似度，确定每个样本所属的类别，再通过每个类别中的样本重新计算每个类的聚类中心，重复这样的过程，直到聚类中心不再改变，最终确定每个样本所属的类别以及每个类的聚类中心。</p>
<h4 id="10-2-2-K-Means算法步骤"><a href="#10-2-2-K-Means算法步骤" class="headerlink" title="10.2.2 K-Means算法步骤"></a>10.2.2 K-Means算法步骤</h4><p>• 初始化常数k，随机初始化k个聚类中心；</p>
<p>• 重复计算以下过程，直到聚类中心不再改变；</p>
<p>○ 计算每个样本与每个聚类中心之间的相似度，将样本划分到最相似的类别中；</p>
<p>○ 计算划分到每个类别中的所有样本特征的均值，并将该均值作为每个类新的聚类中心。</p>
<p>• 输出最终的聚类中心以及每个样本所属的类别。</p>
<h4 id="10-2-3-K-Means算法与矩阵分解"><a href="#10-2-3-K-Means算法与矩阵分解" class="headerlink" title="10.2.3 K-Means算法与矩阵分解"></a>10.2.3 K-Means算法与矩阵分解</h4><p>以上对K-Means算法进行了简单介绍，在K-Means算法中，假设训练数据集X中有m个样本{X  （1） ，X  （2） ，…，X （m）<br>}，其中，每一个样本X  （i） 为n维的向量。此时样本可以表示为一个m×n的矩阵：</p>
<p><img src="Image00298.jpg" alt></p>
<p>假设有k个类，分别为：{C 1 ，…，C k }。在K-Means算法中，利用欧氏距离计算每一个样本X  （i） 与k个聚类中心之间的相似度，并将样本X<br>（i） 划分到最相似的类别中，再利用划分到每个类别中的样本重新计算k个聚类中心。重复以上的过程，直到质心不再改变为止。</p>
<p>K-Means算法的目标是使得每一个样本X  （i） 被划分到最相似的类别中，利用每个类别中的样本重新计算聚类中心C k ：</p>
<p><img src="Image00408.jpg" alt></p>
<p>其中，<img src="Image00507.jpg" alt> 表示的是所有C k 类中的所有的样本的特征向量的和，＃（X  （i） ∈C k ）表示的是类别C k 中的样本的个数。</p>
<p>K-Means算法的停止条件是最终的聚类中心不再改变，此时，所有样本被划分到了最近的聚类中心所属的类别中，即：</p>
<p><img src="Image00605.jpg" alt></p>
<p>其中，样本X  （i） 是数据集X  m×n 的第i行。C  j 表示的是第 j个类别的聚类中心。假设M  k×n 为k个聚类中心构成的矩阵。矩阵Z m×k 是由z ij 构成的0-1矩阵，z ij 为：</p>
<p><img src="Image00714.jpg" alt></p>
<p>对于上述的优化目标函数，其与如下的矩阵形式等价：</p>
<p><img src="Image00806.jpg" alt></p>
<p>其中，对于非矩阵形式的目标函数，可以表示为：</p>
<p><img src="Image00009.jpg" alt></p>
<p>由于<img src="Image00122.jpg" alt> ，即每一个样本X  （i） 只能属于一个类别，则：</p>
<p><img src="Image00245.jpg" alt></p>
<p>其中，m j 表示的是属于第 j个类别的样本的个数。对于矩阵形式的目标函数，其可以表示为：</p>
<p><img src="Image00348.jpg" alt></p>
<p>其中：</p>
<p><img src="Image00457.jpg" alt></p>
<p>因此，上述的两种形式的目标函数是等价的。</p>
<h3 id="10-3-K-Means算法实践"><a href="#10-3-K-Means算法实践" class="headerlink" title="10.3 K-Means算法实践"></a>10.3 K-Means算法实践</h3><p>假设有m个样本{X  （1） ，X  （2） ，…，X （m）<br>}，如图10.1所示，其中，已知这些数据可以划分成4个不同的类别，首先定义k=4，随机初始化4个不同的聚类中心，通过计算每个样本点与聚类中心之间的距离，将样本点划分到不同的类别中。</p>
<p><img src="Image00832.jpg" alt></p>
<p>图10.1 聚类数据（包含了4个不同的类别）</p>
<p>对于K- Means算法，其主要的流程是获取数据集，并对数据进行处理，然后对这些数据计算其聚类中心。首先，为了使得Python能够支持中文的注释和利用numpy，我们需要在“KMeans.py”文件的开始加入：</p>
<p><img src="Image00035.jpg" alt></p>
<p>K-Means算法的主函数如程序清单10-2所示。</p>
<p><strong>程序清单10-2 主函数</strong></p>
<p><img src="Image00147.jpg" alt></p>
<p>在程序清单10-2中，K-Means算法的主要步骤包括：①导入数据，如程序代码中的①所示。load_data函数的主要功能是对数据进行预处理，并导入；②随机初始化k个聚类中心，如程序代码中的②所示，randCent函数主要完成k个聚类中心的初始化；③利用K- Means算法进行聚类计算，如程序代码中的③所示，kmeans函数是K-Means算法的核心程序，利用K- Means算法计算出聚类中心和每个样本所属的类别；④保存每个样本所属的类别到sub文件中，如程序代码中的④所示；⑤保存k个聚类中心到center文件中，如程序代码中的⑤所示，其中save_result函数将最终的结果写回到对应的文件中。</p>
<h4 id="10-3-1-导入数据"><a href="#10-3-1-导入数据" class="headerlink" title="10.3.1 导入数据"></a>10.3.1 导入数据</h4><p>在利用K-Means算法对数据进行聚类运算之前，首先需要导入数据，导入数据的程序代码如程序清单10-3所示。</p>
<p><strong>程序清单10-3 数据的导入</strong></p>
<p><img src="Image00166.jpg" alt></p>
<p><img src="Image00699.jpg" alt></p>
<p>在程序清单 10-3 中，load_data 函数用于对指定文件中的数据进行预处理，并将处理完的数据导入到一个矩阵中。</p>
<h4 id="10-3-2-初始化聚类中心"><a href="#10-3-2-初始化聚类中心" class="headerlink" title="10.3.2 初始化聚类中心"></a>10.3.2 初始化聚类中心</h4><p>在K-Means算法中，需要先指定聚类中心的个数k，并在利用K- Means算法进行聚类之前，随机初始化k个聚类中心。初始化k个聚类中心的过程如程序清单10-4所示。</p>
<p><strong>程序清单10-4 随机初始化4个不同的聚类中心</strong></p>
<p><img src="Image00476.jpg" alt></p>
<p>在程序清单 10-4 中，函数 randCent 用于随机初始化k个聚类中的方法，假设样本维数是2，初始化聚类中心的方法为：找到每一维数据上的最小值 min 和最大值max，生成在此区间范围内的随机值，生成随机值的公式如下所示：</p>
<p><img src="Image00565.jpg" alt></p>
<h4 id="10-3-3-聚类过程"><a href="#10-3-3-聚类过程" class="headerlink" title="10.3.3 聚类过程"></a>10.3.3 聚类过程</h4><p>在K-Means算法中，聚类的核心实现部分是kmeans函数，如程序清单10-5所示。</p>
<p><strong>程序清单10-5 kmeans函数</strong></p>
<p><img src="Image00683.jpg" alt></p>
<p><img src="Image00772.jpg" alt></p>
<p>在程序清单10-5中，kmeans函数是K- Means算法的核心程序，程序的输入是训练数据，聚类中心的个数k和初始化的k个聚类中心，其输出是最终的k个聚类中心和每个样本所属的类别。在while循环中判断每个节点所在的类别是否变化，若不变则退出。在计算每个样本所属的类别的过程中，是判断样本与每个聚类中心之间的相似度，在程序清单10-5中使用欧式距离的平方作为其相似性的度量方法，如程序中的①所示。将每个样本重新划分到每个类别中，需要重新计算k个聚类中心的坐标，计算的方法是每个类别中的坐标的均值，如程序中的②所示。</p>
<h4 id="10-3-4-最终的聚类结果"><a href="#10-3-4-最终的聚类结果" class="headerlink" title="10.3.4 最终的聚类结果"></a>10.3.4 最终的聚类结果</h4><p>最终的结果需要保存到对应的文件中，需要保存的结果有每个样本所属的类别和所有的聚类中心，保存文件的save_result函数如程序清单10-6所示。</p>
<p><strong>程序清单10-6 save_result函数</strong></p>
<p><img src="Image00887.jpg" alt></p>
<p>在程序清单10-6中，函数save_result的输入为数据保存的文件名file_name和所需要保存的数据source，最终将source中的数据写入到file_name文件中。</p>
<p>程序的运算过程为：</p>
<p><img src="Image00088.jpg" alt></p>
<p>当运行完成后，最终的聚类结果如图10.2所示。</p>
<p><img src="Image00779.jpg" alt></p>
<p>图10.2 K-Means聚类结果</p>
<p>在图10.2中，“+”代表的是4个不同的聚类中心，这4个聚类中心的具体值为：</p>
<p><img src="Image00428.jpg" alt></p>
<h3 id="10-4-K-Means-算法"><a href="#10-4-K-Means-算法" class="headerlink" title="10.4 K-Means++算法"></a>10.4 K-Means++算法</h3><h4 id="10-4-1-K-Means算法存在的问题"><a href="#10-4-1-K-Means算法存在的问题" class="headerlink" title="10.4.1 K-Means算法存在的问题"></a>10.4.1 K-Means算法存在的问题</h4><p>由于 K-Means 算法简单且易于实现，因此 K-Means算法得到了很多的应用，但是从上述的K- Means算法的实现过程发现，K-Means算法中的聚类中心的个数k需要事先指定，这一点对于一些未知数据存在很大的局限性。其次，在利用K- Means算法进行聚类之前，需要初始化k个聚类中心，在上述的K- Means算法的过程中，使用的是在数据集中随机选择最大值和最小值之间的数作为其初始的聚类中心，但是聚类中心选择不好，对于K- Means算法有很大的影响，如选取的k个聚类中心为：</p>
<p><img src="Image00037.jpg" alt></p>
<p>此时，得到最终的聚类中心为：</p>
<p><img src="Image00519.jpg" alt></p>
<p>最终的聚类效果如图10.3所示。</p>
<p><img src="Image00623.jpg" alt></p>
<p>图10.3 初始化不好情况下的聚类效果</p>
<p>为了解决因为初始化的问题带来 K-Means 算法的问题，改进的 K-Means 算法即K- Means++算法被提出，K-Means++算法主要是为了能够在聚类中心的选择过程中选择较优的聚类中心。</p>
<h4 id="10-4-2-K-Means-算法的基本思路"><a href="#10-4-2-K-Means-算法的基本思路" class="headerlink" title="10.4.2 K-Means++算法的基本思路"></a>10.4.2 K-Means++算法的基本思路</h4><p>K-Means++算法在聚类中心的初始化过程中的基本原则是使得初始的聚类中心之间的相互距离尽可能远，这样可以避免出现上述的问题。K-Means++算法的初始化过程如下。</p>
<p>• 在数据集中随机选择一个样本点作为第一个初始化的聚类中心；</p>
<p>• 选择出其余的聚类中心：</p>
<p>○ 计算样本中的每一个样本点与已经初始化的聚类中心之间的距离，并选择其中最短的距离，记为d i ；</p>
<p>○ 以概率选择距离最大的样本作为新的聚类中心，重复上述过程，直到k个聚类中心都被确定。</p>
<p>• 对k个初始化的聚类中心，利用K-Means算法计算最终的聚类中心。</p>
<p>在上述的 K-Means++算法中可知 K-Means++算法与 K-Means 算法最本质的区别是k个聚类中心的初始化过程，K-Means++算法的具体实现如程序清单10-7所示。</p>
<p><strong>程序清单10-7 K-Means++聚类中心的初始化</strong></p>
<p><img src="Image00729.jpg" alt></p>
<p><img src="Image00825.jpg" alt></p>
<p>在程序清单10-7中，函数get_centroids实现了K- Means++算法的k个聚类中心的初始化，在初始化的过程中，其具体过程为：①随机从样本中选择出一个样本作为第1 个聚类中心，如程序代码中的①所示；②初始化一个距离序列 d，用于保存每个样本点到已初始化的聚类中心之间的最短距离，如程序代码中的②所示；③对每个样本找到其最近的聚类中心，并将距离保存到距离序列d中，如程序代码中的③所示，函数nearest实现了最短距离的计算，具体的实现如程序清单10-8所示；④为选择出与当前的所有聚类中心距离最远的样本点，需要计算当前的所有距离之和 sum_all，如程序代码中的④所示；⑤随机选择所有和sum_all之间的随机数，如程序代码中的⑤所示；⑥找到最大的距离，如程序代码中的⑥所示，并将其作为新的聚类中心。</p>
<p><strong>程序清单10-8 nearest函数</strong></p>
<p><img src="Image00029.jpg" alt></p>
<p>在程序清单 10-8 中，函数 nearest 用于计算点 point 和聚类中心点集合cluster_centers之间的最短距离，并返回这个最短距离。在函数nearest中，首先定义了一个常量 FLOAT_MAX，用于初始化 min_dist，对于 FLOAT_MAX 的定义如程序代码中的①所示。在最短距离的计算过程中，使用到了K- Means中的distance函数，如程序代码中的②所示，distance函数的具体实现如程序清单10-1所示。</p>
<h4 id="10-4-3-K-Means-算法的过程和最终效果"><a href="#10-4-3-K-Means-算法的过程和最终效果" class="headerlink" title="10.4.3 K-Means++算法的过程和最终效果"></a>10.4.3 K-Means++算法的过程和最终效果</h4><p>由上述可知 K-Means++算法与 K-Means 算法唯一的不同是两者在聚类中心的初始化过程。为了使得 Python 能够支持中文的注释和利用 numpy，我们需要在“KMeanspp.py”文件的开始加入：</p>
<p><img src="Image00515.jpg" alt></p>
<p>在K- Means++算法中还需要使用到random模块中的random方法，同时，还需要使用到“KMeans.py”文件中的load_data函数、kmeans函数、distance函数和save_result函数，因此，在文件“KMeanspp.py”中需要分别导入：</p>
<p><img src="Image00262.jpg" alt></p>
<p>K-Means++算法的主函数如程序清单10-9所示。</p>
<p><strong>程序清单10-9 K-Means++算法的主函数</strong></p>
<p><img src="Image00670.jpg" alt></p>
<p>在程序清单10-9中，与K-Means算法的主函数的唯一区别为聚类中心的初始化过程，如程序代码中的①所示，具体的初始化过程如上所述。</p>
<p>对于K-Means++算法，其运行过程为：</p>
<p><img src="Image00474.jpg" alt></p>
<p>最终的聚类中心为：</p>
<p><img src="Image00562.jpg" alt></p>
<p>计算出来的最终的聚类中心与K-Means在正常情况下计算出来的聚类中心一致。最终的聚类效果如图10.2所示。</p>
<p><strong>参考文献</strong></p>
<p>[1] Bauckhage C.k-Means Clustering Is Matrix Factorization[J].Statistics,2015.</p>
<p>[2] Arthur D,Vassilvitskii S.k-means++:the advantages of careful seeding[C]//Eighteenth Acm-Siam Symposium on Discrete Algorithms,SODA 2007,New Orleans,Louisiana,Usa,January.2007:1027-1035.</p>
<p>[3] 李航.统计学习方法[M].北京：清华大学出版社.2012.</p>
<p>[4] 谢剑斌等.视觉机器学习20讲[M].北京：清华大学出版社.2015.</p>
<p>[5] 陈皓.深入浅出K- Means算法[DB/OL].http：//www.csdn.net/article/2012-07-03/2807073-k-means</p>
<p>[6] wikipedia.K-means++[DB/OL].<a href="https://en.wikipedia.org/wiki/K-means%2B%2B" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/K-means%2B%2B</a></p>
<p>[7] Peter Harrington.机器学习实战[M].王斌，译.北京：人民邮电出版社.2013.</p>
<h2 id="11-Mean-Shift"><a href="#11-Mean-Shift" class="headerlink" title="11 Mean Shift"></a>11 Mean Shift</h2><p>在K-Means算法中，最终的聚类效果受初始的聚类中心的影响，K-Means++算法的提出，为选择较好的初始聚类中心提供了依据，但是在K- Means算法中，聚类的类别个数k仍需要事先指定，对于类别个数未知的数据集，K-Means算法和K- Means++算法将很难对其进行精确求解，对此，有一些改进的算法被提出来处理聚类个数k未知的情形。</p>
<p>MeanShift算法，又被称为均值漂移算法，与K-Means算法一样，都是基于聚类中心的聚类算法，不同的是，Mean Shift算法不需要事先指定类别个数k。在Mean Shift算法中，聚类中心是通过在给定区域中的样本的均值来确定的，通过不断更新聚类中心，直到最终的聚类中心不再改变为止。Mean Shift算法在聚类，图像平滑、分割和视频跟踪等方面有广泛的应用。</p>
<h3 id="11-1-Mean-Shift向量"><a href="#11-1-Mean-Shift向量" class="headerlink" title="11.1 Mean Shift向量"></a>11.1 Mean Shift向量</h3><p>对于给定的n维空间R n 中的m个样本点X  （i） ，i=1，…，m，对于其中的一个样本X，其Mean Shift向量为：</p>
<p><img src="Image00677.jpg" alt></p>
<p>其中，S  h 指的是一个半径为h的高维球区域，如上图中的蓝色的圆形区域。S  h 的定义为：</p>
<p><img src="Image00768.jpg" alt></p>
<p>对于一个半径为h的圆S h ，其均值向量如图11.1所示。</p>
<p><img src="Image00880.jpg" alt></p>
<p>图11.1 均值漂移向量</p>
<p>在图11.1中，“o”表示的是最终的均值漂移点。在计算漂移均值向量的过程中，通过计算圆S h 中的每一个样本点X  （i） 相对于点X的偏移向量（X<br>（i） -X），再对所有的漂移均值向量求和然后再求平均。</p>
<p>如上的均值漂移向量的求解方法存在一个问题，即在S h 的区域内，每一个样本点X  （i） 对样本X的贡献是一样的。而在实际中，每一个样本点X  （i） 对于样本X的贡献是不一样的，这样的贡献可以通过核函数进行度量。</p>
<h3 id="11-2-核函数"><a href="#11-2-核函数" class="headerlink" title="11.2 核函数"></a>11.2 核函数</h3><p>在Mean Shift算法中引入核函数的目的是使得随着样本与被漂移点的距离不同，其漂移量对均值漂移向量的贡献也不同。核函数的定义如下：</p>
<p>设ℵ是输入空间（欧式空间R n 的子集或离散集合），又设H 为特征空间（希尔伯特空间），如果存在一个从ℵ到H 的映射：</p>
<p><img src="Image00082.jpg" alt></p>
<p>使得所有x 1 ，x 2 ∈ℵ，函数K（x 1 ，x 2 ）满足条件：</p>
<p><img src="Image00201.jpg" alt></p>
<p>则称K（x 1 ，x 2 ）为核函数， _φ_ （x）为映射函数。 _φ_ （x 1 ）· _φ_ （x 2 ）表示的是 _φ_ （x 1 ）和 _φ_<br>（x 2 ）的内积。</p>
<p>高斯核函数是使用较多的一种核函数，其函数形式为：</p>
<p><img src="Image00308.jpg" alt></p>
<p>其中，h称为带宽（bandwidth），不同带宽的核函数如图11.2所示。</p>
<p><img src="Image00416.jpg" alt></p>
<p>图11.2 高斯核函数</p>
<p>从图11.2中高斯核函数的图像可以看出，当带宽h一定时，样本点之间的距离越近，其核函数的值越大；当样本点之间的距离相等时，随着高斯核函数的带宽h的增大，核函数的值在减小。</p>
<p>现在，让我们利用Python实现高斯核函数，在高斯核函数中使用到了矩阵的相关运算，在实现之前，我们需要导入numpy：</p>
<p><img src="Image00695.jpg" alt></p>
<p>高斯核函数的程序代码如程序清单11-1所示。</p>
<p><strong>程序清单11-1 高斯核函数</strong></p>
<p><img src="Image00696.jpg" alt></p>
<p>在程序清单11-1中，高斯核函数gaussian_kernel的输入是样本之间的距离distance和指定的高斯核函数的带宽h，其输出是高斯核函数的值gaussian_val。</p>
<h3 id="11-3-Mean-Shift算法原理"><a href="#11-3-Mean-Shift算法原理" class="headerlink" title="11.3 Mean Shift算法原理"></a>11.3 Mean Shift算法原理</h3><h4 id="11-3-1-引入核函数的Mean-Shift向量"><a href="#11-3-1-引入核函数的Mean-Shift向量" class="headerlink" title="11.3.1 引入核函数的Mean Shift向量"></a>11.3.1 引入核函数的Mean Shift向量</h4><p>假设在半径为h的范围S  h 范围内，为了使得每一个样本点X （i） 对于样本X的贡献不一样，向基本的Mean Shift向量形式中增加核函数，得到如下的改进的Mean Shift向量形式：</p>
<p><img src="Image00700.jpg" alt></p>
<p>其中<img src="Image00702.jpg" alt> 是高斯核函数。通常，可以取S h 为整个数据集范围。</p>
<h4 id="11-3-2-Mean-Shift算法的基本原理"><a href="#11-3-2-Mean-Shift算法的基本原理" class="headerlink" title="11.3.2 Mean Shift算法的基本原理"></a>11.3.2 Mean Shift算法的基本原理</h4><p>在Mean Shift算法中，通过迭代的方式找到最终的聚类中心，即对每一个样本点计算其漂移均值，以计算出来的漂移均值点作为新的起始点，重复以上的步骤，直到满足终止的条件，得到的最终的均值漂移点即为最终的聚类中心。其具体的步骤如下：</p>
<p>• 步骤1：在指定的区域内计算每一个样本点的漂移均值，其过程如图11.3所示。</p>
<p><img src="Image00706.jpg" alt></p>
<p>图11.3 计算漂移均值</p>
<p>在图11.3中，“o”表示的是初始的样本点，“*”表示的是漂移均值点。</p>
<p>• 步骤2：移动该点到漂移均值点处，如图11.3所示，从原始样本点“o”处移动到漂移均值点“*”处。</p>
<p>• 步骤3：重复上述的过程（计算新的漂移均值，移动），如图11.4所示。</p>
<p><img src="Image00709.jpg" alt></p>
<p>图11.4 以漂移均值点作为新的起点</p>
<p>• 步骤4：当满足最终的条件时，则退出，最终的漂移均值点如图11.5所示。</p>
<p><img src="Image00712.jpg" alt></p>
<p>图11.5 每一次生成的均值漂移点</p>
<p>从上述过程可以看出，在Mean Shift算法中，最关键的就是计算每个点的偏移均值，然后根据新计算的偏移均值更新点的位置。</p>
<h3 id="11-4-Mean-Shift算法的解释"><a href="#11-4-Mean-Shift算法的解释" class="headerlink" title="11.4 Mean Shift算法的解释"></a>11.4 Mean Shift算法的解释</h3><p>在Mean Shift算法中，实际上利用了概率密度，求得概率密度的局部最优解。</p>
<h4 id="11-4-1-概率密度梯度"><a href="#11-4-1-概率密度梯度" class="headerlink" title="11.4.1 概率密度梯度"></a>11.4.1 概率密度梯度</h4><p>对一个概率密度函数 f （X），已知d维空间中n个采样点X （i） ，i=1，…，n，f （X）的核函数估计（也被称为Parzen窗估计）为：</p>
<p><img src="Image00716.jpg" alt></p>
<p>其中K （x）是一个单位核函数，K （x）可以表示为<img src="Image00720.jpg" alt> 。</p>
<p>概率密度函数 f （X）的梯度∇f （X）的估计为</p>
<p><img src="Image00722.jpg" alt></p>
<p>令g（x）=-k′（x），<img src="Image00693.jpg" alt> ，则有：</p>
<p><img src="Image00727.jpg" alt></p>
<p>其中，第一个方括号内是以G（X）为核函数对概率密度函数 f （X）的估计，记为<img src="Image00732.jpg" alt> ，第二个方括号中的是Mean Shift向量，则<img src="Image00735.jpg" alt> 可以表示为：</p>
<p><img src="Image00738.jpg" alt></p>
<p>则Mean Shift向量M  h （X）可以表示为：</p>
<p><img src="Image00740.jpg" alt></p>
<p>由上式可知，Mean Shift向量M  h （X）与概率密度函数 f （X）的梯度成正比，因此，Mean Shift向量总是指向概率密度增加的方向。</p>
<h4 id="11-4-2-Mean-Shift向量的修正"><a href="#11-4-2-Mean-Shift向量的修正" class="headerlink" title="11.4.2 Mean Shift向量的修正"></a>11.4.2 Mean Shift向量的修正</h4><p>对于Mean Shift向量，可以表示为：</p>
<p><img src="Image00742.jpg" alt></p>
<p>记：<img src="Image00744.jpg" alt> ，则上式变成：</p>
<p><img src="Image00747.jpg" alt></p>
<h4 id="11-4-3-Mean-Shift算法流程"><a href="#11-4-3-Mean-Shift算法流程" class="headerlink" title="11.4.3 Mean Shift算法流程"></a>11.4.3 Mean Shift算法流程</h4><p>Mean Shift算法的算法流程如下：</p>
<p>• 计算m h （X）；</p>
<p>• 令X=m h （X）；</p>
<p>• 如果<img src="Image00749.jpg" alt> ，结束循环，否则，重复上述步骤。</p>
<p>从Mean Shift算法流程可以看出，核心的部分是计算Mean Shift向量，其核心代码如程序清单11-2所示。</p>
<p><strong>程序清单11-2 训练过程</strong></p>
<p><img src="Image00752.jpg" alt></p>
<p>在程序清单11-2中，Mean Shift算法的训练过程函数train_mean_shift的输入是需要训练的样本和高斯核函数的带宽 h，其输出是均值漂移点 mean_shift_points，即聚类中心，每个样本所属的类别group。在指定的误差 MIN_DISTANCE范围内，计算每一个样本点的漂移均值，如程序中的①所示，生成漂移均值向量的函数shift_point如程序清单11-3所示。然后计算该样本点与漂移均值之间的距离，如程序中的②所示，函数euclidean_dist使用的是欧式距离的平方，函数的具体实现如程序清单11-4所示。最终，需要计算每个样本点所属的类别，主要的函数为 group_points，如程序中的③所示，函数的具体实现如程序清单11-5所示。</p>
<p><strong>程序清单11-3 生成漂移向量</strong></p>
<p><img src="Image00277.jpg" alt></p>
<p>程序清单11-3中实现了计算偏移均值的核心部分，函数shift_point的输入为需要求解偏移均值的样本，所有的样本点和高斯核函数的带宽h。首先计算需要求解偏移均值的样本与所有的样本点之间的距离，如程序中的①所示，函数euclidean_dist的实现如程序清单11-4所示。在计算完距离后，计算高斯核函数的值，如程序中的②所示，函数gaussian_kernel的实现如程序清单11-1所示。最终计算出该样本的漂移向量，如程序中的③所示。</p>
<p><strong>程序清单11-4 欧式距离</strong></p>
<p><img src="Image00028.jpg" alt></p>
<p>在程序清单11-4中，实现了欧式距离的计算。欧式距离函数euclidean_dist的输入是两个样本，输出是样本之间的距离。假设有两个点，分别为P和Q，其对应的坐标分别为：</p>
<p><img src="Image00760.jpg" alt></p>
<p>那么，点P和点Q之间的欧式距离定义为：</p>
<p><img src="Image00763.jpg" alt></p>
<p><strong>程序清单11-5 计算所属的类别</strong></p>
<p><img src="Image00766.jpg" alt></p>
<p><img src="Image00769.jpg" alt></p>
<p>程序清单 11-5 中的 group_points 主要用于计算每个样本点所属的类别。函数group_points的输入为计算出的漂移向量，输出为每个样本所属的类别。</p>
<h3 id="11-5-Mean-Shift算法实践"><a href="#11-5-Mean-Shift算法实践" class="headerlink" title="11.5 Mean Shift算法实践"></a>11.5 Mean Shift算法实践</h3><p>对于如图11.6所示的数据点，利用Mean Shift算法对其进行聚类的过程中，其计算过程为：①导入数据；②计算偏移向量；③保存最终的计算结果。</p>
<p><img src="Image00771.jpg" alt></p>
<p>图11.6 原始数据点</p>
<p>首先，为了使得 Python 能够支持中文的注释和利用 numpy，我们需要在“mean_shift.py”文件的开始加入：</p>
<p><img src="Image00774.jpg" alt></p>
<p>同时，在计算高斯核函数时，需要使用到math模块中的sqrt函数和pi函数，因此，需要在“mean_shift.py”文件中导入：</p>
<p><img src="Image00188.jpg" alt></p>
<h4 id="11-5-1-Mean-Shift的主过程"><a href="#11-5-1-Mean-Shift的主过程" class="headerlink" title="11.5.1 Mean Shift的主过程"></a>11.5.1 Mean Shift的主过程</h4><p>主函数如程序清单11-6所示。</p>
<p><strong>程序清单11-6 主函数</strong></p>
<p><img src="Image00782.jpg" alt></p>
<p>在程序清单11-6中，利用Mean Shift算法训练数据的主要过程为：①导入数据。load_data函数将训练数据导入，如程序代码中的①所示，load_data函数的具体实现如程序清单11-7所示；②利用Mean Shift算法对数据进行聚类，函数train_mean_shift实现对数据的聚类，如程序代码中的②所示，train_mean_shift函数的具体实现如程序清单11-2所示；③当训练完成后，保存每个样本所属的类别cluster 和最终的聚类中心shift_points，如程序代码中的③和④所示，函数save_result的具体实现如程序清单11-8所示。</p>
<p><strong>程序清单11-7 导入数据</strong></p>
<p><img src="Image00784.jpg" alt></p>
<p><img src="Image00475.jpg" alt></p>
<p>在程序清单11-7中，设置了一个全局常数MIN_DISTANCE，如程序代码中的①所示，表示的是最小的误差。</p>
<h4 id="11-5-2-Mean-Shift的最终聚类结果"><a href="#11-5-2-Mean-Shift的最终聚类结果" class="headerlink" title="11.5.2 Mean Shift的最终聚类结果"></a>11.5.2 Mean Shift的最终聚类结果</h4><p>程序的运行过程为：</p>
<p><img src="Image00048.jpg" alt></p>
<p>最终的聚类结果如图11.4所示。</p>
<p><img src="Image00482.jpg" alt></p>
<p>图11.7 最终的聚类结果</p>
<p>在图11.7中，“+”表示的是最终的漂移均值，从图11.7中可知，对于上述的数据集，可以划分成3个类别，且这3个聚类中心为：</p>
<p><img src="Image00484.jpg" alt></p>
<p>最终，需要将训练的结果保存，包括每一个样本所属的类别和聚类中心，save_result函数的具体实现如程序清单11-8所示。</p>
<p><strong>程序清单11-8 保存最终的结果</strong></p>
<p><img src="Image00486.jpg" alt></p>
<p>在程序清单11-8中，函数save_result将最终训练好的数据source保存到对应的文件中，其中source的数据类型是numpy.mat类型。</p>
<p><strong>参考文献</strong></p>
<p>[1] Cheng Y.Mean shift,mode seeking,and clustering[J].IEEE Transactions on Pattern Analysis&amp;Machine Intelligence,1995,17(8):790-799.</p>
<p>[2] Comaniciu D,Meer P.Mean shift:a robust approach toward feature space analysis[J].IEEE Transactions on Pattern Analysis&amp;Machine Intelligence,2002,24(5):603-619.</p>
<p>[3] 李航.统计学习方法[M].北京：清华大学出版社.2012.</p>
<h2 id="12-DBSCAN"><a href="#12-DBSCAN" class="headerlink" title="12 DBSCAN"></a>12 DBSCAN</h2><p>K-Means算法、K-Means++算法和Mean Shift算法都是基于距离的聚类算法，基于距离的聚类算法的聚类结果是球状的簇，当数据集中的聚类结果是非球状结构时，基于距离的聚类算法的聚类效果并不好，然而，基于密度的聚类算法能够较好地处理非球状结构的数据。与基于距离的聚类算法不同的是，基于密度的聚类算法可以发现任意形状的聚类。</p>
<p>在基于密度的聚类算法中，通过在数据集中寻找被低密度区域分离的高密度区域，将分离出的高密度区域作为一个独立的类别。DBSCAN （Density-Based Spatial Clustering of Application with Noise）是一种典型的基于密度的聚类算法。</p>
<h3 id="12-1-基于密度的聚类"><a href="#12-1-基于密度的聚类" class="headerlink" title="12.1 基于密度的聚类"></a>12.1 基于密度的聚类</h3><h4 id="12-1-1-基于距离的聚类算法存在的问题"><a href="#12-1-1-基于距离的聚类算法存在的问题" class="headerlink" title="12.1.1 基于距离的聚类算法存在的问题"></a>12.1.1 基于距离的聚类算法存在的问题</h4><p>K-Means算法，K-Means++算法和Mean Shift算法都是基于距离的聚类算法，当数据集中的聚类结果是球状结构时，基于距离的聚类算法能够得到比较好的结果，球状结构的聚类结果如图12.1所示。</p>
<p><img src="Image00488.jpg" alt></p>
<p>图12.1 球状结构的聚类</p>
<p>然而，除了上述的球状结构的聚类数据外，有一些数据集的聚类结果是非球状的结构，如图12.2所示。</p>
<p><img src="Image00490.jpg" alt></p>
<p>图12.2 非球状结构的聚类数据</p>
<p>利用K-Means++聚类算法对图12.2中的数据进行聚类，设置聚类中心的个数为2，得到如图12.3所示的聚类结果。</p>
<p><img src="Image00494.jpg" alt></p>
<p>图12.3 K-Means++算法的聚类结果</p>
<p>在图12.2中，“+”表示的是最终的两个聚类中心，由图12.2可知，对于图中的非球状结构的聚类数据，基于距离的K- Means++算法并不能得到正确的聚类结果。</p>
<p>利用Mean Shift聚类算法对图12.2中的数据进行聚类，设置高斯核函数中的h=1时，得到如图12.4所示的聚类结果。</p>
<p><img src="Image00498.jpg" alt></p>
<p>图12.4 Mean Shift算法的聚类结果</p>
<p>在图 12.4 中，“+”表示最终的聚类中心，与 K-Means++算法类似，基于距离的Mean Shift算法对图中的非球状聚类结构的数据也不能得到正确的聚类结果。</p>
<h4 id="12-1-2-基于密度的聚类算法"><a href="#12-1-2-基于密度的聚类算法" class="headerlink" title="12.1.2 基于密度的聚类算法"></a>12.1.2 基于密度的聚类算法</h4><p>从图12.2中，我们可以看出，数据点在图中呈现上下两个弧形，同时，分别在两个弧形中，数据点之间较为密集，而两个弧形彼此之间较为稀疏。由这样的现象，我们猜测是否存在一种方法能够利用样本之间的紧密程度对数据进行聚类？基于密度的聚类（Density- Based Clustering）便是这样一种利用数据之间的紧密程度来对样本进行聚类的算法。</p>
<h3 id="12-2-DBSCAN算法原理"><a href="#12-2-DBSCAN算法原理" class="headerlink" title="12.2 DBSCAN算法原理"></a>12.2 DBSCAN算法原理</h3><h4 id="12-2-1-DBSCAN算法的基本概念"><a href="#12-2-1-DBSCAN算法的基本概念" class="headerlink" title="12.2.1 DBSCAN算法的基本概念"></a>12.2.1 DBSCAN算法的基本概念</h4><p>DBSCAN（Density-Based Spatial Clustering of Application with Noise）是一种典型的基于密度的聚类算法，在DBSCAN算法中，有两个最基本的邻域参数，分别为 _ε_ 邻域和MinPts。其中 _ε_ 邻域表示的是在数据集D中与样本点x i 的距离不大于 _ε_ 的样本，即：</p>
<p><img src="Image00503.jpg" alt></p>
<p>样本点x i 的 _ε_ 邻域如图12.5所示。</p>
<p><img src="Image00257.jpg" alt></p>
<p>图12.5 x i 的 _ ε _ 邻域</p>
<p>在图 12.5 中，样本点x不在样本点x i 的 _ε_ 邻域内。x i 的密度可由x i 的 _ε_ 邻域内的点的数量来估计。MinPts表示的是在样本点x i 的 _ε_ 邻域内的最少样本点的数目。基于邻域参数 _ε_ 邻域和MinPts，在DBSCAN算法中将数据点分为以下三类：</p>
<p>• 核心点（Core Points）。若样本x i 的 _ε_ 邻域内至少包含了 MinPts 个样本，即<img src="Image00509.jpg" alt> ，则称样本点x i 为核心点。</p>
<p>• 边界点（Border Points）。若样本x i 的 _ε_ 邻域内包含的样本数目小于 MinPts，但是它在其他核心点的邻域内，则称样本点x i 为边界点。</p>
<p>• 噪音点（Noise）。指的是既不是核心点也不是边界点的点。</p>
<p>核心点、边界点和噪音点如图12.6所示。</p>
<p><img src="Image00511.jpg" alt></p>
<p>图12.6 核心点、边界点和噪音点</p>
<p>在图12.6中，设置MinPts的值为9，对应的样本点x 1 的 _ε_ 邻域内包含11个样本点，大于 MinPts，则样本点x 1 为核心点。样本点的x 2 在样本点x 1 的 _ε_ 邻域内，且样本点x 2 的 _ε_ 邻域内只包含8个样本点，小于MinPts，则样本点x 2 为边界点。样本点x 3 为噪音点。</p>
<p>在DBSCAN算法中，还定义了如下的一些概念：</p>
<p>• 直接密度可达（directly density-reachable）。若样本点x  j 在核心点x i 的 _ε_ 邻域内，则称样本点x  j 从样本点x i 直接密度可达。</p>
<p>• 密度可达（density-reachable）。若在样本点x i，1 和样本点x i，n 之间存在序列x i，2 ，…，x i，n-1 ，且x i，j+1 从x i，j 直接密度可达，则称x i，n 从x i，1 密度可达。由密度可达的定义可知，样本点x i，1 ，x i，2 ，…，x i，n-1 均为核心点，直接密度可达是密度可达的特殊情况。</p>
<p>• 密度连接（density-connected）。对于样本点x i 和样本点x  j ，若存在样本点x k ，使得x i 和x  j 都从x k 密度可达，则称x i 和x  j 密度相连。</p>
<p>直接密度可达、密度可达如图12.7所示。</p>
<p><img src="Image00513.jpg" alt></p>
<p>图12.7 直接密度可达与密度可达</p>
<p>在图12.7中，设置MinPts的值为9，则样本点x 1 和样本点x 2 为核心点，样本点x 3 为边界点。样本点x 2 在核心点x 1 的 _ε_ 邻域内，则样本点x 2 从样本点x 1 直接密度可达；样本点x 3 在核心点x 2 的 _ε_ 邻域内，则样本点x 3 从核心点x 2 直接密度可达；在样本点x 1 和x 3 之间存在样本点x 2 ，且样本点x 2 从样本点x 1 直接密度可达，则样本点x 3 从样本点x 1 密度可达。</p>
<h4 id="12-2-2-DBSCAN算法原理"><a href="#12-2-2-DBSCAN算法原理" class="headerlink" title="12.2.2 DBSCAN算法原理"></a>12.2.2 DBSCAN算法原理</h4><p>基于密度的聚类算法通过寻找被低密度区域分离的高密度区域，并将高密度区域作为一个聚类“簇”。在DBSCAN算法中，聚类“簇”定义为：由密度可达关系导出的最大的密度连接样本的集合。</p>
<p>若x为核心对象，由x密度可达的所有样本组成的集合记为：</p>
<p><img src="Image00516.jpg" alt></p>
<p>则X满足连接性和最大性的簇。</p>
<h4 id="12-2-3-DBSCAN算法流程"><a href="#12-2-3-DBSCAN算法流程" class="headerlink" title="12.2.3 DBSCAN算法流程"></a>12.2.3 DBSCAN算法流程</h4><p>在 DBSCAN 算法中，由核心对象出发，找到与该核心对象密度可达的所有样本形成一个聚类“簇”。DBSCAN算法的算法流程为：</p>
<p>• 根据给定的邻域参数 _ε_ 和MinPts确定所有的核心对象</p>
<p>• 对每一个核心对象</p>
<p>• 选择一个未处理过的核心对象，找到由其密度可达的样本生成聚类“簇”</p>
<p>• 重复以上过程</p>
<p>现在，让我们利用 Python 实现 DBSCAN 算法的核心部分，即 dbscan 函数。在dbscan函数中，需要用到矩阵的相关计算，因此，我们需要导入numpy模块：</p>
<p><img src="Image00520.jpg" alt></p>
<p>DBSCAN算法的具体实现如程序清单12-1所示。</p>
<p><strong>程序清单12-1 DBSCAN算法</strong></p>
<p><img src="Image00523.jpg" alt></p>
<p><img src="Image00527.jpg" alt></p>
<p><img src="Image00528.jpg" alt></p>
<p>在程序清单12-1中，函数dbscan是DBSCAN算法的主要部分。dbscan函数的输入为需要聚类的数据集data、半径的大小eps和半径内的最少的数据点的个数MinPts。dbscan函数的输出为每个节点的类型types和每个节点所属的类别sub_class。在dbscan函数中，首先初始化一些参数，包括：①每个样本所属的类型types，其中，在types中，1为核心点，0为边界点，-1为噪音点，如程序代码中的①所示；②每个样本所属的类别sub_class，如程序代码中的②所示；③每个样本是否被处理过的dealed，如程序代码中的③所示；④样本之间的距离的dis，如程序代码中的④所示，函数distance的具体实现如程序清单12-2所示。</p>
<p>在完成初始化后，对每一个样本，区分其是核心点、边界点还是噪音点。对于核心点，找到其密度可达的点，如程序代码中的⑤所示。函数find_eps找到与指定样本之间的距离小于或等于eps的样本的小标，find_eps的具体实现如程序清单12-2所示。</p>
<p><strong>程序清单12-2 find_eps函数</strong></p>
<p><img src="Image00530.jpg" alt></p>
<p>在程序清单12-2中，函数find_eps找到与指定样本之间的距离小于或等于eps的样本的下标，函数find_eps的输入为指定样本与其他所有样本之间的距离distance_D和指定的半径的大小 eps。函数的输出为与指定样本的距离小于或等于 eps 的样本的下标，并将所有下标保存到ind中。</p>
<p>对于图12.2所示的非球状结构的聚类数据，DBSCAN算法得到的结果如图12.8所示。</p>
<p>从图12.8中可以看出，利用DBSCAN算法可以完全将非球状的聚类数据正确划分。对比图12.3中K-Means++算法的聚类结果和图12.4中Mean Shift算法的聚类结果，基于密度的DBSCAN算法能够处理任意形状的聚类问题。</p>
<p><img src="Image00532.jpg" alt></p>
<p>图12.8 利用DBSCAN处理的非球状的聚类数据</p>
<h3 id="12-3-DBSCAN算法实践"><a href="#12-3-DBSCAN算法实践" class="headerlink" title="12.3 DBSCAN算法实践"></a>12.3 DBSCAN算法实践</h3><p>我们以图12.9所示的数据为例，利用DBSCAN算法对其进行聚类操作：</p>
<p><img src="Image00535.jpg" alt></p>
<p>图12.9 原始数据点</p>
<p>首先，为了使得Python能够支持中文的注释和利用numpy，我们需要在“dbscan.py”文件的开始加入：</p>
<p><img src="Image00538.jpg" alt></p>
<p>同时，在epsilon函数中，需要使用到math模块中的gamma函数和sqrt函数，因此，在“dbscan.py”文件中需要加入：</p>
<p><img src="Image00542.jpg" alt></p>
<h4 id="12-3-1-DBSCAN算法的主要过程"><a href="#12-3-1-DBSCAN算法的主要过程" class="headerlink" title="12.3.1 DBSCAN算法的主要过程"></a>12.3.1 DBSCAN算法的主要过程</h4><p>对于上述的数据，利用 DBSCAN 算法对其进行聚类的过程中，其计算过程为：①导入数据；②利用 DBSCAN 算法进行聚类；③保存最终的计算结果，主函数如程序清单12-3所示。</p>
<p><strong>程序清单12-3 主函数</strong></p>
<p><img src="Image00026.jpg" alt></p>
<p>在程序清单 12-3 中，利用 DBSCAN 算法训练数据的主要过程为：①导入数据。load_data函数将训练数据导入，如程序代码中的①所示，load_data函数的具体实现如程序清单12-4所示；②计算出最佳的半径值eps。epsilon函数计算出数据集中最佳的半径大小，如程序代码中的②所示，epsilon 函数的具体实现如程序清单 12-5 所示；③利用DBSCAN算法对数据进行聚类，函数dbscan实现对数据的聚类，如程序代码中的③所示，dbscan 函数的具体实现如程序清单 12-1 所示；④当训练完成后，保存每个样本的类型 types 和每个样本所属的类别 cluster，如程序代码中的④和⑤所示，函数save_result的具体实现如程序清单12-6所示。</p>
<p><strong>程序清单12-4 导入数据</strong></p>
<p><img src="Image00548.jpg" alt></p>
<p>在程序清单 12-4 中，在导入数据之前，首先设置了一个全局常数 MinPts，常数MinPts表示的是半径内的最少的数据点的个数，如程序代码中的①所示。然后，利用函数load_data导入需要聚类的数据。</p>
<p><strong>程序清单12-5 求最佳半径eps</strong></p>
<p><img src="Image00552.jpg" alt></p>
<p>在程序清单12-5中，函数epsilon计算出样本中的最佳的半径大小，函数epsilon的输入为数据集data和半径内的数据点的个数MinPts，函数epsilon的输出为半径eps。半径eps的具体的计算过程如程序代码中的①所示。</p>
<h4 id="12-3-2-Mean-Shift的最终聚类结果"><a href="#12-3-2-Mean-Shift的最终聚类结果" class="headerlink" title="12.3.2 Mean Shift的最终聚类结果"></a>12.3.2 Mean Shift的最终聚类结果</h4><p>程序的运行过程为：</p>
<p><img src="Image00555.jpg" alt></p>
<p>在运行过程中，通过函数 epsilon 设置 _ε_ 值为 1.384，最终的聚类结果如图 12.10所示。</p>
<p><img src="Image00470.jpg" alt></p>
<p>图12.10 最终的聚类结果</p>
<p>在图12.10中，“*”表示的是噪音点，从图12.10中可知，对于上述的数据集，可以划分成 4 个类别。最终，需要将训练的结果保存，包括每一个样本所属的类别，save_result函数的具体实现如程序清单12-6所示。</p>
<p><strong>程序清单12-6 保存最终结果的save_result函数</strong></p>
<p><img src="Image00561.jpg" alt></p>
<p><img src="Image00564.jpg" alt></p>
<p>在程序清单12-6中，函数save_result将需要保存的数据data保存到file_name指定的文件中。其中，函数save_result的输入为需要保存到的文件名file_name和需要保存的数据data，data为numpy.mat格式。</p>
<p>DBSCAN算法的聚类结果与 _ε_ 的取值有关，若 _ε_ 的取值太小，则聚类结果中划分为噪音点的数量变多，如取 _ε_ =0.8时，聚类结果如图12.11所示。</p>
<p><img src="Image00838.jpg" alt></p>
<p>图12.11 _ ε _ =0.8时的聚类结果</p>
<p>在图12.11中，划分为噪音点的数量增多，当继续减小时，噪音点的数量会继续增多，最终会导致大多数样本点都被划为噪音点。若 _ε_ 的取值太大，则聚类结果中划分到噪音点的数量会减少，如取 _ε_ =1.5时，聚类结果如图12.12所示。</p>
<p><img src="Image00041.jpg" alt></p>
<p>图12.12 _ ε _ =1.5时的聚类结果</p>
<p>在图12.12中，被划分为噪音点的数量在减少，当继续增大 _ε_ 的值时，类的数量会减少。以上两种情况都是由于 _ε_ 取值的问题导致的。因此，对于 DBSCAN 算法，要得到正确有效的聚类结果，需要设置较为合适的 _ε_ 值。</p>
<p><strong>参考文献</strong></p>
<p>[1] Ester M,Kriegel H P,Sander J,et al.A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise[C]//2008:226—231.</p>
<p>[2] M.Daszykowski,B.Walczak,D.L.Massart,Looking for Natural Patterns in Data.Part 1:Density Based Approach,Chemometrics and Intelligent Laboratory Systems 56 (2001) 83-92</p>
<p>[3] 周志华.机器学习[M].北京：清华大学出版社.2016.</p>
<h2 id="13-Label-Propagation"><a href="#13-Label-Propagation" class="headerlink" title="13 Label Propagation"></a>13 Label Propagation</h2><p>近年来随着社交网络的发展，先后出现了以Facebook、Twitter为代表的社交网站，国内的社交网站如新浪微博等。人们可以在这些社交网站上进行交流，分享各自的想法，社交网络的发展对信息的传播起到了推波助澜的作用。以微博为例，用户在微博上，可以关注感兴趣的人，同样也会被其他人关注，可以发表各自的观点，也可以转发其他人的博文，在这些数据的背后，不仅存在用户之间的社交关系，还存在着用户之间的兴趣关系。为了能够对社交网站上的用户进行聚类，可以使用社区划分的算法对用户进行聚类。</p>
<p>Label Propagation算法是基于标签传播的社区划分算法，在基于标签传播的社区划分算法中，将网络中的边看作是个体之间的信息传播，其传播的结果通常使得社团内部节点之间共享相同的信息。</p>
<h3 id="13-1-社区划分"><a href="#13-1-社区划分" class="headerlink" title="13.1 社区划分"></a>13.1 社区划分</h3><h4 id="13-1-1-社区以及社区划分"><a href="#13-1-1-社区以及社区划分" class="headerlink" title="13.1.1 社区以及社区划分"></a>13.1.1 社区以及社区划分</h4><p>随着对各种网络结构的研究，人们发现在各类网络中存在社区结构，社区结构指的是在网络中，由一些节点构成特定的分组，在同一个分组内的节点通过节点，之间的连接边紧密地连接在一起，而在分组与分组之间，其连接比较松散，称每一个分组为一个社区。由上可知社区是网络中节点的集合，这些节点内部连接较为紧密而外部连接较为稀疏。</p>
<p>在社交网络中，每一个用户相当于一个节点，用户之间通过互相的关注关系构成了用户之间的社交关系，用户之间通过转发感兴趣的微博，构成了用户之间的兴趣关系。通过将用户划分到不同的社区中，每一个社区具有一些不同的属性以区分其他社区，如兴趣，在该社区内部的节点之间有较为紧密的连接，而在任意两个社区之间，由于两个社区之间的属性不一样，如一个社区的兴趣为电影，另一个社区的兴趣为体育，这两个社区之间的相对连接则较为稀疏。具有两个社区的社区结构如图13.1所示。</p>
<p><img src="Image00152.jpg" alt></p>
<p>图13.1 社区结构</p>
<p>在图13.1中，节点之间通过边的连接构成了整个网络，整个网络被划分成了两个部分，分别为社区A和社区B，其中，社区A或者社区B的内部连接较为紧密，而在社区A和社区B之间的连接则较为稀疏。</p>
<h4 id="13-1-2-社区划分的算法"><a href="#13-1-2-社区划分的算法" class="headerlink" title="13.1.2 社区划分的算法"></a>13.1.2 社区划分的算法</h4><p>假设在网络中，每一个样本点只能属于一个社区，这样的问题称为非重叠社区划分。在非重叠社区划分中，典型的方法主要有基于模块度优化的社区划分算法，基于谱分析的社区划分算法，基于信息论的社区划分算法和基于标签传播的社区划分算法。</p>
<p>在基于模块度优化的社区划分算法中，其基本思想是将社区划分问题转化为对模块度函数的优化，其中，模块度是度量社区划分质量的重要标准。在实际的求解过程中，由于模块度函数不能直接求解，通常采用近似的方式对其进行求解，根据求解方法的不同，主要分为如下的几种方法：</p>
<p>1.凝聚方法（Agglomerative Method）。通过不断合并不同社区，实现对整个网络的社区划分，典型的方法有Newman快速算法、CNM算法和MSG-MV算法；</p>
<p>2.分裂方法（Divisive Method）。通过不断删除网络中边来实现对整个网络的社区划分，典型的方法有Newman提出的GN算法；</p>
<p>3.直接近似求解模块度函数（Optimization Method）。通过优化算法直接对模块度函数进行求解，典型的方法有EO算法。</p>
<h4 id="13-1-3-社区划分的评价标准"><a href="#13-1-3-社区划分的评价标准" class="headerlink" title="13.1.3 社区划分的评价标准"></a>13.1.3 社区划分的评价标准</h4><p>社区划分是将网络划分成多个不同的社区，在每一个社区的内部，其连接比较紧密，在社区与社区之间的连接较为稀疏。根据这样形式化的定义，社区划分的结果会有很多种，为了能够度量社区划分的质量，Newman等人提出了模块度（Modularity）的概念，模块度是一个在[-1，1]区间上的实数，其通过比较每一个社区内部的连接密度和社区与社区之间的连接密度，去度量社区划分的质量。若将连接比较稠密的点划分在一个社区中，这样模块度的值会变大，最终，模块度最大的划分是最优的社区划分。</p>
<h3 id="13-2-Label-Propagation算法原理"><a href="#13-2-Label-Propagation算法原理" class="headerlink" title="13.2 Label Propagation算法原理"></a>13.2 Label Propagation算法原理</h3><h4 id="13-2-1-Label-Propagation算法的基本原理"><a href="#13-2-1-Label-Propagation算法的基本原理" class="headerlink" title="13.2.1 Label Propagation算法的基本原理"></a>13.2.1 Label Propagation算法的基本原理</h4><p>Label Propagation算法是一种基于标签传播的局部社区划分算法。对于网络中的每一个节点，在初始阶段，Label Propagation算法对每个节点初始化一个唯一的标签，在每次的迭代过程中，每个节点根据与其相连的节点所属的标签改变自己的标签，更改的原则是选择与其相连的节点中所属标签最多的社区标签为自己的社区标签，这便是标签传播的含义。随着社区标签不断传播，最终，连接紧密的节点将有共同的标签。</p>
<p>Label Propagation算法最大的优点是其算法逻辑比较简单，相比于优化模块度的过程，算法速度非常快。Label Propagation算法利用网络自身的结构指导标签的传播过程，在这个过程中无需优化任何函数。在算法开始前我们不必知道社区的个数，随着算法的迭代，在最终的过程中，算法将自己决定社区的个数。</p>
<p>对于Label Propagation算法，假设对于节点x，其邻居节点为x 1 ，x 2 ，…x k ，对于每一个节点，都有其对应的标签，标签代表的是该节点所属的社区。在算法迭代的过程中，节点x根据其邻居节点更新其所属的社区。</p>
<p>在初始阶段，令每一个节点都属于唯一的社区，当社区的标签在节点间传播时，紧密相连的节点迅速取得一致的标签。具体过程如图13.2所示。</p>
<p><img src="Image00269.jpg" alt></p>
<p>图13.2 标签传播</p>
<p>在图13.2中所示的标签传播的过程中，对于c节点，在选择了与a节点一致的标签后，与d节点相邻的节点中，属于a社区的节点最多，因此c节点的标签也被设置成 a，这样的过程不断持续下去，直到所有可能聚集到一起的节点都具有了相同的社区标签，此时，图13.2中的所有节点的标签都变成了a。在传播过程的最终，具有相同社区标签的节点被划到相同的社区中成为一个个独立的社区。</p>
<h4 id="13-2-2-标签传播"><a href="#13-2-2-标签传播" class="headerlink" title="13.2.2 标签传播"></a>13.2.2 标签传播</h4><p>在标签传播的过程中，节点的标签是根据其邻接节点的标签进行更新的，如在图13.2中，与节点c相邻的节点分别为a、b和d。节点的标签的更新过程可以分为两种，即：</p>
<p>• 同步更新</p>
<p>• 异步更新</p>
<p>同步更新是指对于节点x，在第t代时，根据其所有邻居节点在第t-1代时的社区标签，对其标签进行更新。即：</p>
<p><img src="Image00719.jpg" alt></p>
<p>其中，C x （t）表示的是节点x在第t代时的社区标签。函数 f 表示的是取的参数节点中所有社区个数最大的社区。同步更新的方法存在一个问题，即对于一个二分或者近似二分的网络来说，这样的结构会导致标签的震荡，如图13.3所示。</p>
<p><img src="Image00480.jpg" alt></p>
<p>图13.3 标签震荡</p>
<p>在图13.3中，在第一步的更新中，若左侧节点的标签更改为a，右侧节点的标签更改为b，在第二步中，左侧的节点又会更改为b，右侧的节点又会更改为a，如此往复，两边的标签会在社区标签a和b间不停地震荡。</p>
<p>对于异步更新方式，其更新公式为：</p>
<p><img src="Image00862.jpg" alt></p>
<p>其中，邻居节点x i1 ，…，x im 的社区标签在第t代已经更新过，则使用其最新的社区标签。而邻居节点x i（m+1） ，…，x ik 在第t代时还没有更新，则对于这些邻居节点还是用其在第（t-1）代时的社区标签。</p>
<p>现在让我们一起使用Python构建Label Propagation算法的异步更新过程，实现的具体过程如程序清单13-1所示，对于节点的更新顺序可以顺序选择。</p>
<p><strong>程序清单13-1 异步更新方式</strong></p>
<p><img src="Image00502.jpg" alt></p>
<p>在代码清单13-1中，函数label_propagation是整个Label Propagation算法的核心部分，其输入为节点以及其所属的社区vector_dict和节点之间的边的信息edge_dict，其输出为最终计算出来的节点以及其所属的社区 vector_dict。在迭代的过程中，首先会判断是否需要迭代，如代码中的①所示，其具体实现见13.2.3节。在迭代的过程中，对每一个节点进行更新，如代码中的②所示。在对该节点计算的过程中，找到与其相连接的节点所属社区最多的社区作为其社区，如代码中的③所示，函数get_max_community_label的具体实现如代码清单13-2所示。</p>
<p><strong>程序清单13-2 修正社区的get_max_community_label函数</strong></p>
<p><img src="Image00775.jpg" alt></p>
<p>在程序清单13-2中，函数get_max_community_label的输入为所有的节点及其所属 社 区 vector_dict 和 节 点 node 的 邻 接 节 点 adjacency_node_list，函 数get_max_community_label的输出为节点node所属的社区。通过统计其邻接节点中每一个节点所属的社区，如代码中的①所示，再将社区按照出现的次数进行降序排列，如代码中的②所示，最终返回出现次数最多的社区作为node节点的社区。</p>
<h4 id="13-2-3-迭代的终止条件"><a href="#13-2-3-迭代的终止条件" class="headerlink" title="13.2.3 迭代的终止条件"></a>13.2.3 迭代的终止条件</h4><p>在迭代过程中，当网络中的每个节点所属的社区不再改变时，此时迭代过程便可以停止。但是在Label Propagation算法中，当某个节点的邻居节点中存在两个或者多个最大的社区标签时，如图13.4所示。</p>
<p><img src="Image00892.jpg" alt></p>
<p>图13.4 迭代的终止条件</p>
<p>节点a与社区A和社区B都有连接，且与社区A的连接权重为2，与社区B的连接权重也为 2。对于节点 a，其所属的社区是随机选取的，此时可以选择社区 A，也可以选择社区 B，因此对于节点 a，其所属的社区会一直改变，这样就不能满足如上的跌倒终止条件。</p>
<p>对于Label Propagation算法，为了避免如上情况，可对上述的迭代终止条件修改为：对于每一个节点，在其所有的邻居节点所属的社区中，其所属的社区标签是最大的，即：如果用C 1 ，…，C p 来表示社区标签，<img src="Image00093.jpg" alt> 表示节点i的所有邻居节点中社区标签为C j 的个数，则算法终止的条件为：对于每一个节点i，如果节点i的社区标签为C m ，则：</p>
<p><img src="Image00211.jpg" alt></p>
<p>这样的停止条件可以使得最终能够获得强壮的社区（Strong Community），但是社区并不是唯一的。终止条件判断的具体实现的代码如程序清单13-3所示。</p>
<p><strong>程序清单13-3 check函数</strong></p>
<p><img src="Image00321.jpg" alt></p>
<p><img src="Image00056.jpg" alt></p>
<p>在程序清单13-3中，函数check的作用是判断社区划分是否结束。check函数的输入为节点以及其所属的社区vector_dict和节点之间的边的信息edge_dict。对于每一个节点计算其邻接节点所属的社区，将邻接节点中所属社区最多的社区作为其新的社区，如代码中的①所示，判断当前所属社区与新的社区是否为同一社区，如代码中的②所示。</p>
<h3 id="13-3-Label-Propagation算法过程"><a href="#13-3-Label-Propagation算法过程" class="headerlink" title="13.3 Label Propagation算法过程"></a>13.3 Label Propagation算法过程</h3><p>Label Propagation算法的过程如下：</p>
<p>• 对网络中的每个节点初始化其所属社区标签，且每个节点所属的社区是唯一的，如对于节点x，初始化其社区标签为C x （0）=x；</p>
<p>• 设置代数t；</p>
<p>• 对于每个节点x∈X，其中X是所有节点的集合，令</p>
<p><img src="Image00584.jpg" alt></p>
<p>• 判断是否可以迭代结束，如果否，则设置t=t+1，重新遍历。</p>
<h3 id="13-4-Label-Propagation算法实践"><a href="#13-4-Label-Propagation算法实践" class="headerlink" title="13.4 Label Propagation算法实践"></a>13.4 Label Propagation算法实践</h3><p>对于图13.5所示的网络结构：</p>
<p><img src="Image00229.jpg" alt></p>
<p>图13.5 无向图的社区</p>
<p>利用Label Propagation算法对其进行社区划分，主要的流程包括：①获取数据集，并对数据进行处理；②利用Label Propagation算法对这些数据计算其所属社区；③保存最终的划分结果。</p>
<p>首先，为了使得 Python 能够支持中文的注释，我们需要在“lp.py”文件的开始加入：</p>
<p><img src="Image00734.jpg" alt></p>
<p>同时，在程序中需要用到string模块中的一些函数，因此，需要导入string模块：</p>
<p><img src="Image00830.jpg" alt></p>
<p>其算法的主函数如程序清单13-4所示。</p>
<p><strong>程序清单13-4 主函数</strong></p>
<p><img src="Image00033.jpg" alt></p>
<p>在程序清单13-4中，Label Propagation算法的主要步骤为：①导入数据，如程序中①的所示，其具体的实现如程序清单 13-5 中的函数 loadData 所示；②利用 label Propagation算法计算每个节点所属的社区，如程序中②的所示，函数label_propagation的具体实现如程序清单13-1所示；③将划分好的每个节点所属的社区保存到最终的文件中，如程序中③的所示，函数save_result的具体实现如程序清单13-6所示。</p>
<h4 id="13-4-1-导入数据"><a href="#13-4-1-导入数据" class="headerlink" title="13.4.1 导入数据"></a>13.4.1 导入数据</h4><p>对于图13.5所示的数据有如下的loadData函数，其具体实现如清单13-5所示。</p>
<p><strong>程序清单13-5 导入数据</strong></p>
<p><img src="Image00144.jpg" alt></p>
<p><img src="Image00265.jpg" alt></p>
<p>在程序清单13-5中，函数loadData的输入为数据的存储位置filePath，函数的输出为节点以及其所属社区vector_dict和节点之间的边和权重edge_dict。在存储节点之间的边和权重的过程中，若是在数据文件 filePath 中保存了权重，则使用其作为节点之间的权重，若没有保存，则统一设置默认的权重，如在程序清单13-5中设置的都是1，如程序中的①和②所示。</p>
<h4 id="13-4-2-社区的划分"><a href="#13-4-2-社区的划分" class="headerlink" title="13.4.2 社区的划分"></a>13.4.2 社区的划分</h4><p>社区划分的实现过程中使用到了 label_propagation 函数，具体实现如程序清单13-1所示。程序的运行过程如下所示：</p>
<p><img src="Image00694.jpg" alt></p>
<p><img src="Image00325.jpg" alt></p>
<h4 id="13-4-3-最终的结果"><a href="#13-4-3-最终的结果" class="headerlink" title="13.4.3 最终的结果"></a>13.4.3 最终的结果</h4><p>对于图13.5所示的无向图，其最终的划分结果如表13-1所示。</p>
<p>表13-1 最终的聚类结果</p>
<p><img src="Image00154.jpg" alt></p>
<p>从表13-1中可知，图13.5中的节点8，9，10，11，12，13，14，15被划分到同一个社区中，而节点 0，1，2，3，4，5，6，7 被划分到了另一个社区中，划分的最终的效果如图13.6所示。</p>
<p><img src="Image00681.jpg" alt></p>
<p>图13.6 最终的聚类效果</p>
<p><strong>参考文献</strong></p>
<p>[1] 骆志刚，丁凡，蒋晓舟，等.复杂网络社团发现算法研究新进展[J].国防科技大学学报，2011，33（1）：47-52.</p>
<p>[2] Raghavan U N,Albert R,Kumara S.Near linear time algorithm to detect community structures in large-scale networks.[J].Physical Review E Statistical Nonlinear&amp;Soft Matter Physics,2007,76(2)</p>
<p>[3] 骆志刚，丁凡，蒋晓舟，等.复杂网络社团发现算法研究新进展[J].国防科技大学学报，2011，33（1）：47-52.</p>
<p>[4] Newman M E.Fast algorithm for detecting community structure in networks.[J].Physical Review E Statistical Nonlinear&amp;Soft Matter Physics,2004,69(6):066133-066133.</p>
<p>[5] Newman M E,Girvan M.Finding and evaluating community structure in networks.[J].Physical Review E Statistical Nonlinear&amp;Soft Matter Physics,2010,69(2 Pt 2):026113-026113.</p>
<p>[6] Blondel V D,Guillaume J L,Lambiotte R,et al.Fast unfolding of communities in large networks[J].Journal of Statistical Mechanics Theory&amp;Experiment,2008,2008(10):155-168.</p>
<h1 id="第四部分-推荐算法"><a href="#第四部分-推荐算法" class="headerlink" title="第四部分 推荐算法"></a>第四部分 推荐算法</h1><p>在如今的大数据时代，数据呈现出爆炸式的增长，生活在其中的人们正饱受着信息过载带来的痛苦，推荐系统（Recommendation System，RS）的出现为用户提供了很多的便利，并为企业带来了很多实际的价值。</p>
<p>第14章将介绍协同过滤（Collaborative Filtering，CF）算法，协同过滤算法是最基本的推荐算法，在推荐系统的产生过程中起到了至关重要的作用。在协同过滤算法中，又可以分为基于用户的协同过滤算法和基于项的协同过滤算法；第 15 章将介绍基于矩阵分解的推荐算法，通过对用户商品矩阵进行分解，可以挖掘出隐含的信息；第 16 章将介绍基于图的推荐算法，用户与商品之间的关系不仅可以由矩阵来表示，也可以使用二部图的形式来表示，在二部图的表示形态下，可以使用基于图的推荐算法。</p>
<h2 id="14-协同过滤算法"><a href="#14-协同过滤算法" class="headerlink" title="14 协同过滤算法"></a>14 协同过滤算法</h2><p>随着互联网技术的发展，网络上的信息量在急剧上升，使得信息过载的问题变得尤为突出，当用户无明确信息需求时，用户无法从大量的信息中获取到感兴趣的信息，同时，信息量的急剧上升也导致了大量的信息被埋没，无法触达到一些潜在的用户。推荐系统（Recommendation System，RS）的出现被称为连接用户与信息的桥梁，一方面帮助用户从海量数据中找到感兴趣的信息，另一方面将有价值的信息传递给潜在的用户。</p>
<p>协同过滤（Collaborative Filtering，CF）算法是最基本的推荐算法，CF算法从用户的历史行为数据中挖掘出用户的兴趣，为用户推荐其感兴趣的项。根据挖掘方法的不同，协同过滤算法可以分为基于用户的（User- based）协同过滤算法和基于项的（Item-based）协同过滤算法。</p>
<h3 id="14-1-推荐系统的概述"><a href="#14-1-推荐系统的概述" class="headerlink" title="14.1 推荐系统的概述"></a>14.1 推荐系统的概述</h3><h4 id="14-1-1-推荐系统"><a href="#14-1-1-推荐系统" class="headerlink" title="14.1.1 推荐系统"></a>14.1.1 推荐系统</h4><p>在信息过载的时代，信息呈现出爆炸式增长，比如每天有大量的微博被创作和转发，信息量的爆炸式增长在给用户不断带来新的信息的同时，也增加了用户筛选的信息的难度，当用户有明确的需求时，比如需要查找“协同过滤算法”，可以利用搜索引擎，如谷歌、百度等，查找到相应的文章。</p>
<p>但是，并不是在任何情况下用户都是有明确需求的，例如在微博上，用户只是想打发时间，在首页中查看每一条微博，此时，用户并没有明确的目的，为了能够帮助用户筛选出一批他们可能感兴趣的信息，就需要分析出该用户的兴趣，从海量的信息中选择出与用户兴趣相似的信息，并将这些信息推荐给用户。推荐系统（Recommendation System，RS）正是在这样的背景下被提出，推荐算法根据用户的历史行为，挖掘出用户的喜好，并为用户推荐与其喜好相符的商品或者信息。推荐系统的任务就是能够连接信息与用户，帮助用户找到其感兴趣的信息，同时让一些有价值的信息能够触达到潜在的用户。</p>
<h4 id="14-1-2-推荐问题的描述"><a href="#14-1-2-推荐问题的描述" class="headerlink" title="14.1.2 推荐问题的描述"></a>14.1.2 推荐问题的描述</h4><p>推荐系统的核心问题是为用户推荐与其兴趣相似度比较高的商品。此时，需要一个函数 f （x），函数 f<br>（x）可以计算候选商品与用户之间的相似度，并向用户推荐相似度较高的商品。为了能够预测出函数 f<br>（x），可以利用到的历史数据主要有：用户的历史行为数据、与该用户相关的其他用户信息、商品之间的相似性、文本的描述等。</p>
<p>假设集合C表示所有的用户，集合S表示所有需要推荐的商品。函数 f 表示商品s到用户c之间的有效性的效用函数，例如：</p>
<p><img src="Image00770.jpg" alt></p>
<p>其中，R是一个全体的排序集合。对于每一个用户c∈C，希望从商品的集合中选择出商品，即s∈S，以使得应用函数 f 的值最大。</p>
<h4 id="14-1-3-推荐的常用方法"><a href="#14-1-3-推荐的常用方法" class="headerlink" title="14.1.3 推荐的常用方法"></a>14.1.3 推荐的常用方法</h4><p>在推荐系统中，常用的推荐算法主要有：</p>
<p>• 协同过滤的推荐（Collaborative Filtering Recommendation）</p>
<p>• 基于内容的推荐（Content-based Recommendation）</p>
<p>• 基于关联规则的推荐（Association Rule-based Recommendation）</p>
<p>• 基于效用的推荐（Utility-based Recommendation）</p>
<p>• 基于知识的推荐（Knowledge-based Recommendation）</p>
<p>• 组合推荐（Hybrid Recommendation）</p>
<h3 id="14-2-基于协同过滤的推荐"><a href="#14-2-基于协同过滤的推荐" class="headerlink" title="14.2 基于协同过滤的推荐"></a>14.2 基于协同过滤的推荐</h3><h4 id="14-2-1-协同过滤算法概述"><a href="#14-2-1-协同过滤算法概述" class="headerlink" title="14.2.1 协同过滤算法概述"></a>14.2.1 协同过滤算法概述</h4><p>协同过滤（Collaborative Filtering，CF）推荐算法是通过在用户的行为中寻找特定的模式，并通过该模式为用户产生有效推荐。它依赖于系统中用户的行为数据，例如通过用户阅读过一些书籍，并且对这些书籍产生过一些评价，利用这些评价推断出该用户的阅读偏好。</p>
<p>基于协同过滤的推荐算法的核心思想是：通过对用户历史行为数据的挖掘发现用户的偏好，基于不同的偏好对用户进行群组划分并推荐品味相似的项。在计算推荐结果的过程中，不依赖于项的任何附加信息或者用户的任何附加信息，只与用户对项的评分有关。</p>
<h4 id="14-2-2-协同过滤算法的分类"><a href="#14-2-2-协同过滤算法的分类" class="headerlink" title="14.2.2 协同过滤算法的分类"></a>14.2.2 协同过滤算法的分类</h4><p>为了能够为用户推荐与其品味相似的项，通常有两种方法：①通过相似用户进行推荐。通过比较用户之间的相似性，越相似表明两者之间的品味越相近，这样的方法被称为基于用户的协同过滤算法（User- based Collaborative Filtering）；②通过相似项进行推荐。通过比较项与项之间的相似性，为用户推荐与其打过分的项相似的项，这样的方法被称为基于项的协同过滤算法（Item- based Collaborative Filtering）。</p>
<p>在基于用户的协同过滤算法中，利用用户访问行为的相似性向目标用户推荐其可能感兴趣的项，如图14.1所示。</p>
<p><img src="Image00884.jpg" alt></p>
<p>图14.1 基于用户的协同过滤算法</p>
<p>在图14.1中，假设用户分别为u 1 、u 2 和u 3 ，其中，用户u 1 互动过的商品有i 1 和i 3 ，用户u 2 互动过的商品为i 2 ，用户u 3 互动过的商品有i 1 、i 3 和i 4 。通过计算，用户u 1 和用户u 3 较为相似，对于用户u 1 来说，用户u 3 互动过的商品i 4 是用户u 1 未互动过的，因此会为用户u 1 推荐商品i 4 。</p>
<p>在基于项的协同过滤算法中，根据所有用户对物品的评价，发现物品和物品之间的相似度，然后根据用户的历史偏好将类似的物品推荐给该用户，如图14.2所示。</p>
<p><img src="Image00086.jpg" alt></p>
<p>图14.2 基于项的协同过滤算法</p>
<p>在图14.2中，假设用户分别为u 1 、u 2 和u 3 ，其中，用户u 1 互动过的商品有i 1 和i 3 ，用户u 2 互动过的商品为i 1 、i 2 和i 3 ，用户u 3 互动过的商品有i 1 。通过计算，商品i 1 和商品i 3 较为相似，对于用户u 3 来说，用户u 1 互动过的商品i 3 是用户u 3 未互动过的，因此会为用户u 3 推荐商品i 3 。</p>
<h3 id="14-3-相似度的度量方法"><a href="#14-3-相似度的度量方法" class="headerlink" title="14.3 相似度的度量方法"></a>14.3 相似度的度量方法</h3><p>相似性的度量方法有很多种，不同的度量方法的应用范围也不一样。相似性的度量方法在不同的机器学习算法中都有应用，如应用在K-Means聚类算法中。</p>
<p>相似性的度量方法必须满足拓扑学中的度量空间的基本条件：假设d是度量空间M 上的度量：d：M×M →R，其中度量d满足：</p>
<p>• 非负性：d （x，y）≥0，当且仅当x=y时取等号；</p>
<p>• 对称性：d（x，y）=d（y，x）；</p>
<p>• 三角不等性：d（x，z）≤d（x，y）+d（y，z）。</p>
<p>本章主要介绍三种相似性的度量方法，分别为：欧氏距离、皮尔逊相关系数和余弦相似度。</p>
<h4 id="14-3-1-欧氏距离"><a href="#14-3-1-欧氏距离" class="headerlink" title="14.3.1 欧氏距离"></a>14.3.1 欧氏距离</h4><p>欧氏距离是使用较多的相似性的度量方法，在第10章的K-Means算法中使用欧氏距离作为样本之间的相似性的度量。</p>
<h4 id="14-3-2-皮尔逊相关系数（Pearson-Correlation）"><a href="#14-3-2-皮尔逊相关系数（Pearson-Correlation）" class="headerlink" title="14.3.2 皮尔逊相关系数（Pearson Correlation）"></a>14.3.2 皮尔逊相关系数（Pearson Correlation）</h4><p>在欧氏距离的计算中，不同特征之间的量级对欧氏距离的影响比较大，例如A=（0.05，1），B=（1，1）和C=（0.05，4），其中，A和B之间的欧氏距离为0.95，而A和C之间的欧氏距离为3，此时我们就不能很好地利用欧氏距离判断A和B，A和C之间的相似性的大小。而皮尔逊相关系数的度量方法对量级不敏感，其具体形式为：</p>
<p><img src="Image00205.jpg" alt></p>
<p>其中<img src="Image00313.jpg" alt> 表示向量X和向量Y内积，<img src="Image00420.jpg" alt> 表示向量X的范数。利用皮尔逊相关系数计算上述的相似性可得，A和B之间的皮尔逊相关系数为 0.5186，而A和C之间的皮尔逊相关系数为-0.4211。</p>
<h4 id="14-3-3-余弦相似度"><a href="#14-3-3-余弦相似度" class="headerlink" title="14.3.3 余弦相似度"></a>14.3.3 余弦相似度</h4><p>余弦相似度是文本相似度度量中使用较多的一种方法，对于两个向量X和Y，其对应的形式如下：</p>
<p><img src="Image00517.jpg" alt></p>
<p>其对应的余弦相似度为：</p>
<p><img src="Image00271.jpg" alt></p>
<p>其中，<img src="Image00289.jpg" alt> 表示的是向量X和向量Y的内积，<img src="Image00311.jpg" alt> 表示的是向量的范数。</p>
<p>现在，让我们一起利用Python实现余弦相似度的计算，在余弦相似度的计算过程中，需要用到矩阵的相关计算，因此需要导入numpy模块：</p>
<p><img src="Image00332.jpg" alt></p>
<p>余弦相似度的具体实现如程序清单14-1所示。</p>
<p><strong>程序清单14-1 余弦相似度</strong></p>
<p><img src="Image00344.jpg" alt></p>
<p>在程序清单14-1中，函数cos_sim用于计算行向量x和行向量y之间的余弦值，具体的计算方法如上述公式所示，对于任意矩阵，计算任意两个行向量之间的相似度的具体实现如程序清单14-2所示。</p>
<p><strong>程序清单14-2 相似度矩阵的计算</strong></p>
<p><img src="Image00377.jpg" alt></p>
<p>在程序清单14-2中，函数similarity计算矩阵data中任意两行之间的相似度，如程序代码中的①所示，并将最终的计算结果保存到矩阵 w中。相似度矩阵 w 是一个对称矩阵，而且在相似度矩阵中，约定自身的相似度的值为 0，如程序代码中的②所示。</p>
<h3 id="14-4-基于协同过滤的推荐算法"><a href="#14-4-基于协同过滤的推荐算法" class="headerlink" title="14.4 基于协同过滤的推荐算法"></a>14.4 基于协同过滤的推荐算法</h3><p>在基于协同过滤的推荐算法中，根据用户或者商品的相似性计算，可以将其分为基于用户的协同过滤算法和基于项的协同过滤算法。假设用户的数据如表14.1所示。</p>
<p>表14.1 用户-商品数据</p>
<p><img src="Image00400.jpg" alt></p>
<p>其中，U_1～U_5表示的是5个不同的用户，D_1～D_5表示的是5个不同的商品，这样便构成了用户- 商品矩阵。在该矩阵中，有用户对每一件商品的打分，其中“-”表示的是用户未对该商品进行打分。</p>
<h4 id="14-4-1-基于用户的协同过滤算法"><a href="#14-4-1-基于用户的协同过滤算法" class="headerlink" title="14.4.1 基于用户的协同过滤算法"></a>14.4.1 基于用户的协同过滤算法</h4><p>基于用户的协同过滤算法，是基于用户之间的相似性计算的。基于用户的协同过滤算法给用户推荐和他兴趣相似的其他用户喜欢的物品。对于表14.1中所示的用户—商品数据，将其转换成用户—商品矩阵：</p>
<p><img src="Image00315.jpg" alt></p>
<p>其中，用户对商品的打分数值大于或等于0，等于0表示用户未对该商品评分。利用程序清单14-2中的similarity函数计算用户- 商品矩阵中的用户之间的相似度，得到用户相似度矩阵：</p>
<p><img src="Image00438.jpg" alt></p>
<p>其中，用户相似度矩阵是一个对称矩阵，且其对角线上是全0。</p>
<p>计算完用户之间的相似度后，利用用户之间的相似度为用户中没有打分的项打分，其方法为：</p>
<p><img src="Image00518.jpg" alt></p>
<p>其中，N （i）表示的是对商品i打过分的用户的集合，如在表14.1中，对商品D_3打过分的用户有U_2、U_3和U_5。w u，v 表示的是用户u和用户v之间的相似度，r v，i 表示的是用户v对商品i的打分。打分的具体实现如程序清单14-3所示。</p>
<p><strong>程序清单14-3 基于用户的协同推荐</strong></p>
<p><img src="Image00481.jpg" alt></p>
<p><img src="Image00496.jpg" alt></p>
<p>在程序清单14-3中，函数user_based_recommend基于用户相似性为用户user推荐商品。在函数user_based_recommend中，主要分为3步：①找到用户user未互动的商品，存放到not_inter中，如程序代码中的①所示；②利用上述公式对没有互动过的商品进行打分，在打分的过程中，首先对用户user未互动过的每一个商品，找到对其互动过的用户，如程序代码中的②所示，再利用上述的打分公式对该商品打分，如程序代码中的③和④所示；③将打分的最终结果按照降序排序并返回，如程序代码中的⑤所示。</p>
<h4 id="14-4-2-基于项的协同过滤算法"><a href="#14-4-2-基于项的协同过滤算法" class="headerlink" title="14.4.2 基于项的协同过滤算法"></a>14.4.2 基于项的协同过滤算法</h4><p>在基于项的协同过滤算法中，是基于项之间的相似性计算的。对于表14.1中所示的用户-商品数据，将其转换成商品-用户矩阵：</p>
<p><img src="Image00514.jpg" alt></p>
<p>利用程序清单14-2中的similarity函数计算商品-用户矩阵中的商品之间的相似度，得到商品的相似度矩阵：</p>
<p><img src="Image00531.jpg" alt></p>
<p>其中，商品相似度矩阵是一个对称矩阵，且其对角线上是全0。</p>
<p>计算完成商品之间的相似度后，利用商品之间的相似度为用户中没有打分的项打分，其方法为：</p>
<p><img src="Image00140.jpg" alt></p>
<p>其中，I （u）表示的是用户u打过分的商品的集合，如在表14.1中，用户U_1打过分的商品为D_1、D_2和D_4。w i，j 表示的是商品i和商品 j之间的相似度，r j，u 表示的是用户u对商品 j的打分。打分的具体实现如程序清单14-4所示。</p>
<p><strong>程序清单14-4 基于项的协同推荐</strong></p>
<p><img src="Image00261.jpg" alt></p>
<p>在程序清单14-4中，函数item_based_recommend基于商品之间的相似性为用户user 推荐商品。在函数 item_based_recommend 中，主要分为 3 步，即：①找到用户user未互动的商品，存放到not_inter中，如程序代码中的①所示；②利用上述公式对没有互动过的商品进行打分，在打分的过程中，首先找到对user互动过的商品，如程序代码中的②所示，再利用上述的打分公式对该商品打分，如程序代码中的③和④所示；③将打分的最终结果按照降序进行排序并返回，如程序代码中的⑤所示。</p>
<h3 id="14-5-利用协同过滤算法进行推荐"><a href="#14-5-利用协同过滤算法进行推荐" class="headerlink" title="14.5 利用协同过滤算法进行推荐"></a>14.5 利用协同过滤算法进行推荐</h3><p>对于表 14.1 中的用户- 商品数据，在利用协同过滤算法进行推荐时，其基本过程包括：①导入数据；②利用基于用户的协同过滤算法或者基于项的协同过滤算法进行推荐。</p>
<h4 id="14-5-1-导入用户-商品数据"><a href="#14-5-1-导入用户-商品数据" class="headerlink" title="14.5.1 导入用户-商品数据"></a>14.5.1 导入用户-商品数据</h4><p>导入用户-商品数据的过程如程序清单14-5所示。</p>
<p><strong>程序清单14-5 导入用户-商品数据</strong></p>
<p><img src="Image00592.jpg" alt></p>
<p>在程序清单14-5中，函数load_data将用户- 商品文件file_path中的数据导入到矩阵data中。在导入过程中，打过分的项转换成浮点数，如程序代码中的①所示，未打过分的项保存为0，如程序代码中的②所示。</p>
<h4 id="14-5-2-利用基于用户的协同过滤算法进行推荐"><a href="#14-5-2-利用基于用户的协同过滤算法进行推荐" class="headerlink" title="14.5.2 利用基于用户的协同过滤算法进行推荐"></a>14.5.2 利用基于用户的协同过滤算法进行推荐</h4><p>有了以上的知识储备，现在，我们利用上面实现好的函数，实现基于用户的协同过滤算法，首先，为了使得Python能够支持中文的注释和使用矩阵的运算，我们需要在“user_based_recommend.py”文件的开始加入：</p>
<p><img src="Image00473.jpg" alt></p>
<p>在基于用户的协同过滤算法中，其主函数如程序清单14-6所示。</p>
<p><strong>程序清单14-6 基于用户的协同过滤算法的主函数</strong></p>
<p><img src="Image00560.jpg" alt></p>
<p>在程序清单14-6中，利用基于用户的协同过滤算法进行推荐，其基本步骤包括：①利用函数 load_data 导入用户商品数据，如程序代码中的①所示；②利用 similarity函数计算用户之间的相似性，如程序代码中的②所示；③利用函数user_based_recommend对用户user未打分的商品进行打分，并对其排序，如程序代码中的③所示；④根据最终的打分结果为用户user推荐top_k个商品，如程序代码中的④所示。函数top_k为用户推荐前k个打分最高的商品，函数top_k的具体实现如程序清单14-7所示。</p>
<p><strong>程序清单14-7 top_k函数</strong></p>
<p><img src="Image00661.jpg" alt></p>
<p><img src="Image00689.jpg" alt></p>
<p>在程序清单14-7中，函数top_k为用户推荐打分最高的k个商品，其输入为根据打分排好序的商品列表predict和需要推荐的商品个数k。</p>
<p>程序的运行过程如下所示：</p>
<p><img src="Image00707.jpg" alt></p>
<p>利用基于用户的协同过滤算法对user为0的用户的推荐结果为：</p>
<p><img src="Image00725.jpg" alt></p>
<p>由上述结果可知，user为0的用户未打分的商品为2和4，最终的打分是2号商品为5.1分，4号商品为2.22分。</p>
<h4 id="14-5-3-利用基于项的协同过滤算法进行推荐"><a href="#14-5-3-利用基于项的协同过滤算法进行推荐" class="headerlink" title="14.5.3 利用基于项的协同过滤算法进行推荐"></a>14.5.3 利用基于项的协同过滤算法进行推荐</h4><p>现在，我们利用上面实现好的函数，实现基于项的协同过滤算法，首先，为了使得 Python 能 够 支 持 中 文 的 注 释 和 使 用 矩 阵 的 运 算，我 们 需 要 在“item_based_recommend.py”文件的开始加入：</p>
<p><img src="Image00743.jpg" alt></p>
<p>同时，在基于项的协同过滤算法中，我们需要使用到“user_based_recommend.py”文件中实现好的 load_data 函数和 similarity 函数，因此，我们需要在“item_based_recommend.py”文件的开始加入：</p>
<p><img src="Image00758.jpg" alt></p>
<p>在基于项的协同过滤算法中，其主函数如程序清单14-8所示。</p>
<p><strong>程序清单14-8 基于项的协同过滤算法的主函数</strong></p>
<p><img src="Image00776.jpg" alt></p>
<p>在程序清单14-7中，利用基于用户的协同过滤算法进行推荐，其基本步骤包括：①利用函数 load_data 导入用户商品数据，如程序代码中的①所示，并将其转换成商品用户矩阵，如程序代码中的②所示；②利用similarity函数计算商品之间的相似性，如程序代码中的③所示；③利用函数 item_based_recommend 对用户user 进行推荐，如程序代码中的④所示；④根据最终的打分结果为用户user推荐top_k个商品，如程序代码中的⑤所示。</p>
<p>程序的运行过程如下所示：</p>
<p><img src="Image00795.jpg" alt></p>
<p>利用基于用户的协同过滤算法对user为0的用户的推荐结果为：</p>
<p><img src="Image00819.jpg" alt></p>
<p>由上述结果可知，user为0的用户未打分的商品为2和4，最终的打分是2号商品为5.5分，4号商品为2.8分。</p>
<p><strong>参考文献</strong></p>
<p>[1] 项亮.推荐系统实践[M].北京：人民邮电出版社.2012.</p>
<p>[2] Peter Harrington.机器学习实战[M].王斌，译.北京：人民邮电出版社.2013.</p>
<p>[3] 刘知远.大数据智能[M].北京：电子工业出版社.2016.</p>
<p>[4] tirozhang.协同过滤算法[DB/OL].https：//segmentfault.com/a/1190000004022134</p>
<h2 id="15-基于矩阵分解的推荐算法"><a href="#15-基于矩阵分解的推荐算法" class="headerlink" title="15 基于矩阵分解的推荐算法"></a>15 基于矩阵分解的推荐算法</h2><p>在基于用户或者基于项的协同过滤推荐算法中，基于用户与用户或者项与项之间的相关性来推荐不同的项。为了能够对指定的用户进行推荐，需要计算用户之间或者项之间的相关性，这样的过程计算量比较大，同时难以实现大数据量下的实时推荐。</p>
<p>基于模型的协同过滤算法有效地解决了实时推荐的问题，在基于模型的协同过滤算法中，利用历史的用户- 商品数据训练得到模型，并利用该模型实现实时推荐。矩阵分解（Matrix Factorization，MF）是基于模型的协同过滤算法中的一种。</p>
<h3 id="15-1-矩阵分解"><a href="#15-1-矩阵分解" class="headerlink" title="15.1 矩阵分解"></a>15.1 矩阵分解</h3><p>假设用户-商品数据如表14.1所示，将其转换成用户—商品矩阵R m×n ，则R m×n 为：</p>
<p><img src="Image00845.jpg" alt></p>
<p>其中矩阵中的“-”表示的是未打分项。矩阵分解是指将一个矩阵分解成两个或者多个矩阵的乘积。对于上述的用户-商品矩阵R m×n ，可以将其分解成两个或者多个矩阵的乘积，假设分解成两个矩阵P m×k 和Q k×n ，我们要使得矩阵P m×k 和Q k×n 的乘积能够还原原始的矩阵R m×n ：</p>
<p><img src="Image00865.jpg" alt></p>
<p>其中，矩阵P m×k 表示的是m×k的矩阵，而矩阵Q k×n 表示的是k×n的矩阵，k是隐含的参数。</p>
<h3 id="15-2-基于矩阵分解的推荐算法"><a href="#15-2-基于矩阵分解的推荐算法" class="headerlink" title="15.2 基于矩阵分解的推荐算法"></a>15.2 基于矩阵分解的推荐算法</h3><p>矩阵分解（Matrix Factorization，MF）算法属于基于模型的协同过滤算法，在基于模型的协同过滤算法中，主要分为：①建立模型；②利用训练好的模型进行推荐。在基于矩阵分解的推荐算法中，上述的两步分别为：①对用户商品矩阵分解；②利用分解后的矩阵预测原始矩阵中的未打分项。</p>
<h4 id="15-2-1-损失函数"><a href="#15-2-1-损失函数" class="headerlink" title="15.2.1 损失函数"></a>15.2.1 损失函数</h4><p>在基于矩阵分解的推荐算法中，首先需要建立模型，即：将原始的评分矩阵R m×n 分解成两个矩阵P m×k 和Q k×n 的乘积：</p>
<p><img src="Image00893.jpg" alt></p>
<p>为了能够求解矩阵P m×k 和Q k×n 的每一个元素，可以利用原始的评分矩阵R m×n 与重新构建的评分矩阵<img src="Image00483.jpg" alt> 之间的误差的平方作为损失函数，即：</p>
<p><img src="Image00501.jpg" alt></p>
<p>最终，需要求解所有的非“-”项的损失之和的最小值：</p>
<p><img src="Image00664.jpg" alt></p>
<h4 id="15-2-2-损失函数的求解"><a href="#15-2-2-损失函数的求解" class="headerlink" title="15.2.2 损失函数的求解"></a>15.2.2 损失函数的求解</h4><p>对于上述的平方损失函数的最小值，可以通过梯度下降法求解，梯度下降法的核心步骤如下所示：</p>
<p>• 求解损失函数的负梯度：</p>
<p><img src="Image00533.jpg" alt></p>
<p>• 根据负梯度的方向更新变量：</p>
<p><img src="Image00554.jpg" alt></p>
<p>通过迭代，直到算法最终收敛。</p>
<h4 id="15-2-3-加入正则项的损失函数即求解方法"><a href="#15-2-3-加入正则项的损失函数即求解方法" class="headerlink" title="15.2.3 加入正则项的损失函数即求解方法"></a>15.2.3 加入正则项的损失函数即求解方法</h4><p>通常在求解的过程中，为了能够有较好的泛化能力，会在损失函数中加入正则项，以对参数进行约束，加入L 2 正则的损失函数为：</p>
<p><img src="Image00572.jpg" alt></p>
<p>利用梯度下降法的求解过程为：</p>
<p>• 求解损失函数的负梯度：</p>
<p><img src="Image00597.jpg" alt></p>
<p>• 根据负梯度的方向更新变量：</p>
<p><img src="Image00622.jpg" alt></p>
<p><img src="Image00642.jpg" alt></p>
<p>通过迭代，直到算法最终收敛。</p>
<p>现在，让我们一起利用 Python 实现矩阵分解的 gradAscent 函数，在实现矩阵分解的过程中，需要使用到矩阵的相关运算，因此需要导入numpy模块：</p>
<p><img src="Image00666.jpg" alt></p>
<p>进行矩阵分解的具体过程如程序清单15-1所示。</p>
<p><strong>程序清单15-1 矩阵分解</strong></p>
<p><img src="Image00607.jpg" alt></p>
<p><img src="Image00717.jpg" alt></p>
<p>在程序清单15-1中，函数gradAscent将输入的用户商品矩阵dataMat分解成矩阵p和矩阵q。函数gradAscent的输入为用户商品矩阵dataMat，分解后矩阵的维数k，梯度下降法中的学习率alpha，正则化参数beta和迭代过程的最大迭代次数maxCycles。在求解的过程中，首先初始化矩阵p和矩阵q，如程序代码中的①和②所示。在训练的过程中，利用梯度下降法不断修改矩阵中的参数，如程序代码中的③和④所示。在每次迭代的过程中，需要计算损失函数的值，如程序代码中的⑤所示，当此时损失函数的值小于某个阈值时，则退出循环，如程序代码中的⑥所示。最终，函数gradAscent输出矩阵p和矩阵q。</p>
<h4 id="15-2-4-预测"><a href="#15-2-4-预测" class="headerlink" title="15.2.4 预测"></a>15.2.4 预测</h4><p>利用上述的过程，我们可以得到矩阵P m×k 和Q k×n ，此时，模型便建立好了。在基于矩阵分解的推荐算法中需要为指定的用户进行推荐其未打分的项，若要计算用户i对商品 j的打分，则计算的方法为：</p>
<p><img src="Image00728.jpg" alt></p>
<p>为用户user预测的具体实现如程序清单15-2所示。</p>
<p><strong>程序清单15-2 为用户user进行推荐</strong></p>
<p><img src="Image00745.jpg" alt></p>
<p><img src="Image00124.jpg" alt></p>
<p>在程序清单15-2中，函数prediction为用户user未打分的项打分，并返回按照打分降序排列的推荐列表。</p>
<h3 id="15-3-利用矩阵分解进行推荐"><a href="#15-3-利用矩阵分解进行推荐" class="headerlink" title="15.3 利用矩阵分解进行推荐"></a>15.3 利用矩阵分解进行推荐</h3><h4 id="15-3-1-利用梯度下降对用户商品矩阵分解和预测"><a href="#15-3-1-利用梯度下降对用户商品矩阵分解和预测" class="headerlink" title="15.3.1 利用梯度下降对用户商品矩阵分解和预测"></a>15.3.1 利用梯度下降对用户商品矩阵分解和预测</h4><p>有了以上的理论分析，现在，我们一起实现矩阵分解和预测的整个过程，首先，在整个计算过程中，我们需要用到矩阵的相关运算，同时，为了使得Python能够支持中文注释，我们在“mf.py”文件开始加入：</p>
<p><img src="Image00780.jpg" alt></p>
<p>在利用基于矩阵分解的推荐算法的过程中，其主函数如程序清单15-3所示。</p>
<p><strong>程序清单15-3 主函数</strong></p>
<p><img src="Image00351.jpg" alt></p>
<p><img src="Image00824.jpg" alt></p>
<p>在主函数中，为了实现对用户商品矩阵的分解和预测，主要的步骤为：①导入用户商品矩阵，如程序代码中的①所示，函数load_data的具体形式如程序清单15-4所示；②利用梯度下降法对用户商品矩阵进行分解，如程序代码中的②所示，函数gradAscent的具体形式如程序清单15-1所示，在得到分解后的矩阵p和矩阵q后，保存这两个矩阵，如程序代码中的③和④所示，函数 save_file 的具体形式如程序清单15-5所示；③为用户user进行打分，得到按照打分降序的推荐列表predict，如程序代码中的⑤所示，函数prediction的具体实现如程序清单15-2所示；④从推荐列表predict中得到top_k推荐，如程序代码中的⑥所示，函数top_k的具体实现如程序清单15-6所示。</p>
<p><strong>程序清单15-4 导入数据</strong></p>
<p><img src="Image00848.jpg" alt></p>
<p>在程序清单 15-4 中，load_data 函数将用户商品数据导入到矩阵中，在数据中存在没有打分的项，如表15.1所示的“-”，在导入矩阵的过程中，用0代替，如程序代码中的①所示。</p>
<p><strong>程序清单15-5 保存结果</strong></p>
<p><img src="Image00870.jpg" alt></p>
<p>在程序清单15-5中，save_file函数将source中的结果保存到file_name指定的文件中。</p>
<p><strong>程序清单15-6 top_k推荐</strong></p>
<p><img src="Image00897.jpg" alt></p>
<p>在top_k函数中，从排好序的商品列表predict中选择前k个作为最终的推荐结果。</p>
<h4 id="15-3-2-最终的结果"><a href="#15-3-2-最终的结果" class="headerlink" title="15.3.2 最终的结果"></a>15.3.2 最终的结果</h4><p>在利用梯度下降法对矩阵进行分解和预测的过程中，其运行的过程如下所示：</p>
<p><img src="Image00579.jpg" alt></p>
<p>最终分解后的矩阵p为：</p>
<p><img src="Image00225.jpg" alt></p>
<p>矩阵q为：</p>
<p><img src="Image00051.jpg" alt></p>
<p>其中，利用分解后的矩阵p和矩阵q为用户user推荐的结果为：</p>
<p><img src="Image00294.jpg" alt></p>
<p>该推荐结果表示的是用户可能会对2号商品的打分为4.3538544776255756分，首先为用户推荐2号商品。</p>
<h3 id="15-4-非负矩阵分解"><a href="#15-4-非负矩阵分解" class="headerlink" title="15.4 非负矩阵分解"></a>15.4 非负矩阵分解</h3><p>通常在矩阵分解的过程中，需要分解后的矩阵的每一项都是非负的，即：</p>
<p><img src="Image00097.jpg" alt></p>
<p>这便是非负矩阵分解（Non-negtive Matrix Factorization，NMF）的来源。</p>
<h4 id="15-4-1-非负矩阵分解的形式化定义"><a href="#15-4-1-非负矩阵分解的形式化定义" class="headerlink" title="15.4.1 非负矩阵分解的形式化定义"></a>15.4.1 非负矩阵分解的形式化定义</h4><p>上面简单介绍了非负矩阵分解的基本含义，非负矩阵分解是在矩阵分解的基础上对分解完成的矩阵加上非负的限制条件，即对用户-商品矩阵R m×n ，找到两个矩阵P m×k 和Q k×n ，使得：</p>
<p><img src="Image00504.jpg" alt></p>
<p>同时要求：</p>
<p><img src="Image00600.jpg" alt></p>
<h4 id="15-4-2-损失函数"><a href="#15-4-2-损失函数" class="headerlink" title="15.4.2 损失函数"></a>15.4.2 损失函数</h4><p>为了能够定量比较矩阵R m×n 和矩阵<img src="Image00163.jpg" alt> 的近似程度，除了上述的平方损失函数，还可以使用KL散度：</p>
<p><img src="Image00802.jpg" alt></p>
<p>其中，在KL散度的定义中，<img src="Image00216.jpg" alt> ，当且仅当A=B时，取等号。</p>
<p>当定义好损失函数后，需要求解的问题就变成了如下的形式：</p>
<p><img src="Image00238.jpg" alt></p>
<h4 id="15-4-3-优化问题的求解"><a href="#15-4-3-优化问题的求解" class="headerlink" title="15.4.3 优化问题的求解"></a>15.4.3 优化问题的求解</h4><p>为了保证在求解的过程中P≥0，Q≥0，可以使用乘法更新规则（Multiplicative Update Rules），具体的操作如下：</p>
<p>对于平方距离的损失函数：</p>
<p><img src="Image00448.jpg" alt></p>
<p>对于KL散度的损失函数：</p>
<p><img src="Image00539.jpg" alt></p>
<p>上述的乘法规则主要是为了在计算的过程中保证非负，而基于梯度下降的方法中，加减运算无法保证非负，其实上述的乘法更新规则与基于梯度下降的算法是等价的，下面以平方距离为损失函数说明上述过程的等价性。</p>
<p>平方损失函数可以写成：</p>
<p><img src="Image00649.jpg" alt></p>
<p>使用损失函数对Q r，j 求偏导数：</p>
<p><img src="Image00868.jpg" alt></p>
<p>则按照梯度下降法的思路：</p>
<p><img src="Image00543.jpg" alt></p>
<p>即：</p>
<p><img src="Image00175.jpg" alt></p>
<p>令<img src="Image00691.jpg" alt> ，即可以得到上述的乘法更新规则的形式。训练的过程如程序清单15-7所示。</p>
<p><strong>程序清单15-7 非负矩阵分解</strong></p>
<p><img src="Image00285.jpg" alt></p>
<p><img src="Image00853.jpg" alt></p>
<p>在程序清单15-7中，函数train实现了非负矩阵的分解，函数train的输入为用户评分矩阵V、分解后矩阵的维数r、最大的迭代次数maxCycles和误差e，函数train的输出为分解后的矩阵 W 和 H。在函数非负矩阵的分解过程中，首先是初始化分解后的矩阵 W 和 H，如程序代码中的①和②所示。在完成初始化后，利用乘法规则对其进行训练，具体训练的过程如程序代码中的③和④所示。</p>
<h3 id="15-5-利用非负矩阵分解进行推荐"><a href="#15-5-利用非负矩阵分解进行推荐" class="headerlink" title="15.5 利用非负矩阵分解进行推荐"></a>15.5 利用非负矩阵分解进行推荐</h3><h4 id="15-5-1-利用乘法规则进行分解和预测"><a href="#15-5-1-利用乘法规则进行分解和预测" class="headerlink" title="15.5.1 利用乘法规则进行分解和预测"></a>15.5.1 利用乘法规则进行分解和预测</h4><p>在利用乘法规则进行非负矩阵分解的过程中，需要使用的头文件如程序清单15-8所示。</p>
<p><strong>程序清单15-8 头文件</strong></p>
<p><img src="Image00492.jpg" alt></p>
<p>在程序清单15-8中，需要用到矩阵分解程序中的load_data函数和save_file函数，如程序代码中的①所示。函数 load_data 导入用户商品矩阵，其具体实现如程序清单15-4所示，函数save_file将最终的结果保存到对应的文件中，其具体实现如程序清单15-5所示，函数prediction通过分解后的矩阵得到指定用户的推荐列表，其具体实现如程序清单15-2所示，函数top_k根据计算出的推荐列表选择前k个作为最终的推荐结果，其具体实现如程序清单15-6所示。</p>
<p>非负矩阵分解的主函数如程序清单15-9所示。</p>
<p><strong>程序清单15-9 非负分解主函数</strong></p>
<p><img src="Image00586.jpg" alt></p>
<p><img src="Image00609.jpg" alt></p>
<p>在主函数中，为了实现对用户商品矩阵的非负矩阵分解和预测，主要的步骤为：①导入用户商品矩阵，如程序代码中的①所示，函数 load_data 的具体形式如程序清单15-4所示；②利用乘法规则对用户商品矩阵进行非负矩阵分解，如程序代码中的②所示，函数train的具体形式如程序清单15-7所示，在得到分解后的矩阵W和矩阵H后，保存这两个矩阵，如程序代码中的③和④所示，函数save_file的具体形式如程序清单 15-5 所示；③对指定的用户计算其推荐列表，如程序代码中的⑤所示，函数prediction的具体实现如程序清单15-2所示；④根据推荐列表prediction得到top_k的推荐结果，如程序代码中的⑥所示，函数top_k的具体实现如程序清单15-6所示。</p>
<h4 id="15-5-2-最终的结果"><a href="#15-5-2-最终的结果" class="headerlink" title="15.5.2 最终的结果"></a>15.5.2 最终的结果</h4><p>利用非负矩阵分解对用户商品矩阵分解的过程中，其运行的过程如下所示：</p>
<p><img src="Image00280.jpg" alt></p>
<p>最终分解后的矩阵W为：</p>
<p><img src="Image00786.jpg" alt></p>
<p>矩阵H为：</p>
<p><img src="Image00436.jpg" alt></p>
<p>其中，利用分解后的矩阵W和矩阵H为用户user推荐的结果为：</p>
<p><img src="Image00232.jpg" alt></p>
<p><strong>参考文献</strong></p>
<p>[1] 刘知远.大数据智能[M].北京：电子工业出版社.2016.</p>
<p>[2] 项亮.推荐系统实践[M].北京：人民邮电出版社.2012.</p>
<p>[3] Lee D D,Seung H S.Algorithms for Non-negative Matrix Factorization[C]//NIPS.2001:556—562.</p>
<p>[4] winone361.基于矩阵分解的推荐算法[DB/OL].http：//blog.csdn.net/winone361/article/details/50705752</p>
<p>[5] tuicool.基于矩阵分解的推荐算法，简单入门- kobeshow[DB/OL].http：//www.tuicool.com/articles/BnEJ7n</p>
<h2 id="16-基于图的推荐算法"><a href="#16-基于图的推荐算法" class="headerlink" title="16 基于图的推荐算法"></a>16 基于图的推荐算法</h2><p>在推荐系统中，用户-商品数据可以转换成用户-商品矩阵的存储形式，利用前两章的协同过滤算法或者基于矩阵分解的方法为实现推荐的功能。同时，用户- 商品数据可以转换成二部图的存储形式，其中，在转化后的用户-商品二部图中，两个子集V 1 和V 2 分别为用户节点的集合和商品节点的集合。</p>
<p>PersonalRank 算法是计算图中节点相对于某个节点的重要性的算法，利用PersonalRank 算法可以计算所有其他节点相对于用户（user）节点的重要性，从而实现为用户（user）推荐。</p>
<h3 id="16-1-二部图与推荐算法"><a href="#16-1-二部图与推荐算法" class="headerlink" title="16.1 二部图与推荐算法"></a>16.1 二部图与推荐算法</h3><h4 id="16-1-1-二部图"><a href="#16-1-1-二部图" class="headerlink" title="16.1.1 二部图"></a>16.1.1 二部图</h4><p>在许多实际问题中常用到二部图，常见的二部图如图16.1所示。</p>
<p><img src="Image00335.jpg" alt></p>
<p>图16.1 二部图</p>
<p>二部图是无向图的一种，若无向图G=V，E 中，其中，V 是无向图中顶点的集合，E是无向图中边的集合。在无向图G中，边的集合V 可以分成两个子集V 1 和V 2 ，且满足：</p>
<p>• V=V 1 ∪V 2 ,V 1 ∩V 2 =∅</p>
<p>• ∀e=（u，v）∈E，均有u∈V 1 ，v∈V 2</p>
<p>则称无向图G为二部图（Bipartite Graph），V 1 和V 2 称为互补顶点子集。特别的，如果V 1 中的每个顶点都与V 2 中的所有顶点邻接，则称G为完全二部图（Complete Bipartite Graph）。</p>
<h4 id="16-1-2-由用户商品矩阵到二部图"><a href="#16-1-2-由用户商品矩阵到二部图" class="headerlink" title="16.1.2 由用户商品矩阵到二部图"></a>16.1.2 由用户商品矩阵到二部图</h4><p>在推荐算法中，通常利用用户的行为，如用户对商品的打分，如表16.1所示。</p>
<p>表16.1 用户-商品数据</p>
<p><img src="Image00220.jpg" alt></p>
<p>其中，用户U对商品D的打分范围为：{1，2，…，5}，“-”表示的是未打分。在基于协同过滤的推荐算法中，通常将上述的用户-商品数据转换成如下所示的用户- 商品矩阵：</p>
<p><img src="Image00739.jpg" alt></p>
<p>其中，未打分的项“-”用0表示。</p>
<p>对于表16.1中的用户-商品数据，可以由上述的二部图表示，表示的形式如图16.2所示。</p>
<p><img src="Image00379.jpg" alt></p>
<p>图16.2 用户商品的二部图表示</p>
<p>在图16.2所示的二部图中，左侧是用户节点的集合{U 1 ，U 2 ，…，U 5 }，右侧是商品节点的集合{D 1 ，D 2 ，…，D 5 }。用户节点U i 和商品节点D j 之间的边表示的是用户U i 对商品D j 有过打分行为。</p>
<p>在推荐系统中，其最终的目的是为用户U i 推荐相关的商品，此时，对于用户U i ，需要计算商品列表{D 1 ，D 2 ，…，D 5<br>}中的商品对其重要性程度，并根据重要性程度生成最终的推荐列表。PageRank算法是用于处理图上的重要性排名的算法。</p>
<h3 id="16-2-PageRank算法"><a href="#16-2-PageRank算法" class="headerlink" title="16.2 PageRank算法"></a>16.2 PageRank算法</h3><h4 id="16-2-1-PageRank算法的概念"><a href="#16-2-1-PageRank算法的概念" class="headerlink" title="16.2.1 PageRank算法的概念"></a>16.2.1 PageRank算法的概念</h4><p>PageRank算法，即网页排名算法，是由佩奇和布林在1997年提出来的链接分析算法。PageRank是用来标识网页的等级、重要性的一种方法，是衡量一个网页的重要指标。PageRank 算法在谷歌的搜索引擎中对网页质量的评价起到了重要的作用，在PageRank 算法提出之前，已经有人提出使用网页的入链数量进行链接分析，但是PageRank算法除了考虑入链数量之外，还参考了网页质量因素，通过组合入链数量和网页质量因素两个指标，使得网页重要性的评价更加准确。</p>
<p>在搜索引擎中，有些网页为了使得自己的的排名能够靠前，想出了很多的方法来作弊，这样的作弊被称为链接作弊（Link Spam）。简单来讲，对于上述的仅考虑入链数量的网页级别算法，有人为了使自己的网页（称为网页A）的级别更高，便做了很多的网页以使得这些网页都指向网页A，这样网页A的入链数量就会变得很多，自然，网页A 的网页级别就会变得很高，但是这样的网页A 的高级别只是所有者作弊的结果，并不等价于网页的质量就很好。</p>
<p>PageRank算法可以避免这样的情况，因为PageRank值同时考虑了入链数量和网页质量，入链数量之前已经说了，然而，对于自己作弊生成的指向网页A的网页质量本身就很低，这样综合的结果是网页 A 的网页质量也不会变得很高，这样 PageRank算法就对网页质量的评价起到了很好的效果。网页的链接分析可以抽象成图模型，如图16.3所示。</p>
<p><img src="Image00746.jpg" alt></p>
<p>图16.3 有向图模型</p>
<p>在上图中，链接关系分别为：1—＞2，1—＞3，1—＞4，2—＞1，2—＞4，4—＞2，4—＞3。</p>
<h4 id="16-2-2-PageRank的两个假设"><a href="#16-2-2-PageRank的两个假设" class="headerlink" title="16.2.2 PageRank的两个假设"></a>16.2.2 PageRank的两个假设</h4><p>对于某个网页的PageRank的计算是基于以下两个假设：</p>
<p>• 数量假设。在 Web 图模型中，如果一个页面节点接收到的其他网页指向的链接数量越多，那么这个页面就越重要。即：链接到网页 A 的链接数越多，网页A越重要。</p>
<p>• 质量假设。指向页面 A 的入链的质量不同，质量高的页面会通过链接向其他的页面传递更多的权重，所以越是质量高的页面指向页面 A，则页面 A 越重要。即：链接到网页A的原网页越重要，则网页A也会越重要。</p>
<p>数量假设，简单来讲就是在互联网中，如果一个网页接收到的其他的网页指向的入链数量越多，那么这个页面就越重要。质量假设，简单来讲就是在互联网中，对于一个页面，越是质量高的网页指向该页面，则该页面越重要。PageRank算法很好地组合了这两个假设，使得对网页的重要性评价变得更加准确。</p>
<h4 id="16-2-3-PageRank的计算方法"><a href="#16-2-3-PageRank的计算方法" class="headerlink" title="16.2.3 PageRank的计算方法"></a>16.2.3 PageRank的计算方法</h4><p>利用PageRank算法计算节点的过程分别为：①将有向图转换成图的邻接矩阵M；②计算出链接概率矩阵；③计算概率转移矩阵；④修改概率转移矩阵；⑤迭代求解PageRank值。对于图16.3中的有向图模型，其邻接矩阵为：</p>
<p><img src="Image00685.jpg" alt></p>
<p>其中，邻接矩阵M 中的每一行代表的是每个节点的出链。</p>
<p>对上述的邻接矩阵M，计算其链接概率矩阵，即对出链进行归一化，得到链接概率矩阵M′：</p>
<p><img src="Image00146.jpg" alt></p>
<p>这样，即表示有多少概率链接到其他的点，如从节点 1 分别以概率<img src="Image00165.jpg" alt> 链接到节点2、节点3和节点4。对上述的网页链接概率矩阵M′求转置，即可得到概率转移矩阵P：</p>
<p><img src="Image00281.jpg" alt></p>
<p>概率转移矩阵P可以描述一个用户在网上的下一步的访问行为。若此时初始化用户对每一个网页节点的访问概率相等，即：</p>
<p><img src="Image00391.jpg" alt></p>
<p>则当该用户下一次访问各节点的概率为：</p>
<p><img src="Image00489.jpg" alt></p>
<p>但是，此时存在这样的一个问题，一个用户不可能一直按照链接进行操作，有时会重新进入新的页面，即以一定的概率按照转移概率浏览网页节点。在上述转移矩阵中加入跳出当前链接的概率 _α_ ，此时转移矩阵变为：</p>
<p><img src="Image00581.jpg" alt></p>
<p>通常取 _α_ =0.85。最终通过迭代公式：</p>
<p><img src="Image00698.jpg" alt></p>
<p>求解PageRank值，当v′和v的误差在一定的范围内，即为最终的PageRank值。对于如图16.3所示的网络结构，最终的PageRank值为：</p>
<p><img src="Image00787.jpg" alt></p>
<p>对于上述的PageRank算法，其计算公式可以表示为：</p>
<p><img src="Image00906.jpg" alt></p>
<p>其中，PR（i）表示的是图中i节点的PageRank值， _α_ 表示转移概率，N表示的是网页的总数，in（i）表示的是指向网页i的网页集合，out（j）表示的是网页 j指向的网页集合。</p>
<h3 id="16-3-PersonalRank算法"><a href="#16-3-PersonalRank算法" class="headerlink" title="16.3 PersonalRank算法"></a>16.3 PersonalRank算法</h3><h4 id="16-3-1-PersonalRank算法原理"><a href="#16-3-1-PersonalRank算法原理" class="headerlink" title="16.3.1 PersonalRank算法原理"></a>16.3.1 PersonalRank算法原理</h4><p>在PageRank算法中，计算出的PR值是每个节点相对于全局的重要性程度，而在推荐问题中，我们希望求解的是所有的商品节点相对于某个用户节点的重要性程度，如在图16.2所示的用户- 商品二部图中，当需要为用户U 1 推荐时，需要计算的是所有的商品节点D  j 相对于用户U 1 的重要性，而不是每一个节点相对于全局的重要性。</p>
<p>PersonalRank算法为PageRank算法的变形形式，用于计算所有的商品节点D  j 相对于某个用户节点U 的重要性程度。假设用户为U 1 ，则从节点U 1 开始在用户-商品二部图中游走，游走到任意一个节点时，与 PageRank 算法一样，会按照一定的概率选择停止游走或者继续游走。假设选择继续游走，则以当前的节点作为新的出发点，重复以上的游走过程，直到每个节点的访问概率不再变化为止。</p>
<h4 id="16-3-2-PersonalRank算法的流程"><a href="#16-3-2-PersonalRank算法的流程" class="headerlink" title="16.3.2 PersonalRank算法的流程"></a>16.3.2 PersonalRank算法的流程</h4><p>PersonalRank算法对通过连接的边为每个节点打分，在PersonalRank算法中，不区分用户和商品，因此上述的计算用户U 1 对所有的商品的感兴趣的程度，就变成了对用户U 1 计算各个节点U 2 ，…，U 5 ，D 1 ，…，D 5 的重要程度。PersonalRank 算法的具体过程如下（对用户U 1 来说）：</p>
<p>• 初始化：</p>
<p><img src="Image00012.jpg" alt></p>
<p>• 开始在图上游走，每次选择PR值不为0的节点开始，沿着边往前的概率为 _α_ ，停在当前点的概率为1 _-α_ ：</p>
<p>• 首先从U 1 开始，从U 1 到D 1 、D 2 和D 4 的概率为<img src="Image00038.jpg" alt> ，则此时D 1 、D 2 和D 4 的PR值为：<img src="Image00059.jpg" alt> ，U 1 的PR值变成了1 _-α_ 。</p>
<p>• 此时PR值不为0的节点为U 1 、D 1 、、D 2 D 4 ，则此时从这三点出发，继续上述的过程，直到收敛为止。</p>
<p>由此，可以得出以下的PR计算方法：</p>
<p><img src="Image00080.jpg" alt></p>
<p>其中，r i 为：</p>
<p><img src="Image00104.jpg" alt></p>
<p>现在，让我们一起利用 Python 实现 PersonalRank 算法的过程，在计算过程中，我们需要用到矩阵的相关计算，因此需要导入numpy模块：</p>
<p><img src="Image00330.jpg" alt></p>
<p>PersonalRank算法的具体过程如程序清单16-1所示。</p>
<p><strong>程序清单16-1 PersonalRank算法过程</strong></p>
<p><img src="Image00149.jpg" alt></p>
<p>在程序清单16-1中，函数PersonalRank是PersonalRank算法的核心部分，在函数PersonalRank中，首先初始化rank，并将user的rank值标记为1，表示从user节点开始游走，如程序代码中的①所示；在初始化完成后，根据上述的公式开始迭代求解，求解的具体过程如程序代码中的②和③所示。</p>
<h3 id="16-4-利用PersonalRank算法进行推荐"><a href="#16-4-利用PersonalRank算法进行推荐" class="headerlink" title="16.4 利用PersonalRank算法进行推荐"></a>16.4 利用PersonalRank算法进行推荐</h3><h4 id="16-4-1-利用PersonalRank算法进行推荐"><a href="#16-4-1-利用PersonalRank算法进行推荐" class="headerlink" title="16.4.1 利用PersonalRank算法进行推荐"></a>16.4.1 利用PersonalRank算法进行推荐</h4><p>有了以上的知识储备，现在，我们一起实现PersonalRank算法的推荐过程，首先，为了能够使用矩阵的相关函数，同时，为了使得Python能够支持中文的注释，因此，在“personal_rank.py”文件中加入：</p>
<p><img src="Image00171.jpg" alt></p>
<p>在利用PersonalRank算法对用户U 1 进行推荐时，其主函数如程序清单16-2所示。</p>
<p><strong>程序清单16-2 PersonalRank推荐的主函数</strong></p>
<p><img src="Image00199.jpg" alt></p>
<p>在主函数中，利用 PersonalRank 进行推荐时，主要的步骤为：①导入用户- 商品数据，如程序代码中的①所示，函数load_data的具体实现如程序清单16-3所示；②将用户-商品矩阵转换成二部图的表示，如程序代码中的②所示，函数 generate_dict的具体实现如程序清单 16-4 所示；③利用 PersonalRank 算法计算出打分列表，如程序代码中的③所示，函数 PersonalRank 的具体实现如程序清单 16-1 所示；④根据打分列表，得到最终的推荐列表，如程序代码中的④所示，函数recommend的具体实现如程序清单16-5所示。</p>
<p><strong>程序清单16-3 load_data函数</strong></p>
<p><img src="Image00224.jpg" alt></p>
<p>在程序清单16-3中，函数load_data主要是导入用户-商品数据，在导入的过程中，对于未打分的项“-”，标记为0。</p>
<p><strong>程序清单16-4 generate_dict函数</strong></p>
<p><img src="Image00247.jpg" alt></p>
<p><img src="Image00267.jpg" alt></p>
<p>在程序清单16-4中，函数generate_dict将用户-商品矩阵转换成用户-商品的二部图表示。在用户- 商品矩阵中，行表示用户，列表示商品，在转换的过程中，为了区分用户节点和商品节点，以“U_”表示用户节点，以“D_”表示商品节点，如“U_0”表示第1个用户。</p>
<p><strong>程序清单16-5 recommend函数</strong></p>
<p><img src="Image00837.jpg" alt></p>
<p>在程序清单16-5 中，recommend 函数根据PersonalRank 算法计算出来的打分结果rank，计算出用户user未互动过的商品的排序。在recommend函数中，主要分为3步：①从用户- 商品二部图中找到用户user曾互动过的商品，如程序代码中的①所示；②从打分 rank 中取出所有对商品的打分，如程序代码中的②所示，并排除用户 user已经打过分的项，得到未打分商品；③对未打分的项根据打分降序排列并返回，如程序代码中的③所示。</p>
<h4 id="16-4-2-最终的结果"><a href="#16-4-2-最终的结果" class="headerlink" title="16.4.2 最终的结果"></a>16.4.2 最终的结果</h4><p>在利用PersonalRank算法对用户user进行推荐的过程中，其运行过程如下所示：</p>
<p><img src="Image00307.jpg" alt></p>
<p>最终为用户user推荐的结果为：</p>
<p><img src="Image00329.jpg" alt></p>
<p><strong>参考文献</strong></p>
<p>[1] 项亮.推荐系统实践[M].北京：人民邮电出版社.2012.</p>
<p>[2] Arasu A,Cho J,Garcia-Molina H,et al.Searching the Web[J].Acm Transactions on Internet Technology,2002,1(1):2—43.</p>
<p>[3] 卢昌海.谷歌背后的数学.http：//www.changhai.org/articles/technology/misc/google_math.php</p>
<p>[4] Haveliwala T H.Topic-sensitive PageRank[C]//International Conference on World Wide Web.ACM,2002:16-23.</p>
<p>[5] HarryHuang1990.用 PersonalRank 实现基于图的推荐算法 http：//blog.csdn.net/harryhuang1990/article/details/10048383</p>
<h1 id="第五部分-深度学习"><a href="#第五部分-深度学习" class="headerlink" title="第五部分 深度学习"></a>第五部分 深度学习</h1><p>深度学习因其强大的特征表示和特征学习，可以显著提高机器学习算法的效果。与前几部分的基本的机器学习算法不同，深度学习算法自动学习到特征的表示方法，不需要人工参与特征的提取，深度学习算法在语音、图像和文本方面得到了广泛的应用。</p>
<p>在第 17 章将介绍特征学习和特征表示的基本概念，并在此基础上介绍AutoEncoder算法，AutoEncoder算法通过自我学习可以实现特征的学习；在第18章将介绍卷积神经网络（Convolutional Neural Network，CNN），在CNN中，通过卷积层和池化层的方式实现特征的学习。</p>
<h2 id="17-AutoEncoder"><a href="#17-AutoEncoder" class="headerlink" title="17 AutoEncoder"></a>17 AutoEncoder</h2><p>在前面的章节中，我们介绍的机器学习算法都需要人工指定其特征的具体形式，这个过程被称为特征处理，通过对原始数据进行处理，得到原始数据的特征，再通过具体的算法，如分类算法、回归算法或者聚类算法对其进行处理，得到最终的处理结果。对于上述特征处理的工作，需要大量的先验知识，如果选取的特征能够较好地表征原始数据，则最终的结果也比较好，反之，效果并不会很好。对于这样的需要大量先验知识的特征提取工作，是否存在一种可以自动学习出其特征的方式，深度学习很好地解决了这样的问题。</p>
<p>深度学习是指利用神经网络的技术能够自动提取出数据中的特征，这个过程被称为特征学习。AutoEncoder 是最基本的特征学习方式，对于一些无标注的数据，AutoEncoder通过重构输入数据以达到自我学习的目的。</p>
<h3 id="17-1-多层神经网络"><a href="#17-1-多层神经网络" class="headerlink" title="17.1 多层神经网络"></a>17.1 多层神经网络</h3><h4 id="17-1-1-三层神经网络模型"><a href="#17-1-1-三层神经网络模型" class="headerlink" title="17.1.1 三层神经网络模型"></a>17.1.1 三层神经网络模型</h4><p>对于传统的三层神经网络模型，其基本组成部分包括输入层、隐含层和输出层，典型的三层结构的神经网络模型如图17.1所示。</p>
<p><img src="Image00350.jpg" alt></p>
<p>图17.1 三层神经网络模型</p>
<p>在图17.1所示的三层结构的神经网络模型中，L 1 层被称为输入层，L 2 层被称为隐含层，L 3 层被称为输出层。在三层结构的神经网络模型中，最典型的例子是BP神经网络，参见本书的第10章。BP神经网络是监督式学习算法。</p>
<p>以分类任务为例，假设有m个训练样本为{（X  （1） ，y （1） ），…，（X  （m） ，y （m） ）}，其中，<img src="Image00373.jpg" alt> ，y∈{-1，1}。在BP神经网络中，其隐含层的输出为：</p>
<p><img src="Image00098.jpg" alt></p>
<p>其中，W 1 被称为输入层到隐含层的权重，b 1 被称为输入层到隐含层的偏置， _σ_ 为非线性函数，常用的非线性函数主要有Sigmoid函数和tanh函数。输出层的输出为：</p>
<p><img src="Image00217.jpg" alt></p>
<p>其中，W 2 被称为隐含层到输出层的权重，b 2 被称为隐含层到输出层的偏置。在BP 神经网络模型中，其参数为W 1 、b 1 、W 2 和b 2 ，为了求解这些参数，需要构造损失函数，在BP神经网络中，可以选择预测的输出是实际的标签之间的差异作为最终的损失函数，即：</p>
<p><img src="Image00324.jpg" alt></p>
<p>通过优化损失函数，求得最终的BP神经网络模型中的参数。</p>
<h4 id="17-1-2-由三层神经网络到多层神经网络"><a href="#17-1-2-由三层神经网络到多层神经网络" class="headerlink" title="17.1.2 由三层神经网络到多层神经网络"></a>17.1.2 由三层神经网络到多层神经网络</h4><p>在图17.1所示的三层神经网络模型中，隐含层通过对训练样本进行线性变换和非线性变换实现对样本空间的变换，能否通过增加更多的层数，实现对输入样本的更高级的抽象呢？即构造如图17.2所示的多层网络。</p>
<p><img src="Image00430.jpg" alt></p>
<p>图17.2 多层神经网络模型</p>
<p>然而，对于图17.2所示的多层神经网络模型，隐含层的层数增加，同时增加了训练的难度，在利用梯度下降对网络中的权重和偏置训练的过程中，会出现诸如梯度弥散等现象。能够充分利用多层神经网络来对样本进行更高层的抽象，Hinton等人提出了逐层训练的概念。</p>
<p>在逐层训练模型中，每次训练两层模型，如图17.3所示。</p>
<p><img src="Image00323.jpg" alt></p>
<p>图17.3 逐层训练</p>
<p>在图17.3中，每次通过训练相邻的两层，前一层的输出作为下一层的输入，通过这样的方式构建多层的神经网络。</p>
<h3 id="17-2-AutoEncoder模型"><a href="#17-2-AutoEncoder模型" class="headerlink" title="17.2 AutoEncoder模型"></a>17.2 AutoEncoder模型</h3><h4 id="17-2-1-AutoEncoder模型结构"><a href="#17-2-1-AutoEncoder模型结构" class="headerlink" title="17.2.1 AutoEncoder模型结构"></a>17.2.1 AutoEncoder模型结构</h4><p>在图17.3中，每次通过训练相邻的两层，并将训练好的模型堆叠起来，构建深层网络模型。自编码器AutoEncoder是一种用于训练相邻两层网络模型的一种方法。</p>
<p>自编码器AutoEncoder是典型的无监督学习算法，其结构如图17.4所示。</p>
<p><img src="Image00304.jpg" alt></p>
<p>图17.4 AutoEncoder的结构</p>
<p>在图17.4中，最左侧是输入层，中间是隐含层，最右边是输出层。对于输入X，假设输入X=（x 1 ，x 2 ，…，x d ），且x i<br>∈[0，1]，自编码器首先将输入X映射到一个隐含层，利用隐含层对其进行表示为H=（h 1 ，h 2 ，…，h d′ ），且h i<br>∈[0，1]，这个过程被称为编码（Encode），隐含层的输出H 的具体形式为：</p>
<p><img src="Image00737.jpg" alt></p>
<p>其中， _σ_ 为一个非线性映射，如Sigmoid函数。</p>
<p>隐含层的输出H 被称为隐含的变量，利用该隐含的变量重构Z。这里输出层的输出Z 与输入层的输入X具有相同的结构，这个过程被称为解码（Decode）。输出层的输出Z 的具体形式为：</p>
<p><img src="Image00836.jpg" alt></p>
<p>输出层的输出Z 可以看成是利用特征H 对原始数据X的预测。</p>
<p>从上述的过程中可以看出，解码的过程是编码过程的逆过程。对于解码过程中的权重矩阵W 2 可以被看成是编码过程的逆过程，即<img src="Image00040.jpg" alt> 。</p>
<h4 id="17-2-2-AutoEncoder的损失函数"><a href="#17-2-2-AutoEncoder的损失函数" class="headerlink" title="17.2.2 AutoEncoder的损失函数"></a>17.2.2 AutoEncoder的损失函数</h4><p>为了使得重构后Z 和原始的数据X之间的重构误差最小，首先需要定义重构误差。定义重构误差的方法有很多种，如使用均方误差：</p>
<p><img src="Image00151.jpg" alt></p>
<p>或者使用交叉熵（cross-entropy）作为其重构误差：</p>
<p><img src="Image00268.jpg" alt></p>
<p>对于AutoEncoder的损失函数的具体求解过程，可以参见本书第6章BP神经网络的求解。</p>
<h3 id="17-3-降噪自编码器Denoising-AutoEncoder"><a href="#17-3-降噪自编码器Denoising-AutoEncoder" class="headerlink" title="17.3 降噪自编码器Denoising AutoEncoder"></a>17.3 降噪自编码器Denoising AutoEncoder</h3><h4 id="17-3-1-Denoising-AutoEncoder原理"><a href="#17-3-1-Denoising-AutoEncoder原理" class="headerlink" title="17.3.1 Denoising AutoEncoder原理"></a>17.3.1 Denoising AutoEncoder原理</h4><p>在AutoEncoder算法中，对于一个输入样本，首先对其进行编码，得到隐含层的输出，再对隐含层的输出进行解码，以重构该样本，在这个过程中，使得最终的重构结果能够尽可能还原输入样本。然而，在很多情况下，原始的数据中通常含有噪音，如图17.5所示的MNIST数据集中的数字是倾斜的：</p>
<p><img src="Image00375.jpg" alt></p>
<p>图17.5 斜着的数字0</p>
<p>对于这样含有噪音的数据，我们希望解码后的数据中不含有噪音，这就需要编码器不仅有编码功能，还得有去噪音的作用，通过这种方式训练出的模型具有更强的鲁棒性，即使得训练出来的模型对一些含有噪音的数据具有较强的泛化能力。Bengio等人在 2008 年提出了降噪自动编码器（Denoising AutoEncoder）的概念，Denoising AutoEncoder 就是在 AutoEncoder 的基础上，为了防止训练出来的自编码器模型过拟合，对输入的数据中加入了噪音，使学习得到的编码器具有更强的鲁棒性，从而增强模型的泛化能力。其结构如图17.6所示。</p>
<p><img src="Image00479.jpg" alt></p>
<p>图17.6 Denoising AutoEncoder的结构</p>
<p>在如图17.6所示的Denoising AutoEncoder的结构中，Denoising AutoEncoder的输入样本为X，Denoising Autoencoder以概率p将输入层节点的值置为0，得到含有噪音的输入样本X～。利用含有噪音的输入样本X～训练自编码器模型，通过编码 encode过程得到隐含层的输出H，并通过解码decode过程得到最终的重构样本Z，最终通过度量重构样本Z 和不含噪音的样本X之间的误差L（X，Z）。</p>
<h4 id="17-3-2-Denoising-AutoEncoder实现"><a href="#17-3-2-Denoising-AutoEncoder实现" class="headerlink" title="17.3.2 Denoising AutoEncoder实现"></a>17.3.2 Denoising AutoEncoder实现</h4><p>我们已经简单了解了降噪自编码器的原理，下面让我们利用TensorFlow框架实现降噪自编码器Denoising AutoEncoder。首先，我们需要导入tensorflow模块和numpy模块：</p>
<p><img src="Image00567.jpg" alt></p>
<p>接下来，我们为降噪自编码器构建一个类，具体的实现如程序清单17-1所示。</p>
<p><strong>程序清单17-1 降噪自编码器类的实现</strong></p>
<p><img src="Image00687.jpg" alt></p>
<p>在程序清单17-1中，实现了降噪自编码器的类Denoising_AutoEncoder的构建，类中的参数包括：输入层到隐含层的权重W以及W的值W_eval，输入层到隐含层的偏置b以及b的值b_eval，隐含层的输出encode_r，隐含层节点的个数layer_size，输入样本input_data以及在降噪自编码器中保持特征不变的比例keep_prob。</p>
<p>当定义好降噪自编码器类 Denoising_AutoEncoder 的初始化函数后，我们需要在降噪自编码器类中定义降噪自编码器的训练过程，其具体的训练过程如程序清单17-2所示。</p>
<p><strong>程序清单17-2 降噪自编码器的训练</strong></p>
<p><img src="Image00024.jpg" alt></p>
<p><img src="Image00319.jpg" alt></p>
<p>在程序清单 17-2 中，fit 函数用于对降噪自编码器模型进行训练，在对降噪自编码器训练的过程中，首先需要对网络中权重和偏置进行初始化，在初始化的过程中，由于选用Sigmoid函数，因此在权重的初始化中，可以选择的方法为：以均匀分布从如下区间</p>
<p><img src="Image00068.jpg" alt></p>
<p>中随机取值，其中，n in 表示的是前一层的节点个数，n out 表示的是后一层的节点个数，具体过程如程序代码中的①所示。使用均方误差作为最终的损失函数，如程序代码中的②所示，最后利用梯度下降法求解损失函数，如程序代码中的③所示。对于TensorFlow的具体操作可以参见本书附录B。</p>
<p><strong>程序清单17-3 取得降噪自编码器的参数</strong></p>
<p>＃得到网络的参数</p>
<p><img src="Image00094.jpg" alt></p>
<p>在程序清单17-3中，函数get_value用于返回降噪自编码器的输入层到隐含层的权重的值W_eval，偏置的值b_eval和隐含层的输出值encoder_r。</p>
<h3 id="17-4-利用Denoising-AutoEncoders构建深度网络"><a href="#17-4-利用Denoising-AutoEncoders构建深度网络" class="headerlink" title="17.4 利用Denoising AutoEncoders构建深度网络"></a>17.4 利用Denoising AutoEncoders构建深度网络</h3><p>在利用Denoising AutoEncoders构建深度网络的过程中，主要包括两个过程：①无监督的逐层训练，即依次训练多个降噪自编码器Denoising AutoEncoder；②有监督的微调，即将训练好的多个降噪自编码器的编码Encoder层组合起来，利用样本标签对训练好的降噪自编码器的编码Encoder层的参数。</p>
<h4 id="17-4-1-无监督的逐层训练"><a href="#17-4-1-无监督的逐层训练" class="headerlink" title="17.4.1 无监督的逐层训练"></a>17.4.1 无监督的逐层训练</h4><p>在无监督的逐层训练的过程中，依次训练每一个降噪自编码器，假设训练完成的第i个降噪自编码器模型如图17.7所示。</p>
<p><img src="Image00113.jpg" alt></p>
<p>图17.7 第i个降噪自编码器结构</p>
<p>其中，隐含层的输出为：</p>
<p><img src="Image00134.jpg" alt></p>
<p>其中，<img src="Image00160.jpg" alt> 表示的是第 i 个降噪自编码器的输入层到隐含层的权重，<img src="Image00185.jpg" alt> 表示的是第i个降噪自编码器的输入层到隐含层的偏置，X  i 表示的是第i个自编码器的输入，此时，保留输入层和隐含层之间的权重<img src="Image00212.jpg" alt> 和偏置<img src="Image00235.jpg" alt> ，并将隐含层的输出H  i 作为第i+1 个降噪自编码器的输入。通过逐层训练的方式，得到m个降噪自编码器的编码过程，这样逐层训练的过程被称为预训练。</p>
<h4 id="17-4-2-有监督的微调"><a href="#17-4-2-有监督的微调" class="headerlink" title="17.4.2 有监督的微调"></a>17.4.2 有监督的微调</h4><p>通过无监督的方式，训练得到m个降噪自编码器的编码过程，将这些降噪自编码器的编码过程串联起来构成深层神经网络，如图17.8所示。</p>
<p><img src="Image00367.jpg" alt></p>
<p>图17.8 堆叠降噪自编码器结构</p>
<p>通过如图17.8所示的方式将训练好的降噪自编码器堆叠在一起，并在最后一层加入有监督的分类，如多分类的Softmax Regression或者二分类的Logistic Regression等。</p>
<p>在如图 17.8 所示的深层网络中，其训练过程为：①初始化各层网络的权重W 和偏置b；②利用损失函数，调整各层网络的权重和偏置。在堆叠降噪自编码器的初始化过程中，充分利用预训练过程的结果，将预训练过程中得到的网络权重和偏置作为堆叠降噪自编码圣经网络的初始值，并利用整个网络的损失函数，对这些初始值进行调整，对权重和偏置调整的过程被称为有监督的微调。</p>
<p>现在，我们利用 TensorFlow 框架实现堆叠降噪自编码器 Stacked Denoising AutoEncoder。</p>
<p><strong>程序清单17-4 堆叠降噪自编码器类的实现</strong></p>
<p><img src="Image00276.jpg" alt></p>
<p><img src="Image00297.jpg" alt></p>
<p>在程序清单 17-4 中，在堆叠降噪自编码器类 Stacked_Denoising_AutoEncoder 的<strong>init</strong>函数中，ecod_W表示的是一系列降噪自编码器输入层到隐含层的权重的集合，ecod_b表示的是一系列降噪自编码器输入层到隐含层的偏置的集合。hidden_list表示的是所有隐含层节点个数的集合，input_data_trainX 表示的是训练样本的特征，input_data_trainY表示的是训练样本的标签，input_data_validX表示的是验证集的特征，input_data_validY表示的是验证集的标签，input_data_testX表示的是测试集的特征，input_data_testY表示的是测试集的标签。</p>
<p><strong>程序清单17-5 堆叠降噪自编码器类的训练</strong></p>
<p><img src="Image00679.jpg" alt></p>
<p><img src="Image00338.jpg" alt></p>
<p><img src="Image00359.jpg" alt></p>
<p>在程序清单17-5中，对堆叠降噪自编码器的训练过程中，主要分为3个部分：</p>
<p>首先，分别训练每一个降噪自编码器，如程序代码中的①所示，第一个降噪自编码器的输入为原始的训练集，当训练完成后，保存输入层到隐含层的权重和偏置，如程序代码中的②和③所示，并将隐含层的输出作为下一个降噪自编码器的输入，如程序代码中的④所示。</p>
<p>其次，当训练完成所有的降噪自编码器后，将所有的降噪自编器的输入层- 隐含层串联起来，并以训练好的值作为初始的值，如程序代码中的⑤所示。最后加入输出层，最终以交叉熵作为损失函数，如程序代码中的⑥所示。利用梯度下降的方法求解损失函数的值，如程序代码中的⑦所示。</p>
<p>最后，在构建完深层网络后，利用梯度下降法对损失函数进行求解，这个过程被称为微调，即对各层的权重和偏置进行调整。最终，训练完成整个网络。</p>
<h3 id="17-5-利用TensorFlow实现Stacked-Denoising-AutoEncoders"><a href="#17-5-利用TensorFlow实现Stacked-Denoising-AutoEncoders" class="headerlink" title="17.5 利用TensorFlow实现Stacked Denoising AutoEncoders"></a>17.5 利用TensorFlow实现Stacked Denoising AutoEncoders</h3><p>为了测试Stacked Denoising AutoEncoders的效果，我们选择MNIST手写体识别的数据集作为测试数据集，对于 MNIST 手写体识别的数据集的具体描述，可以参见本书的第2章。</p>
<h4 id="17-5-1-训练Stacked-Denoising-AutoEncoders模型"><a href="#17-5-1-训练Stacked-Denoising-AutoEncoders模型" class="headerlink" title="17.5.1 训练Stacked Denoising AutoEncoders模型"></a>17.5.1 训练Stacked Denoising AutoEncoders模型</h4><p>现在让我们一起利用上面构建好的降噪自编码器的类和堆叠降噪自编码器的类，对MNIST手写体识别的数据集进行训练，首先，为了能够使得Python代码支持中文的注释以及导入代码中使用到的函数，我们在“stacked_denoising_autoencoder.py”文件中加入：</p>
<p><img src="Image00385.jpg" alt></p>
<p>接下来，我们开始利用TensorFlow实现堆叠降噪自编码器，其主函数如程序清单17-6所示。</p>
<p><strong>程序清单17-6 堆叠降噪自编码器训练的主函数</strong></p>
<p><img src="Image00407.jpg" alt></p>
<p>在程序清单17-6中，为了训练堆叠降噪自编码器，首先，我们需要导入数据集，如程序代码中的①所示；其次，利用训练数据集训练堆叠降噪自编码器，在训练的过程中，我们先初始化网络结构，在此，我们构建了包含3个隐含层的堆叠降噪自编码器，其中，每一个隐含层的节点个数都为1000，如程序代码中的②所示；在初始化完成后，利用fit函数训练堆叠降噪自编码器，如程序代码中的③所示。</p>
<h4 id="17-5-2-训练的过程"><a href="#17-5-2-训练的过程" class="headerlink" title="17.5.2 训练的过程"></a>17.5.2 训练的过程</h4><p>对于堆叠降噪自编码器的训练，其训练过程为：</p>
<p><img src="Image00312.jpg" alt></p>
<p><img src="Image00419.jpg" alt></p>
<p>首先是分别训练3个降噪自编码器，最后将训练好的降噪自编码器的输入层—隐含层堆叠起来，并对这些权重和偏置进行微调，最终在验证集上的准确性达到97.58%，对于测试集，其最终的结果为97.34%：</p>
<p><img src="Image00617.jpg" alt></p>
<p><strong>参考文献</strong></p>
<p>[1] Vincent P,Larochelle H,Bengio Y,et al.Extracting and composing robust features with denoising autoencoders[C]//International Conference.2008:1096-1103.</p>
<p>[2] Schölkopf B,Platt J,Hofmann T.Greedy Layer-Wise Training of Deep Networks[C]//Conference on Advances in Neural Information Processing Systems.MIT Press,2006:153-160.</p>
<p>[3] DeepLearning 0.1.Denoising Autoencoders<br>(dA)[DB/OL].<a href="http://www.deeplearning.net/tutorial/dA.html" target="_blank" rel="noopener">http://www.deeplearning.net/tutorial/dA.html</a></p>
<p>[4] DeepLearning 0.1.Stacked Denoising Autoencoders<br>(SdA)[DB/OL].<a href="http://www.deeplearning.net/tutorial/SdA.html" target="_blank" rel="noopener">http://www.deeplearning.net/tutorial/SdA.html</a></p>
<h2 id="18-卷积神经网络"><a href="#18-卷积神经网络" class="headerlink" title="18 卷积神经网络"></a>18 卷积神经网络</h2><p>在上一章中，我们介绍了最基本的多层神经网络模型，在多层神经网络模型中，如果直接对神经网络中的参数进行训练，通常难以训练得到较好的参数。为了能够对多层神经网络进行训练，逐层训练的概念被提出，通过逐层训练得到每一层神经网络中的初始化的参数，并通过微调得到最终的神经网络的参数。</p>
<p>卷积神经网络（Convolutional Neural Networks，CNN）是多层神经网络模型的一个变种，主要是受到生物学的启发，卷积神经网络 CNN 在图像领域得到了广泛的应用。在卷积神经网络 CNN 中，充分利用图像数据在局部上的相关性，这样可以尽可能减少网络中参数的个数，以方便网络中参数的求解。</p>
<h3 id="18-1-传统神经网络模型存在的问题"><a href="#18-1-传统神经网络模型存在的问题" class="headerlink" title="18.1 传统神经网络模型存在的问题"></a>18.1 传统神经网络模型存在的问题</h3><p>对于传统的多层神经网络结构，通常是由一个输入层、多个隐含层和一个输出层组成，对于一个具有3个隐含层的神经网络结构如图18.1所示。</p>
<p><img src="Image00254.jpg" alt></p>
<p>图18.1 包含3个隐含层的神经网络结构</p>
<p>假设对于一张图像，其大小为28×28，在图18.1所示的包含3个隐含层的神经网络结构中，其输入层的节点个数为28×28=784个，假设在多层神经网络中，每一个隐含层的节点个数为 1000 个，则输入层到隐含层的权重W 为784×1000的矩阵，输入层到隐含层的偏置b为1×1000的向量。同样，第一个隐含层到第二个隐含层的权重W 为1000×1000的矩阵，第一个隐含层到第二个隐含层的偏置为1×1000的向量，第二个隐含层到第三个隐含层的权重W 为1000×1000的矩阵，第二个隐含层到第三个隐含层的偏置为1×1000的向量。对于手写体识别MNIST数据集，其输出层的节点个数为10个，因此，第三个隐含层到输出层的权重W 为1000×10，第三个隐含层到输出层的偏置b为1×10的向量。综上，对于图18.1所示的包含3个隐含层的神经网络结构，其参数的个数为：</p>
<p><img src="Image00726.jpg" alt></p>
<p>对于一个仅包含3个隐含层的神经网络，需要训练的参数有2797010个，这样的数字对于训练过程来说是相当庞大的，那么，是否存在一种深层的神经网络模型，在该神经网络模型中，其参数个数能够得到削减，但是不影响其模型的精度？这就是我们接下来要介绍的卷积神经网络模型。</p>
<h3 id="18-2-卷积神经网络"><a href="#18-2-卷积神经网络" class="headerlink" title="18.2 卷积神经网络"></a>18.2 卷积神经网络</h3><h4 id="18-2-1-卷积神经网络中的核心概念"><a href="#18-2-1-卷积神经网络中的核心概念" class="headerlink" title="18.2.1 卷积神经网络中的核心概念"></a>18.2.1 卷积神经网络中的核心概念</h4><p>为了能够减少参数的个数，在卷积神经网络（Convolutional Neural Networks，CNN）中，提出了3个重要的概念：①稀疏连接（Sparse Connectivity）；②共享权值（Shared</p>
<p>Weights）；③池化（Pooling）。其中，稀疏连接主要是通过对数据中的局部区域进行建模，以发现局部的一些特性；共享权值的目的是为了简化计算的过程，使得需要优化的参数变少；子采样的目的是解决图像中的平移不变性，即所要识别的内容与其在图像中的具体位置无关。</p>
<p>对于图像来说，其特征在空间上存在局部的相关性。在卷积神经网络 CNN 中，通过在邻接层的神经元之间使用局部连接来发现输入特征在空间上存在的局部相关性，其具体过程如图18.2所示。</p>
<p><img src="Image00822.jpg" alt></p>
<p>图18.2 稀疏连接</p>
<p>对于图18.2 中所示的神经网络，第m层节点的输入是第m-1层的神经元的一个子集，其中，被选择的子集的大小被称为感受野。假设第m层的神经元具有在第m-1层上宽度为 3 的感受野，因此只连接第m-1层上的 3 个邻接神经元，如在第m层上的y 1 节点，与其连接的m-1层上的节点为x 1 、x 2 和x 3 。在第m+1层的神经元与第m层的神经元之间具有相似的连接特性，即第m+1层的神经元在第m层上的感受野的宽度也是3，但是他们关于输入层m-1层的感受野却变大了，如图18.2所示，第m+1层的神经元在第m-1层上的感受野的宽度为5。</p>
<p>稀疏连接特性相对于图18.1中所示的全连接网络减少了网络中边的数量，在卷积神经网络 CNN 中，在每一组感受野中，其参数是相互共享的，即权值共享，其具体过程如图18.3所示。</p>
<p><img src="Image00016.jpg" alt></p>
<p>图18.3 权值共享</p>
<p>在图18.3中，当设置感受野的大小为3时，此时包含了3个参数w 1 、w 2 和w 3 ，在每一组感受野中，其参数是共享的，即对于第m层的节点y 1 、y 2 和y 3 ，其参数是一致的，权值共享的方式可以极大地缩减需要学习的参数数量，这样就可以加快学习速度。</p>
<p>池化（Pooling）是卷积神经网络CNN中另一个比较重要的概念，一般可以采用最大池化（max-pooling）。在max- pooling中，将输入图像划分成为一系列不重叠的正方形区域，然后对于每一个子区域，输出其中的最大值，其具体过程如图18.4所示。</p>
<p><img src="Image00136.jpg" alt></p>
<p>图18.4 max-pooling</p>
<p>在图 18.4 中，对原始的4×4图像，将其划分成 4 个不重叠的正方形区域，每个正方形的大小为2×2，如对于第一个正方形内的4个数，利用max- pooling策略，其输出值为这4个数中的最大值。利用max-pooling策略能够进一步降低计算量。通过消除非最大值，为上层降低了计算量。</p>
<h4 id="18-2-2-卷积神经网络模型"><a href="#18-2-2-卷积神经网络模型" class="headerlink" title="18.2.2 卷积神经网络模型"></a>18.2.2 卷积神经网络模型</h4><p>在上面，我们介绍了卷积神经网络CNN中的基本概念，在卷积神经网络CNN的发展过程中，出现了一些经典的卷积神经网络模型，主要包括：LeNet、AlexNet、ZF Net、GoogLeNet和VGGNet，下面我们以LeNet为例。</p>
<p>LeNet是由Yann LeCun设计，在LeNet中，其基本操作包括：卷积和max-pooling，这两个基本操作分别对应着卷积层（Convolution Layer）和下采样层（Sub-Sampling Layer）。通过卷积层和下采样层的交替，构造成深层的网络结构，其具体的网络结构如图18.5所示。</p>
<p><img src="Image00258.jpg" alt></p>
<p>图18.5 LeNet的网络结构</p>
<p>在图18.5所示的LeNet网络结构中，对于一幅图像，首先经过卷积层，经过卷积层后的特征映射的个数为 4，然后对每一个特征映射采用 max- pooling，再对max-pooling 后的结果应用卷积操作和 max-pooling 操作，最后是一个全连接的 MLP层，即传统的包含一个隐含层的神经网络，将图像划分到指定的类别。有了如上的直观理解，我们接下来将会详细介绍在卷积层、池化层和全连接层中所要完成的工作。</p>
<h3 id="18-3-卷积神经网络的求解"><a href="#18-3-卷积神经网络的求解" class="headerlink" title="18.3 卷积神经网络的求解"></a>18.3 卷积神经网络的求解</h3><p>在卷积神经网络 CNN 中，最重要的是卷积层（Convolution Layer）、下采样层（Sub-Sampling Layer）和全连接层（Fully- Connected Layer），这3层分别对应着卷积神经网络中最重要的 3 个操作，即：卷积操作、max-pooling 操作和全连接的 MLP操作。</p>
<h4 id="18-3-1-卷积层（Convolution-Layer）"><a href="#18-3-1-卷积层（Convolution-Layer）" class="headerlink" title="18.3.1 卷积层（Convolution Layer）"></a>18.3.1 卷积层（Convolution Layer）</h4><p>在卷积层中，最重要的操作是卷积操作，卷积操作主要是 f<br>（x）g（x）在重合区域的积分。接下来，我们分别对一维数据下的卷积操作、二维数据下的卷积操作和三维数据下的卷积操作进行讨论。</p>
<p>一维数据下的卷积的定义：</p>
<p><img src="Image00361.jpg" alt></p>
<p>对于图18.6所示的卷积操作，其中W=（1，0，-1）：</p>
<p><img src="Image00672.jpg" alt></p>
<p>图18.6 一维卷积操作</p>
<p>在图 18.6 所示的一维数据下的卷积操作中，对第m层的神经元采用卷积操作，得到第m+1层卷积后的值，对于第m+1层中的第一个神经元，其卷积后的值为：</p>
<p><img src="Image00558.jpg" alt></p>
<p>同样，通过对第m层神经元采用同样的权值向量得到最终的卷积后的值。</p>
<p>对于二维数据下的卷积，与一维数据下的卷积不同的是，在二维数据下的卷积中，其权重组成的是矩阵，不是向量。二维数据下的卷积定义为：</p>
<p><img src="Image00673.jpg" alt></p>
<p>对于图18.7所示的二维数据的卷积操作，其权重矩阵为：</p>
<p><img src="Image00260.jpg" alt></p>
<p><img src="Image00362.jpg" alt></p>
<p>图18.7 二维卷积操作</p>
<p>在图18.7所示的二维数据下的卷积操作中，通过原始数据与权重矩阵的点积，得到卷积后的结果，对于图中的阴影部分的数据，卷积后的结果为：</p>
<p><img src="Image00300.jpg" alt></p>
<p>对于三维数据下的卷积操作，其是二维数据下的卷积操作的推广形式，比如一张RGB图片，对应了3个通道（Channel），对每一个通道采用二维数据下的卷积操作，并将3个通道上的值累加，得到最终的卷积操作的结果。</p>
<p>那么，我们应该如何求解卷积操作中的权重的值呢？其基本思路与第6章中介绍的BP神经网络的求解类似，其基本过程分为信号的正向传播和误差的反向传播。</p>
<p>对于信号的正向传播，我们之前已经进行了详细论述，接下来，我们以二维数据为例，讨论如何利用误差的反向传播对参数进行调整。对于误差的反向传播，如图18.7所示的信号的正向传播过程中，假设左侧的矩阵为X，权重矩阵为W，右侧的矩阵为Y，其中：</p>
<p><img src="Image00601.jpg" alt></p>
<p>那么，与权重w 11 相关的所有项为：</p>
<p><img src="Image00340.jpg" alt></p>
<p>那么，<img src="Image00363.jpg" alt> ，因此，如果误差矩阵d为：</p>
<p><img src="Image00389.jpg" alt></p>
<p>则权重矩阵W 的梯度可由图18.8所示。</p>
<p><img src="Image00410.jpg" alt></p>
<p>图18.8 误差的反向传播</p>
<p>由图18.8可知，权重的梯度是输入与误差矩阵的卷积操作。</p>
<h4 id="18-3-2-下采样层（Sub-Sampling-Layer）"><a href="#18-3-2-下采样层（Sub-Sampling-Layer）" class="headerlink" title="18.3.2 下采样层（Sub-Sampling Layer）"></a>18.3.2 下采样层（Sub-Sampling Layer）</h4><p>在卷积神经网络CNN中，在卷积层后面，通常为下采样层（Sub-Sampling Layer），也被称为pooling。pooling的种类有很多种，主要是用一个特征来表达一个局部特征，这就使得参数大为减少，常见的有 max- pooling、mean-pooling 和 L2-pooling。max-pooling就是用局部特征的最大值来表达这个区域的特征。对于max- pooling的具体过程如图18.4所示。</p>
<h4 id="18-3-3-全连接层（Fully-Connected-Layer）"><a href="#18-3-3-全连接层（Fully-Connected-Layer）" class="headerlink" title="18.3.3 全连接层（Fully-Connected Layer）"></a>18.3.3 全连接层（Fully-Connected Layer）</h4><p>对于全连接层，其实质为包含一个隐含层的神经网络模型。在卷积神经网络中，利用卷积层和下采样层的交替叠加，得到特征的高层抽象，再对高层抽象的特征进行全连接的映射，最终对其进行分类，对于全连接层的具体操作可以参见本书第 6 章BP神经网络的具体操作。</p>
<h3 id="18-4-利用TensorFlow实现CNN"><a href="#18-4-利用TensorFlow实现CNN" class="headerlink" title="18.4 利用TensorFlow实现CNN"></a>18.4 利用TensorFlow实现CNN</h3><h4 id="18-4-1-CNN的实现"><a href="#18-4-1-CNN的实现" class="headerlink" title="18.4.1 CNN的实现"></a>18.4.1 CNN的实现</h4><p>我们已经对卷积神经网络CNN的基本原理做了介绍，现在，我们利用TensorFlow框架实现卷积神经网络CNN，首先，我们需要导入tensorflow模块和numpy模块：</p>
<p><img src="Image00429.jpg" alt></p>
<p>接下来，我们为卷积神经网络CNN构建一个类，具体的实现如程序清单18-1所示。</p>
<p><strong>程序清单18-1 卷积神经网络类的实现</strong></p>
<p><img src="Image00451.jpg" alt></p>
<p><img src="Image00472.jpg" alt></p>
<p>在程序清单18-1中，我们实现了卷积神经网络CNN类的构造。类中的参数包括：第一个卷积层的权重w，第一个卷积层的偏置b，第二个卷积层的权重w 2 ，第二个卷积层的偏置 b 2 ，第三个卷积层的权重 w 3 ，第三个卷积层的偏置 b 3 ，全连接层中输入层到隐含层的权重w 4 ，全连接层中输入层到隐含层的偏置b 4 ，隐含层到输出层的权重w_o 和隐含层到输出层的偏置 b_o，其次，还包括卷积层中样本保持不变的比例p_keep_conv和全连接层中样本保持不变的比例p_keep_hidden。</p>
<p>当定义好卷积神经网络类 CNN 的初始化函数后，我们需要在卷积神经网络类中定义卷积神经网络CNN的训练过程，其具体的训练过程如程序清单18-2所示。</p>
<p><strong>程序清单18-2 卷积神经网络的训练</strong></p>
<p><img src="Image00487.jpg" alt></p>
<p><img src="Image00508.jpg" alt></p>
<p><img src="Image00526.jpg" alt></p>
<p>在程序清单 18-2 中，fit 函数用于对卷积神经网络模型进行训练，在如上的卷积神经网络中，包含了三个卷积层和下采样层。当构建完整个卷积神经网络模型后，将卷积神经网络模型的对数据的预测值与样本的真实标签之间的交叉熵作为最终的损失函数，并求损失函数中的最小值，如程序代码中的①所示，利用优化方法求解如上的损失函数，得到整个卷积神经网络模型的值，如程序代码中的②所示。</p>
<h4 id="18-4-2-训练CNN模型"><a href="#18-4-2-训练CNN模型" class="headerlink" title="18.4.2 训练CNN模型"></a>18.4.2 训练CNN模型</h4><p>现在让我们一起利用上面构建好的卷积神经网络的类，对 MNIST 手写体识别的数据集进行训练，首先，为了能够使得Python代码支持中文的注释以及导入代码中使用到的函数，我们在“cnn.py”文件中加入：</p>
<p><img src="Image00540.jpg" alt></p>
<p>接下来，我们开始利用TensorFlow实现卷积神经网络的训练，其主函数如程序清单18-3所示。</p>
<p><strong>程序清单18-3 卷积神经网络训练的主函数</strong></p>
<p><img src="Image00559.jpg" alt></p>
<p>在程序清单18-3中，为了训练卷积神经网络，首先，我们需要导入数据集，如程序代码中的①所示；其次，利用训练数据集训练卷积神经网络，在训练的过程中，我们先初始化网络结构，如程序代码中的②所示；在初始化完成后，利用 fit 函数训练卷积神经网络模型，如程序代码中的③所示。</p>
<h4 id="18-4-3-训练的过程"><a href="#18-4-3-训练的过程" class="headerlink" title="18.4.3 训练的过程"></a>18.4.3 训练的过程</h4><p>对于卷积神经网络CNN的训练，其训练过程为：</p>
<p><img src="Image00176.jpg" alt></p>
<p>利用训练数据集对卷积神经网络模型进行训练，最终在验证集上的准确性达到99.3%，对于测试集，其最终的结果为99.29%：</p>
<p><img src="Image00608.jpg" alt></p>
<p><strong>参考文献</strong></p>
<p>[1] DeepLearning 0.1.Convolutional Neural Networks<br>(LeNet)[DB/OL].<a href="http://www.deeplearning.net/tutorial/lenet.html#tips-and-tricks" target="_blank" rel="noopener">http://www.deeplearning.net/tutorial/lenet.html#tips-and-tricks</a></p>
<p>[2] Lecun Y,Bottou L,Bengio Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</p>
<h1 id="第六部分-项目实践"><a href="#第六部分-项目实践" class="headerlink" title="第六部分 项目实践"></a>第六部分 项目实践</h1><p>在前面各章节中，我们介绍了机器学习中的分类算法、回归算法和聚类算法，并在此基础上，介绍了机器学习的一个实践领域，即在推荐系统中的机器学习算法，最后，我们将对近年来比较流行的深度学习算法进行介绍，机器学习是一个实践性较强的方向，每一个算法都能被用来求解实际的问题。</p>
<p>在第 19 章中，我们将介绍如何利用多种机器学习算法完成微博的精准推荐。在一个具体的项目中，通常包含多种不同的机器学习算法，利用组合多种机器学习算法完成一个复杂的项目。</p>
<h2 id="19-微博精准推荐"><a href="#19-微博精准推荐" class="headerlink" title="19 微博精准推荐"></a>19 微博精准推荐</h2><p>在第四部分，我们介绍了几种不同的推荐算法，还有很多其他的推荐算法。一个完整的推荐系统通常是由多种算法组合而成。在构建工业级的推荐系统的过程中，会涉及很多不同学科的知识，包括大规模搜索、文本分析、机器学习、信息检索等。</p>
<p>在推荐系统中，通过对用户数据的挖掘，抽象出用户感兴趣的“商品”，在不同的应用场景中，“商品”的表现形态也不一样，以微博的博文推荐为例，“商品”表现为用户的博文，在博文精准推荐中，其核心问题是在给定的环境下，为用户推荐高质量且符合用户兴趣的博文。在本章中，我们将从实际问题出发，详细介绍如何利用前面介绍的各项机器学习的技术构建一个完整的推荐系统。</p>
<h3 id="19-1-精准推荐"><a href="#19-1-精准推荐" class="headerlink" title="19.1 精准推荐"></a>19.1 精准推荐</h3><h4 id="19-1-1-精准推荐的项目背景"><a href="#19-1-1-精准推荐的项目背景" class="headerlink" title="19.1.1 精准推荐的项目背景"></a>19.1.1 精准推荐的项目背景</h4><p>在社交网络中，每一个用户只是整个网络中的一个节点，一个简单的网络结构如图19.1所示。</p>
<p><img src="Image00630.jpg" alt></p>
<p>图19.1 网络结构</p>
<p>在微博中，用户可以通过“关注”行为成为另一个用户的粉丝，“关注”行为是有向的。通过“关注”一个用户后，我们可以在我们的feed流中看到对方的信息。在微博中，通过这样的方式，我们可以接触到更多的信息。</p>
<p>然而，在信息过载的时代，信息呈现爆炸式增长，如在微博中，每天有大量的微博被创作和转发，信息量的爆炸式增长在给用户不断带来新的信息的同时，也增加了用户筛选信息的难度，为了能够为用户推荐其感兴趣的信息，我们首先要分析出该用户的兴趣，从海量的信息中选择出与用户兴趣相似的信息，并将这些信息推荐给用户。推荐系统（Recommendation System，RS）正是在这样的背景下被提出的，推荐算法根据用户的历史行为，挖掘出用户的喜好，并为用户推荐与其喜好相符的商品或者信息。推荐系统的任务就是能够连接信息与用户，帮助用户找到其感兴趣的信息，同时让一些有价值的信息能够触达到潜在的用户中。此时，对用户兴趣的精准挖掘，成为为用户精准推荐博文的关键任务。</p>
<h4 id="19-1-2-精准推荐的技术架构"><a href="#19-1-2-精准推荐的技术架构" class="headerlink" title="19.1.2 精准推荐的技术架构"></a>19.1.2 精准推荐的技术架构</h4><p>在构建推荐系统的过程中，为了能够为用户提供精准的博文推荐，其架构的设计主要包括四层：数据生产层、存储层、候选过滤层和排序层。最终输出排序后的结果，具体的精准推荐的架构设计如图19.2所示。</p>
<p><img src="Image00497.jpg" alt></p>
<p>图19.2 精准推荐的架构设计</p>
<p>在图19.2所示的精准推荐的架构设计中，首先，在数据生产层，我们需要利用离线挖掘的方法对用户兴趣进行挖掘，挖掘完成后，将用户数据存储到对应的数据库中，我们称为用户数据库。同时，我们需要将待推荐的微博也存储在数据库中，我们称为推荐微博数据库。在用户数据挖掘中，通常使用到的方法包括协同过滤算法、标签传播算法、word2vec等。</p>
<p>在存储层，将挖掘好的用户兴趣存储到对应的数据库中，我们通常可以使用Redis等NoSQL数据库。</p>
<p>在候选过滤层，当用户请求时，首先从用户数据库中查找到用户的兴趣，再根据查找到的用户兴趣、到推荐微博数据库中进行请求，查找到对应的待推荐的微博，其具体过程如图19.3所示。</p>
<p><img src="Image00675.jpg" alt></p>
<p>图19.3 查询操作</p>
<p>当通过如上的操作查询出了最终的结果后，我们需要对其中的结果进行合并和过滤，以保证最终结果的唯一性。</p>
<p>在排序层，我们需要对所有的候选进行排序，以确定最终的曝光顺序，在排序阶段，使用的评价指标通常为点击率CTR，即：</p>
<p><img src="Image00697.jpg" alt></p>
<p>其中，＃impression表示的是曝光的次数，＃click表示的是点击的次数，在微博中，点击的行为主要包括：“转发”、“评论”、“点赞”、“点击短链接”等。通常采用机器学习的算法对候选进行排序，排序的主要方法有：Logistic Regression算法、因子分解机FM算法、梯度提升决策树GBDT算法等。</p>
<h4 id="19-1-3-离线数据挖掘"><a href="#19-1-3-离线数据挖掘" class="headerlink" title="19.1.3 离线数据挖掘"></a>19.1.3 离线数据挖掘</h4><p>在精准推荐中，对用户的离线数据挖掘是很关键的步骤，常用的用户定向主要有：①人群属性定向（Demographic Targeting）；②行为定向（Behavioral Targeting）；③地理位置的定向（Geo Targeting）；④相似用户的定向（Look- Alike Targeting）。</p>
<p>人群属性定向指基于用户基本属性进行定向，包括年龄、性别等定向，比如为女性用户推荐化妆品类的微博。行为定向指的是基于用户的历史行为数据挖掘用户的兴趣，比如通过对微博中用户对博文的“转发”、“评论”、“点赞”等数据的分析，发现用户的兴趣。地理位置定向指的是利用移动设备记录用户的地理位置，为用户推荐相关的微博，比如用户在某个景点，我们为其推荐邻近地点的微博。相似用户的定向指的是利用已经找出的一些人，找到与其相似的用户进行定向。</p>
<p>以上简单介绍了4种离线数据挖掘的方法，还有很多其他的挖掘方法。在本章中，我们重点关注行为定向和相似用户的定向。</p>
<h3 id="19-2-基于用户行为的挖掘"><a href="#19-2-基于用户行为的挖掘" class="headerlink" title="19.2 基于用户行为的挖掘"></a>19.2 基于用户行为的挖掘</h3><p>在微博中，有两方面的数据可以使用，一方面是用户之间的关注关系，这不部分数据体现了用户的社交属性；另一方面是用户的行为数据，主要包括用户的原创、“转发”、“评论”、“收藏”、“点赞”、“点击短链接”等，这部分体现了用户的兴趣属性，通过对不同类型的数据挖掘，我们可以获得用户不同维度上的相似性。</p>
<p>在基于用户的行为的挖掘中，主要包括：</p>
<p>• 基于互动内容的兴趣挖掘</p>
<p>• 基于与待推荐微博博主的互动</p>
<h4 id="19-2-1-基于互动内容的兴趣挖掘"><a href="#19-2-1-基于互动内容的兴趣挖掘" class="headerlink" title="19.2.1 基于互动内容的兴趣挖掘"></a>19.2.1 基于互动内容的兴趣挖掘</h4><p>在微博中，用户的互动行为主要包括“转发”、“评论”、“点赞”、“收藏”和“点击短链接”等。这些行为的背后，表明用户对这条微博的内容在某种程度上产生了共鸣，但是，在不同的行为之间，其能够代表用户的兴趣程度也是不一样的，如“点赞”行为只是对博文内容的认同，而转发行为，则更多地表明用户希望让自己认同的微博内容被更多人看到，更能表明用户的兴趣。</p>
<p>基于互动内容的兴趣挖掘是指利用一些机器学习或者文本处理的方法，提取出用户互动微博文本中的核心词，一般提取核心词的主要步骤为：</p>
<p>• 对文本进行分词，常用的分词工具有：paoding、FudanNLP、CRF++、jieba等•去掉停用词，并计算剩余词的T F-I D F值，取T D-I D F值较高的词作为核心词</p>
<p>以这些核心词作为用户的标签，并将这些信息保存到对应的数据库中，其具体过程如图19.4所示。</p>
<p><img src="Image00718.jpg" alt></p>
<p>图19.4 基于互动内容的兴趣挖掘</p>
<h4 id="19-2-2-基于与博主互动的兴趣挖掘"><a href="#19-2-2-基于与博主互动的兴趣挖掘" class="headerlink" title="19.2.2 基于与博主互动的兴趣挖掘"></a>19.2.2 基于与博主互动的兴趣挖掘</h4><p>当用户A与待推荐微博的博主之间有过互动行为时，在一定程度上表明该用户与博主之间存在某种兴趣上的相似性，对于博主发布的微博，用户A互动的可能性比较大，因此，可以选择将这部分用户作为待推广的候选集。</p>
<p>在基于与博主互动的兴趣挖掘中，是指将微博博主的微博投放给与其互动过的一些用户。基于与博主互动的兴趣挖掘的主要任务是对历史的“转发”、“评论”、“点赞”、“收藏”等数据进行处理，从中提取出博主与互动用户之间的关系，并将这样的对应关系存入对应的数据库中，其具体的过程如图19.5所示。</p>
<p><img src="Image00807.jpg" alt></p>
<p>图19.5 基于与博主互动的兴趣挖掘</p>
<h3 id="19-3-基于相似用户的挖掘"><a href="#19-3-基于相似用户的挖掘" class="headerlink" title="19.3 基于相似用户的挖掘"></a>19.3 基于相似用户的挖掘</h3><p>“相似用户”的概念在不同的应用场景下的理解是不同的，如基于相似兴趣的相似用户，基于不同群体的相似用户等。在基于相似兴趣的相似用户中，这些用户都对同一个事物有兴趣，如一群对“机器学习”感兴趣的用户的集合。在基于不同群体的相似用户中，这些用户可能是年龄区间也可能是消费能力相似，如大学生群体等。在精准推荐中，我们主要考虑的是基于相似兴趣的相似用户。</p>
<h4 id="19-3-1-基于“＠”人的相似用户挖掘"><a href="#19-3-1-基于“＠”人的相似用户挖掘" class="headerlink" title="19.3.1 基于“＠”人的相似用户挖掘"></a>19.3.1 基于“＠”人的相似用户挖掘</h4><p>从上面的分析中，我们知道，在微博中，一个用户与其粉丝之间的关系大致可以分为：</p>
<p>• 社交关系：如亲戚、朋友、同事、同学等</p>
<p>• 兴趣关系：如机器学习爱好者等</p>
<p>一个用户与其粉丝之间存在某种相似性，或者是兴趣维度的相似，或者是群体间的相似。在微博中，为了能够定向让某个人看到，我们会在这条微博中加入“＠”该用户的标记。“＠”标记在一定程度上说明该信息与被“＠”用户之间存在关系，而由上述的分析可知，用户与其粉丝之间存在社交关系或者兴趣关系，因此，这样的挖掘方法能够充分利用微博数据的特殊性。</p>
<p>在基于“＠”人的相似用户挖掘中，对于包含“＠”信息的微博，通过对“＠”用户的提取，查找到该用户的粉丝，并进行投放，需要查找的库为用户的粉丝库。</p>
<h4 id="19-3-2-基于社区的相似用户挖掘"><a href="#19-3-2-基于社区的相似用户挖掘" class="headerlink" title="19.3.2 基于社区的相似用户挖掘"></a>19.3.2 基于社区的相似用户挖掘</h4><p>社区划分是社交网络中研究比较多的一个话题，对于不同结构的社交网络有不同的社区划分算法，在本书的第13章中，我们介绍了标签传播Label Propagation算法。对于社区并没有明确的定义，通常对于社区的理解是：在网络中，由一些节点构成特定的分组，在同一个分组内的节点，通过节点之间的连接边紧密地连接在一起，而在分组与分组之间，其连接比较松散，称每一个分组为一个社区。社区划分算法通过某种方式将用户划分到不同的社区中，社区内部的连接较为强烈，社区与社区之间有比较明显的界限。</p>
<p>在微博中，用户与用户之间的连接主要分为两种，一种是通过“关注”操作连接两个用户，另外一种是通过“转发”、“评论”、“点赞”、“收藏”、“点击短链接”等行为连接两个用户。在上述的两种连接中，前者的关系不仅包含了兴趣关系，也包含社交关系，而后者，更多倾向于兴趣关系。在这里，我们想要得到的更多的是用户之间的兴趣关系，因此，我们这里使用到的数据是用户之间的“转发”、“评论”、“点赞”、“收藏”、“点击短链接”等行为数据。</p>
<p>从这些行为数据中我们可以知道，这些行为数据连接的两个用户之间的边是存在方向的，即构成的图是有向图。有向图是指图中的边是带有方向的图。对于有向图，每两个节点之间的边的条数是两条，分别为流出的边和流入的边，其流出边的总数为出度，流入边的总数为入度，有向图如图19.6所示。</p>
<p><img src="Image00750.jpg" alt></p>
<p>图19.6 有向图</p>
<p>对于节点5，其出度为2，入度也为2。对于更多的有向图的知识，可参阅相关图论的书籍。</p>
<p>而对于标签传播 Label Propagation 算法，其对数据的要求是无向图，为了使得Label Propagation算法能够利用上述的行为数据对用户进行社区划分，我们将图中的流出边和流入边进行合并，合并的公式为：</p>
<p><img src="Image00767.jpg" alt></p>
<p>其中w i，j 表示的是节点 j到节点i的权重， _λ i，j _ 表示的是节点i到节点 j的权重， _λ j，i _ 表示的是节点 j到节点i的权重。通过参数 _α_ 和参数 _β_ 可以调节不同的权重比例。此时，我们可以利用Label Propagation算法对微博中的社区进行划分。</p>
<p>我们对参数 _α_ 与参数 _β_ 进行了不同的取值，并利用30天的行为数据，最终得到，当 _α_ =0.6， _β_ =0.4时效果比较好，最终识别出12629个社区。虽然我们挖掘出了这些社区，但是这些社区的质量参差不齐，有的社区内部较为活跃，而有些社区，内部并不活跃，我们试图将一些不活跃的社区从我们挖掘好的社区中去除，此时，计算每一个社区中的信息熵，熵越大表明该社区越活跃，因此，我们过滤一些不活跃的社区，保留活跃的社区。</p>
<p>当有微博需要投放时，选择某几个社区，将微博投放给社区中的住户，选择社区的方式有很多种，比如：</p>
<p>•微博的主题与社区标签的匹配</p>
<p>• 微博博主所在的社区</p>
<p>在基于社区的相似用户的挖掘中，利用 Label Propagation 算法对社区进行挖掘，最终将社区对应的用户列表存储到对应的数据库中，其具体的过程如图19.7所示。</p>
<p><img src="Image00785.jpg" alt></p>
<p>图19.7 基于社区的相似用户挖掘</p>
<h4 id="19-3-3-基于协同过滤的相似用户挖掘"><a href="#19-3-3-基于协同过滤的相似用户挖掘" class="headerlink" title="19.3.3 基于协同过滤的相似用户挖掘"></a>19.3.3 基于协同过滤的相似用户挖掘</h4><p>对于相似用户的挖掘，除了上述的社区挖掘的方法外，还可以使用协同过滤的方法。在协同过滤算法中，主要分为基于用户的协同过滤算法和基于项的协同过滤算法，其主要的区别是在相似度的计算过程中。对于这两种协同过滤算法的详细介绍，可以参见本书的第14章。</p>
<p>我们以基于用户的协同过滤算法为例，在基于用户的协同过滤算法中，主要计算任意两个用户A和B之间的相似度，并利用该相似度将用户B互动过而用户A没有互动过的商品推荐给用户A。</p>
<p>在微博中，每个用户都有自己的粉丝列表，我们可以利用两个用户的粉丝列表来度量这两个用户之间的相似度，假设用户A的粉丝列表集合为F A ，用户B的粉丝列表集合为F B ，那么，用户A和用户B的相似度为：</p>
<p><img src="Image00809.jpg" alt></p>
<p>其中，F A ∩F B 表示的是F A 和F B 的交集，<img src="Image00834.jpg" alt> 表示的是集合F A 中元素的个数。</p>
<h3 id="19-4-点击率预估"><a href="#19-4-点击率预估" class="headerlink" title="19.4 点击率预估"></a>19.4 点击率预估</h3><p>点击率预估是广告计算中的核心问题，点击率预估的目的是为了广告的排序，同样，在精准推荐中，我们的最终目的是为了使得推荐给用户的结果，用户对其互动率最高。因此，在产生候选后，我们需要对这些候选进行排序，以产生最终的曝光顺序。</p>
<h4 id="19-4-1-点击率预估的概念"><a href="#19-4-1-点击率预估的概念" class="headerlink" title="19.4.1 点击率预估的概念"></a>19.4.1 点击率预估的概念</h4><p>在精准推荐中，我们的目标是使得推荐给用户的微博，用户对其能有较高的互动率，以此来评价推荐质量的好坏，在这里，我们借用广告计算中的一个重要概念：点击率。此时，对于精准推荐，点击率（Click Through Rate，CTR）成为一个重要的指标，点击率CTR的计算方法为：</p>
<p><img src="Image00855.jpg" alt></p>
<p>其中，＃impression表示的是曝光的次数，＃click表示的是互动的次数。</p>
<p>为了能够使得整体的互动率最高，我们需要对候选集进行排序，排序的依据便是预测的点击率（pCTR）。因此，对于候选集，我们必须有点击率的预估方法。</p>
<h4 id="19-4-2-点击率预估的方法"><a href="#19-4-2-点击率预估的方法" class="headerlink" title="19.4.2 点击率预估的方法"></a>19.4.2 点击率预估的方法</h4><p>近年来，机器学习技术被广泛应用于点击率预估模型中，其中，使用最多的方法是Logistic Regression算法，对于Logistic Regression算法的介绍，可以参见本书的第1章。在训练Logistic Regression模型的过程中，主要分为：①收集数据；②清洗数据并进行特征转换；③利用转换后的特征训练Logistic Regression模型。</p>
<p>首先，我们需要收集用于训练CTR模型的数据。在CTR预估中，有两种特征选择的方法：静态特征和动态特征。其中，静态特征指的是年龄、性别等一些自然属性，而动态特征指的是利用历史的 CTR 作为特征，在我们的环境中，为兼顾到不同来源的CTR之间的对比，我们选用动态特征。</p>
<p>选择好特征之后，我们对特征进行转换，也称为特征处理。在这里，我们使用的方法是离散化的方法，针对动态特征，可以采用等频离散的方式对特征进行离散化，离散化后的是特征成为0或者1的特征组合。</p>
<p>利用离散化后的特征训练Logistic Regression算法，得到最终的CTR预测模型。</p>
<p>以上便是 CTR 预估的基本方法，近年来，随着行业内对 CTR 预估的深入探索，提出了以下几种改进的方法：</p>
<p>• 以梯度提升决策树的方法代替特征处理中的等频离散</p>
<p>在利用梯度提升决策树的方法进行特征处理时，首先，对于梯度提升决策树GBDT算法在本书中并未涉及，读者可查阅相关的GBDT的文献。利用训练数据训练梯度提升决策树GBDT，每一个样本都会落到一棵树中的某一个叶子节点上，以叶子节点的编号作为离散化后的特征，其具体过程如图19.8所示。</p>
<p><img src="Image00878.jpg" alt></p>
<p>图19.8 GBDT+LR的CTR预估</p>
<p>在图19.8中，原始特征分别经过两棵回归树Tree1和Tree2，分别落到Tree1的第 2 个叶子节点上和 Tree2 的第 1 个叶子节点上，此时，对于处理后的特征X为{0，1，0，1，0}，再利用离散化后的特征训练Logistic Regression模型。</p>
<p>• 利用FM算法代替LR算法</p>
<p>对于FM算法，具体可以参见本书的第2章。在FM算法中，能够发现LR所不能发现的交叉特征，因此利用FM算法在一定程度上能够提高CTR预估的效果。</p>
<p>• 利用DNN算法代替LR算法</p>
<p>目前对于DNN算法在CTR预估上的探索还比较少，但是，这必将是未来一个很大的应用方向，对于各种DNN算法，可以参见本书的第五部分。</p>
<h3 id="19-5-各种数据技术的效果"><a href="#19-5-各种数据技术的效果" class="headerlink" title="19.5 各种数据技术的效果"></a>19.5 各种数据技术的效果</h3><p>以上便是精准推荐的核心部分，在精准推荐中，首先利用离线挖掘技术挖掘候选数据，挖掘的主要方法包括：基于互动内容的兴趣挖掘、基于与博主互动的兴趣挖掘、基于“＠”人的相似用户挖掘、基于社区的相似用户挖掘和基于协同过滤的相似用户挖掘。挖掘完后将各部分数据存储到对应的数据库中，当有微博需要被推荐时，从此数据库中查询出对应的候选集，并对其排序，进而输出推荐结果。</p>
<p>最终，在实际的环境中，各数据源的曝光量如图19.9所示。</p>
<p><img src="Image00904.jpg" alt></p>
<p>图19.9 各数据源的曝光量</p>
<p>在图19.9中显示了各数据源在实际中的曝光量，从曝光量的数据可以看出，基于与博主互动和“＠”人的粉丝的曝光量相较其他两项很少，这两部分数据对于兴趣的挖掘也会相对比较精准，对于各数据源的互动率如图19.10所示。</p>
<p><img src="Image00715.jpg" alt></p>
<p>图19.10 各数据源的互动率</p>
<p>从图19.10中可以看出，基于与博主互动与“＠”的人这两部分的互动率最高，这也与我们的推断相似，且这两部分的数据量都比较小。基于社区+协同过滤的相似用户挖掘可以作为数据的一种补充方式，这部分数据的曝光量也不是很大。</p>
<p><strong>参考文献</strong></p>
<p>[1] 项亮.推荐系统实践[M].北京：人民邮电出版社.2012.</p>
<p>[2] Chapelle O,Manavoglu E,Rosales R.Simple and Scalable Response Prediction for Display Advertising[J].Acm Transactions on Intelligent Systems&amp;Technology,2014,5(4):1-34.</p>
<p>[3] He X,Pan J,Jin O,et al.Practical lessons from predicting clicks on adsat facebook[C].Proceedings of 20th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining.ACM,2014:1-9.</p>
<p><strong>附录A</strong></p>
<p><strong>A.1 Python的安装</strong></p>
<p>本书中的 Python 程序是在 Python 2.7.9 版本下开发完成的，建议大家选择安装Python 2.7。</p>
<p>可以从 https：//www.python.org/downloads/找到对应的 Python 版本，如选择了Python 2.7.9 版本，则跳转到下载页面 https：//www.python.org/downloads/release/python-279/，选择对应的平台，目前 Mac OS X 系统和 Linux 系统中都默认安装了Python 2.7，对于Windows平台的用户，可以直接选择对应的安装包进行安装。</p>
<p>在Windows平台上，下载好安装包后，根据“下一步”的指示，完成安装。安装完成后，通过“Win+R”方式打开控制台，输入以下的命令：</p>
<p><img src="Image00337.jpg" alt></p>
<p>启动 Python，若此时安装正确，则可以看到 Python 对应的版本号等其他一些信息：</p>
<p><img src="Image00010.jpg" alt></p>
<p><strong>A.2 numpy的安装</strong></p>
<p>在本书的程序中，我们需要使用到numpy模块，因此需要安装numpy模块，可以通过以下命令：</p>
<p><img src="Image00491.jpg" alt></p>
<p>来安装numpy，对于Windows平台的用户，可以通过https：//sourceforge.net/projects/numpy/files/NumPy/网站选择对应的安装包进行安装。</p>
<p><strong>A.3 Python的基本操作</strong></p>
<p><strong>A.3.1 Python中的基本数据类型</strong></p>
<p>Python的基本数据类型包括整型、浮点型、布尔型和字符串等：</p>
<p><img src="Image00108.jpg" alt></p>
<p><strong>A.3.2 Python中的数据结构</strong></p>
<p>Python的数据结构包括：列表（List）、字典（Dict）、集合（Set）和元组（Tuple）。</p>
<p><img src="Image00640.jpg" alt></p>
<p>在集合中存储的元素都是互异的元素，在字典中，是以 key：value 的形式存储元素的。</p>
<p><strong>A.3.3 Python中函数的定义和使用</strong></p>
<p>Python函数使用def来定义函数，以符号函数sign为例：</p>
<p><img src="Image00278.jpg" alt></p>
<p>通过以下的方式直接调用：</p>
<p><img src="Image00546.jpg" alt></p>
<p><strong>A.3.4 Python中类的定义和使用</strong></p>
<p><img src="Image00657.jpg" alt></p>
<p>当定义好类后，需要声明和初始化：</p>
<p><img src="Image00755.jpg" alt></p>
<p>更多有关Python的语法知识，可以自行参见Python教程。</p>
<p><strong>A.4 Numpy的基本操作</strong></p>
<p>Numpy是Python中用于科学计算的核心库。利用numpy模块，可以在Python中方便地实现矩阵的相关运算。</p>
<p><strong>A.4.1 数组array</strong></p>
<p>numpy中的数组是一组相同类型的数据的集合，可以通过numpy中的array函数生成：</p>
<p><img src="Image00860.jpg" alt></p>
<p>在numpy中，还提供了其他的创建数组的方法：</p>
<p><img src="Image00197.jpg" alt></p>
<p>对numpy中数组元素的访问，可以使用下标的方式：</p>
<p><img src="Image00387.jpg" alt></p>
<p>对于数组的计算，通常包含加法、减法和乘法：</p>
<p><img src="Image00781.jpg" alt></p>
<p><img src="Image00730.jpg" alt></p>
<p>需要注意的是，在乘法中，a*b表示的是对应元素相乘，而要是实现矩阵的乘积，应该使用dot函数。</p>
<p><strong>A.4.2 矩阵mat</strong></p>
<p>与数组相似的还有一种直接转换成矩阵的表示：</p>
<p><img src="Image00524.jpg" alt></p>
<p>这样，就可以使用各种矩阵的计算，如求转置、求逆、矩阵的乘积等：</p>
<p><img src="Image00598.jpg" alt></p>
<p>有关 numpy 的更多操作，可以查阅官方文档 https：//docs.scipy.org/doc/numpy/user/index.html。</p>
<p><strong>附录B</strong></p>
<p><strong>B.1 TensorFlow的安装</strong></p>
<p>对于TensorFlow的安装，在完成这本书的过程中，其还未支持Windows平台，因此，我的实验是在 Ubuntu 环境下完成的。对于 Ubuntu 环境，可以直接选择安装CPU版本：</p>
<p>sudo pip install—upgrade <a href="https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-" target="_blank" rel="noopener">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-</a> linux_x86_64.whl</p>
<p>对于更多其他的安装可以参见TensorFlow官方文档。</p>
<p><strong>B.2 TensorFlow的基本操作</strong></p>
<p><strong>B.2.1 TensorFlow操作的特点</strong></p>
<p>在TensorFlow中，使用图来表示计算任务，图中的节点被称为操作，每一个操作获得0个或者多个Tensor，对这些Tensor执行计算，产生0个或者多个Tensor。在TensorFlow中，操作是以图的形式来描述的，为了进行计算，图必须在会话里被启动。对于TensorFlow的基本操作如下节所示。</p>
<p><strong>B.2.2 TensorFlow的基本操作</strong></p>
<p>首先，为了能够使用TensorFlow，我们需要导入tensorflow：</p>
<p><img src="Image00711.jpg" alt></p>
<p>现在，我们就可以使用TensorFlow的基本功能了。</p>
<p>• 常量的定义</p>
<p><img src="Image00800.jpg" alt></p>
<p>这样就定义了一个值为10的常量x。</p>
<p>• 变量的定义</p>
<p>在TensorFlow中，变量用Variable来定义，并且必须初始化，如：</p>
<p><img src="Image00443.jpg" alt></p>
<p>这样分别定义了一个3×3的全1矩阵x和一个3×3的全0矩阵y。当变量定义完后，还必须使用如下的操作：</p>
<p><img src="Image00117.jpg" alt></p>
<p>这样，变量才能被使用。</p>
<p>• 占位符</p>
<p>我们已经介绍了变量在定义时要初始化，但是如果有些变量我们刚开始并不知道它们的值，这样就无法完成初始化，此时，可以利用占位符来表示：</p>
<p><img src="Image00318.jpg" alt></p>
<p>这样，就指定了这个变量的类型和大小。</p>
<p>• 图（graph）</p>
<p>在TensorFlow中，要实现具体的运算，如两个变量的加法运算，我们不能直接定义两个变量，并将两个数相加，输出结果。在TensorFlow中，每一个变量都是一个tensor对象，对象间的运算称之为操作（op），TensorFlow不会去一条条地执行各个操作，而是把所有的操作都放入到一个图（graph）中，图中的每一个结点就是一个操作。然后将整个graph的计算过程交给一个TensorFlow的Session，此Session可以运行整个计算过程，如计算两个变量加法的过程为：</p>
<p><img src="Image00342.jpg" alt></p>
<p>其中 sess.run（）是执行操作，注意要先执行变量初始化操作，再执行运算操作。Session 需要先创建，使用完后还需要释放。如果使用占位符，则需要使用 feed 为占位符赋值，如：</p>
<p><img src="Image00453.jpg" alt></p>
<p>对于TensorFlow的其他操作可以参见TensorFlow的官方文档。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      <div>    
       
       
      <ul class="post-copyright">
        <li class="post-copyright-author">
            <strong>本文作者：</strong>hac_lang
        </li>
        <li class="post-copyright-link">
          <strong>本文链接：</strong>
          <a href="/2017/08/20/book-《Python机器学习算法》/" title="book_《Python机器学习算法》">2017/08/20/book-《Python机器学习算法》/</a>
        </li>
        <li class="post-copyright-license">
          <strong>版权声明： </strong>
          许可协议，请勿用于商业，转载注明出处！
        </li>
      </ul>
      
      </div>
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/编程/" rel="tag"># 编程</a>
          
            <a href="/tags/计算机科学/" rel="tag"># 计算机科学</a>
          
            <a href="/tags/计算机/" rel="tag"># 计算机</a>
          
            <a href="/tags/自评/" rel="tag"># 自评</a>
          
            <a href="/tags/books/" rel="tag"># books</a>
          
            <a href="/tags/算法/" rel="tag"># 算法</a>
          
            <a href="/tags/豆瓣6/" rel="tag"># 豆瓣6</a>
          
            <a href="/tags/更毕/" rel="tag"># 更毕</a>
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/计算机算法/" rel="tag"># 计算机算法</a>
          
            <a href="/tags/数学/" rel="tag"># 数学</a>
          
            <a href="/tags/T-工业技术/" rel="tag"># T-工业技术</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/02/book-《面向机器智能的TensorFlow实践》/" rel="next" title="book_《面向机器智能的TensorFlow实践》">
                <i class="fa fa-chevron-left"></i> book_《面向机器智能的TensorFlow实践》
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/06/book-《白话深度学习与TensorFlow》/" rel="prev" title="book_《白话深度学习与TensorFlow》">
                book_《白话深度学习与TensorFlow》 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
    <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="hac_lang">
  
  <p class="site-author-name" itemprop="name">hac_lang</p>
  <div class="site-description motion-element" itemprop="description">小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理</div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    

    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>







  <div class="links-of-author motion-element">
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://github.com/HACLANG" title="GitHub &rarr; https://github.com/HACLANG" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://stackoverflow.com/yourname" title="StackOverflow &rarr; https://stackoverflow.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://gitter.im" title="Gitter &rarr; https://gitter.im" rel="noopener" target="_blank"><i class="fa fa-fw fa-github-alt"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.jianshu.com/u/442ddccf3f32" title="简书 &rarr; https://www.jianshu.com/u/442ddccf3f32" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="Quora &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://plus.google.com/yourname" title="Google &rarr; https://plus.google.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="mailto:haclang.org@gmail.com" title="E-Mail &rarr; mailto:haclang.org@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="skype:haclang?call|chat" title="Skype &rarr; skype:haclang?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://twitter.com/haclang2" title="Twitter &rarr; https://twitter.com/haclang2" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="FaceBook &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
    
  </div>








          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-绪论"><span class="nav-text">0 绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0-1-机器学习基础"><span class="nav-text">0.1 机器学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-1-1-机器学习的概念"><span class="nav-text">0.1.1 机器学习的概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-1-2-机器学习算法的分类"><span class="nav-text">0.1.2 机器学习算法的分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-2-监督学习"><span class="nav-text">0.2 监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-2-1-监督学习"><span class="nav-text">0.2.1 监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-2-2-监督学习的流程"><span class="nav-text">0.2.2 监督学习的流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-2-3-监督学习算法"><span class="nav-text">0.2.3 监督学习算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-3-无监督学习"><span class="nav-text">0.3 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-3-1-无监督学习"><span class="nav-text">0.3.1 无监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-3-2-无监督学习的流程"><span class="nav-text">0.3.2 无监督学习的流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-3-3-无监督学习算法"><span class="nav-text">0.3.3 无监督学习算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-4-推荐系统和深度学习"><span class="nav-text">0.4 推荐系统和深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-4-1-推荐系统"><span class="nav-text">0.4.1 推荐系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-4-2-深度学习"><span class="nav-text">0.4.2 深度学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-5-Python和机器学习算法实践"><span class="nav-text">0.5 Python和机器学习算法实践</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#第一部分-分类算法"><span class="nav-text">第一部分 分类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Logistic-Regression"><span class="nav-text">1 Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Logistic-Regression模型"><span class="nav-text">1.1 Logistic Regression模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-线性可分VS线性不可分"><span class="nav-text">1.1.1 线性可分VS线性不可分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-Logistic-Regression模型"><span class="nav-text">1.1.2 Logistic Regression模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-3-损失函数"><span class="nav-text">1.1.3 损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-梯度下降法"><span class="nav-text">1.2 梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-梯度下降法的流程"><span class="nav-text">1.2.1 梯度下降法的流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-凸优化与非凸优化"><span class="nav-text">1.2.2 凸优化与非凸优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-3-利用梯度下降法训练Logistic-Regression模型"><span class="nav-text">1.2.3 利用梯度下降法训练Logistic Regression模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-梯度下降法的若干问题"><span class="nav-text">1.3 梯度下降法的若干问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-选择下降的方向"><span class="nav-text">1.3.1 选择下降的方向</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-步长的选择"><span class="nav-text">1.3.2 步长的选择</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Logistic-Regression算法实践"><span class="nav-text">1.4 Logistic Regression算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-利用训练样本训练Logistic-Regression模型"><span class="nav-text">1.4.1 利用训练样本训练Logistic Regression模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-最终的训练效果"><span class="nav-text">1.4.2 最终的训练效果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-3-对新数据进行预测"><span class="nav-text">1.4.3 对新数据进行预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Softmax-Regression"><span class="nav-text">2 Softmax Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-多分类问题"><span class="nav-text">2.1 多分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Softmax-Regression算法模型"><span class="nav-text">2.2 Softmax Regression算法模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-Softmax-Regression模型"><span class="nav-text">2.2.1 Softmax Regression模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-Softmax-Regression算法的代价函数"><span class="nav-text">2.2.2 Softmax Regression算法的代价函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Softmax-Regression算法的求解"><span class="nav-text">2.3 Softmax Regression算法的求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Softmax-Regression与Logistic-Regression的关系"><span class="nav-text">2.4 Softmax Regression与Logistic Regression的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-Softmax-Regression中的参数特点"><span class="nav-text">2.4.1 Softmax Regression中的参数特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-2-由Softmax-Regression到Logistic-Regression"><span class="nav-text">2.4.2 由Softmax Regression到Logistic Regression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Softmax-Regression算法实践"><span class="nav-text">2.5 Softmax Regression算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-1-对Softmax-Regression算法的模型进行训练"><span class="nav-text">2.5.1 对Softmax Regression算法的模型进行训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-2-最终的模型"><span class="nav-text">2.5.2 最终的模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-3-对新的数据的预测"><span class="nav-text">2.5.3 对新的数据的预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Factorization-Machine"><span class="nav-text">3 Factorization Machine</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Logistic-Regression算法的不足"><span class="nav-text">3.1 Logistic Regression算法的不足</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-因子分解机FM的模型"><span class="nav-text">3.2 因子分解机FM的模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-因子分解机FM模型"><span class="nav-text">3.2.1 因子分解机FM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-因子分解机FM可以处理的问题"><span class="nav-text">3.2.2 因子分解机FM可以处理的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-二分类因子分解机FM算法的损失函数"><span class="nav-text">3.2.3 二分类因子分解机FM算法的损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-FM算法中交叉项的处理"><span class="nav-text">3.3 FM算法中交叉项的处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-交叉项系数"><span class="nav-text">3.3.1 交叉项系数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-模型的求解"><span class="nav-text">3.3.2 模型的求解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-FM算法的求解"><span class="nav-text">3.4 FM算法的求解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-随机梯度下降（Stochastic-Gradient-Descent）"><span class="nav-text">3.4.1 随机梯度下降（Stochastic Gradient Descent）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-基于随机梯度的方式求解"><span class="nav-text">3.4.2 基于随机梯度的方式求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-3-FM算法流程"><span class="nav-text">3.4.3 FM算法流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-因子分解机FM算法实践"><span class="nav-text">3.5 因子分解机FM算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-1-训练FM模型"><span class="nav-text">3.5.1 训练FM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-最终的训练效果"><span class="nav-text">3.5.2 最终的训练效果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-3-对新的数据进行预测"><span class="nav-text">3.5.3 对新的数据进行预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-支持向量机"><span class="nav-text">4 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-二分类问题"><span class="nav-text">4.1 二分类问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-二分类的分隔超平面"><span class="nav-text">4.1.1 二分类的分隔超平面</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-感知机算法"><span class="nav-text">4.1.2 感知机算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-3-感知机算法存在的问题"><span class="nav-text">4.1.3 感知机算法存在的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-函数间隔和几何间隔"><span class="nav-text">4.2 函数间隔和几何间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-函数间隔"><span class="nav-text">4.2.1 函数间隔</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-几何间隔"><span class="nav-text">4.2.2 几何间隔</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-支持向量机"><span class="nav-text">4.3 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-间隔最大化"><span class="nav-text">4.3.1 间隔最大化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-支持向量和间隔边界"><span class="nav-text">4.3.2 支持向量和间隔边界</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-3-线性支持向量机"><span class="nav-text">4.3.3 线性支持向量机</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-支持向量机的训练"><span class="nav-text">4.4 支持向量机的训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-1-学习的对偶算法"><span class="nav-text">4.4.1 学习的对偶算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-2-由线性支持向量机到非线性支持向量机"><span class="nav-text">4.4.2 由线性支持向量机到非线性支持向量机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-3-序列最小最优化算法SMO"><span class="nav-text">4.4.3 序列最小最优化算法SMO</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-支持向量机SVM算法实践"><span class="nav-text">4.5 支持向量机SVM算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-1-训练SVM模型"><span class="nav-text">4.5.1 训练SVM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-2-利用训练样本训练SVM模型"><span class="nav-text">4.5.2 利用训练样本训练SVM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-3-利用训练好的SVM模型对新数据进行预测"><span class="nav-text">4.5.3 利用训练好的SVM模型对新数据进行预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-随机森林"><span class="nav-text">5 随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-决策树分类器"><span class="nav-text">5.1 决策树分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-决策树的基本概念"><span class="nav-text">5.1.1 决策树的基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-选择最佳划分的标准"><span class="nav-text">5.1.2 选择最佳划分的标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-3-停止划分的标准"><span class="nav-text">5.1.3 停止划分的标准</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-CART分类树算法"><span class="nav-text">5.2 CART分类树算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-CART分类树算法的基本原理"><span class="nav-text">5.2.1 CART分类树算法的基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-CART分类树的构建"><span class="nav-text">5.2.2 CART分类树的构建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-3-利用构建好的分类树进行预测"><span class="nav-text">5.2.3 利用构建好的分类树进行预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-集成学习（Ensemble-Learning）"><span class="nav-text">5.3 集成学习（Ensemble Learning）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-1-集成学习的思想"><span class="nav-text">5.3.1 集成学习的思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-2-集成学习中的典型方法"><span class="nav-text">5.3.2 集成学习中的典型方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-随机森林（Random-Forests）"><span class="nav-text">5.4 随机森林（Random Forests）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-1-随机森林算法模型"><span class="nav-text">5.4.1 随机森林算法模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-2-随机森林算法流程"><span class="nav-text">5.4.2 随机森林算法流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-随机森林RF算法实践"><span class="nav-text">5.5 随机森林RF算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-1-训练随机森林模型"><span class="nav-text">5.5.1 训练随机森林模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-2-最终的训练结果"><span class="nav-text">5.5.2 最终的训练结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-3-对新数据的预测"><span class="nav-text">5.5.3 对新数据的预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-BP神经网络"><span class="nav-text">6 BP神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-神经元概述"><span class="nav-text">6.1 神经元概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-1-神经元的基本结构"><span class="nav-text">6.1.1 神经元的基本结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-激活函数"><span class="nav-text">6.1.2 激活函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-神经网络模型"><span class="nav-text">6.2 神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-神经网络的结构"><span class="nav-text">6.2.1 神经网络的结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-神经网络中的参数说明"><span class="nav-text">6.2.2 神经网络中的参数说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-3-神经网络的计算"><span class="nav-text">6.2.3 神经网络的计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-神经网络中参数的求解"><span class="nav-text">6.3 神经网络中参数的求解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-1-神经网络损失函数"><span class="nav-text">6.3.1 神经网络损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-2-损失函数的求解"><span class="nav-text">6.3.2 损失函数的求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-3-BP神经网络的学习过程"><span class="nav-text">6.3.3 BP神经网络的学习过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-BP神经网络中参数的设置"><span class="nav-text">6.4 BP神经网络中参数的设置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-1-非线性变换"><span class="nav-text">6.4.1 非线性变换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-2-权重向量的初始化"><span class="nav-text">6.4.2 权重向量的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-3-学习率"><span class="nav-text">6.4.3 学习率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-4-隐含层节点的个数"><span class="nav-text">6.4.4 隐含层节点的个数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-BP神经网络算法实践"><span class="nav-text">6.5 BP神经网络算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-1-训练BP神经网络模型"><span class="nav-text">6.5.1 训练BP神经网络模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-2-最终的训练效果"><span class="nav-text">6.5.2 最终的训练效果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-3-对新数据的预测"><span class="nav-text">6.5.3 对新数据的预测</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第二部分-回归算法"><span class="nav-text">第二部分 回归算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-线性回归"><span class="nav-text">7 线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-基本线性回归"><span class="nav-text">7.1 基本线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-1-线性回归的模型"><span class="nav-text">7.1.1 线性回归的模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-2-线性回归模型的损失函数"><span class="nav-text">7.1.2 线性回归模型的损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-线性回归的最小二乘解法"><span class="nav-text">7.2 线性回归的最小二乘解法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-线性回归的最小二乘解法"><span class="nav-text">7.2.1 线性回归的最小二乘解法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-广义逆的概念"><span class="nav-text">7.2.2 广义逆的概念</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-牛顿法"><span class="nav-text">7.3 牛顿法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-1-基本牛顿法的原理"><span class="nav-text">7.3.1 基本牛顿法的原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-2-基本牛顿法的流程"><span class="nav-text">7.3.2 基本牛顿法的流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-3-全局牛顿法"><span class="nav-text">7.3.3 全局牛顿法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-4-Armijo搜索"><span class="nav-text">7.3.4 Armijo搜索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-5-利用全局牛顿法求解线性回归模型"><span class="nav-text">7.3.5 利用全局牛顿法求解线性回归模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-利用线性回归进行预测"><span class="nav-text">7.4 利用线性回归进行预测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-1-训练线性回归模型"><span class="nav-text">7.4.1 训练线性回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-2-最终的训练结果"><span class="nav-text">7.4.2 最终的训练结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-3-对新数据的预测"><span class="nav-text">7.4.3 对新数据的预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-局部加权线性回归"><span class="nav-text">7.5 局部加权线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-1-局部加权线性回归模型"><span class="nav-text">7.5.1 局部加权线性回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-2-局部加权线性回归的最终结果"><span class="nav-text">7.5.2 局部加权线性回归的最终结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-岭回归和Lasso回归"><span class="nav-text">8 岭回归和Lasso回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-线性回归存在的问题"><span class="nav-text">8.1 线性回归存在的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-岭回归模型"><span class="nav-text">8.2 岭回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-1-岭回归模型"><span class="nav-text">8.2.1 岭回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-2-岭回归模型的求解"><span class="nav-text">8.2.2 岭回归模型的求解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-Lasso回归模型"><span class="nav-text">8.3 Lasso回归模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-拟牛顿法"><span class="nav-text">8.4 拟牛顿法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-1-拟牛顿法"><span class="nav-text">8.4.1 拟牛顿法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-2-BFGS校正公式的推导"><span class="nav-text">8.4.2 BFGS校正公式的推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-3-BFGS校正的算法流程"><span class="nav-text">8.4.3 BFGS校正的算法流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-L-BFGS求解岭回归模型"><span class="nav-text">8.5 L-BFGS求解岭回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-1-BGFS算法存在的问题"><span class="nav-text">8.5.1 BGFS算法存在的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-2-L-BFGS算法思路"><span class="nav-text">8.5.2 L-BFGS算法思路</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-6-岭回归对数据的预测"><span class="nav-text">8.6 岭回归对数据的预测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-6-1-训练岭回归模型"><span class="nav-text">8.6.1 训练岭回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-6-2-最终的训练结果"><span class="nav-text">8.6.2 最终的训练结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-6-3-利用岭回归模型预测新的数据"><span class="nav-text">8.6.3 利用岭回归模型预测新的数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-CART树回归"><span class="nav-text">9 CART树回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-复杂的回归问题"><span class="nav-text">9.1 复杂的回归问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-1-线性回归模型"><span class="nav-text">9.1.1 线性回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-2-局部加权线性回归"><span class="nav-text">9.1.2 局部加权线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-3-CART算法"><span class="nav-text">9.1.3 CART算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-CART回归树生成"><span class="nav-text">9.2 CART回归树生成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-1-CART回归树的划分"><span class="nav-text">9.2.1 CART回归树的划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-2-CART回归树的构建"><span class="nav-text">9.2.2 CART回归树的构建</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-CART回归树剪枝"><span class="nav-text">9.3 CART回归树剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-3-1-前剪枝"><span class="nav-text">9.3.1 前剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-3-2-后剪枝"><span class="nav-text">9.3.2 后剪枝</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-CART回归树对数据预测"><span class="nav-text">9.4 CART回归树对数据预测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-1-利用训练数据训练CART回归树模型"><span class="nav-text">9.4.1 利用训练数据训练CART回归树模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-2-最终的训练结果"><span class="nav-text">9.4.2 最终的训练结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-3-利用训练好的CART回归树模型对新的数据预测"><span class="nav-text">9.4.3 利用训练好的CART回归树模型对新的数据预测</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第三部分-聚类算法"><span class="nav-text">第三部分 聚类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#10-K-Means"><span class="nav-text">10 K-Means</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-相似性的度量"><span class="nav-text">10.1 相似性的度量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-1-闵可夫斯基距离"><span class="nav-text">10.1.1 闵可夫斯基距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-2-曼哈顿距离"><span class="nav-text">10.1.2 曼哈顿距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-3-欧氏距离"><span class="nav-text">10.1.3 欧氏距离</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-K-Means算法原理"><span class="nav-text">10.2 K-Means算法原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-2-1-K-Means算法的基本原理"><span class="nav-text">10.2.1 K-Means算法的基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-2-2-K-Means算法步骤"><span class="nav-text">10.2.2 K-Means算法步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-2-3-K-Means算法与矩阵分解"><span class="nav-text">10.2.3 K-Means算法与矩阵分解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-3-K-Means算法实践"><span class="nav-text">10.3 K-Means算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-3-1-导入数据"><span class="nav-text">10.3.1 导入数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-3-2-初始化聚类中心"><span class="nav-text">10.3.2 初始化聚类中心</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-3-3-聚类过程"><span class="nav-text">10.3.3 聚类过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-3-4-最终的聚类结果"><span class="nav-text">10.3.4 最终的聚类结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-4-K-Means-算法"><span class="nav-text">10.4 K-Means++算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-4-1-K-Means算法存在的问题"><span class="nav-text">10.4.1 K-Means算法存在的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-4-2-K-Means-算法的基本思路"><span class="nav-text">10.4.2 K-Means++算法的基本思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-4-3-K-Means-算法的过程和最终效果"><span class="nav-text">10.4.3 K-Means++算法的过程和最终效果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-Mean-Shift"><span class="nav-text">11 Mean Shift</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-Mean-Shift向量"><span class="nav-text">11.1 Mean Shift向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-核函数"><span class="nav-text">11.2 核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-3-Mean-Shift算法原理"><span class="nav-text">11.3 Mean Shift算法原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-1-引入核函数的Mean-Shift向量"><span class="nav-text">11.3.1 引入核函数的Mean Shift向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-2-Mean-Shift算法的基本原理"><span class="nav-text">11.3.2 Mean Shift算法的基本原理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-4-Mean-Shift算法的解释"><span class="nav-text">11.4 Mean Shift算法的解释</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-4-1-概率密度梯度"><span class="nav-text">11.4.1 概率密度梯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-4-2-Mean-Shift向量的修正"><span class="nav-text">11.4.2 Mean Shift向量的修正</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-4-3-Mean-Shift算法流程"><span class="nav-text">11.4.3 Mean Shift算法流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-5-Mean-Shift算法实践"><span class="nav-text">11.5 Mean Shift算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-5-1-Mean-Shift的主过程"><span class="nav-text">11.5.1 Mean Shift的主过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-5-2-Mean-Shift的最终聚类结果"><span class="nav-text">11.5.2 Mean Shift的最终聚类结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-DBSCAN"><span class="nav-text">12 DBSCAN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-基于密度的聚类"><span class="nav-text">12.1 基于密度的聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#12-1-1-基于距离的聚类算法存在的问题"><span class="nav-text">12.1.1 基于距离的聚类算法存在的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-1-2-基于密度的聚类算法"><span class="nav-text">12.1.2 基于密度的聚类算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-DBSCAN算法原理"><span class="nav-text">12.2 DBSCAN算法原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#12-2-1-DBSCAN算法的基本概念"><span class="nav-text">12.2.1 DBSCAN算法的基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-2-2-DBSCAN算法原理"><span class="nav-text">12.2.2 DBSCAN算法原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-2-3-DBSCAN算法流程"><span class="nav-text">12.2.3 DBSCAN算法流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-DBSCAN算法实践"><span class="nav-text">12.3 DBSCAN算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#12-3-1-DBSCAN算法的主要过程"><span class="nav-text">12.3.1 DBSCAN算法的主要过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-3-2-Mean-Shift的最终聚类结果"><span class="nav-text">12.3.2 Mean Shift的最终聚类结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-Label-Propagation"><span class="nav-text">13 Label Propagation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-社区划分"><span class="nav-text">13.1 社区划分</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-1-社区以及社区划分"><span class="nav-text">13.1.1 社区以及社区划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-2-社区划分的算法"><span class="nav-text">13.1.2 社区划分的算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-3-社区划分的评价标准"><span class="nav-text">13.1.3 社区划分的评价标准</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-2-Label-Propagation算法原理"><span class="nav-text">13.2 Label Propagation算法原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#13-2-1-Label-Propagation算法的基本原理"><span class="nav-text">13.2.1 Label Propagation算法的基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-2-2-标签传播"><span class="nav-text">13.2.2 标签传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-2-3-迭代的终止条件"><span class="nav-text">13.2.3 迭代的终止条件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-3-Label-Propagation算法过程"><span class="nav-text">13.3 Label Propagation算法过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-4-Label-Propagation算法实践"><span class="nav-text">13.4 Label Propagation算法实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#13-4-1-导入数据"><span class="nav-text">13.4.1 导入数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-4-2-社区的划分"><span class="nav-text">13.4.2 社区的划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-4-3-最终的结果"><span class="nav-text">13.4.3 最终的结果</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第四部分-推荐算法"><span class="nav-text">第四部分 推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#14-协同过滤算法"><span class="nav-text">14 协同过滤算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-1-推荐系统的概述"><span class="nav-text">14.1 推荐系统的概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-1-推荐系统"><span class="nav-text">14.1.1 推荐系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-2-推荐问题的描述"><span class="nav-text">14.1.2 推荐问题的描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-3-推荐的常用方法"><span class="nav-text">14.1.3 推荐的常用方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-基于协同过滤的推荐"><span class="nav-text">14.2 基于协同过滤的推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-2-1-协同过滤算法概述"><span class="nav-text">14.2.1 协同过滤算法概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-2-2-协同过滤算法的分类"><span class="nav-text">14.2.2 协同过滤算法的分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-3-相似度的度量方法"><span class="nav-text">14.3 相似度的度量方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-1-欧氏距离"><span class="nav-text">14.3.1 欧氏距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-2-皮尔逊相关系数（Pearson-Correlation）"><span class="nav-text">14.3.2 皮尔逊相关系数（Pearson Correlation）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-3-余弦相似度"><span class="nav-text">14.3.3 余弦相似度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-基于协同过滤的推荐算法"><span class="nav-text">14.4 基于协同过滤的推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-4-1-基于用户的协同过滤算法"><span class="nav-text">14.4.1 基于用户的协同过滤算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-4-2-基于项的协同过滤算法"><span class="nav-text">14.4.2 基于项的协同过滤算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-5-利用协同过滤算法进行推荐"><span class="nav-text">14.5 利用协同过滤算法进行推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-5-1-导入用户-商品数据"><span class="nav-text">14.5.1 导入用户-商品数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-5-2-利用基于用户的协同过滤算法进行推荐"><span class="nav-text">14.5.2 利用基于用户的协同过滤算法进行推荐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-5-3-利用基于项的协同过滤算法进行推荐"><span class="nav-text">14.5.3 利用基于项的协同过滤算法进行推荐</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-基于矩阵分解的推荐算法"><span class="nav-text">15 基于矩阵分解的推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-1-矩阵分解"><span class="nav-text">15.1 矩阵分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-2-基于矩阵分解的推荐算法"><span class="nav-text">15.2 基于矩阵分解的推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-2-1-损失函数"><span class="nav-text">15.2.1 损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-2-2-损失函数的求解"><span class="nav-text">15.2.2 损失函数的求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-2-3-加入正则项的损失函数即求解方法"><span class="nav-text">15.2.3 加入正则项的损失函数即求解方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-2-4-预测"><span class="nav-text">15.2.4 预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-3-利用矩阵分解进行推荐"><span class="nav-text">15.3 利用矩阵分解进行推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-1-利用梯度下降对用户商品矩阵分解和预测"><span class="nav-text">15.3.1 利用梯度下降对用户商品矩阵分解和预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-2-最终的结果"><span class="nav-text">15.3.2 最终的结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-4-非负矩阵分解"><span class="nav-text">15.4 非负矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-4-1-非负矩阵分解的形式化定义"><span class="nav-text">15.4.1 非负矩阵分解的形式化定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-4-2-损失函数"><span class="nav-text">15.4.2 损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-4-3-优化问题的求解"><span class="nav-text">15.4.3 优化问题的求解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-5-利用非负矩阵分解进行推荐"><span class="nav-text">15.5 利用非负矩阵分解进行推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-5-1-利用乘法规则进行分解和预测"><span class="nav-text">15.5.1 利用乘法规则进行分解和预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-5-2-最终的结果"><span class="nav-text">15.5.2 最终的结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16-基于图的推荐算法"><span class="nav-text">16 基于图的推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#16-1-二部图与推荐算法"><span class="nav-text">16.1 二部图与推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-1-1-二部图"><span class="nav-text">16.1.1 二部图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-1-2-由用户商品矩阵到二部图"><span class="nav-text">16.1.2 由用户商品矩阵到二部图</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-2-PageRank算法"><span class="nav-text">16.2 PageRank算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-2-1-PageRank算法的概念"><span class="nav-text">16.2.1 PageRank算法的概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-2-2-PageRank的两个假设"><span class="nav-text">16.2.2 PageRank的两个假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-2-3-PageRank的计算方法"><span class="nav-text">16.2.3 PageRank的计算方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-3-PersonalRank算法"><span class="nav-text">16.3 PersonalRank算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-3-1-PersonalRank算法原理"><span class="nav-text">16.3.1 PersonalRank算法原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-3-2-PersonalRank算法的流程"><span class="nav-text">16.3.2 PersonalRank算法的流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-4-利用PersonalRank算法进行推荐"><span class="nav-text">16.4 利用PersonalRank算法进行推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-4-1-利用PersonalRank算法进行推荐"><span class="nav-text">16.4.1 利用PersonalRank算法进行推荐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-4-2-最终的结果"><span class="nav-text">16.4.2 最终的结果</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第五部分-深度学习"><span class="nav-text">第五部分 深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#17-AutoEncoder"><span class="nav-text">17 AutoEncoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-多层神经网络"><span class="nav-text">17.1 多层神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#17-1-1-三层神经网络模型"><span class="nav-text">17.1.1 三层神经网络模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-1-2-由三层神经网络到多层神经网络"><span class="nav-text">17.1.2 由三层神经网络到多层神经网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-2-AutoEncoder模型"><span class="nav-text">17.2 AutoEncoder模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#17-2-1-AutoEncoder模型结构"><span class="nav-text">17.2.1 AutoEncoder模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-2-2-AutoEncoder的损失函数"><span class="nav-text">17.2.2 AutoEncoder的损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-3-降噪自编码器Denoising-AutoEncoder"><span class="nav-text">17.3 降噪自编码器Denoising AutoEncoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#17-3-1-Denoising-AutoEncoder原理"><span class="nav-text">17.3.1 Denoising AutoEncoder原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-3-2-Denoising-AutoEncoder实现"><span class="nav-text">17.3.2 Denoising AutoEncoder实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-4-利用Denoising-AutoEncoders构建深度网络"><span class="nav-text">17.4 利用Denoising AutoEncoders构建深度网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#17-4-1-无监督的逐层训练"><span class="nav-text">17.4.1 无监督的逐层训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-4-2-有监督的微调"><span class="nav-text">17.4.2 有监督的微调</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-5-利用TensorFlow实现Stacked-Denoising-AutoEncoders"><span class="nav-text">17.5 利用TensorFlow实现Stacked Denoising AutoEncoders</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#17-5-1-训练Stacked-Denoising-AutoEncoders模型"><span class="nav-text">17.5.1 训练Stacked Denoising AutoEncoders模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-5-2-训练的过程"><span class="nav-text">17.5.2 训练的过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-卷积神经网络"><span class="nav-text">18 卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#18-1-传统神经网络模型存在的问题"><span class="nav-text">18.1 传统神经网络模型存在的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-2-卷积神经网络"><span class="nav-text">18.2 卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#18-2-1-卷积神经网络中的核心概念"><span class="nav-text">18.2.1 卷积神经网络中的核心概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-2-2-卷积神经网络模型"><span class="nav-text">18.2.2 卷积神经网络模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-3-卷积神经网络的求解"><span class="nav-text">18.3 卷积神经网络的求解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#18-3-1-卷积层（Convolution-Layer）"><span class="nav-text">18.3.1 卷积层（Convolution Layer）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-3-2-下采样层（Sub-Sampling-Layer）"><span class="nav-text">18.3.2 下采样层（Sub-Sampling Layer）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-3-3-全连接层（Fully-Connected-Layer）"><span class="nav-text">18.3.3 全连接层（Fully-Connected Layer）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-4-利用TensorFlow实现CNN"><span class="nav-text">18.4 利用TensorFlow实现CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#18-4-1-CNN的实现"><span class="nav-text">18.4.1 CNN的实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-4-2-训练CNN模型"><span class="nav-text">18.4.2 训练CNN模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-4-3-训练的过程"><span class="nav-text">18.4.3 训练的过程</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第六部分-项目实践"><span class="nav-text">第六部分 项目实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#19-微博精准推荐"><span class="nav-text">19 微博精准推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#19-1-精准推荐"><span class="nav-text">19.1 精准推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#19-1-1-精准推荐的项目背景"><span class="nav-text">19.1.1 精准推荐的项目背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-1-2-精准推荐的技术架构"><span class="nav-text">19.1.2 精准推荐的技术架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-1-3-离线数据挖掘"><span class="nav-text">19.1.3 离线数据挖掘</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-2-基于用户行为的挖掘"><span class="nav-text">19.2 基于用户行为的挖掘</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#19-2-1-基于互动内容的兴趣挖掘"><span class="nav-text">19.2.1 基于互动内容的兴趣挖掘</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-2-2-基于与博主互动的兴趣挖掘"><span class="nav-text">19.2.2 基于与博主互动的兴趣挖掘</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-3-基于相似用户的挖掘"><span class="nav-text">19.3 基于相似用户的挖掘</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#19-3-1-基于“＠”人的相似用户挖掘"><span class="nav-text">19.3.1 基于“＠”人的相似用户挖掘</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-3-2-基于社区的相似用户挖掘"><span class="nav-text">19.3.2 基于社区的相似用户挖掘</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-3-3-基于协同过滤的相似用户挖掘"><span class="nav-text">19.3.3 基于协同过滤的相似用户挖掘</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-4-点击率预估"><span class="nav-text">19.4 点击率预估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#19-4-1-点击率预估的概念"><span class="nav-text">19.4.1 点击率预估的概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-4-2-点击率预估的方法"><span class="nav-text">19.4.2 点击率预估的方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-5-各种数据技术的效果"><span class="nav-text">19.5 各种数据技术的效果</span></a></li></ol></li></ol></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hac_lang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  





  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="true"></script>









  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  
  

  
  

  


  

  

  

  

  


  


  




  




  




  



<script>
// GET RESPONSIVE HEIGHT PASSED FROM IFRAME

window.addEventListener("message", function(e) {
  var data = e.data;
  if ((typeof data === 'string') && (data.indexOf('ciu_embed') > -1)) {
    var featureID = data.split(':')[1];
    var height = data.split(':')[2];
    $(`iframe[data-feature=${featureID}]`).height(parseInt(height) + 30);
  }
}, false);
</script>


  

  

  


  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":250,"height":500},"mobile":{"show":false,"scale":0.5},"react":{"opacity":0.7},"log":false,"tagMode":false});</script></body>
</html>
