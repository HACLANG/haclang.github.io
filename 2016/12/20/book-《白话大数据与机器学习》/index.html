<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">






<link rel="stylesheet" href="/css/main.css?v=7.2.0">






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="作者: 高扬 / 卫峥 / 尹会生出版社: 机械工业出版社出版年: 2016-6ISBN: 9787111538479 前言为什么要写这本书不知从何时开始我们已周身没入大数据时代的潮流，不知不觉被卷入了大数据时代。 无论是每天上网看网页、聊QQ、聊微信，或者登录银行、网购、买票，或者出行、投宿，甚至是出入任何公众场合、驾车、用水用电……我们无时无刻不在生产着各种数据。而同时我们也在消费着其他人生产">
<meta name="keywords" content="机器学习,计算机,人工智能,books,互联网,更毕,豆瓣7,AI,科普,数据分析,大数据,自评5">
<meta property="og:type" content="article">
<meta property="og:title" content="book_《白话大数据与机器学习》">
<meta property="og:url" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/index.html">
<meta property="og:site_name" content="Hac_lang">
<meta property="og:description" content="作者: 高扬 / 卫峥 / 尹会生出版社: 机械工业出版社出版年: 2016-6ISBN: 9787111538479 前言为什么要写这本书不知从何时开始我们已周身没入大数据时代的潮流，不知不觉被卷入了大数据时代。 无论是每天上网看网页、聊QQ、聊微信，或者登录银行、网购、买票，或者出行、投宿，甚至是出入任何公众场合、驾车、用水用电……我们无时无刻不在生产着各种数据。而同时我们也在消费着其他人生产">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00004.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00005.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00006.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00007.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00008.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00009.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00010.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00011.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00012.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00013.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00014.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00015.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00016.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00017.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00018.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00019.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00020.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00021.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00022.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00023.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00024.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00025.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00026.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00027.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00028.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00029.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00030.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00031.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00032.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00033.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00034.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00035.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00036.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00037.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00038.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00039.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00040.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00041.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00042.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00043.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00044.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00045.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00046.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00047.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00048.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00049.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00050.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00051.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00052.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00053.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00054.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00055.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00056.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00057.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00058.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00059.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00060.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00061.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00062.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00063.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00064.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00065.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00066.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00067.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00068.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00069.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00070.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00071.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00072.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00073.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00074.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00075.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00076.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00077.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00078.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00079.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00080.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00081.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00082.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00083.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00084.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00085.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00086.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00087.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00088.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00089.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00090.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00091.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00092.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00093.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00094.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00095.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00096.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00097.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00098.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00099.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00100.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00101.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00102.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00103.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00104.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00105.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00106.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00107.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00108.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00109.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00110.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00111.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00112.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00113.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00114.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00115.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00116.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00117.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00118.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00119.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00120.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00121.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00122.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00123.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00124.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00125.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00126.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00127.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00128.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00129.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00130.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00131.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00132.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00133.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00134.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00135.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00136.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00137.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00138.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00139.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00140.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00141.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00142.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00143.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00144.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00145.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00146.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00147.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00148.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00149.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00150.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00151.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00152.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00153.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00154.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00155.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00156.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00157.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00158.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00159.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00160.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00161.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00162.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00163.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00164.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00165.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00166.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00167.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00168.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00169.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00170.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00171.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00172.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00173.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00174.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00175.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00176.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00177.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00178.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00179.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00180.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00181.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00182.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00183.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00184.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00185.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00186.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00187.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00188.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00189.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00190.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00191.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00192.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00193.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00194.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00195.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00193.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00194.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00196.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00197.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00198.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00199.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00200.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00201.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00202.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00203.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00204.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00205.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00206.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00207.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00208.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00209.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00210.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00211.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00212.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00213.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00214.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00215.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00216.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00217.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00218.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00219.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00220.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00221.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00222.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00223.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00224.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00225.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00226.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00227.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00228.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00229.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00230.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00231.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00232.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00233.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00234.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00235.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00236.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00237.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00238.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00239.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00240.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00241.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00242.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00243.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00244.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00245.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00246.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00247.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00248.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00249.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00250.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00251.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00252.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00253.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00254.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00255.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00256.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00257.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00258.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00259.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00260.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00261.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00262.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00263.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00264.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00265.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00266.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00267.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00268.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00269.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00270.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00271.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00272.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00273.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00274.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00275.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00276.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00277.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00278.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00279.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00280.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00281.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00282.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00283.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00284.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00285.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00286.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00287.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00288.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00289.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00290.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00291.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00292.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00284.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00285.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00286.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00287.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00288.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00289.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00290.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00291.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00292.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00293.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00294.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00295.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00296.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00297.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00298.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00299.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00300.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00301.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00302.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00303.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00304.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00305.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00306.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00307.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00308.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00309.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00310.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00311.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00312.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00313.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00314.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00315.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00316.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00317.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00318.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00319.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00320.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00321.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00322.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00323.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00324.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00325.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00326.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00327.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00328.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00329.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00330.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00331.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00332.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00333.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00334.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00335.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00336.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00337.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00338.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00339.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00340.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00341.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00342.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00343.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00344.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00345.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00346.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00347.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00348.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00349.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00350.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00351.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00352.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00353.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00354.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00355.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00356.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00357.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00358.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00359.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00360.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00361.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00362.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00363.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00364.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00365.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00366.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00367.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00368.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00369.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00369.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00369.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00369.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00369.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00370.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00371.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00372.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00373.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00374.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00375.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00376.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00377.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00378.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00379.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00380.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00381.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00382.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00383.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00384.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00385.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00386.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00387.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00388.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00389.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00390.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00391.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00392.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00393.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00394.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00394.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00395.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00396.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00395.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00397.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00398.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00399.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00398.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00400.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00401.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00400.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00402.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00403.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00404.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00405.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00406.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00407.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00408.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00409.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00410.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00411.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00412.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00413.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00414.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00415.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00416.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00417.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00418.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00419.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00420.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00421.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00422.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00423.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00424.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00425.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00426.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00427.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00428.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00429.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00430.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00431.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00432.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00433.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00434.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00435.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00436.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00437.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00438.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00439.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00440.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00441.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00442.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00443.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00444.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00445.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00446.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00447.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00448.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00449.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00450.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00451.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00452.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00453.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00454.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00458.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00459.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00460.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00461.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00462.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00463.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00464.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00465.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00466.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00467.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00468.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00469.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00470.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00471.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00472.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00473.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00474.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00475.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00476.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00477.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00478.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00479.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00480.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00481.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00482.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00483.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00484.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00485.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00486.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00487.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00488.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00489.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00490.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00491.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00492.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00493.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00494.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00495.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00496.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00497.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00498.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00499.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00500.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00501.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00502.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00503.jpg">
<meta property="og:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00504.jpg">
<meta property="og:updated_time" content="2020-08-13T15:00:52.585Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="book_《白话大数据与机器学习》">
<meta name="twitter:description" content="作者: 高扬 / 卫峥 / 尹会生出版社: 机械工业出版社出版年: 2016-6ISBN: 9787111538479 前言为什么要写这本书不知从何时开始我们已周身没入大数据时代的潮流，不知不觉被卷入了大数据时代。 无论是每天上网看网页、聊QQ、聊微信，或者登录银行、网购、买票，或者出行、投宿，甚至是出入任何公众场合、驾车、用水用电……我们无时无刻不在生产着各种数据。而同时我们也在消费着其他人生产">
<meta name="twitter:image" content="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/Image00004.jpg">





  
  
  <link rel="canonical" href="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  
  <title>book_《白话大数据与机器学习》 | Hac_lang</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hac_lang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">小白hac_lang的笔记，涉及内容包含但不限于：人工智能   基因工程    信息安全   软件工程   嵌入式   天文物理</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-news">

    
    
      
    

    

    <a href="/news/" rel="section"><i class="menu-item-icon fa fa-fw fa-rss"></i> <br>news</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    
  
  

  

  <a href="https://github.com/HACLANG" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://haclang.github.io/2016/12/20/book-《白话大数据与机器学习》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hac_lang">
      <meta itemprop="description" content="小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hac_lang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">book_《白话大数据与机器学习》

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-12-20 16:05:47" itemprop="dateCreated datePublished" datetime="2016-12-20T16:05:47+08:00">2016-12-20</time>
            </span>
          

          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>作者: 高扬 / 卫峥 / 尹会生<br>出版社: 机械工业出版社<br>出版年: 2016-6<br>ISBN: 9787111538479</p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h4 id="为什么要写这本书"><a href="#为什么要写这本书" class="headerlink" title="为什么要写这本书"></a>为什么要写这本书</h4><p>不知从何时开始我们已周身没入大数据时代的潮流，不知不觉被卷入了大数据时代。</p>
<p>无论是每天上网看网页、聊QQ、聊微信，或者登录银行、网购、买票，或者出行、投宿，甚至是出入任何公众场合、驾车、用水用电……我们无时无刻不在生产着各种数据。而同时我们也在消费着其他人生产的数据，我们使用的众多家电产品，每一个设计细节都融入了设计者对用户体验数据的调查与分析；我们使用的每一部手机、每一台电脑，每一个部件的产出都融入着多得无法想象的指标数据控制下的生产与监控；我们访问的每一个网页、每一个软件，每一次享受到的贴心的产品改动和服务的升级，无不浸透着无数的数据汇集与精细的分析和反馈。这是一场慢慢到来的、贯穿所有产业的革命，这是一次润物细无声的各行业精耕细作的开端。</p>
<p>不管我们是不是愿意，不管我们有没有意识到，我们现在已经身处大数据时代的奇点，而未来要迎接的是大数据奇点爆炸给我们带来的冲击力。我们需要力量来驾驭浪里的航船，我们需要乘风破浪前进的动力。</p>
<p>在这一次远航中，我们不必担心自己的能力水平无法感知数据这种磅礴之力的气魄，不必担心晦涩难懂的公式定理会让我们感到阻力。</p>
<p>请相信我，这是一本通俗易懂的大数据图书，这是一本轻松愉悦的数据挖掘和机器学习的读本，这是一本没有门槛的机器学习实战手册。让我们一起扬帆远航吧！</p>
<h4 id="本书特色"><a href="#本书特色" class="headerlink" title="本书特色"></a>本书特色</h4><p>从行为脉络来看，本书基本上是从数据统计、数据指标理解、数据模型、聚类/分类与机器学习、数据应用、大数据框架补充知识，以及扩展讨论这样的角度来层层深入完成的。</p>
<p>这种方式会给读者比较好的带入感，让大家——尤其是不擅长数学的读者降低对大数据与机器学习算法的恐惧感。如果读者朋友对排列组合、统计分布这些基础知识比较了解，完全可以考虑跳过这些部分直接去读后面更感兴趣的内容。</p>
<p>为了调节阅读气氛，我们还尝试加入了一些漫画插图。为了让读者朋友能够更快地进行实践，我们几乎在每一个算法讲解后都配有Python或者SQL语言的实现部分。相信这些能够帮助大家更快、更轻松地阅读本书。</p>
<h4 id="读者对象"><a href="#读者对象" class="headerlink" title="读者对象"></a>读者对象</h4><p>（1）对大数据感兴趣但是完全不了解的技术人员。</p>
<p>（2）对机器学习和数据挖掘比较感兴趣的技术人员。</p>
<p>（3）大数据初级从业人员。</p>
<h4 id="如何阅读本书"><a href="#如何阅读本书" class="headerlink" title="如何阅读本书"></a>如何阅读本书</h4><p>本书一共分为18章。</p>
<p>第1章～第5章为入门所需基础知识及对数据指标运营的阐述。</p>
<p>第6章～第10章是对数据挖掘基础知识与算法的介绍。</p>
<p>第11章～第18章为生产应用与高级扩展。</p>
<p>其中，第1章～第15章正文内容，以及第17章、第18章的正文内容由高扬编写。</p>
<h1 id="第1章-大数据产业"><a href="#第1章-大数据产业" class="headerlink" title="第1章 大数据产业"></a>第1章 大数据产业</h1><h2 id="1-1-大数据产业现状"><a href="#1-1-大数据产业现状" class="headerlink" title="1.1 大数据产业现状"></a>1.1 大数据产业现状</h2><p>大数据是近几年来都一直非常火热的一个名词，似乎是伴随着“互联网”的逐渐发展所出现的一个新名词。我们在天天听着“互联网+”的同时也在听说“大数据+”。</p>
<p>大数据其实是一个比较抽象和笼统的概念，应该说这个词是为了涵盖性地表达一系列生产和业务行为的一个统称。但是也正是由于这种抽象和过于简略的称谓方式，让每个人都容易对这个词产生见仁见智的不同视角的印象或者看法。</p>
<p>大数据是一个以数据为核心的产业，是一个围绕大数据生命周期不断循环往复的生产过程，同时也是由多种行业分工和协同配合而产生的一个复合性极高的行业。</p>
<p>在我看来，大数据产业生产流程从数据的生命周期的传导和演变上可以分为这样几个部分：数据收集、数据存储、数据建模、数据分析、数据变现 。</p>
<p>其中每个环节都是非常重要的数据生命环节，每个环节的生产加工行为都是有其价值的，并且每个环节做到极致都可以成就一个伟大的公司。整个完整的产业生态圈就是大数据，它的缩影也渗透在任何一家以数据作为运营基础的公司中。</p>
<p>根据麦肯锡2011年发布的一份研究报告，到2018年世界范围内将会出现高达14万～19万的“大数据”岗位空缺。而艾瑞咨询集团在“2014年会”上曾指出，全球数据量每18个月翻一番，到2015年，中国专用数据分析人员预计缺口1400万。</p>
<p>可以看到，在仅仅三四年的时间间隔上，两家咨询公司做出的预测都很大胆，但是两个估算数字相差也确实非常悬殊。究竟哪个数字更贴近“事实”并不好判断，因为大家对“大数据”的概念边界理解可能有很大的偏差，估算出现偏差是必然的，但是有一点可以肯定，大数据人才缺口一定是未来几年非常显著的问题。</p>
<p>2015年12月21日，全球第一家大数据交易所——贵阳大数据交易所经过半年多的发展，交易金额已突破6000万元人民币，会员数量超过300家，接入贵阳大数据交易所的数据源公司超过100家，数据总量超过10 PB，已发生实际交易的会员超过70家。预计在未来3～5年，交易所日交易额将突破100亿元 <a href="#ch1_back">[1]</a> 。</p>
<p>截至目前，中国境内除了贵阳大数据交易所以外，还有长江大数据交易所、东湖大数据交易所、崇州大数据交易所等十余家大数据交易所挂牌营业。</p>
<p>2016年1月，阿里云的“数加”大数据平台和金山云的KMR平台等国内大品牌云产品供应商的重磅产品先后登场，几乎所有有远见的云产品巨擘资本都在向大数据产业链集中。但是这块蛋糕似乎有点太大了，只能边烘焙边分割，谁也没办法一下子全吃掉。</p>
<p><a href="#ch1">[1]</a> 来自《新华网》的报道。</p>
<h2 id="1-2-对大数据产业的理解"><a href="#1-2-对大数据产业的理解" class="headerlink" title="1.2 对大数据产业的理解"></a>1.2 对大数据产业的理解</h2><p>“大数据”这个人造词汇其实很容易产生不少误解，尤其是这个“大”字，很容易让人感觉，数据量必须大，而且特别大，越大越能形成产业，也越有价值。其实这真的是“大数据”给人带来的误导。大数据产业的存在其实和其他产业并无二致，本身是为了给其他产业提供服务。</p>
<p>做个假设，假如现在给石油产业冠以“大石油”产业的名字，那么会影响石油行业本身对其他行业的服务样态吗？应该不会。</p>
<p>在“大石油”产业里，同样有人从事着这样的工作内容：石油勘探、石油开采、石油运输、石油提炼、石油产品销售 等多个细分领域和环节。</p>
<p>最后提供给社会的是由大量人工和智慧凝结在石油产品上的服务，而这些服务极大地方便并满足了社会各领域对于工业能源、建筑材料、食品包装、服装面料、模型器具、日杂用品等多种制造与使用的需求。试想如果没有石油，也就没有廉价汽车与航空动力，尤其是没有乙烯等重要化工原材料的来源，是否存在塑料这样一种廉价的工业制造材料都很难说，那么各个产业则需要用其他造价更为高昂的材料对其进行取代，更不用提家用的天然气和液化石油气了，人们只能再去寻找其他能源：要么不洁净——如柴火和煤炭，要么价格昂贵——如氢气。人们之所以选用石油作为整个产业链的根源，并把它发展成一个完整的产业也是由于这样的原因，大概这个逻辑是比较容易理解的。</p>
<p>类比一下“大数据”产业，数据收集、数据传输、数据存储、数据建模、数据分析、数据交易贯穿了大数据产业的完整产业链。在这个产业链里同样蕴含着和“大石油”一样的东西，这个东西是什么？</p>
<p>数据通过各种软件进行收集，通过网络进行传输，通过云数据中心进行存储，通过数据科学家或者行业专家进行建模和加工，最后数据分析得到的是一种知识，是一种人们通过数据洞悉世界的能力。数据之间本来彼此错综复杂的潜在关系会使得大量孤立而多来源的数据同时出现在一个舞台后显得更为有趣，大量看似不相关的事情却能够通过观察与分析后告诉人们更多背后的因果。这些因果联系的意义会让人们在各个方面能够推测未来趋势，减少试错的机会，减少成本，降低风险，解放劳动力。笔者认为这才是大数据产业本身的价值与意义所在。</p>
<h2 id="1-3-大数据人才"><a href="#1-3-大数据人才" class="headerlink" title="1.3 大数据人才"></a>1.3 大数据人才</h2><h3 id="1-3-1-供需失衡"><a href="#1-3-1-供需失衡" class="headerlink" title="1.3.1 供需失衡"></a>1.3.1 供需失衡</h3><p>大数据产业既然如此炙手可热，那么大数据人才的待遇如何呢？这一点其实不用多说，只要大家时常关注一些猎头QQ群的状态，或者猎头朋友的签名档内容，再或者干脆到“猎聘网”、“前程无忧”等专业的人才中介网站去看看就会了然于胸——30万年薪找不到人，40万年薪找不到人，50万、60万还是找不到人，一时间可谓洛阳纸贵，似乎市场上的大数据人才是“一将难求”。这也从一个侧面说明，很多公司愿意花这么多薪水雇佣一位大数据人才，不管他的头衔是大数据科学家，还是大数据架构师，抑或是大数据产品经理，很显然这些公司都是把大数据产业发展作为自己的经营战略的重要组成部分来看待。</p>
<p>大数据人才的一将难求其实不奇怪，因为人才既然是市场的一部分，是一种特殊的“商品”，那就必然受到市场因素的调节，供需严重失衡才会有这样的现象。但是，为什么大数据人才会供需严重失衡呢？原因有以下两个。</p>
<p>（1）大数据产业发展迅速，很多公司都越来越意识到要将大数据作为自己公司经营战略不可或缺的一部分，就像销售、生产、公关这样的重要环节一样缺一不可。人才需求旺盛！</p>
<p>（2）大数据人才培养成本居高不下，培养周期长，成材率相对较低，这也是导致大数据人才缺乏的一个非常重要的原因。人才供应不足！</p>
<h3 id="1-3-2-人才方向"><a href="#1-3-2-人才方向" class="headerlink" title="1.3.2 人才方向"></a>1.3.2 人才方向</h3><p>从目前市场上的人才需求观点来看，大数据人才大致可以分为以下3个方向。</p>
<p>（1）偏重基建与架构的“大数据架构”方向。</p>
<p>（2）偏重建模与分析的“大数据分析”方向。</p>
<p>（3）偏重应用实现的“大数据开发”方向。</p>
<p>当然，也有理想主义者会认为能来个三合一的人才就更好了，但是知识宽度和知识深度本身就是一组矛盾，毕竟对于有限的学习时间和精力，能够在一方面做到运用自如已属不易。</p>
<p>1.大数据架构方向</p>
<p>大数据架构方向的人才更多注重的是Hadoop、Spark、Storm等大数据框架的实现原理、部署、调优和稳定性问题，以及它们与Flume、Kafka等数据流工具以及可视化工具的结合技巧，再有就是一些工具的商业应用问题，如Hive、Cassandra、HBase、PrestoDB等。能够将这些概念理解清楚，并能够用辩证的技术观点进行组合使用，达到软/硬件资源利用的最大化，服务提供的稳定化，这是大数据架构人才的目标。</p>
<p>以下是大数据架构方向研究的主要方面。</p>
<p>（1）架构理论：关键词有高并发、高可用、并行计算、MapReduce、Spark等。</p>
<p>（2）数据流应用：关键词有Flume、Fluentd、Kafka、ZMQ等。</p>
<p>（3）存储应用：关键词有HDFS、Ceph等。</p>
<p>（4）软件应用：关键词有Hive、HBase、Cassandra、PrestoDB等。</p>
<p>（5）可视化应用，关键词有HightCharts、ECharts、D3、HTML5、CSS3等。</p>
<p>大数据架构师除了最后可视化的部分不需要太过注意（但是要做基本的原理了解）以外，其他的架构理论层面、数据流层面、存储层面、软件应用层面等都需要做比较深入的理解和落地应用。尤其是需要至少由每一个层面中挑选一个可以完全纯熟应用的产品，然后组合成一个完整的应用场景，在访问强度、实现成本、功能应用层面都能满足需求，这是一个合格的大数据架构师必须完成的最低限要求。</p>
<p>2.大数据分析方向</p>
<p>大数据分析方向的人才更多注重的是数据指标的建立，数据的统计，数据之间的联系，数据的深度挖掘和机器学习，并利用探索性数据分析的方式得到更多的规律、知识，或者对未来事物预测和预判的手段。</p>
<p>以下是大数据分析方向研究的主要方面。</p>
<p>（1）数据库应用：关键词有RDBMS、NoSQL、MySQL、Hive、Cassandra等。</p>
<p>（2）数据加工：关键词有ETL、Python等。</p>
<p>（3）数据统计：关键词有统计、概率等。</p>
<p>（4）数据分析：关键词有数据建模、数据挖掘、机器学习、回归分析、聚类、分类、协同过滤等。</p>
<p>此外还有一个方面是业务知识。</p>
<p>其中，数据库应用、数据加工是通用的技术技巧或者工具性的能力，主要是为了帮助分析师调用或提取自己需要的数据，毕竟这些技巧的学习成本相对较低，而且在工作场景中不可或缺，而每次都求人去取数据很可能会消耗过多的时间成本。</p>
<p>数据统计、数据分析是分析师的重头戏，一般来说这两个部分是分析师的主业，要有比较好的数学素养或者思维方式，而且一般来说数学专业出身的人会有相当的优势。最后的业务知识方面就是千姿百态了，毕竟每家行业甚至每家公司的业务形态都是千差万别的，只有对这些业务形态和业务流程有了充分的理解才能对数据分析做到融会贯通，才有可能正确地建立模型和解读数据。</p>
<p>3.大数据开发方向</p>
<p>大数据开发方向的人才更多注重的是服务器端开发，数据库开发，呈现与可视化，人机交互等衔接数据载体和数据加工各个单元以及用户的功能落地与实现。</p>
<p>以下是大数据开发研究的主要方面。</p>
<p>（1）数据库开发：关键词有RDBMS、NoSQL、MySQL、Hive等。</p>
<p>（2）数据流工具开发：关键词有Flume、Heka、Fluentd、Kafka、ZMQ等。</p>
<p>（3）数据前端开发：关键词有HightCharts、ECharts、JavaScript、D3、HTML5、CSS3等。</p>
<p>（4）数据获取开发：关键词有爬虫、分词、自然语言学习、文本分类等。</p>
<p>可以注意到，大数据开发职种和大数据架构方向有很多关键词虽然是重合的，但是措辞不一样，一个是“应用”，一个是“开发”。区别在于，“应用”更多的是懂得这些这种技术能为人们提供什么功能，以及使用这种技术的优缺点，并擅长做取舍；“开发”更注重的是熟练掌握，快速实现。</p>
<p>最后一个方面——数据获取开发与前面的数据库开发、数据流工具开发、数据前端开发略有不同，它出现的时间相对较晚，应用面相对较窄。现在很多数据公司，如汤森路透、彭博等咨询公司的数据除了从专业行业公司直接得到以外，很多也是从互联网上爬取的，这个过程中也涉及一些关键技术。</p>
<h3 id="1-3-3-环节和工具"><a href="#1-3-3-环节和工具" class="headerlink" title="1.3.3 环节和工具"></a>1.3.3 环节和工具</h3><p>前面提到过，大数据从数据的生命周期的传导和演变上可以分为这样几个部分：数据收集，数据存储，数据建模，数据分析，数据变现，如图1-1所示。</p>
<p><img src="Image00004.jpg" alt></p>
<p>图1-1 数据生命周期中的环节</p>
<p>1.数据收集</p>
<p>完成数据收集是做大数据的第一步，这里请注意，数据的收集和平时在业务生产库上的做法不太一样，而是更像以前数据仓库里的收集方式。</p>
<p>方法一：<br>快照法。可以每天、每周、每月用数据快照的方式把当前这一瞬间的某数据的状态复制下来放入相应的位置——这个位置就是大数据的数据中心所采用的数据容器，可以用Hive实现，也可以用Oracle或者其他专业的数据仓库软件实现。</p>
<p>方法二： 可以使用一些工具来进行流式的数据导入，如TCP流或者HTTP长/短链接。</p>
<p>2.数据存储</p>
<p>数据存储的方式也是比较多样的，当数据收集进入数据中心时，可以考虑使用HDFS或者Ceph等开源并且低成本的方案，数据量较小的时候可以采用NAS直接mount到一台Linux服务器的某挂载点。比较推荐HDFS和Ceph主要是因为这两种框架在业界已经有了长时间的应用，社区活跃，方案成熟稳定，部署价格低廉且扩展性极好。</p>
<p>3.数据建模</p>
<p>数据建模是一个人为因素影响比较大的环节，我们这里提到的数据建模是指数理关系的梳理，并根据数据建立一定的数据计算方法和数据指标。一般来说，在一个比较成熟的行业里，数据指标相对是比较固定的，只要对业务有足够的了解是比较容易建立起运营数据模型的。使用人们熟悉的SQL语言就可以对存储容器中的数据进行筛选和洗涤，如果数据存储的容器是其他的异构容器，如HBase或者Mongodb等，就只能使用它们自己的操作Shell去操作了。</p>
<p>在这里需要提一句，有一个比较重要的环节是数据清洗。不同的业务习惯下，清洗有着不同的解释，但核心思想都是让数据中那些由于误传、漏传、叠传等原因产生的数据失真部分被摒弃在计算之外。此外，原始数据从非格式化变成格式化需要一个“整形”的过程，目的是让它能够和其他数据进行参照来运算，清洗同样涵盖这个“整形”的过程。也有人习惯把这个环节直接放在数据收集的部分一次性完成，究竟哪种方式比较好不能一概而论：在数据收集的时候就直接“整形”完毕，可能会使后面的数据存储、建模等环节处理起来成本更低一些，这是它的好处；但是在这个过程中会发生一部分数据裁剪的动作，而裁减掉的数据所蕴含的信息以后再想找回是不可能的。孰优孰劣还是因地制宜地进行讨论比较好。</p>
<p>4.数据分析</p>
<p>数据分析是这些环节里面一个比较重要的环节。“分析”两个字的含义可以包含两个方面的内容：一个是在数据之间尝试寻求因果关系或影响的逻辑；另一个是对数据的呈现做适当的解读。</p>
<p>这两个方面或许有重叠的部分，但是笔者认为这两个方面还是可以分开来理解的，前者偏重数据挖掘、试错与反复比对；后者偏重业务结合、行业情景带入等。但是两者都是货真价实的分析工作，这点毫无疑问。数据分析的工具在“市面”上有不少，有开源的，也有收费的，到现在其实没有特别好用的，大多使用的时候门槛较高而且使用习惯十分西方化。目前收费的软件里比较好的有IBM的SPSS、SAP的BW/BO，以及微软的SSAS和SSRS；开源的软件里有Mahout、Spark ML Lib、Python Pandas等。收费软件里通常会把挖掘分析和可视化结合得比较好，而开源软件里主要是封装的算法比较多，但是环节较为孤立，绘图的丰富程度和美观程度会大打折扣或者干脆没有，那么这个环节就需要使用者自己想办法了。</p>
<h3 id="1-3-4-门槛障碍"><a href="#1-3-4-门槛障碍" class="headerlink" title="1.3.4 门槛障碍"></a>1.3.4 门槛障碍</h3><p>大数据开发不是一个由大数据带来的新方向，它更多是迁移性地使用旧有技术，技术突破也比较有限，所以这里不做过多的介绍。</p>
<p>1.大数据架构方向</p>
<p>对于大数据架构方向，在资料方面虽然近两年涌现出很多翻译资料，也有很多国内高手写出不少实战心得，但是对于层出不穷的优秀开源框架来说，资料的更新多少显得有些跟不上脚步。这大概是自学成才的大数据架构师们很多不得不一边翻看着英文文档，一边翻看着源代码，一边试来试去的原因。而同时，越来越多的架构师们也将自己的心得写成越来越多的参考书籍，这些书籍的丰富也使大数据架构领域奋战的同行们有了更多的参考，有了更快进步的动力。</p>
<p>2.大数据分析方向</p>
<p>对于大数据分析方向，在淘宝或者京东上试着翻看一下，其实书籍比前者要缺乏得多。虽然有一些不错的数据挖掘与机器学习的书，也有一些关于Mahout、Spark ML Lib和文本挖掘、神经网络的书被摆上货架，但是笔者也经常听一些同行抱怨这类书有不少问题不尽如人意。英译中版本的书有不少翻译生涩、阅读困难，而且解析不够细致，默认读者已经掌握了大量的数理统计或概率学的知识；而Mahout之类开源数据挖掘项目的书籍，要么就是由官方文档翻译而来新意全无，要么就是由于成书较慢早已落后于当前的最新版本，所以让人读起来如吮鸡肋。</p>
<p>3.市场参与</p>
<p>在我看来，大数据行业目前不够繁荣的原因众多，但是究其根本就是因为它目前还不是一门“生意”，距离大量自由交易有价值的数据这一目标还相差太远。这种交易既存在于公司和组织之间，也存在于公司内部。现在去看身边最繁荣的市场，如大超市、大型网店，它们火爆的原因关键在于流量大、交易自由、交易成本低、交易参与方众多。大数据行业想要真正实现良性和快速发展，关键在于提高行业的市场化普及度。换句话说，大数据市场供需越丰富市场就越繁荣。而市场供需的繁荣靠的是参与方的丰富，以及交易内容的丰富。</p>
<p>那么支持参与方的不断丰富和交易内容不断丰富的源动力是什么呢？当然是人们经常说的去中心化和降低参与门槛。说到底，让尽可能多的人成为大数据行业中的价值制造者，这可能才是大数据行业进步的关键点所在。大数据产业中最有价值的层面在哪里？应该说，在数据收集、数据存储、数据建模、数据分析、数据变现这几个主要环节中，只有数据建模和数据分析这两个环节离数据变现，即创造价值更近；而数据收集和数据存储这两个环节做得再好也只是离节约成本更近，而对促进市场化普及的帮助较为间接。</p>
<p>因目前数据分析书籍的缺乏，阅读门槛的障碍，业内人士对知识的渴求，种种因素使我决心尝试着写一本门槛更低，更易理解，更“平民化”的数据挖掘与机器学习的书籍。</p>
<h2 id="1-4-小结"><a href="#1-4-小结" class="headerlink" title="1.4 小结"></a>1.4 小结</h2><p>大数据产业已经向我们敞开了大门，整个产业才刚刚开始萌芽，只要我们肯多进行观察、学习和思考，任何领域任何业务都会享受到大数据产业为我们带来的各种好处。</p>
<p>笔者问过一些试读过本书的朋友，他们有的是大专毕业，有的是大学本科毕业但是由于专业设定的原因没有学过高等数学，基本还是能够看懂。</p>
<p>如果读者已经完成大专或者大本的学业，而且加减乘除、幂指对函数这些概念基本没问题；</p>
<p>如果读者对“一个六面的骰子在丢出后出现2点的概率是1/6”基本没问题；</p>
<p>如果读者对“一个匀质的硬币在扔出1000次后，正面朝上和反面朝上的次数基本各为500次”没问题；那么请读者放心大胆地跟随我们，我们将用最令人放松的聊天方式开始这次轻松的白话数据挖掘与机器学习之旅。</p>
<h1 id="第2章-步入数据之门"><a href="#第2章-步入数据之门" class="headerlink" title="第2章 步入数据之门"></a>第2章 步入数据之门</h1><p>数据与数据应用中的许多概念彼此有着千丝万缕的联系，同时也有着概念上的偏重与区别，这里先从数据应用领域中的常见概念聊起。</p>
<h2 id="2-1-什么是数据"><a href="#2-1-什么是数据" class="headerlink" title="2.1 什么是数据"></a>2.1 什么是数据</h2><p>数据是什么？这几乎成为一个人们熟视无睹的问题。</p>
<p>有不少朋友脑子里可能会直接冒出一个词“数字”——“数字就是数据”，我相信会有一些朋友斩钉截铁地说。</p>
<p>一些朋友会在稍作思考后回答“数字和字符、字母，这些都是数据”。</p>
<p><img src="Image00005.jpg" alt></p>
<p>图2-1 例1</p>
<p><img src="Image00006.jpg" alt></p>
<p>图2-2 例2</p>
<p>不知道你现在是不是正在纠结哪个回答更正确，抑或第二个回答更合理一些，这里先放一放。先看下面这组例子（图2-1）：</p>
<p>这里有6个0，请问它是数据吗？</p>
<p>再看这样的例子（图2-2）：</p>
<p>这里有4个1和2个a，那么它是数据吗？</p>
<p>也许你可能会问，“这到底是什么意思？”不错，这就是我们在认识数据的过程中存在的一个很要命的问题，几乎在我们出发时就拦住了我们的去路。</p>
<p>我们回过头再想想刚才的问题可能会得到比较令自己和他人信服的回答：“承载了信息的东西”才是数据，换句话说，不管是石头上刻的画，或者是小孩子在沙滩上歪歪扭扭写出的字迹，或者是嬉皮士们在墙上的涂鸦，只要它表达一些确实的含义，那么这种符号就可以被认为是数据。而没有承载信息的符号就不是数据。这个观点似乎看上去要比前面的回答理性得多，也科学得多，但是这个观点真的不需要补充了吗？</p>
<p>我们假设这两个例子都有一些比较特殊的场景，假设第一组里出现的6个0其实是时分秒的简写，000000表示00点00分00秒，而如果写作112349则表示11点23分49秒，那么它是不是也是数据呢？假设第二组出现的4个1和2个a其实是一组密码，4个1代表一个被约定的地点，aa代表一种被约定的事件，那这组数字和字母的意义也有了相应的解读，那么它是不是也是数据呢？</p>
<p>不难看出，一些符号如果想要被认定为数据，那就必须承载一定的信息。而信息很可能是因场景而定，因解读者的认知而定，所以一些符号是不是可以被当做数据，有相当的因素是取决于解读者的主观视角的。不知道这个观点你是不是认可，总之这点很重要。</p>
<h2 id="2-2-什么是信息"><a href="#2-2-什么是信息" class="headerlink" title="2.2 什么是信息"></a>2.2 什么是信息</h2><p>说到这里，我的同事娟娟非常认真且煞有介事地跟我说：“我觉得数字、字母、图像，这些都是数据，跟信息不信息的没什么关系。”看着她认真地跟我抬杠，我觉得蛮好，至少在认识数据过程中积极思考只有好处。</p>
<p>信息一词，在没有学术背景的情况下其实有着很多解释，例如，广播中的声音、互联网上的消息、通信系统中传输和处理的语音对象，甚至是小区和校园的消息看板（图2-3），也就是人类社会传播的一切内容。</p>
<p><img src="Image00007.jpg" alt></p>
<p>图2-3 信息的表现形式</p>
<p>1948年，数学家香农（Claude Elwood Shannon）在题为《通信的数学理论》的论文中指出：“信息是用来消除随机不定性的东西”。这句话如果举个例子说明，大概可以想象这样一个场景（图2-4）。</p>
<p><img src="Image00008.jpg" alt></p>
<p>图2-4 场景1</p>
<p>我说了两句话：“我今年33岁。”，“我明年34岁。”</p>
<p>那么第一句话如果是为了向不了解我的人介绍我的年龄而可以算作信息，第二句话则不是信息。至少你会觉得说了第一句以后，后面这句简直就是废话，因为从第一句话完全可以推导出来。</p>
<p>再如，某一天巴西足球队和中国足球队进行了比赛。</p>
<p>结果第二天张三告诉笔者，“昨天巴西队赢了。”</p>
<p>而后李四告诉笔者，“昨天中国队输了。”</p>
<p>再而后王五告诉笔者，“昨天的比赛不是平局。”（图2-5）</p>
<p><img src="Image00009.jpg" alt></p>
<p>图2-5 场景2</p>
<p>前提是只要他们都是说实话的人，那么对于我来说，也就只有张三的话能算信息，李四和王五说的则不能算作信息。甚至连张三说的“昨天巴西队赢了”这句话是否能够被算作信息，我们都要表示怀疑，因为这也有点“废话”的意味——但凡对足球运动有点认识的人都几乎可以认定，即便你不告诉我昨天巴西队赢了，我也能猜个八九不离十，因为可能性实在是太大太大了，大到几乎是一定的，几乎是毋庸置疑的。国足的粉丝们请放下手中的臭鸡蛋和烂西红柿，听我把例子讲完。</p>
<p>现在对信息是什么清晰多了吧？我们可以粗略地认为，信息就是那些把我们不清楚的事情阐明的描述，而已经明确或者知晓的东西让我们再“知晓”一遍，这些被知会的内容就不再是信息了。这个概念是很有用的，我们后面在讲信息论的时候也会再做定量的说明，现在只做一个定性的了解。</p>
<p>数据和信息是我们在数据挖掘和机器学习领域天天要打交道的基础，也是我们研究的主要对象。所以对数据和信息有一个比较一致性的认识对后面咱们讨论问题是非常有好处的。</p>
<h2 id="2-3-什么是算法"><a href="#2-3-什么是算法" class="headerlink" title="2.3 什么是算法"></a>2.3 什么是算法</h2><p>算法这个名称大家应该不陌生，如果你是一个信息相关专业的本科学生，至少在本科一年级或者二年级就接触过不少算法了。随便打开一个人力资源网站去搜索“算法工程师”，好的算法工程师的年薪能到三五十万甚至上百万。</p>
<p>算法是什么？算法可以被理解为“计算的方法和技巧”，在计算机中，算法大多数指的就是一段或者几段程序，告诉计算机用什么样的逻辑和步骤来处理数据和计算，然后得到处理的结果。</p>
<p>科班出身的信息相关专业的朋友看到这里就会觉得比较亲切了，经典的算法有很多，如“冒泡排序”算法，这几乎是所有学习高级语言和“数据结构”课程的入门必学；再如“八皇后问题”算法，这几乎也是在讲穷举计算时的经典保留算法案例（就是在国际象棋棋盘上放8个能够横、竖、斜无限制前进的皇后，让它们之间互相不能攻击，求有多少种解）；以及MD5算法、ZIP2压缩算法等各种不胜枚举的算法。图2-6所示为八皇后问题的一组解，经过穷举是可以求出所有92组解的。</p>
<p><img src="Image00010.jpg" alt></p>
<p>图2-6 八皇后问题的一组解</p>
<p>应该说算法是数据加工的灵魂。如果说数据和信息是原始的食材，数据分析的结论是菜肴，那么算法就是烹调过程；如果说数据是玉璞，数据中蕴含的知识是价值连城的美碧，那么算法就是玉石打磨和加工的机床和工艺流程。</p>
<p>算法在高级语言发展了很多年之后，更多地被封装成了独立的函数或者独立的类，开放接口供人调用，然而算法封装得再好也是不能不假思索地使用就能获益的东西，要知道，这些封装只是在一定程度上避免了重复发明轮子而已。</p>
<p>大家不要以为算法全都是算法工程师的事情，跟普通的程序员或者分析人员无关，算法说到底是对处理逻辑理解的问题。</p>
<p>《孙子兵法·作战篇》有云，“不尽知用兵之害者，则不能尽知用兵之利”，意思是说，不对用兵打仗的坏处与弊端进行充分了解同样不可能对用兵打仗的好处有足够的认识。算法的应用是一个辩证的过程，不仅在于不同算法间的比较和搭配使用有着辩证关系，在同一个算法中，不同的参数和阈值设置同样会带来大相径庭的结果，甚至影响数据解读的科学性。这一点请大家务必有所注意。</p>
<h2 id="2-4-统计、概率和数据挖掘"><a href="#2-4-统计、概率和数据挖掘" class="headerlink" title="2.4 统计、概率和数据挖掘"></a>2.4 统计、概率和数据挖掘</h2><p>统计、概率、数据挖掘，这几个词经常伴随出现，尤其是统计和概率两个概念，几乎就像自然界的伴生矿一样分不了家，有很多出版社都出版过叫做《概率统计》的书籍。</p>
<p>本书不准备从学术的角度对统计和概率做严格的区分，在平时工作中用的统计大多为计数功能，如在使用Excel时会用到COUNT、SUM、AVERAGE等统计函数；如软件开发中，在用SQL语言对数据库的某些字段进行计数（count）、求和（sum）、求平均（avg）等函数。而概率的应用大多则是根据样本的数量以及占比得到“可能性”和“分布比例”等描述数值。当然，概率的用法远不止这些，在数据挖掘中同样用到大量概率相关的算法，后面会有相当的篇幅进行说明。</p>
<p>数据挖掘这个词很多时候是和机器学习一起出现的，现在网上对这两个词的关系也是莫衷一是。有的说数据挖掘包含机器学习，有的说机器学习是数据挖掘发展的更高阶段。在笔者看来，数据挖掘和机器学习这样的词汇命名应该是信息科学自然进化和衍生出来的，带有一定的约定俗成的色彩，人们的看法见仁见智也在情理之中。</p>
<p>我的观点是这样。</p>
<p>首先我认为没有必要一定要给两个词汇划一个界限，或者一定要对它们做严格的概念区分，因为区分的标准到目前本就没有科学而无争议的界定，况且能不能分清一个算法属于数据挖掘的范畴还是机器学习的范畴对于算法本身使用是没有任何影响的，这两个词大家如果想听解释的话，不妨只从字面意思去理解就已经足够了。</p>
<p>数据挖掘——首先是有一定量的数据作为研究对象，挖掘——顾名思义，说明有一些东西并不是放在表面上一眼就能看明白，要进行深度的研究、对比、甄别等工作，最终从中找到规律或知识，“挖掘”这个词用得很形象。</p>
<p>机器学习——先想想人类学习的目的是什么，是掌握知识，掌握能力，掌握技巧，最终能够进行比较复杂或者高要求的工作。那么类比一下机器，我们让机器学习，不管学习什么，最终目的都是让它独立或至少半独立地进行相对复杂或者高要求的工作。这里提到的机器学习更多是让机器帮助人类做一些大规模的数据识别、分拣、规律总结等人类做起来比较花时间的事情。但是请注意，与数据挖掘一起出现的这个机器学习概念和我们说的“人工智能”还是相差甚远，因为这里面对“智能”的考究程度实在是太低了。</p>
<h2 id="2-5-什么是商业智能"><a href="#2-5-什么是商业智能" class="headerlink" title="2.5 什么是商业智能"></a>2.5 什么是商业智能</h2><p>另一个经常和大数据一起出现的词汇是商业智能（图2-7），也就是我们平时简称的BI（Business Intelligence）。</p>
<p>商业智能——业界比较公认的说法是在1996年最早由加特纳集团（Gartner Group）提出的一个商业概念，通过应用基于事实的支持系统来辅助商业决策的制定。商业智能技术提供使企业迅速分析数据的技术和方法，包括收集、管理和分析数据，将这些数据转化为有用的信息。如果这个书本式的概念读起来还是比较费解，那么请看一个形象的比喻。</p>
<p>公司在日常运营过程中是需要做很多决策的，无时无刻都存在于公司的各个方面，而决策不管是股东大会讨论还是企业领导、部门领导直接发布行政命令，最终可能是很多因素共同影响做出的结果，无论其来自主观还是客观。</p>
<p><img src="Image00011.jpg" alt></p>
<p>图2-7 商业智能</p>
<p>这些决策可以如何得出呢？可以由领导直接凭经验决定；可以群策群力开会决定；可以问询行业专家；甚至可以找个算卦先生来占卜……从概念来说都是属于辅助决策。而显然，我们都期望不论最终是如何做出的这些决策和命令，它们都应该是更为理性、科学、正确的。但是如何帮助他们做出更为理性、科学、正确的决策呢？商业智能整体就是研究这样一个课题，到目前为止，业界普遍比较认可的方式就是基于大量的数据所做的规律性分析。因而，市面上成熟的商业智能软件大多都是基于数据仓库做数据建模和分析，以及数据挖掘和报表的。</p>
<p>可以说，商业智能是一个具体的、大的应用领域，也是数据挖掘和机器学习应用的一个天然亲密的场景。而且商业智能这个解决问题的理念其实不仅仅可以应用于商业，还可以应用于国防军事、交通优化、环境治理、舆情分析、气象预测等。</p>
<h2 id="2-6-小结"><a href="#2-6-小结" class="headerlink" title="2.6 小结"></a>2.6 小结</h2><p>数据的认识和数据的应用是大数据与机器学习的基础，数据、信息、算法、概率、数据挖掘、商业智能，这些是大数据最为核心的基础概念与要素。当我们对这些概念有了清楚的认识，并能够清楚说出这些概念之间的辩证关系时，我们就已经在数据大门的里面了，怎么样，是不是很简单？下面就让我们一步一步地深入理解这些概念的细节以及它们的应用技巧吧。</p>
<h1 id="第3章-排列组合与古典概型"><a href="#第3章-排列组合与古典概型" class="headerlink" title="第3章 排列组合与古典概型"></a>第3章 排列组合与古典概型</h1><p>记得我在上中学和上大学的时候，有一些身边的同学就抱怨：“真不知道这些定理们都是哪儿来的，是天上掉下来的还是哪个数学家哪天睡觉做梦突然梦见的”。大家听了不住地发笑，有的同学则忍俊不禁，捂着嘴点起头来（图3-1）。</p>
<p><img src="Image00012.jpg" alt></p>
<p>图3-1 对数学定理感到疑惑</p>
<p>在我印象里，高中的时候当时其实有不少同学的数学成绩还是相当不错的，但是也会偶尔发发牢骚，“每个公式我都背得很熟，每个题我也都会做，我就是不知道将来这些东西能干什么用……”</p>
<p>还有的朋友对数学确实有兴趣，但是看到复杂的公式就两眼发花、瞳孔放大，抱着《概率论》、《高等数学》这些大部头的书满头大汗，难以入定。不知道你身边的同学朋友里有没有对数学又爱又恨的人。</p>
<p>其实数学本身本不应是高高在上的学科，它来源于人类的生产生活，也扎根于人类的生产生活，它本身无处不在，只是我们早已习惯了它以孤傲的脸孔出现在书本上，才让我们感到格外地陌生和疏远。我们有没有尝试着重新用更平和和更朴实的目光再一次认识一下这些和我们本应水乳交融的平民数学？我们需要平民化的数学应用，需要用更为平民化的语言把简洁生涩的公式定理表述得更加接地气。</p>
<h2 id="3-1-排列组合的概念"><a href="#3-1-排列组合的概念" class="headerlink" title="3.1 排列组合的概念"></a>3.1 排列组合的概念</h2><h3 id="3-1-1-公平的决断——扔硬币"><a href="#3-1-1-公平的决断——扔硬币" class="headerlink" title="3.1.1 公平的决断——扔硬币"></a>3.1.1 公平的决断——扔硬币</h3><p>排列组合是本书介绍的第一个概率论概念，也是在高中学过的一个概率学的入门概念。概念记不清了也不要紧，现在回忆一下在中学学过的排列组合都有哪些经典问题来着。</p>
<p>首先是扔硬币（图3-2）。</p>
<p><img src="Image00013.jpg" alt></p>
<p>图3-2 排列组合的经典场景——扔硬币（见彩插）</p>
<p>如果一个匀质的硬币——也就是扔出正面朝上和反面朝上各有一半可能性的硬币，我们连扔3次，产生3次朝上的可能性有多大？</p>
<p>这个计算应该不算难，首先每一次扔出，每一个面的可能性是一样的，即正面1/2的可能性，反面也是1/2的可能性。</p>
<p>那么第一次扔，正面朝上是1/2的可能性，反面朝上也是1/2的可能性。</p>
<p>在第一次正面朝上的情况下，第二次扔，正面朝上的可能性仍然是1/2，反面朝上也是1/2的可能性。（即正正，正反。）</p>
<p>而在第一次反面朝上的情况下，第二次扔，正面朝上的可能性仍然是1/2，反面朝上也是1/2的可能性。（即反正，反反。）</p>
<p>也就是说连扔两次，两次结果为“正正”、“正反”、“反正”、“反反”的可能性都是完全一样的，各是1/4。</p>
<p>以此类推，连扔3次，3次都是正面朝上的可能性应该为1/8，即概率为1/8或12.5%。也就是说，3次朝上分别为“正正正”、“正正反”、“正反正”、“正反反”、“反正正”、“反正反”、“反反正”、“反反反”。这几种的可能性是一样大的（图3-3）。</p>
<p><img src="Image00014.jpg" alt></p>
<p>图3-3 正反面朝上的可能性</p>
<p>我们可以想想在生活中的例子，扔硬币和扔骰子很多时候都作为大家凭运气讲公平的一种裁决手段，如两个人打赌赌单双数或者大小数，4个人打麻将决定抓牌位置，我们都会借助硬币或者骰子这样的几率产生均等的工具来将公平进行到底，当然那些手法出众或者出老千的情况除外。</p>
<p>在影视作品里曾看到过一些赌徒为了让自己扔骰子掷出6点的概率增加而在6点的正对面放置铅弹一类的重物，使得骰子的6个面中6点被掷出的几率远高于其他几面（图3-4）。而一旦被人识破，该赌徒则会被其他赌徒殴打甚至是杀害。显然，在事先得知骰子被做了如此手脚之后，是不会再有兴趣和该赌徒博弈的，因为掌握这种严重不对称信息的人会成为不败的赢家，因为这种机会的均等性被破坏了，造成极大的“不公平”。</p>
<p><img src="Image00015.jpg" alt></p>
<p>图3-4 “不公平”的骰子</p>
<p>如果一个随机试验所包含的单位事件（就是刚才说的3次朝上分别为“正正正”、“正正反”……这其中每一种情况都是单位事件）是有限的，且每个单位事件发生的可能性均相等，则这个随机试验叫做拉普拉斯试验，这种条件下的概率模型就叫古典概型。古典概型也叫传统概率，该定义是由法国著名数学家拉普拉斯（Laplace）提出的。</p>
<p>这种使用穷举有限多个可能性，并且根据可能性在所有事件中所占比例求出可能性的问题，就可以使用排列组合的方式来进行计算。</p>
<h3 id="3-1-2-非古典概型"><a href="#3-1-2-非古典概型" class="headerlink" title="3.1.2 非古典概型"></a>3.1.2 非古典概型</h3><p>上述“古典概型”的特点是“包含的单位事件是有限的，且每个单位事件发生的可能性均相等”。单位事件指的就是抛出一个“正正正”或者“正正反”这种一个确定的试验结果的事件。可能性均等就是“正正正”、“正正反”……一共8种情况，每种情况产生的机会是一样的。</p>
<p>那么是不是也有不符合古典概型的反例呢？也就是说“包含的单位事件不是有限的或每个单位事件发生的可能性不均等”则不算是古典概型，有这样的例子吗？</p>
<p>有的。首先，刚刚提到的赌徒改造骰子的例子就是“每个单位事件发生的可能性不均等”的例子，那么这种情况下就不能使用穷举、排列组合的方法进行计算，算出来也和试验结果不一致；再者，还是使用骰子掷数的例子，用两个骰子来掷。因为每个骰子的掷出范围为1～6个点，所以两个骰子扔出的范围是2～12个点。但是需要注意，虽然骰子掷出每个点的机会是一样的，但是2～12这11个点产生的可能性不是一样的。两个骰子都扔出1才产生2，所以概率为1/36，同理12的概率也是1/36。但是6就不一样了，两个骰子的点数可以为1和5、2和4、3和3、4和2、5和1，每种情况的概率都是1/36，相加得5/36。所以对于两个骰子扔出2～12个点，每个点产生的概率可就不一样了，那每个点的概率必然不能是1/11。好在产生2～12这11个点的每种情况中，各自是由两个古典概型组成的，还能分解以后各自求解（图3-5）。</p>
<p><img src="Image00016.jpg" alt></p>
<p>图3-5 两个骰子掷出的点数</p>
<p>而“包含的单位事件不是有限的”这种例子其实也很多，例如，我想知道我每天出门碰到熟人的概率。这种问题用古典概型也是不能解决的，所有单位事件的定义非常复杂，每个单位事件也不能通过类似扔骰子这么简单的事情就描述清楚，还有时间、地点等各种复杂的情况，当然是没有办法用古典概型来获解的。</p>
<h2 id="3-2-排列组合的应用示例"><a href="#3-2-排列组合的应用示例" class="headerlink" title="3.2 排列组合的应用示例"></a>3.2 排列组合的应用示例</h2><h3 id="3-2-1-双色球彩票"><a href="#3-2-1-双色球彩票" class="headerlink" title="3.2.1 双色球彩票"></a>3.2.1 双色球彩票</h3><p>双色球彩票在中国的历史不算短了，大概是从2003年2月就开始在中国联网发售。虽然有很多人都在诟病说双色球开奖的方式不够公平透明，但是还有相当多的彩民一直在执着地研究双色球开奖的规律（图3-6）。</p>
<p><img src="Image00017.jpg" alt></p>
<p>图3-6 双色球</p>
<p>这里只从数学的角度来看一下双色球彩票的头奖和你花两块钱下注购买的彩票一致性的概率，也就是人们平时说的买一注然后就能中头奖的概率有多大。这里必须先明确一个前提，就是确实没有人对彩票购买和抽奖小球的抽出做干预，换而言之，就是你下的这一注是在完全不知道开奖结果的情况下买的，抽奖也是在每个球被抽出的概率一样的情况下进行的。</p>
<p>我们购买一注彩票的时候，首先选择红球，从01～33共33个号码中选择6个号码。再选择蓝球，从01～16共16个号码中选择1个号码。6红1蓝一共7个号码组成完整的一注彩票。</p>
<p>最终抽奖的时候也会是从01～33共33个红色号码中选择6个号码，再从01～16共16个蓝色号码中选择1个号码。6红1蓝一共7个号码组成完整的一注头奖彩票。</p>
<p>如果选择的6红1蓝和头奖的6红1蓝完全一致那就算中了头奖，奖金怎么算……这个大家去问福利彩票中心吧，咱们这里只算概率。</p>
<p>先算算挑选6红1蓝一共有多少种挑法。</p>
<p>首先先从33个红球中挑选6个红球，用组合的方式计算<img src="Image00018.jpg" alt> ：</p>
<p><img src="Image00019.jpg" alt></p>
<p>也就是1107568种选法。</p>
<p>再从16个蓝色球中选1个，一共有16种选法。</p>
<p>这样6红1蓝的选法一共有1107568×16=17721088种。</p>
<p>举个形象点的例子，老天爷在想1到17721088中的一个整数，你也在想1到17721088中的一个整数，你们俩想的完全一样的概率有多大？没错，是1/17721088，大约是0.0000000564%的概率。</p>
<p>不少人说，这没关系，反正有一些破解方法。有哪些破解方法？支持以下两种方法的人比较多。</p>
<p>方法一： 多买几种组合。</p>
<p>那就算算看，一共17721088种可能，全部买下来——也就是俗称的全餐彩票，一共要花35442176元人民币。奖池是不是在所有中头奖的人平分后还能至少分到手这么多不好说（加上二等奖、三等奖一共能领到多少钱都可以自己算）。按照比例缩小一些试试呢？比如买一半，那就是中奖概率变成1/2——要花17721088元，还有一半的可能性是不中。其他比例读者可以自己计算。每一种比例在降低投入的同时，也在降低中奖概率。所以这种方式并没有提高买彩票的投入产出比。</p>
<p>方法二： 只买一种组合，坚持到底，就能提高胜率。</p>
<p>有这样思想的朋友估计是这么一个思路，就是这一次这种组合不中，由于每种组合概率一样，所以在多次随机过程里前面出现过的组合后面出现的概率就低，前面没出现过的组合后面出现的概率就高。有这样思路的朋友，想想这样一个事情，交通事故其实是一个典型的随机事件，平均每个月发生交通事故的数量是相对“固定”的，只是发生的地点、发生的时间、发生的车型、涉及的人可能不同而已。</p>
<p>那么如果要避免交通事故，就要先人为制造一些无害的交通事故，造够了次数，这个月就不会再发生交通事故了，大家也可以安心上路了。这个逻辑就变得顺理成章，但是事实真的会是这样吗？</p>
<p>这种随机产生的每一次结果之间其实是独立的概率，换句话说每一次结果是不会影响前后随机事件里产生的结果的，也不会影响到前后的随机事件的结果。在静态概型里，这个结论请大家牢记。也有人表示怀疑，说我明明在一些事情里看到前一件事发生后会影响后面事件发生的可能性，那这种事情怎么解释。这种事情，首先不是古典概型的范畴，如果要归类的话可以算作条件概率的研究范畴，条件概率在后文会详细讲解。</p>
<h3 id="3-2-2-购车摇号"><a href="#3-2-2-购车摇号" class="headerlink" title="3.2.2 购车摇号"></a>3.2.2 购车摇号</h3><p>北京是一个以拥堵著称的城市，拥堵的问题也是由来已久，而且几乎是越来越严重。在万般无奈的情况下，专家们最后祭出了一个大招——摇号。</p>
<p>摇号是一个带有比较浓郁配给制色彩的手段。大概的形式就是，每个已经具备摇号资格的人登记一下身份证号码，所有登记过身份证号码的人都放在一个大“池子”里，然后每两个月通过“随机”的方式产生20000个号码，这20000个幸运儿就是中签者，就拥有了购买一辆汽油动力汽车的配额（图3-7）。</p>
<p><img src="Image00020.jpg" alt></p>
<p>图3-7 汽车摇号</p>
<p>中签概率多大呢？有人想到直接用20000除以1420000就是自己中签的概率。但是为什么是这么算呢？有理论依据吗？下面试着推导一下。</p>
<p>以真实数据为例，2015年9月这个“池子”里大约有1420000个号，从里面选出20000个号，一个人中签的概率有多大？稍微想想看，这个数值也不会是<img src="Image00021.jpg" alt><br>，因为不是要求1420000个号里找出20000个号一组的不同组合。</p>
<p>在没有其他政策进行干预而将1420000个号码进行等概率选出的情况下，选出20000个号，而自己的号正好在其中。相当于用一个1420000面的骰子投掷一次选出一个号，然后把这个号抹掉，再用剩余的1419999个号做成一个1419999面的骰子，再投掷一次，选出一个号，然后把这个号再抹掉……一次一次下去，直到20000次为止。实际相当于这么一个过程。</p>
<p>想不清梦的话试试用小一点的数字找找感觉。</p>
<p>如果是有3个人参与摇号，摇出2个，是怎么计算呢？</p>
<p>按照这种扔骰子的方法来玩，假设我们有个3面的骰子（其实真的是没办法做出一个3个面的等概率骰子，我们就当真的能做出来好了）。第一次我被骰子选中的概率为1/3，还有2/3是没被选中的概率。在没选中的情况下，换2个面的骰子，这一次我被骰子选中的概率为1/2。</p>
<p>算算我能被选中的概率一共是多少吧，</p>
<p><img src="Image00022.jpg" alt></p>
<p>如果是6个人参与摇号，摇出3个，是怎么计算呢？</p>
<p>仍然用扔骰子的方法，同理：</p>
<p>第一次，选中的概率为1/6，没选中的概率为5/6，现在该换5面的骰子了。</p>
<p>第二次，选中的概率为1/5，没选中的概率为4/5，现在该换4面的骰子了。</p>
<p>第三次，选中的概率为1/4，没选中的概率为3/4，结束。</p>
<p>被选中的概率是多少呢？</p>
<p><img src="Image00023.jpg" alt></p>
<p>如果有兴趣可以继续用其他例子去算，我们现在直接说结论了，这种情况其实就是用掷骰子的次数除以最开始骰子的总面数，也就是一共选出的次数除以全样本空间的大小。20000/1420000这个答案是没有问题的，也就是中签率为1.4%左右，一年摇号6次的话，估计运气最差的人要11.8年才能抽中，听到这样的消息现在整个人都不好了。不过别忘了，每个月这个“池子”还在变大，究竟等多久可能只有老天知道了。我们这里只从理论上讲解了计算的原理，但是和实际的计算方法还是有区别的，毕竟实际的遴选规则也是在不断变化，例如对长时间未选中的号码加遴选权重，这样计算起来更为复杂一些。</p>
<h3 id="3-2-3-德州扑克"><a href="#3-2-3-德州扑克" class="headerlink" title="3.2.3 德州扑克"></a>3.2.3 德州扑克</h3><p>七零后和八零后的朋友估计对香港影星周润发很熟悉，尤其是发哥在《赌神》系列中风流倜傥的表演给人留下很深的印象，其中最后发哥和大BOSS单挑基本玩的都是“梭哈”——英文名称Show Hand。梭哈和我们今天要说的德州扑克在牌点大小比较的规则上是非常近似的。</p>
<p>德州扑克是很多年轻人都喜欢的扑克竞技游戏，全称是Texas Hold’em Poker，中文简称德州扑克。这里研究一下各种牌型出现的概率。</p>
<p>对于不熟悉德州扑克规则的读者来说，还是有必要先简单描述一下德州扑克的规则。</p>
<p>一张台面至少2人，最多22人，一般是由2～10人参加。德州扑克一共有52张牌，没有王牌。每个玩家分2张牌作为“底牌”，5张由荷官（专业发牌的人）陆续朝上发出的公共牌。开始的时候，每个玩家会有2张面朝下的底牌。经过所有押注圈后，若仍不能分出胜负，游戏会进入“摊牌”阶段，也就是让所剩的玩家亮出各自的底牌以较高下，持大牌者获胜。</p>
<p>第一轮是在每位玩家只能看到自己2张底牌的情况下加注。</p>
<p>第二轮是在每位玩家能看到自己2张底牌，以及桌面上3张公共牌的情况下加注。</p>
<p>第三轮是在每位玩家能看到自己2张底牌，以及桌面上4张公共牌的情况下加注。</p>
<p>第四轮是在每位玩家能看到自己2张底牌，以及桌面上5张公共牌的情况下加注。</p>
<p>最多只会经历这4轮，一局游戏结束。</p>
<p>游戏的输赢就是看玩家自己的2张底牌与桌面上当前已开出的公共牌，一共挑选出5张，组成最“大”的牌，哪位玩家的牌组合最“大”，哪位玩家就获得胜利。</p>
<p>牌的组合大小怎么定义呢？</p>
<p>对博弈类游戏有所了解的读者可能会有一些常识性的体会——组合出现的可能性越小的通常牌越“大”。那德州扑克里都有哪些组合呢？</p>
<p>第一等：同花大顺。相同花色的A、K、Q、J、10（图3-8）。</p>
<p>第二等：同花顺。相同花色的5张牌相连。例如，红桃6、7、8、9、10，黑桃9、10、J、Q、K等（图3-9）。</p>
<p><img src="Image00024.jpg" alt></p>
<p>图3-8 同花大顺</p>
<p><img src="Image00025.jpg" alt></p>
<p>图3-9 同共顺</p>
<p>第三等：四条。4张相同点数的牌。例如，4张8，4张Q等（图3-10）。</p>
<p>第四等：满堂红（也叫葫芦）。3张相同点数的牌，再加2张相同点数的牌。例如，3张5和2张9，3张K和2张10等（图3-11）。</p>
<p><img src="Image00026.jpg" alt></p>
<p>图3-10 四条</p>
<p><img src="Image00027.jpg" alt></p>
<p>图3-11 满堂红</p>
<p>第五等：同花。5张相同花色的牌，但不是同花顺。例如，5张牌都是方块，5张牌都是梅花等（图3-12）。</p>
<p>第六等：顺子。5张点数相连的牌，但至少包含两种花色。例如，方块2、方块3、梅花4、红桃5、红桃6，红桃8、方块9、梅花10、红桃J、黑桃Q等（图3-13）。</p>
<p><img src="Image00028.jpg" alt></p>
<p>图3-12 同花</p>
<p><img src="Image00029.jpg" alt></p>
<p>图3-13 顺子</p>
<p>第七等：三条。3张相同点数的牌，再加2张不同点数的牌。例如，3张9和1张3、1张K，3张Q和1张A，1张6等（图3-14）。</p>
<p>第八等：两对。2张相同点数的牌作为一对，两对牌，再加1张单牌。例如，2张5、2张9、1张A，2张10、2张J、1张K等（图3-15）。</p>
<p><img src="Image00030.jpg" alt></p>
<p>图3-14 三条</p>
<p><img src="Image00031.jpg" alt></p>
<p>图3-15 两对</p>
<p>第九等：一对。2张相同点数的牌作为一对，一对牌，再加3张单牌。例如，2张10、1张7、1张8、1张9，2张A、1张K、1张9、1张5等（图3-16）。</p>
<p><img src="Image00032.jpg" alt></p>
<p>图3-16 一对</p>
<p>第十等：高牌。高牌即单牌，不满足前面九等牌中任何一种的，就只能按照点数大小按顺序决定高低了。A比K大，K比Q大，以此类推，2最小。</p>
<p>这里试求一下，一个人自己摸牌（没有任何第二个玩家参与的情况下），前三等牌被摸到的概率有多大。</p>
<p>请注意，在没有开始摸牌之前，如果牌被洗过若干次（没有其他人为干扰因素），牌的发放是随机的。而一旦底牌发放以后，尤其是玩家自己看过牌以后，这个时候的概率计算和现在要讨论的这种概率计算是不一样的——显然，一个是完全随机的，一个是有一定条件的，条件就是刚刚看到的那两张底牌，而这种情况暂时不讨论。</p>
<p>那么这种情况下，整个选牌的过程相当于从整副牌52张中选出7张，并从中组合出最大牌的过程，即</p>
<p><img src="Image00033.jpg" alt></p>
<p>7张牌的组合一共有133784560种。</p>
<p>1.同花大顺</p>
<p>在所有的组合中有多少是同花大顺的呢？同花大顺一共4种，分别是黑桃、红桃、梅花、方块的10、J、Q、K、A。7张牌里面，5张已经确定，另外2张怎么选都无所谓。以黑桃为例，黑桃的同花大顺选出后，其实还有47张牌没有发，挑出2张，即</p>
<p><img src="Image00034.jpg" alt></p>
<p>同理，红桃、梅花、方块的同花大顺也是一样的，都是1081种组合，即同花大顺共计有4324种组合。因此概率是</p>
<p><img src="Image00035.jpg" alt></p>
<p>2.同花顺</p>
<p>同花顺有多少种情况呢？以黑桃为例，假设A～5组成同花顺，黑桃6是不能发的，还剩下46张可以组合，则这种情况下组合数量为</p>
<p><img src="Image00036.jpg" alt></p>
<p>2～6组成同花顺，7是不能发的，A可以发（A作为散牌），所以还是</p>
<p><img src="Image00037.jpg" alt></p>
<p>以此类推，黑桃的组合为A～5，2～6，……，9～K，一共9种，那么黑桃一种花色的牌型种类就为</p>
<p>1035×9=9315</p>
<p>4种花色的组合数就是</p>
<p>9315×4=37260</p>
<p>得到结果概率为</p>
<p><img src="Image00038.jpg" alt></p>
<p>网上还有一种算法：</p>
<p><img src="Image00039.jpg" alt></p>
<p>这种算法是有问题的。错误发生的地方大概在这里：“以黑桃为例，A～5，2～6，……，9～K，一共9种，47张牌里挑出两张，计算：</p>
<p><img src="Image00040.jpg" alt></p>
<p>那么黑桃的同花顺的牌型种数为</p>
<p>1081×9=9729</p>
<p>同理，红桃、梅花、方块的同花顺都有9729种组合，共计38916种组合，得到结果</p>
<p><img src="Image00041.jpg" alt></p>
<p>这里一旦选好了5张牌作为“核心组合”以后，其他牌的选择其实不是自由的，因为有的牌配进来以后就发现这个一开始就认定的组合不是最后在台面上最大的牌。</p>
<p>3.四条</p>
<p>四条有多少种呢，计算方法类同，4张已经确定，还有48张没有发：</p>
<p><img src="Image00042.jpg" alt></p>
<p>注意这里4张的组合有多少种——13种，所以四条可能出现的组合数量为</p>
<p>17296×13=224848</p>
<p>除一下得到结果</p>
<p><img src="Image00043.jpg" alt></p>
<p>虽然看上去机会仍然很渺茫，但是比同花大顺和同花顺的概率还是大了不少，是不是？</p>
<p>其他的组合方式大家有兴趣可以自己慢慢去算，网上也有现成算好的对照表。</p>
<p>提示一下，两对牌这种情况比较难算，因为情况比较复杂。它复杂的地方在于在满足两对牌的情况下，还要将满足同花大顺、同花顺、四条、满堂红、同花、顺子、三条的情况全部剔除才行。两对牌的牌型为31433400种，概率为23.5%。还有一些其他形式的对照表，就是在手里底牌为已知固定组合的情况下，最终与公共牌组合成为各等牌的概率。这里温馨提醒一下各位牌友，刚刚我们计算的概率是在一个人自己摸牌的情况下产生的概率。一旦是5个人、10个人玩的时候就大不相同了。有一点是确定的，人越多，公共牌和其他玩家一起组成的牌的种类可能性也越多，“罕见组合”在一局中出现的可能性也比一个人自己摸牌要高很多，请一定注意哦。</p>
<h2 id="3-3-小结"><a href="#3-3-小结" class="headerlink" title="3.3 小结"></a>3.3 小结</h2><p>排列组合以及利用排列组合计算的古典概型在生产生活中可以解决很多问题。刚刚这些例子我们已经看到了不少用法和技巧。</p>
<p>在这里有几个概念可能会被误读，我们需要在这里澄清一下。</p>
<p>最容易发生的误解是，扔硬币的时候，如果前3次出现“正”，那第4次出现“反”的概率就增大。</p>
<p>这里面的误解我认为有两个层面。</p>
<p>误解1：对“概率”一词本身的理解有偏差。</p>
<p>“概率”一词的汉语含义是几率、可能性、可能程度。我们通常会以我们自己臆想的方式去猜测某件事情的可能性比较高或者比较低，这会导致我们对概率大小理解的偏差。</p>
<p>在使用排列组合与古典概型的方法时，有一个大原则就是这些概率实际上是通过统计计算出来的，请注意，由统计得出概率是人们得到概率最原始的方法，包括后面将要介绍的条件概率也是一样的道理。也就是说，硬币扔出正面和反面各50%的概率是多少，这不是因为硬币本身有两个面，而是通过多次扔硬币，然后用得到正面的次数除以总数得到扔出正面的概率——这个才是定义。而如果硬币本身不是匀质的，如由于图案雕花构造或者铸币金属本身的特性导致正面较重，反面较轻，很有可能导致扔出正面的概率为60%，反面的概率为40%的情况（抑或其他比例）。请注意，这个结论同样是通过多次扔硬币得出来的，例如扔1000次，发现有600次是正面，400次是反面。这时再计算扔3次硬币会产生3个正面的概率就不是3个1/2相乘了，而是3个0.6相乘了。</p>
<p>既然如此，概率本身的解释就是对于大量样本分布比例的解释，而不是对单次事件的可能性的解释。我们说扔硬币产生正面概率50%，反面概率50%，其实是在说扔1000次硬币，理论上会有500次产生正面，500次产生反面；扔10000次硬币，理论上会有5000次产生正面，5000次产生反面。这才是概率本身的含义，而对于单次扔硬币的解释没有意义。</p>
<p>误解2：事件之间的独立性。</p>
<p>扔出一次硬币，得到正面，下一次重新再扔，那么这一次扔硬币和上一次扔硬币有关系吗？学过概率论的朋友都不会陌生，答案是“没有关系”。没学过概率论的朋友其实稍微想一想也能得出这个结论。</p>
<p>这里不妨再做一个实验，这个实验略显复杂且无厘头，但是这个过程大家想想很快能想明白。</p>
<p>让100个人，每个人都手持一枚同款匀质硬币，让他们各自开始扔，一次、两次、三次……任何一个人都是一直在扔硬币直到出现最近3次连续都是正面的时候停下来。最后，这100个人都会在那里静静地停下来等待下一个指令，这个指令就是让他们同时进行一次抛硬币的动作，然后比较这100枚硬币正反面出现的比例。对于每一位参与实验的人来说，如果由于前3次投掷都产生正面而使得第4次投掷出现反面的概率变高的话，那么会在100人同时投掷的实验中看到一个奇怪的现象，那就是出现反面比正面多很多的情况。真的会这样吗？人们甚至还可以观察更为极端的情况，那就是等待最近5次连续都是正面的时候停下来，结果又当如何？如果在一个试验中直接扔100枚硬币，那么产生正面和反面应该都是50次左右。这又和刚刚的假设看上去如此矛盾。究竟哪种说法是对的呢？统计的定义交给统计来验证吧。</p>
<h1 id="第4章-统计与分布"><a href="#第4章-统计与分布" class="headerlink" title="第4章 统计与分布"></a>第4章 统计与分布</h1><p>前一章简单复习了一下排列组合，就当热热身吧。本章开始再复习一下统计和分布的知识。</p>
<p>统计学的内容其实非常庞杂，应用领域也很广，也有着不同的学派。但是如果不做学术研究而只是关注应用，我认为未必需要进行面面俱到的深入学习。</p>
<p>不管是什么学科，尤其是工科和理科中的各个学科，它们绝不会是人们单纯觉得好玩，绝不会是为了消遣娱乐而创立的，而是它的发展最终能够降低人类认知或描述世界的成本，带来工作效益的直接或间接的提升。有了这个思路，我们在理解很多现象的时候都会更自然，因此这里从应用的思路来入手。</p>
<h2 id="4-1-加和值、平均值和标准差"><a href="#4-1-加和值、平均值和标准差" class="headerlink" title="4.1 加和值、平均值和标准差"></a>4.1 加和值、平均值和标准差</h2><p>上学是每个人几乎都经历过的过程，拿来做例子也许会更加亲切。</p>
<p>假设在一所高中，有3个年级，每个年级有10个班，每个班有40到60个学生不等。要对这些老师和学生进行管理喊口号是不好使的，作为学校的教学主任，他需要了解这些学生的学习情况，知道学生学习成绩的变化，老师教学水平的高低，以及调整的方式（图4-1）。</p>
<p>最早的考试不知道是哪位聪明人发明的，因为考试是一种天然有着“数字化管理”基因的东西，天然就是一种指标坯子。例如，一次学校期末考试以后，所有的学生成绩都汇总上来，假设考试的科目有语文、数学、英语3个学科，一个包含1000多个学生的四五千个单位的数据就会摆在这位教学主任的眼前（图4-2）。</p>
<p>倘若你现在就是这位教学主任，需要你和校长汇报一下这次考试各班的情况如何，你会怎么办？</p>
<p><img src="Image00044.jpg" alt></p>
<p>图4-1 学生管理</p>
<p><img src="Image00045.jpg" alt></p>
<p>图4-2 学生成绩汇总</p>
<p>把所有的学生的每一门课的成绩都逐个给校长读一遍？恐怕是要花费太多的时间，搞不好开始汇报还没有3分钟校长已经睡着了。</p>
<h3 id="4-1-1-加和值"><a href="#4-1-1-加和值" class="headerlink" title="4.1.1 加和值"></a>4.1.1 加和值</h3><p>这里插入一段小小的联想，想想平时到超市里购物最后在收银台做了什么事情。收银员把每件货品的价格加和，不管是10件还是20件还是更多，最终只给出一个价格的合计值。顾客按照这个合计值付账一次性结束整个交易，而这显然是比对每件货品都单独结算一次的时间成本低很多的。这里面用到统计学的知识了吗？用到了，只是它太稀松平常了以至于人们几乎没有意识到而已。这种用一个加和值来概括性地描述一群事物的方法几乎不需要教学就能直观地实现早市上那些即便没有什么学历的菜贩也不用非要找个数学老师来系统教学一下或者深造一个统计学专业的文凭才能开始给买菜大妈们报价和卖菜吧。所以使用一个性状数值的加和值来对一群事物进行描述是一种非常自然的描述方式（图4-3 <a href="#ch1_back">[1]</a> ），这简直太棒了。</p>
<p><img src="Image00046.jpg" alt></p>
<p>图4-3 价格加和</p>
<p>这种例子其实到处都是，如平时说的GDP（Gross Domestic Product，国内生产总值，我们常常口口相传的国民生产总值实际是GNP——Gross National Product），再如“上个月我出差一共花了2000元”，这都是非常典型的用总和值来进行概括描述的例子。人们不需要具体阐述千万个工厂每个工厂有多大产值，千万家公司每家公司有多大产值；或者出差吃某一顿饭花了多少钱，坐出租车某一次花了多少钱，这种细节的陈述太繁琐。这就是人们从加和值描述中得到的最大好处——直奔主题，只关心人们最关心的总和数字，忽略里面的细节。通常把加和值的概念用希腊字母∑来表示，读作“西格玛”，后面还会经常碰到这个标记。</p>
<p>然后再回来看一下教学主任的问题。是不是也可以很自然地先想到，干脆用一个加和值来描述，这一个班所有的学生所有的成绩加起来一共多少分。如果真的这么做了会出现什么情况？</p>
<p>“一年级一班分数总和9600分，一年级二班分数总和13500分……”这一描述看上去是非常简洁的，但是这种描述带来的信息几乎没有什么价值。可以根据这个数值比较说一年级二班的学生比一年级一班的学生学习好吗？直观去看的话，这种似乎感觉很奇怪，但是怎么个奇怪法呢？</p>
<p>事实上可能是这样，一年级一班有40名学生，每个人3门功课每一门都是80分；一年级二班有60名学生，每个人3门功课每一门都是75分。需要陈述到这个级别才能明确究竟哪个班更好，这显然和我们用简洁数值做描述的初衷背道而驰。对学生成绩的描述如果能够成为对整个班级的成绩概括描述，同时兼有对每个个体的描述，套用现在流行的一句话——“那真是极好的”。有这样一种数吗？有的，如平均值。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<h3 id="4-1-2-平均值"><a href="#4-1-2-平均值" class="headerlink" title="4.1.2 平均值"></a>4.1.2 平均值</h3><p>平均值的计算方法大家肯定很熟悉，我们在学生时代就已经经历过无数的例子。上述例子就是以班级为单位把每个人的每门课程加在一起除以总的学生数量，再除以课程数量。</p>
<p>“一年级一班有40名学生，3门课程平均分为80分”。</p>
<p>“一年级二班有60名学生，3门课程平均分为75分”。</p>
<p>“一年级三班有50名学生，3门课程平均分为80分”。</p>
<p>从这里基本还是能得到一个清晰的感性认识，那就是一年级一班和一年级三班的总体水平是“一样的”，而且他们比一年级二班的水平高。因为在使用平均值进行比较时，实际直观感觉是在对比3个班级中每一个学生个体。</p>
<p>所有这类用单一的数据定义来概括性描述一些抽象或复杂数据的方式方法都叫做“指标”。平均分在这里就是一个很好的指标，因为它用一个简洁的数据定义概括了众多数据的特性。平均值和样本数量（学生数）这两个值就基本可以描述清楚学生分数的高低情况了。在上述例子里，平均分这种指标恐怕不是由某个数学家或者智商殊绝于人的家伙特意发明出来的，而是在生活中由于要进行对象数据的宏观描述而自然而然产生的一种方便的数值计算和描述方法。</p>
<p>另外，指标在很多企事业单位、学术技术领域都有广泛的应用。如证券交易中有很多价格指标——用来描述价格震荡的剧烈程度、价格变化的趋势等；环保领域有PM2.5浓度指标；以及交通警察在测量司机是否酒驾时使用的血液酒精浓度——BAC指标等。图4-4所示为家用多功能环境测量仪器的各种污染指标显示，有甲醛、PM2.5、PM10、VOC和电磁辐射，这些数值化的读数都是指标。</p>
<p><img src="Image00047.jpg" alt></p>
<p>图4-4 污染指标显示</p>
<p>指标的使用有助于我们简练地描述对象。再回到班级成绩统计的例子。</p>
<p>“一年级一班有40名学生，3门课程平均分为80分”。</p>
<p>“一年级二班有60名学生，3门课程平均分为75分”。</p>
<p>“一年级三班有50名学生，3门课程平均分为80分”。</p>
<p>从这组数据来看，基本可以得到一个印象，就是一年级一班的成绩“普遍”比一年级二班“好”，至少是从“宏观体现”上看比二班好，它和一年级三班“一样好”。但是一年级一班和一年级三班这两个班的每个人的成绩都是一样的吗？至少人数是不一样的。那么也许还需要进一步地描述这平均下来的80分和每个学生具体的课程分数之间的差异性有多大，这就涉及另一个描述的需求——标准差。</p>
<h3 id="4-1-3-标准差"><a href="#4-1-3-标准差" class="headerlink" title="4.1.3 标准差"></a>4.1.3 标准差</h3><p>我们先上公式，标准差公式如下：</p>
<p><img src="Image00048.jpg" alt></p>
<p>下面解释一下这个公式的含义。</p>
<p>我们以一年级一班所有40个学生为例，那么3门考试的情况下全班就有120个分数参与统计，也就是n=120。把每个学生每门课的成绩减去全班的3个学科总的平均分80分，这样得到120个差值，再把这些差值分别平方（主要是为了去掉负数，因为在分数差距里面，不管是比这个平均值多，还是比这个平均值少，都被视为偏差），将这些平方的结果再加和，之后除以参与统计的学科数量120，最后开平方，这个数字只可能是一个大于等于零的数字。用汉字描述起来很啰嗦，但是一旦变成一个标准差的指标以后，由于是约定俗成的，所以只需要“标准差”这3个字就能表示了。</p>
<p>这个数字表示的是什么含义？从这个数字得到的过程其实不难看出来。</p>
<p>如果所有的人的所有课程成绩都是和平均分一样，那么算出来的标准差就是0，因为每一个<img src="Image00049.jpg" alt> 肯定都是02<br>；反之，如果所有的人的课程成绩与平均分的差距都很大，好的很好，差的很差，那么结果就是这个值会很大。如果一个班级成绩标准差比另一个班级成绩的标准差小，说明学生之间的考试成绩水平差不多，标准差大则说明学生之间的考试成绩水平相差比较大。</p>
<p>需要说明的是，一般来说为了在教学战术指导层面让平均值和标准差更有针对性，通常是不会像例子里这样来操作的。更多的是以一个班为单位，求班里某一个学科成绩的平均值和标准差，或者求某一个学生所有学科的平均值和标准差。这两种计算分别用来描述一个教师教学的成果和某个学生的成绩以及偏科的程度。</p>
<p>例如，“一年级三班有50名学生，英语考试平均分为80分，标准差为4.25”，“张三同学，语文、数学、英语三门课的平均分为90分”。</p>
<p>前者能够反映教授这个班的英语教师的教学情况，后者能够反映张三这名学生的各学科学习情况——当然都是粗犷的概述性描述。</p>
<p>加和值（总和值）、平均值、标准差，这几个值是在生产生活中大量应用的统计学指标。不过在此需要强调的是，也是很容易被人误读的地方。那就是，平均值、标准差是客观的计算结果，是描述性的说明，但是绝非对比和评价的标准。</p>
<p>不少人认为，某学校某老师的学生的高考平均分比另一学校另一老师的学生平均分要高，这一定说明这个学校这个老师的教学水平要高。这个因果关系不一定是正确的，因为一旦在生活中应用，客观场景的细节会让这种对比变得毫无意义。虽然从广大家长的视角去看，不管怎么样，只要有选择的余地，比较两个班的平均成绩来判断自己的孩子进入哪个班未来会更有利是有道理的。</p>
<p>举个反例。如果这两个学校的老师的生源本身就有很悬殊的差距：一个老师的学生平均分都在80分左右，只能上一般的大学；另一个老师则有不少85分以上的学生，还有大量60、70分的“关系户”学生，如图4-5所示。那么或许后者的班级里诞生清华北大的学生的可能性还会比前一个班更大也未可知。</p>
<p><img src="Image00050.jpg" alt></p>
<p>图4-5 学生水平悬殊</p>
<p><img src="Image00051.jpg" alt></p>
<p>图4-5 （续）</p>
<p>所以，请读者注意，平均分、标准差只能做描述用，只是一种简洁的描述方法，最多只能帮助我们让数据宏观的“画面感”更饱满。它们既不是对比的标尺，也不是用来具体做规则制定用的硬性尺度，更不能用来孤立地评价好坏，因为“好坏”这种含有大量主观判断色彩以及个性化好恶的东西本身就很抽象而且标准繁多。</p>
<h2 id="4-2-加权均值"><a href="#4-2-加权均值" class="headerlink" title="4.2 加权均值"></a>4.2 加权均值</h2><p>平均值这种指标有一个“兄弟”，就是加权均值。权（Weight）指的是权重，也就是指所占的“比重”或“重要”程度。在前一节的例子里，我们看到全班学生的平均值计算方法，是把每个样本值（学生成绩）直接加和，然后除以学生数再除以学科数量来得到全班每个人每学科的平均值。这里一视同仁地把所有学生和所有学科进行等同看待，没有丝毫的偏倚，分数直接相加直接平均。然而这种方法并非在所有的场景里都是最合理的，我们再来看一个生活中的小例子。</p>
<h3 id="4-2-1-混合物定价"><a href="#4-2-1-混合物定价" class="headerlink" title="4.2.1 混合物定价"></a>4.2.1 混合物定价</h3><p>在超市里买可能都到过卖出售糖果零食副食品的柜台去，或者至少是看过那里有卖糖的。有一类比较受欢迎的糖叫做什锦糖，有的地方叫杂拌糖，就是把几种不同类型的糖混在一起卖（图4-6），过年的时候通常卖得不错。毕竟对于那些不是追求某一种糖品口味的人来说购买“一种糖”就等于买了若干种糖是很省事的——至少不需要挑选多次称重多次。但是这种糖如何定价呢？</p>
<p>例如，有一种什锦糖是4种单品糖按照1：1：1：1的比例组合而成的，它们分别定价为：水果糖15元一斤，奶糖25元一斤，牛轧糖30元一斤，巧克力糖40元一斤（图4-7）。</p>
<p><img src="Image00052.jpg" alt></p>
<p>图4-6 什锦糖的比例混合（见彩插）</p>
<p><img src="Image00053.jpg" alt></p>
<p>图4-7 什锦糖中每种单品按1：1：1：1</p>
<p>均匀混合以后，可以认为，理想状态下，一位客户如果正好称4斤糖就恰好为1斤水果糖，1斤奶糖，1斤牛轧糖和1斤巧克力糖。那这4斤糖一共应该是多少钱呢？15元+25元+30元+40元=110元，也就是平均一斤为27.5元。那么这种糖定价就应该为27.5元才为合理。因为也确实是这样，顾客购买的一斤什锦糖里面应该有水果糖、奶糖、牛轧糖、巧克力糖各0.25斤，而0.25斤的4种糖价格分别为3.75元，6.25元，7.5元，10元，加和仍然是27.5元——两种算法是一样的。</p>
<p>如果这几种糖的混合比例为1：2：3：4（图4-8），那么也比较容易得出，在混合均匀的情况下，10斤糖里有1斤水果糖，2斤奶糖，3斤牛轧糖和4斤巧克力糖。15元×1+25元×2+30元×3+40元×4=315元，也就是平均31.5元才为合理。这其实就是一种加权平均的算法了，因为每种糖的价格在什锦糖的均价上体现出来的是不同的比重。如果不按照这种方法定，还是按照非加权平均的方式去算会怎么样呢？按照前面的例子那就是27.5元一斤，而10斤的什锦糖为275元，比应售价低40元，显然是亏本的买卖。</p>
<p><img src="Image00054.jpg" alt></p>
<p>图4-8 什锦糖中每种单品按1：2：3：4的比例混合</p>
<p>这种由于混合比例所产生的权重不同，进而使用加权平均来进行计算的思路还有很多。如某些混合液体的成本估算，要把每种液体的成本和份数考虑在内，而不能直接用液体单价加和了直接平均。尤其是牛奶、豆浆、白酒这些通过原浆和水混合勾兑出来的液体，在评估成本的时候必须是用比例来计算。如以重量1：4的原浆和水的比例来勾兑白酒，勾兑完的白酒成本怎么计算？</p>
<p>1kg白酒成本=（1kg白酒原浆成本×1+1kg水成本×4）÷（1+4）</p>
<p>而绝对不会是</p>
<p>（1kg白酒原浆成本+1kg水成本）÷2</p>
<h3 id="4-2-2-决策权衡"><a href="#4-2-2-决策权衡" class="headerlink" title="4.2.2 决策权衡"></a>4.2.2 决策权衡</h3><p>除了计算混合物的定价，还有很多场合都会用到加权平均的概念。</p>
<p>在一些决策的场合我们会用到加权均值的概念。例如，在股东大会上投票决议是否通过某一决定的情况下，以所持股作为投票单位，那么这种情况显然大股东——持有比一般股东多很多股票的股东对决策意见的左右能力就要强很多。控股的大股东拥有超过50%的股份，这种情况显然是一家说了算（图4-9）。这是一种典型的加权均值的概念，权重就是股份比例。</p>
<p><img src="Image00055.jpg" alt></p>
<p>图4-9 股东大会上的投票（见彩插）</p>
<p>再如，一些重大的国家重点建设项目要进行广泛的意见听取和决策投票。这种时候的投票往往也是加权性质的，形式上可能略有不同但是实质没有区别。</p>
<p>举例来说，决定某地区水电站建设最后方案是否通过，假设项目组每个参与决策的人员都有打分的权利，百分制，规则如下。</p>
<p>（1）100分为完全同意，0分为不同意，50分为完全中立。</p>
<p>（2）只有全部人打分超过75分为通过（立刻按方案执行）。</p>
<p>（3）26～74分为再修改（意见正反两派分歧较大，需要修改）。</p>
<p>（4）0～25分为否决（挂起或无限期延迟）。</p>
<p>这种情况下，通常是不会把每个人的打分直接加和然后求平均值的，取而代之的或许会是以下方案。</p>
<p>与会的两院院士的意见分数有可能是不动的，也就是权值为1。</p>
<p>与会的当地主要领导权值为0.8。</p>
<p>与会的当地能源口的领导权值为0.6……</p>
<p>最后用各自打的分数乘以这个权重，自然分数会在一定程度上向着权值高的打分方倾斜（图4-10）。</p>
<p>总之，在决策中做加权平均的目的是为了让整个决策既融合众多参与方、利益方的意见，同时也尽量使它向着更权威、更理性、更科学的方面倾斜，这是它的核心指导思想。这里只是泛泛地用这样一个场景做说明，实际操作起来会更加复杂、严谨与合理。加权平均在决策中的用法是比较常见的，在经济管理学领域的“德尔菲法则”（Delphi Method）中加权平均是一个重要的思想。</p>
<p><img src="Image00056.jpg" alt></p>
<p>图4-10 投票中的权值分配</p>
<p>据称德尔菲法则是在20世纪40年代由赫尔默（Helmer）和戈登（Gordon）首创。1946年，美国兰德公司为避免集体讨论存在的屈从于权威或盲目服从多数的缺陷，首次用这种方法进行定性预测，后来该方法被迅速广泛采用。20世纪中期，当美国政府执意发动朝鲜战争时，兰德公司又提交了一份预测报告，预告这场战争必败。政府完全没有采纳，结果几年后一败涂地。从此以后，德尔菲法得到广泛认可。</p>
<p>这些在经济学、管理学等领域使用的加权均值的应用都是其推广和引申，都是在决策中广泛应用的场景，大家有兴趣还可以自己去发现更多的例子。</p>
<h2 id="4-3-众数、中位数"><a href="#4-3-众数、中位数" class="headerlink" title="4.3 众数、中位数"></a>4.3 众数、中位数</h2><p>不只是平均值、标准差这样的数值能够用来对一群对象进行描述，众数和中位数也有相关的作用。</p>
<h3 id="4-3-1-众数"><a href="#4-3-1-众数" class="headerlink" title="4.3.1 众数"></a>4.3.1 众数</h3><p>看这样一个例子，一个小区的理发师，在对当天所有前来理发的人做了年龄登记后，得到这样一个年龄列表“15、20、22、22、23、35、50、72”，一共8位顾客。其中，22就是众数。众数反映的是一个多数的概念，即一个数字比其他的数字出现得多，或者更普遍。</p>
<p>再如，我的同事娟娟也为我贡献了两个例子：其一，她每个月都要读几本书，去年一年每个月读书的数量分别为“2、2、3、2、3、2、1、1、1、0、1、6”，这里面有两个众数——1和2，都是出现最多的，均为4次，而其他数值出现的次数比4次少，这说明普遍每个月会读1本书或者2本书；其二，她每周要看一部电影，两个月统计下来每周看的电影类型如下“文艺、警匪、喜剧、惊悚、惊悚、喜剧、科幻、喜剧”。其中，“喜剧”是众数，出现3次（图4-11）。</p>
<p><img src="Image00057.jpg" alt></p>
<p>图4-11 看电影</p>
<p>首先，我们可以感性地理解众数就是在样本对象中出现最多的那个数字。但是在最后的例子里我们也看到，虽然叫做“众数”，可是不一定是数字，也有可能是别的数据类型，例子里给的是一个电影类型的文字枚举值。当然，如果在样本对象中没有任何一个数值比其他对象多（如所有的数值都只出现一次，或者都出现两次……），这种情况下是不存在众数的，也就是说没有一个数字比其他的数字出现得更多。</p>
<p>众数在我们日常生活中应用的例子也是有的，如娟娟每周都要看电影，而如果喜剧电影是众数这个信息被她的崇拜者知道了，那买两张喜剧电影票请她看电影的情况下会比买两张战争题材或者其他故事片电影更有成功的把握。这种众数的应用场景就是对人偏好特点的描述。</p>
<p>对于做数据库开发的人来说，平时在做</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT（*），XXX FROM TABLE GROUP BY XXX；</span><br></pre></td></tr></table></figure>

</details>

<p>进行这种操作时也是在尝试求众数，只是可能很多时候的结果中有可能得不到众数而已。</p>
<h3 id="4-3-2-中位数"><a href="#4-3-2-中位数" class="headerlink" title="4.3.2 中位数"></a>4.3.2 中位数</h3><p>中位数，顾名思义，就是位于中间位置的数字。</p>
<p>举个例子，一组新毕业的大学生参加新员工入职体检，身高测量样本如下（单位为厘米），从小到大排序“168、172、175、175、177、185、205”，一共7个数字，中位数是175——从高到低数也是第4个，从低到高数也是第4个。如果是8个数字，“168、172、175、175、177、177、185、205”、那么中位数是176，即177+175/2。找到中位数就是这么简单。</p>
<p>在这个例子里面可以发现一个特性，就是有一个比一般人高很多的新员工——身高205厘米的这位，这样7个人平均值是179.57，比中位数175明显大一些。如果去掉205这个样本再求一次平均值，平均值为175.33，中位数是175。用中位数来描述样本的分布，在一定程度上可以消除个别极端值对整个样本平均值的影响。</p>
<p>我们平时在生活中用平均值来描述样本的情况比较多，而较少用众数和中位数，主要是因为平时生活中的场景中多为正态分布，所以平均值、中位数、众数非常接近，那么只用平均值最多加上标准差来表示即能够满足一般性的描述需求。至于正态分布是什么，后文将会介绍。</p>
<h2 id="4-4-欧氏距离"><a href="#4-4-欧氏距离" class="headerlink" title="4.4 欧氏距离"></a>4.4 欧氏距离</h2><p>在刚刚讲述标准差的例子里其实我们还用到一个概念，就是欧氏距离（Euclidean Distance），只是当时没有提出这个概念。回想一下计算标准差的过程，“把每个学生每门课的成绩减去平均分，再把这些差值分别平方，将这些平方的结果再加和，之后除以学生数量，最后开平方”。</p>
<p>注意中间这个过程：“每门课程的成绩减去平均分，再把差值平方”，这其实就是在求“欧氏距离”的过程。</p>
<p>所谓欧氏距离中的“欧”指的是被称作几何之父的古希腊数学家欧几里得。欧氏距离是在其巨著《几何原本》中提到的一个非常重要的概念。欧氏距离的定义大概是这样的：在一个N维度的空间里，求两个点的距离，这个距离肯定是一个大于等于0的数字（也就是说没有负距离，最小也就是两个点重合的零距离），那么这个距离需要用两个点在各自维度上的坐标相减，平方后加和再开平方。</p>
<p>欧氏距离使用的范围实在是太广泛了，我们几乎每天都在使用。</p>
<p>一维的应用就相当多，如在地图上有一条笔直的东西向或者南北向的路，在上面有两个点，怎么量取它们在地图上的距离？数轴标识如图4-12所示，可以用尺子的刻度贯穿两个点，大值减小值就能直接得出结果，最多再乘以一个比例尺就能得到实际的大小。或者用其中一个点的读数减去另外一个点的读数，不管结果正负，将它平方后再开方，还是一个非负数的值，这两种办法本质上没有什么区别。地图明明是一个二维平面的概念，为什么非要说是一维的呢？只是因为量取的手段和一维一样，只参考一个维度的读数就可以了。</p>
<p><img src="Image00058.jpg" alt></p>
<p>图4-12 欧氏距离一维的应用</p>
<p>二维的应用也是很多的，其中最典型的莫过于人们熟知的“勾股定理”。公式如下：</p>
<p><img src="Image00059.jpg" alt></p>
<p>即</p>
<p><img src="Image00060.jpg" alt></p>
<p>在一个直角三角形里，斜边长度等于两个直角边平方之和再开方。这其实就是求斜边两个端点的欧氏距离，别忘了这里有一个隐含条件，就是斜边距离是不能用尺子直接去量的，只能用两个已知的直角边的长度做条件。至于斜边长度等于两个直角边之和的定理在不同的阶段用三角函数作为工具证明过，用面积作为工具证明过，用相似三角形的方法证明过，方法实在是太多了，在百度网页搜索中我们至少可以找到16种完全不同而且都是正确的方法。图4-13所示为一种二维空间中的向量计算方式示意图，在知道向量分别在x轴和y轴的投影之后就能用勾股定理求出欧氏距离。</p>
<p><img src="Image00061.jpg" alt></p>
<p>图4-13 二维空间中的向量计算</p>
<p>推广到三维的应用，还是可以使用勾股定理的思路进行计算。每个点都向3个平面各做一条垂线段，可以看出，两个点的距离其实就是一个长方体的对角线长度。最后得到距离为3个维度的坐标差值分别平方加和再开平方：</p>
<p><img src="Image00062.jpg" alt></p>
<p>这只是在两个平面空间中用了两次勾股定理而已。</p>
<p>例如，在忽略地球自身弧度的情况下，求两个距离较近的不同高度的楼宇顶部距离，完全可以使用这种欧氏距离的定义直接求解。如图4-14所示，在三维空间中，实际需要使用两次二维空间上的勾股定理就能计算出三维空间中的两点之间的欧氏距离。</p>
<p><img src="Image00063.jpg" alt></p>
<p>图4-14 三维空间中的欧氏距离应用</p>
<p>根据上述一维、二维、三维的欧氏距离计算方法：</p>
<p><img src="Image00064.jpg" alt></p>
<p>可以推断四维、五维一直到N维空间上的欧氏距离的计算公式，一定是N个维度的“读数”差的平方再开方。</p>
<p>欧氏距离除了刚刚举的例子，在后面数据挖掘部分会有很多应用场景。它主要用来描述两个多维点之间的“距离”，遗憾的是，三维以下的点和点的距离通过刚刚的讲解很容易出现画面感，四维和四维以上的距离就只能凭想象了，只是在计算中确实存在且有对应的含义解释。这种解释通常也用来直接判断两个点在多维关系上谁与谁更“近”，虽然超过三维的情况下这个“近”已经没有办法用手边的工具量出来。例如，在一个五维空间里，A点和B点的距离为6，A点和C点的距离为10，那么就可以认为B点到A点的距离比C点到A点的距离更近，这样就足够了。</p>
<h2 id="4-5-曼哈顿距离"><a href="#4-5-曼哈顿距离" class="headerlink" title="4.5 曼哈顿距离"></a>4.5 曼哈顿距离</h2><p>介绍了欧氏距离，再来介绍一下曼哈顿距离（Manhattan Distance）。</p>
<p>欧氏距离是人们在解析几何里最常用的一种计算方法，但是计算起来比较复杂，要平方，加和，再开方，而人们在空间几何中度量距离很多场合其实是可以做一些简化的。曼哈顿距离就是由19世纪著名的德国犹太人数学家——赫尔曼·闵可夫斯基发明的（图4-15）。<br><a href="#ch1_back">[1]</a></p>
<p><img src="Image00065.jpg" alt></p>
<p>图4-15 赫尔曼·闵可夫斯基</p>
<p>赫尔曼·闵可夫斯基在少年时期就在数学方面表现出极高的天分，他是后来四维时空理论的创立者，也曾经是著名物理学家爱因斯坦的老师。</p>
<p>曼哈顿距离也叫出租车距离，用来标明两个点在标准坐标系上的绝对轴距总和。简单来说，对比一下欧氏距离。</p>
<p>欧氏距离里的距离计算：</p>
<p><img src="Image00066.jpg" alt></p>
<p>曼哈顿距离中的距离计算：</p>
<p><img src="Image00067.jpg" alt></p>
<p>曼哈顿距离中的距离计算公式比欧氏距离的计算公式看起来简洁很多，只需要把两个点坐标的x坐标相减取绝对值，y坐标相减取绝对值，再加和。</p>
<p>从公式定义上看，曼哈顿距离一定是一个非负数，距离最小的情况就是两个点重合，距离为0，这一点和欧氏距离一样。曼哈顿距离和欧氏距离的意义相近，也是为了描述两个点之间的距离，不同的是曼哈顿距离只需要做加减法，这使得计算机在大量的计算过程中代价更低，而且会消除在开平方过程中取近似值而带来的误差。不仅如此，曼哈顿距离在人脱离计算机做计算的时候也会很方便。举例如下。</p>
<p><img src="Image00068.jpg" alt></p>
<p>图4-16 国际象棋</p>
<p>在国际象棋棋盘（图4-16）上，有这种横平竖直的格子，描述格子和格子之间的距离可以直接用曼哈顿距离。如A1格子到C4格子的曼哈顿距离计算如下：</p>
<p><img src="Image00069.jpg" alt></p>
<p>两个格子之间的曼哈顿距离为5。</p>
<p>之所以曼哈顿距离又被称为出租车距离是因为在像纽约曼哈顿区这样的地区有很多由横平竖直的街道所切成的街区（Block），出租车司机计算从一个位置到另一个位置的距离，通常直接用街区的两个坐标分别相减，再相加，这个结果就是他即将开车通过的街区数量，而完全没有必要用欧氏距离来求解——算起来超级麻烦还没有意义，毕竟谁也没办法从欧氏距离的直线上飞过去。如图4-17 <a href="#ch2_back">[2]</a><br>所示，假设一辆出租车要从上面的圆圈位置走到下面的圆圈位置，无论是左边的线路，还是右边的线路，都要经过11个街区，而这个11就是曼哈顿距离。</p>
<p>从曼哈顿距离的定义就能看出，曼哈顿距离的创立，与其说有很大的学术意义不如说更多的是应用意义。这也是本书一直想说的一点，数学就在我们身边，它是我们的工具，能帮我们解决问题而不是带来麻烦。</p>
<p><img src="Image00070.jpg" alt></p>
<p>图4-17 曼哈顿距离的应用</p>
<p>上面的公式只给了二维空间上的曼哈顿距离公式，三维、四维或者更多维度的计算原理是一样。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<p><a href="#ch2">[2]</a> 图片来源于谷歌地图，曼哈顿街区。</p>
<h2 id="4-6-同比和环比"><a href="#4-6-同比和环比" class="headerlink" title="4.6 同比和环比"></a>4.6 同比和环比</h2><p>在看一些公司的财务报告或者看新闻的时候，常常会听到“今年7月销售额1000万元，同比去年增长100%，环比增长25%”诸如此类的说法。</p>
<p>何为同比？就是“与相邻时段的同一时期相比”的意思，在这个例子里，今年7月同比增长10%的意思就是今年7月的销售额和去年7月的销售额相比增长10%，这样推断来看，去年7月销售额应该是500万元（图4-18（a））。</p>
<p>何为环比？就是直接和上一个报告期进行比较，在这个例子里，环比增长25%的意思就是今年7月的销售额和今年6月的销售额相比增长25%，这样推断来看，今年6月的销售额为800万元（图4-18（b））。</p>
<p><img src="Image00071.jpg" alt></p>
<p>图4-18 同比与环比（见彩插）</p>
<p>但是，在真实应用的时候通常是不会这样来反推前值的，而是先得到前值和当期值然后做比较得出同比和环比。</p>
<p>这种比对其实也是天然形成的，要知道在公司或组织运营中通常喜欢用周期性单位来做计划，不管是预算计划，还是工作任务分派计划，用周、月、季度、年都是可以的，选取的周期大小完全取决于工作场景中这个周期是不是容易把握和调整。在一个周期结束的时候，通常要对这个周期的工作内容进行总结，这种总结的目的就是对比和经验归纳，手段通常就用同比和环比。</p>
<p>同比、环比的周期在选用时要适当，太小不方便，太大同样不方便。我们可以把同比和环比看成体检，就好比每个人做体检，一年一次，一年两次即可。再有钱也没有必要一天体检一次，而十年八年才体检一次恐怕对疾病预防起不到什么作用。</p>
<p>举个生产中常见的例子，例如，互联网公司用周和月甚至是天做短期的运营时间单位，因为可以在比较短的时间内看到反馈和周期性变化的规律特点，用年做单位则显得有些笨重，反馈慢。互联网公司常用的同比环比的对象有什么呢？日/月活跃用户数，简称“日活”和“月活”，这是经常用来做同比和环比的对象；还有日PV数（Page Views），也就是俗称的点击量，一个用户发生一次网页访问就算一个PV，很多互联网公司在做运营时把PV数当成一个网站活跃程度重要的指标。图4-19所示为新浪网（<a href="http://www.sina.com.cn" target="_blank" rel="noopener">www.sina.com.cn</a><br>）的Alexa网站排名，其中“日均PV[周平均]”的概念就是统计7天的PV数然后除以7，大约5.64亿次——真是不得了。</p>
<p><img src="Image00072.jpg" alt></p>
<p>图4-19 新浪网的Alexa网站排名</p>
<p>如果把国家也看成一个公司来运营，它的大政方针通常用年，甚至是5年来做运营时间单位，如果用日和周则会使得计划过于细碎，对于细节也非常难以做到有效的反馈和调整。</p>
<p>同比和环比在我们平时制作报表的时候会经常用到，对比的对象也很丰富，可以对比某些项目的加和值，也可以对比平均值，只要是同一对象同一单位的值对比就是有意义的。另外，同比和环比也是在平时公司报表中最常用的比较手段之一，几乎所有的营业指标都可以使用同比和环比进行自我比较，很直观也很方便。</p>
<p>而在所有的营业指标里最常用的周期指标通常是“月同比”和“月环比”，除了周期大小较为合适以外，月环比能够与最近一个经营周期做对比，便于快速反应；而月同比是和去年的同期月相比，这种比较会过滤掉一些周期性的波动的影响因素。</p>
<p>举个例子，某网吧2014年9月份在其网吧装机规模一直没有变化的情况下上座率环比下降25%，同比上升10%，这说明它经营情况下滑了吗？不一定，要知道9月份和8月份有一个很大的区别是中小学生暑假假期正好在8月底结束，很多孩子没有假期那么自由能够去上网了，网吧的上座率由于这种周期性波动产生的环比下降是一种自然且正常的波动。而同比上升10%就说明今年9月份比去年9月份上座率还是提高了10%，因此不能断定网吧的经营有下滑。</p>
<p>这样的季节或人为性规律的周期性影响在生产生活中有很多，尤其是跟行业结合的时候会有很多细节值得关注，请各位读者特别注意这一点，切莫生搬硬套。</p>
<h2 id="4-7-抽样"><a href="#4-7-抽样" class="headerlink" title="4.7 抽样"></a>4.7 抽样</h2><p>抽样（Sampling）是一种非常好的了解大量样本空间分布情况的方法，样本越大则抽样带来的成本减少的收益就越明显。</p>
<p>例如，一个大型食品加工厂一天要出货100万包方便面。为了检验方便面的质量或者说合格率，在出厂的时候每一包都打开检验一下是很不现实的。就算时间允许，所有的方便面都开包测重金属、菌群数量、酸价……即便这种方法检测出来的结果能够覆盖所有的出厂方便面，也确实是一种极为精确的方法，然而这些方便面在被检验后也只能扔进垃圾堆了，完全不具备可操作性。图4-20 <a href="#ch1_back">[1]</a> 所示为用已经粉末化的方便面做酸价检测，看上去搞一次也蛮复杂的。</p>
<p>而在实际生产中通常怎么做呢？</p>
<p>在方便面出厂的时候一般是这样做的——其实别的很多产品操作方法也相近，用类似扔骰子的方法来选择取哪几箱，哪几包方便面，取出一定的比例来做随机抽测，这在我国的产品质量检验国家标准中是有明文规定的。GB 10111标准就规定了利用随机数骰子进行随机抽样的方法。这种骰子不是我们平时麻将用的6面的骰子，而是有20面的骰子，标有0～9的读数，每个读数占两面，如图4-21 <a href="#ch2_back">[2]</a> 所示。</p>
<p><img src="Image00073.jpg" alt></p>
<p>图4-20 方便面的酸价测试</p>
<p>倘若100万包里只挑出100包来做测试，如果发现有1包有质量问题，按照抽样比例会推定所有的100万包里有可能有10000包面有相同的质量问题；同理，2包有质量问题就推定会有20000包面有相同的质量问题。在真实的操作中，这种抽检的比例有可能会更低，不会天天都检测，如半年一次，一次若干包。这种方式虽然看上去可能会产生“漏网之鱼”，会有一定的几率让不合格产品流向市场，但是从工业生产的角度来说操作性大大增强，操作成本也降到极低。毕竟检验手段的发明和进步，究其根本还是为了保证产业的良性发展而绝非阻碍其发展。</p>
<p><img src="Image00074.jpg" alt></p>
<p>图4-21 20面的骰子</p>
<p>抽样也可以应用于许多别的领域，如在类似民意调查中使用抽样，同样可以事半功倍起到非常好的作用。有一个例子也很经典，就是美国民意调查的例子，这个例子也见于涂子沛先生的《数据之巅》一书，非常有趣。</p>
<p>1936年，一本叫做《文学文摘》的杂志在对240万名普通美国民众进行了民意调查问卷后，得到结论，认为兰登（Alfred Landon）会当选第33任美国总统。要知道240万的民众可不算小数目，印发、邮寄、统计，这些都会消耗大量的人工成本。但是《文学文摘》选取这么大的一个样本进行统计，从目的来看显然是认为调查对象越多则调查结果越准确，为此他们不惜血本。此时，一所刚刚成立不久的研究所出现了，它只对5000人进行了调查，根据调查结果他们判断罗斯福会胜出。这家研究所就是1935年成立的美国舆论研究所（AIPO），它的奠基人是美国民意调查科学化的先驱——乔治·盖洛普（George Gallup）。</p>
<p>在1936年到2008年间，一共进行了18次总统选举，盖洛普民意调查成功预测了16次，这是一个非常惊人的成就。但是盖洛普是怎么通过5000张问卷就能击败240万张问卷的调查结果呢？难道240万张问卷数量多反而更不准吗？</p>
<p>这就是我们在应用抽样中需要注意的一个问题，那就是抽样对象要更加有代表性和分散性，这样才会体现出与整个样本空间更为相近的分布特点。</p>
<p>前面例子提到的“用类似扔骰子的方法来选择取哪几箱，哪几包方便面，取出一定的比例来做随机抽测”本身就是为了规避人为选择的偏向性，让每一箱、每一包都有相等的机会被选中。而总统选举这种场景可以想见，投票人里面涵盖了大量的不同阶级、不同种族、不同利益团体的对新总统期望的价值取向，那么在设计抽样对象时就应该考虑按照人口比例进行缩小并兼顾各种利益团体的样本才会更为准确，如白人、黑人的比例，不同州的人口比例，工人、中产阶级、资本家的比例，男、女性的比例等因素。</p>
<p>随着计算机存储能力和计算能力的不断增强，全样本空间的统计和计算的成本变得越来越低，所以抽样统计现在更多地应用于一些对于样本收集和存储成本过高的领域，或者由于种种原因不能做全样本收集的情况，如食品检验、人口统计、大气/水污染检验等领域。</p>
<p>抽样的合理运用在人们生产中会发挥其“轻巧”的作用，对于那些只需少量看统计效果，快速出反馈的试探性操作来说，抽样应该是再合适不过的高效、低成本的操作方式了。</p>
<p><a href="#ch1">[1]</a> 图片来自百度图库。</p>
<p><a href="#ch2">[2]</a> 图片来自百度图库。</p>
<h2 id="4-8-高斯分布"><a href="#4-8-高斯分布" class="headerlink" title="4.8 高斯分布"></a>4.8 高斯分布</h2><p>前面的章节曾经提到过一个概念，叫做“正态分布”。</p>
<p>正态分布（Normal Distribution）又名高斯分布（Gaussian Distribution），是一个在数学、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响力。</p>
<p>约翰·卡尔·弗里德里希·高斯（Johann Carl Friedrich Gauss）是德国著名数学家、物理学家、天文学家、大地测量学家，他是近代数学奠基者之一，被认为是历史上最重要的数学家之一，并享有“数学王子”的美誉。他的头像也被印在以前德国的官方货币——德国马克10马克上，图4-22 <a href="#ch1_back">[1]</a><br>所示为10德国马克上的高斯头像。有一种说法认为，高斯和阿基米德、牛顿并列为世界三大数学家——虽然这个说法没有得到书面和史料方面的支持。但是客观地评价，高斯、阿基米德、牛顿这3位科学家对于数学发展的贡献确实都是丰碑性质的，这点毋庸置疑。</p>
<p>先来看一下高斯分布的概率密度函数：</p>
<p><img src="Image00075.jpg" alt></p>
<p><img src="Image00076.jpg" alt></p>
<p>图4-22 10德国马克上的高斯头像</p>
<p>图4-23所示为高斯密度函数的函数曲线。</p>
<p><img src="Image00077.jpg" alt></p>
<p>图4-23 高斯密度函数的函数曲线</p>
<p>熟悉高斯分布的人自然觉得非常亲切，不熟悉高斯分布的朋友估计会感觉有些不知所云，这里简单介绍一下。</p>
<p>先介绍一下什么是概率密度函数，大家知道，y=f（x）这种表达式是以前在中学学习函数时使用的一种表达式，表示函数值y和自变量x函数关系，f（x）展开之后就具体解释了x参与运算的过程。而概率密度实际指的是y=f（x），x是样本特性自变量，y是x在这个样本特性上的数量比例。exp指的是自然常数e的幂函数，即e的多少次幂的概念（e是一个无理数，也就是无限不循环小数，e≈2.71828…）。这个函数的峰值在x=μ的位置，此时对应的函数值y为<img src="Image00078.jpg" alt>。其实这里样本数量的计算用的是定积分的定义，即整个函数曲线在其下方围住的与y=0（x轴）所围成的面积占比。它在x=μ左右两侧的函数是对称的，x在μ-<br>σ和μ+σ之间的样本数量占到整个样本数量的68.2%，x在μ-2σ和μ+2σ之间的样本数量占到整个样本数量的95.4%，x在μ-3σ和μ+3σ之间的样本数量占到整个样本数量的99.6%。</p>
<p>高斯分布作为分布特性的一种，首先是用来描述统计对象的，如果统计对象的分布特性符合高斯分布，那么所有针对高斯分布的定理和“经验值”就能够直接套用。而高斯分布本身在自然界的应用是非常广泛的，用一句话解释高斯分布所表现的分布特点就是“一般般的很多，极端的很少”。</p>
<p>这里举一个具体的例子，假如对某一地区的男性身高做了一个随机抽样，一共1000人，结果发现他们的身高是一个μ=175cm的高斯分布，σ=10cm。那么首先，这样一个描述就已经能够清晰地说明这个抽样检查的结果了，而以下结论也就随之成立（图4-24）。</p>
<p>身高165～175cm的人（大约）有341名。</p>
<p>身高175～185cm的人（大约）有341名。</p>
<p>身高155～165cm的人（大约）有136名。</p>
<p>身高185～195cm的人（大约）有136名。</p>
<p>身高145～155cm的人（大约）有21名。</p>
<p>身高195～205cm的人（大约）有21名。</p>
<p><img src="Image00079.jpg" alt></p>
<p>图4-24 1000人的身高分布</p>
<p>这些数量基本已经涵盖了统计总人数的99.6%。需要注意的是，根据统计的情况在不同的条件下μ和σ的值可能会不同。</p>
<p>μ较大，则整个函数图像的中轴向右挪动比较多。</p>
<p>μ较小，则函数图像的中轴向左挪动比较多。</p>
<p>σ较大，则整个曲线绵延比较长，整个坡度显得平缓。</p>
<p>σ较小，整个曲线窄而立陡。</p>
<p>符合高斯分布特性的对象是非常多的，平时也会看到很多这种“一般般的很多，极端的很少”的现象。如平时小区里的汽车，其中中档的比较多，高级的比较少，特别破的也比较少（在不同档次的社区注意μ可能会不同，就是平均水平在不同小区之间可能偏差很多，高档小区的车普遍比较好，μ就比较大；低档小区的车普遍不大好，μ就比较小）。如某小区如图4-25所示，大部分人买的汽车都30万左右，价格高的和低的汽车数量都随着与30万的距离变大而逐渐变少。</p>
<p>我们平时接触的人里，智慧一般的人很多，非常聪明的人较少，非常愚笨的人也较少（在一些大公司或者重点学校里虽然整体的聪明程度提高，但是还是存在这个小范围内的高斯分布，即μ比较偏右，而σ比较小）。如某公司全体员工集体做了一次IQ测试（智商测试），测试结果表明智商在110附近的人最多，智商在90到100之间的较少，同时智商在120到130之间的较少，而智商在80到90之间以及130到140之间的就更少了（图4-26）。这也符合人们一般性的认知。</p>
<p><img src="Image00080.jpg" alt></p>
<p>图4-25 汽车的价格与数量关系</p>
<p><img src="Image00081.jpg" alt></p>
<p>图4-26 智商与人数关系</p>
<p>再如，全社会范围内的收入，中档次收入的人比较多，特别贫穷和特别富裕的人较少，但是他们在地域上的分布和职业类别上的分布可能就不那么均匀了。诸如此类的例子还有很多。</p>
<p>高斯分布有什么用呢？</p>
<p>首先刚才说过，如果在统计过程中发现一个样本呈现高斯分布的特性，只需要把样本总数量、μ和σ表述出来，就已经能够形成一个完整的画面感了。这对人们描述对象是有很大帮助的。</p>
<p>还有一个好处，就是我们发现了这样一个特性以后，在生产制造、商业等领域会有很多对应性的用法能够减少不必要的投入或损失。</p>
<p>例如，在设计一款服装后，S/M/L/XL这些号码怎么设计比较合理呢？设计完了制造多少比较合理呢？这时就可以在抽样后在高斯分布曲线上找到这些合适的点。既然μ-<br>σ和μ+σ之间已经占68.2%了，那么如果没有足够的预算或者精力，可以只先尝试做一个以μ为标准的板式，针对一部分人打板做市场推广。因为再做μ-<br>σ和μ+σ这两个如此不同的板式，打板成本将会再提高2倍，但是增益仅有不到50%（这从概率密度函数上就可以看出来）。这其实就是一种针对市场迎合的分析和尝试——优先做那些受众情况最一般、人数最集中的部分。</p>
<p>再如，常常会听到“二八法则”这种说法——在不同的场景里这可能是高斯分布的一种形式。假设正在经营一家游戏公司，公司有一款刚起步不久的产品A游戏，A游戏有1万用户，如果想做这一款游戏的用户拓展工作应该怎么去考虑呢？或许可以尝试这样：先看看这1万用户中每个用户平均在游戏里充值花多少钱，做一个排名。不花钱玩的人会不少，还有一些花极多的钱来玩游戏的玩家，中间的是中坚力量——用户数量大，每个人花费的额度适中，持续周期较长，这样的一群人更值得关注。对于这些用户，如果能够知道他们加入游戏的渠道的分布比例，就有理由相信这些渠道的特点和它们覆盖这些用户的特点是有相关性的。例如，这些表现活跃的用户究竟是经由在大学校园里做宣传活动加入的，还是由于在某些游戏门户网站发的广告加入的，还是通过某些免费软件的推广渠道加入的。那么如果想扩大这部分用户的数量可以对应地加大这部分渠道的流量。至少直观上看，这比盲目地进行全方位立体交叉的广告投放效果要好。</p>
<p><a href="#ch1">[1]</a> 来源于百度图库。</p>
<h2 id="4-9-泊松分布"><a href="#4-9-泊松分布" class="headerlink" title="4.9 泊松分布"></a>4.9 泊松分布</h2><p>泊松（Poisson）分布是一种统计与概率学中常见的离散概率分布，由法国数学家西莫恩·德尼·泊松（Simeon-Denis Poisson）（图4-27 <a href="#ch1_back">[1]</a> ）在1838年发表。泊松分布是概率论中最重要的概念之一。</p>
<p>泊松分布的概率函数如下：</p>
<p><img src="Image00082.jpg" alt></p>
<p>泊松分布的参数λ是单位时间（或单位面积）内随机事件的平均发生率。泊松分布适合于描述单位时间内随机事件发生的次数。其中k！是指k的阶乘，也就是k×（k-1）×（k-2）×…×2×1，k取非负整数。泊松分布概率密度函数如图4-28 <a href="#ch2_back">[2]</a> 所示。</p>
<p><img src="Image00083.jpg" alt></p>
<p>图4-27 西莫恩·德尼·泊松</p>
<p><img src="Image00084.jpg" alt></p>
<p>图4-28 泊松分布概率密度函数</p>
<p>还是根据认识高斯分布的经验来认识一下泊松分布。也就是说在一个标准的时间里，发生这件事的发生率是λ次（注意，这是一个具体的次数，不是一个概率值），那发生k次的概率是多少。</p>
<p>泊松分布适用的事件需要满足以下3个条件。</p>
<p>（1）这个事件是一个小概率事件。</p>
<p>（2）事件的每次发生是独立的不会相互影响。</p>
<p>（3）事件的概率是稳定的。</p>
<p>下面举一个公共汽车到站的例子。</p>
<p>假设在一个公共汽车站上有很多不同线路的公交车，而且平均每5分钟会来2辆公交车。求5分钟内来5辆公交车的概率有多大。</p>
<p>这里λ为2，k为5。</p>
<p><img src="Image00085.jpg" alt></p>
<p>概率仅3.61%。</p>
<p>还有一个比较经典的例子：已知有一个书店，售卖许多图书，其中工具书销售一直较为稳定而且数量较少（概率较小的事件），新华字典平均每周卖出4套。作为书店老板，新华字典应该备多少本为宜？</p>
<p>所有生产中解决的都是“为宜”的问题，也就是做投入产出的权衡。本例中，在没有做计算之前我们先想一下，如果备货过少，那么每周很可能都会有用户“流失”掉去买别的书店的新华字典或者由于无法满足客户的购书需求而引起客户的忠诚度下降等问题，而如果备货过多，那么就会占用大量的库存空间导致库存成本过高。</p>
<p>这是一个典型的泊松分布问题，因为在条件叙述里它是满足这三个前置条件的。这里λ是4，求k是多少“为宜”。</p>
<p>这里需要用到“累积概率”，其实“累积概率”的用法在前面高斯分布的研究中已经用过了，就是指自变量取值在一个区间内的所有概率的加和，在高斯分布的例子里从μ-<br>σ到μ+σ之间的自变量取值会涵盖68.2%的样本空间，这就是“累积概率”，即有68.2%的样本都存在于x的μ-σ到μ+σ的区间内。</p>
<p>在这个例子里，也求一下累积概率。由于是离散概率函数，可以先求出k所对应的各个概率的大小，再计算累积概率的大小。</p>
<p><img src="Image00086.jpg" alt></p>
<p>对应的表格如表4-1所示。</p>
<p>表4-1 不同k值对应的累积概率</p>
<p><img src="Image00087.jpg" alt></p>
<p>对应的概率图如图4-29所示。</p>
<p>表4-1表示k的取值，即每周备货多少本新华字典，以及销售周有多大概率会有k本的销售数量。最后一列的累积概率指的是备货为k本的情况下，会有多少个销售周的销售数量小于等于备货数量。这里只算到k=9的情况，其他情况读者有兴趣可以自己再算。图4-9所示的概率图中，横轴为次数k，纵轴为概率%。因为k是离散值所以画成离散的点即可，在有的资料上会用曲线把每个点顺序连接起来，这种画法也没有问题，只要读者知道k的取值为正整数即可。</p>
<p>具体看k=5，新华字典备货为5件的情况下，大概有76.63%的销售周不会有供不应求的情况，这些销售周内会有7.33%的销售周卖出1本，14.7%的销售周卖出2本，19.5%的销售周卖出3本，19.5%销售周卖出4本，15.6%销售周卖出5本，总之不会超过5本，也就是一年的52周里有40周可以满足消费者需求，还有12周会脱销。</p>
<p><img src="Image00088.jpg" alt></p>
<p>图4-29 对应的概率周</p>
<p>当选择k=7时，新华字典备货为7件的情况下，大概有92.98%的销售周不会有供不应求的情况，也就是一年的52周里有48周可以满足消费者需求，还有4周会脱销。</p>
<p>在泊松分布的例子里，可以看到一个现象，就是k每增加1，在k小于λ的时候，累积函数增加是很快的，而且每次增加的量比上一次增加的要多；而在k越过λ之后，虽然开始还在增加，但是每次增加的量比上一次增加的要少，然后越来越少。所以这个技巧在解决类似的问题时请根据实际情况斟酌采纳。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<p><a href="#ch2">[2]</a> 图片来源于维基百科。</p>
<h2 id="4-10-伯努利分布"><a href="#4-10-伯努利分布" class="headerlink" title="4.10 伯努利分布"></a>4.10 伯努利分布</h2><p>伯努利分布（Bernoulli Distribution）是一种离散分布，在概率学中非常常用，有两种可能的结果，1表示成功，出现的概率为p（其中0＜p＜1）；0表示失败，出现的概率为q=1-p。这很好理解，除去成功都是失败，p是成功的概率，概率100%减去p就是失败的概率。</p>
<p><img src="Image00089.jpg" alt></p>
<p>图4-30 雅各布·伯努利</p>
<p>伯努利分布是为纪念瑞士科学家雅各布·伯努利（Jakob Bernoulli）（图4-30 <a href="#ch1_back">[1]</a><br>）而命名的。这里值得一提的是伯努利家族。瑞士的伯努利家族（也译作贝努力）是一个很伟大的家族，一个家族3代人中产生了8位科学家，后裔有不少于120位被人们系统地追溯过，他们在数学、自然科学、技术、工程乃至法律、管理、文学、艺术等方面享有名望，有的甚至声名显赫。</p>
<p>伯努利分布的分布律如下：</p>
<p><img src="Image00090.jpg" alt></p>
<p>看上去像个分段函数是不是，它也可以写作</p>
<p><img src="Image00091.jpg" alt></p>
<p>这两个写法其实说的是一回事，你自己可以把n=0和n=1分别带进去算一算。</p>
<p>伯努利分布的应用需满足以下条件。</p>
<p>（1）各次试验中的事件是互相独立的，每一次n=1和n=0的概率分别为p和q。</p>
<p>（2）每次试验都只有两种结果，即n=0，或n=1。</p>
<p>如果不满足这两个条件，则分布不是伯努利分布。</p>
<p>满足伯努利分布的样本有一个非常重要的性质，即满足下面公式：</p>
<p><img src="Image00092.jpg" alt></p>
<p>我们解释一下这个公式的含义。</p>
<p>其中，X指的是试验的次数，<img src="Image00093.jpg" alt> 指的是组合，也就是<img src="Image00094.jpg" alt><br>，<img src="Image00095.jpg" alt> 就是p的n次幂与（1-p）的n-k次幂的乘积。</p>
<p>这个公式表示，如果一个试验满足<img src="Image00096.jpg" alt><br>的伯努利分布，那么在连续试验n次的情况下，出现n=1的情况发生恰好k次的概率为<img src="Image00097.jpg" alt> 。n=1就是对应概率为p的情况。</p>
<p>下面用一个小例子来说明。</p>
<p>例如，张三参加英语雅思考试，每次考试通过的概率为1/3，不通过的概率为2/3。如果他连续考试4次，那么恰好通过2次的概率为多少？</p>
<p>在这个例子里可以比较容易看到，P=1/3，n=4，k=2。代入公式：</p>
<p><img src="Image00098.jpg" alt></p>
<p>因此概率为8/27。</p>
<p>这个例子也可以用排列组合来计算。一共4次考试，2次通过，一共有6种情况，如表4-2所示。</p>
<p>表4-2 通过的情况</p>
<p><img src="Image00099.jpg" alt></p>
<p>试着求每次的概率，情况1，即第1次通过且第2次通过且第3次不通过且第4次不通过。这里千万不要漏掉后面两个条件，后面两次必须是不通过，否则条件就和公式不匹配了。</p>
<p>那么，第1次通过，概率为1/3，第2次通过，概率为1/3，第3次不通过，概率为2/3，第4次不通过，概率为2/3。这4个条件都发生的概率为</p>
<p><img src="Image00100.jpg" alt></p>
<p>同理，情况2到情况6的概率都是4/81。所以最后的结果是</p>
<p><img src="Image00101.jpg" alt></p>
<p>结果是完全一样的。</p>
<p>对于满足伯努利分布的试验来说，用古典概型进行计算显得复杂和繁琐，尤其是n和k比较大的时候用古典概型来做就太不方便了。</p>
<p>伯努利分布的应用场景其实远比这个例子丰富，读者有兴趣可以再继续寻找其他题目试解。</p>
<p><a href="#ch1">[1]</a> 图片来自维基百科。</p>
<h2 id="4-11-小结"><a href="#4-11-小结" class="headerlink" title="4.11 小结"></a>4.11 小结</h2><p>在前4章里学习了一些统计和概率的基本知识，如建立指标，是使用加和值，还是使用加权平均值，在制作报表的时候是否应该适当使用指标的同比、环比进行对比，是否应该适时地使用抽样来进行用户调研，是否可以在报表中加入一些分布图来让阅读者有更直观性的认识，是否能用排列组合的方式算出一些事件在生产中发生的概率……</p>
<p>统计和分布这个部分是统计和概率学的基础部分，这些知识能用来解哪些题？能够用在什么场合？</p>
<p>要回答这些问题需要先理解统计和分布本身的意义，它们是为描述大量样本的宏观样态而出现的，究其根本也是描述为目的，它不是算法，所以通常无法直接拿来解题，但是它能用最简洁的方式给我们带来大量样本宏观样态下的画面感，更为直观。至于使用的场合，如果描述的对象是大量的样本，那么就用简洁的方式描述它的宏观状态的，即使用统计和分布中的描述方法。分布可以用来建模，也可以用来解决生产生活中的问题，上述例子就是很好的样本，读者可以试着再去找一些案例，只要满足分布的前置条件都是可以套用分布的结论和推广使用的。</p>
<h1 id="第5章-指标"><a href="#第5章-指标" class="headerlink" title="第5章 指标"></a>第5章 指标</h1><h2 id="5-1-什么是指标"><a href="#5-1-什么是指标" class="headerlink" title="5.1 什么是指标"></a>5.1 什么是指标</h2><p>指标，顾名思义，就是指定的标准。词典里的解释是“衡量目标的单位或方法”。指标就是为了描述一些对象的状态而制定出来的标准，在日常生产生活中有着非常广泛的应用。</p>
<p>如果你参加工作已经有一段时间了，那么对于指标可能不会太陌生，甚至在上学的时候我们身边同样有过各种指标。</p>
<p>我们上学的时候，每门功课的期末考试分数为百分制，即90分为优秀，60分为及格，不及格就留级。这就是一套简单但是完整的指标体系（图5-1）。其中，“指标”就是期末考试分数，“百分制”就是指标的取值范围，“90分为优秀，60分为及格，不及格就留级就是指标满足程度的奖惩手段。</p>
<p>看似简单的一个指标定义却完成了一个完整的指标检验闭环。在这个闭环里，学生就是指标负担的主体，学生为了达到指标而学习，学习的情况用指标量化表示，最后根据量化的指标进行奖惩。有了这个指标体系，学校的名声、教师的水平高低、学生成绩水平的优劣都有了量化的准绳；这为由此产生的一系列“运营”带来了驱动能力——虽然我们一直非常不情愿把教育体系当作运营的对象，但我们却不得不把教育体系的学校作为运营的对象。</p>
<p>学生成绩有了高低，就能有不同的晋升途径；教师的水平有了高低，就能有评级评奖，薪金浮动；学校的名声有了体现，就能有口碑和好坏的差别，吸引师资和学生的差别以及赞助费的差别。这些是我们在设置考试的时候就精心设计出来的吗？未必，这是由考试这个指标体系自然而然衍生出来的一系列游戏规则。</p>
<p><img src="Image00102.jpg" alt></p>
<p>图5-1 学生成绩指标体系</p>
<p>再如企业里有不同的岗位，其中销售岗位是最容易被赋予指标的，如“月销售额”、“年回款率”等。其中“月销售额”对于一个销售人员来说是最容易让人想到的一种指标。一个销售人员是不是“优秀”，是不是“有能力”、“有价值”，最简单的就是用卖了多少价值的产品来衡量。而“年回款率”指标是在“月销售额”指标运作的前提下才会有的，指每年的销售额究竟有多少是真正可以兑现的。因为在很多企业里，产品销售有一些特殊的规则，如客户先拿货后付款，先签框架合同后交易。所以“月销售额”有可能会由于具体生产细节上操作的问题使得这个孤立指标的意义不够丰满，还需要其他指标来补充。毕竟，找个光拿货不给钱的客户，销售额再多也不算销售人员有能耐。企业里的指标运营类别比学校丰富得多，有了这些指标，企业内部可以进行人员的左迁和右迁，刺激企业良性发展，这已经在很多优秀的企业中得到验证。</p>
<p>再如体检时的化验项目，如眼睛近视度数、身高、体重、心跳、血压、血糖浓度、血小板浓度……以及尿酸浓度、各种转氨酶浓度等专业的指标。小小一张化验单据，多则百余项，少的只有一二十项，已经把人的身体状况描述得即简洁又具体，这就是指标的力量。</p>
<p>如果没有指标我们的体检应该怎么做呢？那体检报告可能要写成长篇的文章来描述了，血液比较粘稠，像什么一样粘稠，粘稠到什么程度，肝部也许有轻微炎症，炎症严重或者不太严重，心跳听起来跳得比较快……这样的报告不只是参与体检的人看不懂，估计医生看了也不知道究竟是多粘稠，炎症多轻微或多严重，心跳得有多快。更关键的是，下一次体检描述完了还是这样一堆文字，那就更没有办法做每次体检之间的比较了，谁知道这次“心跳听起来跳得比较快”和上次的“心跳听起来跳得比较快”哪个更快一些，是更快了还是更慢了（图5-2）。如果连针对不同器官的指标分类都不存在，那体检就显得更滑稽，甚至检查了半天结果只能说“看上去很健康”“看上去好像不太健康”，这哪里叫体检，连街边5块钱一次的相面测字都不如——这种体检想想也是醉了。</p>
<p><img src="Image00103.jpg" alt></p>
<p>图5-2 没有指标的体检报告</p>
<p>指标给我们带来的便利是非常明显的，这也就是在各行各业广泛使用指标的重要原因。</p>
<p>在指标建立的过程中，我们实际做的是一个建模的过程，围绕建模要做很多辅助工作。</p>
<p>在综合类或者理工类大学会有一门选修课叫作“数学建模”，我们在书店里也能找到关于数学建模的书籍。这里的建模和数学建模的意思相近，至少目的是相近的。我们要做的建模工作实际上是抽象在生产生活中的各种数字，并尝试建立数字和数字之间的逻辑关系假说，并通过分析手段来进行逻辑关系的调整，最终让建立的这种逻辑关系和客观事实一致。这个逻辑关系可能就是这些数字之间的加减乘除或幂指对函数等。</p>
<p>使用数学建模的手段来建立数据之间逻辑关系的例子有很多，生物学家可能会观测一个地区的温度、湿度以及当地某种微生物种群数量，进而得到一个3种变量之间的逻辑关系；社会学家可能会通过观察一个地区的收入水平平均值、标准差和犯罪率来做一个模型描述，描述犯罪率可能由收入水平平均值和标准差这两个值的一些运算逻辑来表述。</p>
<p>我们平时在生产生活中所做的各种数值的统计工作，并把它们定义成指标，再用这些指标的运算组合来定义新的指标，这个过程就是一个建模的过程。这个模型是直接对生产中的各种指标数据进行逻辑解释的。例如，在制作互联网广告系统时，在网页上推送一个广告，做一个计数指标“推送数”，再做一个实际进行广告点击的计数指标“点击数”，定义如下：</p>
<p>转化率=推送数÷点击数</p>
<p>在经过一段时间的统计后，可以发现转化率很可能是一个定值或者是围绕某μ值的正态分布。那么再进行广告投放时，就能够根据预期的推送数来做对应的点击数的预判，进而估算出收入大小，那么广告定价或者投放策略调整也都有了参考。</p>
<h2 id="5-2-指标化运营"><a href="#5-2-指标化运营" class="headerlink" title="5.2 指标化运营"></a>5.2 指标化运营</h2><p>在现代化的公司里，指标化运营是必不可少的手段。</p>
<p>指标化运营会让公司的每个人，不只是领导层，甚至是各个岗位的各个工作人员都清晰地知道公司当前所处的状况以及自己当前的表现情况。就像日常化的体检，或者汽车的仪表盘那样一目了然。</p>
<p>公司指标化运营的好处显而易见，对于老牌公司来说，已经形成了比较完备的指标体系，可能并不会困惑；但是对于刚成立的新公司、新部门、新项目来说，应当怎样选择合适的运营指标呢？我们不妨从众多优秀的指标中寻找一下这些指标的共性。</p>
<h3 id="5-2-1-指标的选择"><a href="#5-2-1-指标的选择" class="headerlink" title="5.2.1 指标的选择"></a>5.2.1 指标的选择</h3><p>我们先来看看在互联网行业都有哪些常用的指标。</p>
<p>PV （Page Views）：页面浏览数，通常指的是每天的点击数，用户访问一次网站的页面就算一次PV。如果说一个网站每天有100万PV，那就是说这个网站所有的网页每天一共被点击100万次。</p>
<p>UV （Unique Visitors）：独立用户数（浏览数），通常指的是每天的用户浏览数，与PV的不同之处是，一个相同的用户如果点击页面10次，算10个PV，但是只算1个UV。有的网站在没有用户体系的情况下有可能会用独立IP来代替这个指标。</p>
<p>DAU （Daily Activated Users）：日活跃用户数，即每天活跃的用户数量。假设一个网站的注册用户有100万，但是一般不会100万人每天全数登录，可能只有5万人登录，那么这5万人就是活跃用户数。活跃用户的定义在不同网站可能是不同的，只是一般习惯上说当天登录过至少一次的用户就算是活跃用户。</p>
<p>MAU （Monthly Activated Users）：月活跃用户数，每月活跃的用户数量。算法与DAU类似，但是统计周期是一个月。</p>
<p>LTV （Life-Time Value）：用户生命周期价值，这个指标在游戏行业里用得比较多，指的是在一个用户“生命周期”——从开始玩这个游戏，到最终抛弃这个游戏为止，一共会为这个游戏付费多少钱。</p>
<p>ARPU （Average Revenue per User）：每用户平均收入。这个指标并不是用来评价用户收入水平，而是站在互联网产品的角度，了解每年（某一年）平均从每个用户身上可以收入多少钱。这个指标在互联网产品、电信运营产品、游戏产品等很多领域都会用到，是一个很常用的指标。</p>
<p>这些指标的共性如下。</p>
<p>1.数字化</p>
<p>首先，这个指标的设立一定是数字化的，不论是整数还是小数，因为只有数字化才能够比较，数字化之后才能参与各种运算。想想刚刚我们在体检的例子里说的那些奇怪的情况就知道数字化有多么重要了。</p>
<p>2.易衡量</p>
<p>指标所衡量的对象一定是相对比较容易衡量出来的。上述指标都具有这个特性，如PV数，在访问日志里用以下语句描述：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT（*）</span><br><span class="line">FROM</span><br><span class="line">VIEW_LOGS</span><br><span class="line">WHERE VISIT_TIME BETWEEN &apos;2016-01-05 00：00：00&apos; AND &apos;2016-01-05 23：59：59&apos;；</span><br></pre></td></tr></table></figure>

</details>

<p>可以获得2016年1月5日这一天的PV数。或者在Linux系统里用Shell命令切割日志后，用wc–l命令也可以得到PV数，非常方便。</p>
<p>UV实际是对PV的用户维度的去重，ARPU值是一个“收入总数/用户数”的商。</p>
<p>这些都是一些用相对简单的统计手段衡量而比较容易得到的值。</p>
<p>指标的获取成本要低，无论是人力成本还是时间成本，毕竟作为工具来说花费过多的成本，甚至成本高于收益是不划算的。</p>
<p>3.意义清晰</p>
<p>意义清晰是非常值得注意的一点。</p>
<p>首先设立的指标的定义是不容易产生二义性的，不会令人对指标本身的含义产生多种不同的理解。</p>
<p>例如，UV指的是在一个周期里有多少不同的用户来访。在一个网站用户系统里，不同的用户就是以不同用户登录名来定义的，一个页面访问就是一个访问。这个定义在说明一次以后会很容易被大家记住。</p>
<p>此外，UV上涨和UV下降，对业务也有清晰的解释。如一个网站的UV持续上涨，那就说明注册用户的访问量在增加，反之在减少。</p>
<p>再如ARPU指标，ARPU值在上涨，说明平均从每个用户身上赚到的钱在增加，运营健康程度应该说也是在增加的。ARPU值下降，说明平均从每个用户身上赚到的钱在减少，至少从宏观上来看，每个人愿意为产品买单的热情越来越低，产品运营是不健康的。</p>
<p>除了指标本身解释的意义清晰以外，这些指标基本也是可以通过改变运营策略在短时间内立竿见影看到反馈效果的。请注意，这一点也非常重要。因为公司运营的视角和其他很多带有“创新”元素的产业一样，规律和经验只能解决一部分问题，很多运营手段，包括公司活动、发布新版本、新广告上线、公司在媒体方面的新消息等，都是带有比较多的试探性意味的。换句话说，在做这些事情之前，只是对这件事会产生好的结果有期许，但是不能也无法决定这件事的结果是好是坏，程度如何也未可知。所以一旦有了指标，就会关注在一件事情以后指标上的反馈，有了对指标的解读，就有了衡量和比较的标准，它能够让我们比较方便地对经营中的试探做出适当的评价。</p>
<p>例如，在制作一个游戏的过程中，上线了一个新版本，第二天发现DAU（日活跃用户）下降了20%，那么我们就很容易推断是新版本的发布导致DAU值的下降。这时，我们要及时进行原因的排查，若确实是新版本的市场影响不好，那么一定要及时做版本回滚，亡羊补牢总比在月度结束的时候看收入报表减少了再来找原因要迅速得多。</p>
<p>4.周期适当</p>
<p>指标的周期设置要适当，这也是非常重要的一点。</p>
<p>上述例子里，PV的周期通常取一天。一天的长度是比较合适的，如果太短，每小时计算一次PV，PV本身会由于很多随机事件而变得波动剧烈，而指标对趋势性变化的解读通常更为有效，尝试对指标波动的解释通常都很让人困扰。网站今天9点到10点间的PV数比7天前9点到10点（周同比）的PV数多20%，请问这是由于公司的良好经营策略增加了20%的用户吗？这个推论无法成立。这个波动可能仅仅是由某一次宣传上的转发动作而发生的瞬时性的访问激增，最终一天的PV算下来可能只在周同比上有1%的提高，这什么都说明不了。</p>
<p>周期设置长一些会怎么样？反馈迟钝。如果用周作为单位来统计PV，那么如果要看经营策略效果，则至少要到一周以后才能看到结果，而且结果还混杂在整整一周的PV合计里，难以区分策略贡献的真实大小。</p>
<p>除了周期不当会产生的指标波动难以解读和反馈迟钝两种问题以外，周期不当本身可能都让指标的侦测行为没有现实意义。</p>
<p>如上述体检的例子，半年一次体检，一年一次体检，这些周期是较为合适的。周期过短，如一天体检一次，即便体检都是在早上空腹后，有极大的准确性，也确实能反映每一天身体各器官各指标的真实情况，但是存在以下问题。</p>
<p>问题1： 体检会花费大量的时间和金钱。一次体检的时间和金钱的成本平摊在一年中几乎可以忽略不计，但是每一天都有体检开销则太为昂贵。</p>
<p>问题2：<br>即便体检是公家报销，时间也充裕。但是如此频繁和短周期的体检结果对于行为有什么指导价值？对于很多疾病来说，治疗周期都是几个月甚至半年、一年，如此频繁的体检对于观察治疗效果显然是没有必要，因为这并不比半年检查一次有更多效益。平白地提高成本而没有可预期的收益，这种成本花费的意义是值得质疑的。</p>
<p>5.尽量客观</p>
<p>指标选择的最后一个标准是尽量客观。</p>
<p>说实话，这一点的难度其实并不低。人本身就是一种靠器官感知世界的动物，既然是感知本身就带有主观性，如何做到完全客观呢？</p>
<p>有一些指标是相对容易由客观的统计结果得到的数字，如PV、UV，以及上述所有指标都是由记录结果统计而来的比较客观的指标，因为它们受到观察者的意愿性影响较小。</p>
<p>有没有主观性较强的指标呢？有的。如在一些企业的一些部门考核中的各人打分，这些评分的客观性就不如由非感性的数字统计而来的结果客观性好。例如，部门员工评价中会看到类似表5-1所示的评价标准。</p>
<p>表5-1 部门员工评价标准</p>
<p><img src="Image00104.jpg" alt></p>
<p>表面上看，这样的评分标准也是使用了数字化的手段，衡量标准（印象）几乎是装在脑子里瞬间就能有，意义也做了解释，周期也可以调整为半年或者一年。但是这样的方式很可能经不住推敲，因为这个评价实际上完全依赖一种模糊的印象，是一个没有标准的标准。所以这种评分的高低会因为打分人的不同而产生巨大的波动，他心情好与不好，他对别人的要求严格与不严格，他是否认真对待这次打分，他是否有其他因素的个人成见渗透其中，这些都令人对这种评分体系的公平性有很大的担忧。</p>
<p>事实上，在日常生产生活中，确实有很多评分是极难避开人类主观判断的，特别在一些无法做到数字化评判的场景里，例如，奥运会里的自由体操运动，油画比赛里的作品打分，再如CCTV的青歌赛或者歌手选秀节目（图5-3）。除了为了满足娱乐层面和商业运作层面的需求外，这个评分的体系还是被期望有一定的公平性，那要怎么做呢？调和的思路是，既然是主观，那一个人的主观就不如多个人的主观更公平。</p>
<p><img src="Image00105.jpg" alt></p>
<p>图5-3 主观性的评分场景</p>
<p>办法1： 多个裁判同时打分，求平均值。这样由个别裁判主观因素形成的不公平因素就会被削弱。</p>
<p>办法2： 去掉极值。如在很多比赛里会有多位裁判同时打分，然后去掉两个最高分，去掉两个最低分。这种办法也是为了削弱由个别裁判主观因素形成的不公平。</p>
<p>办法3：<br>加权平均。加权平均在这种情况下也可以发挥作用，例如，一些选秀类节目除了现场裁判打分还有场外观众打分，当然权重可能不一样。还有大学里一些课程评分体系也是有类似的思路，期末考试占70%的总评成绩，平时的作业占30%的总评成绩。大概思路也是显而易见，既然都是主观评分，那裁判有权威多占一些比重，群众相对更不明真相少占一点权重；或者，期末考试比较重要比较正规多占一些成绩比重，平时的成绩也能说明学习的质量但是相对重要性较低少占一些成绩比重。通过这样的均衡来降低某一个人或者某一次主观性的偏差对我们期望的尽可能客观的评价所造成的影响。但是加权平均的手法在实际生产生活中作为指标评价的手段其实并不容易操作。主要问题在于分权引入哪些因素更为合理，因为这些被引入的因素的成分很可能不可控。还有一个问题是各因素赋予多大的权重更为合理，这通常没有一个可靠的标准。所以只要能用客观数字来做指标，尽量使用客观数字，这样可操作性会更大一些。</p>
<h3 id="5-2-2-指标体系的构建"><a href="#5-2-2-指标体系的构建" class="headerlink" title="5.2.2 指标体系的构建"></a>5.2.2 指标体系的构建</h3><p>当准备为公司设立或调整指标体系时，请先确保它满足数字化、易衡量、意义清晰、周期恰当等基本要求。在此基础之上，还要在指标的设置上进行一些额外的考究。</p>
<p>1.指标考核的对象</p>
<p>指标考核的对象是谁？换句话说，谁来背这个指标？</p>
<p>不能背指标的部门会失去活力，从老板的角度来看，无法评价员工的好坏，进步与否，作用大小；从部门自己的角度来看，不知道生产调整的方向是进步还是退步以及幅度多大。所以，如果要想让部门变得有活力，部门以及部门中的每个人都应该背负一定的指标。而指标的评价方应该是直接享受这一指标收益的人。例如，公司的后勤部门为公司员工提供各种后勤服务，后勤部门背负的指标里应该有诸如“员工满意度”这种指标的，而这种指标的评价方应该是享受后勤部门服务的员工或部门。当然，不建议直接使用员工打分的形式，应该尽量用“备货延误率”、“投诉率”等相对客观可循的数字比较好。</p>
<p>2.指标的周期</p>
<p>对于个人或者部门来说，指标的目的是为了进行工作质量的评价和调整，所以周期要设置为与生产周期匹配为宜。在很多劳动密集型的产业，如工厂里，工作指标是用小时做单位的，在一些超市里打零工的码货员一类的职位同样用的是小时为单位的指标，因为他们的劳动场景以小时为单位就可以做出完整评价，如“平均每小时成品数量”、“平均每小时工资”等。</p>
<p>在互联网企业里很多产品指标的周期是天，某个部门和某个人的指标周期是周或者月，对于公司级别的指标则很多是用月、季度、年来做单位的。“Q4销售额”（第四季度销售额）、“2015年度盈亏”，这两个指标一个以季度为单位进行统计，另一个则以年为单位进行统计。</p>
<p>3.指标的比较</p>
<p>指标可以横向对比也可以纵向对比，但是不能“斜着比”。</p>
<p>所谓横向，就是指同一部门或同一工种之间的同一指标的对比。横向通常用在某一特定时段，对员工进行排名或者评优等评价。同一生产线上的同一工种的每个工人的“平均每小时成品数量”是可以比较的，比较的结果就是谁平均每小时成品能力更强。</p>
<p>所谓纵向，就是同一部门或者同一员工，自己某一指标在不同时段的对比。</p>
<p>纵向一般只用好同比和环比就可以了，只比较不同时段的情况。例如，销售部门的销售额，8月为1000万，同比增加20%，环比增加10%，这种比较就是典型的纵向比较。</p>
<p>但是“斜着比”通常没有可比性。同一时间段不同部门的不同指标之间，单纯从数字大小是能比较的，但是比较的结果没有任何解释性的意义，所以一般不做这种对比。</p>
<p>4.复合指标</p>
<p>复合指标是针对基础指标而言的。</p>
<p>基础指标一般认为是不可再分的指标，如PV数，这个指标再分是没有业务意义解释的。</p>
<p>复合指标一般是由基础指标和复合指标进行运算得到的。如游戏中常用的“用户留存率”就是一个复合指标，因为它是由两个基础指标计算而来的，一个是“注册用户数”，一个是“留存用户数”，用“留存用户数”除以“注册用户数”得到的商作为“用户留存率”指标。</p>
<p>复合指标在指标体系里的数量还是很多的，尤其是越宏观的数据指标越是复合指标，这也符合“抓大放小”的管理原则。在经济学和社会学上用到的很多指标都是复合指标，如，全球繁荣指数、社会基尼系数等。</p>
<p>除了一些比较大型、层级多、涉及行业或生产环节多的公司以外，一般的中小型公司通常不推荐使用过于复杂的复合指标体系，因为这让指标反映变得不直观，反馈也不直观。这样的指标值上升了或者下降了，我们无法快速地找到诱发的基础指标，更无法快速地找到对应的环节、部门或人——让反映变得迟钝肯定不会是管理的目的。</p>
<h2 id="5-3-小结"><a href="#5-3-小结" class="headerlink" title="5.3 小结"></a>5.3 小结</h2><p>读到这里我们已经读完了前5章内容。如果觉得有些乏累，可以先喘口气，因为我们可以告一段落了。在前5章里我们讨论了排列组合、统计、概率、分布、指标等内容，附录里提供了数据收集以及其他辅助技术，如果使用好这些知识，日常运营中的多数问题就都能解决了。</p>
<p>后面的内容会更多偏重数据的深度挖掘和机器学习，对于基础运营人员来说这是要求更高的内容了。如果感觉前面的内容没什么困难那请放松心情继续往下读吧，它们虽然离生产生活会略远一些，但是同样不难。</p>
<h1 id="第6章-信息论"><a href="#第6章-信息论" class="headerlink" title="第6章 信息论"></a>第6章 信息论</h1><p>从本章开始就要逐步展开数据挖掘和机器学习相关的内容了。</p>
<p>虽然信息论和很多数据挖掘算法没有直接的关联应用，但是也有相当比例的机器学习算法中应用到了信息论的概念，所以这里还是要提一下信息论。</p>
<p>信息论无疑是20世纪最伟大的发明之一。信息论的奠定为后来的通信系统、数据传输、密码学、数据压缩等学科领域带来了更多的提示和理论依据，也极大地促进了这些学科领域的长足发展。</p>
<p>克劳德·香农（Claude Shannon）被称为“信息论之父”。人们通常将香农于1948年10月发表于《贝尔系统技术学报》上的论文《A Mathematical Theory of Communication》（《通信的数学理论》）作为现代信息论研究的开端。图6-1 <a href="#ch1_back">[1]</a> 所示为克劳德·香农。</p>
<p><img src="Image00106.jpg" alt></p>
<p>图6-1 克劳德·香农</p>
<p>信息论涉及的内容非常多，这里只挑选和相关的最基本的知识点来讲述。</p>
<p><a href="#ch1">[1]</a> 图片来自百度百科。</p>
<h2 id="6-1-信息的定义"><a href="#6-1-信息的定义" class="headerlink" title="6.1 信息的定义"></a>6.1 信息的定义</h2><p>前面的章节已经定性地介绍了信息的含义。这里引用最被大家广泛认可的一种科学性的信息定义——“信息是被消除的不确定性。”这是1928年由哈特莱（R.V.L.Hartley）提出的概念。这句话读起来还是不够具象，下面举例来说明。</p>
<p>还是讨论前面提到的足球赛结果的例子吧（图6-2）。</p>
<p>某一天巴西足球队和中国足球队进行了比赛。</p>
<p>结果第二天张三说“昨天巴西队赢了。”</p>
<p>而后李四说“昨天中国队输了。”</p>
<p>再而后王五说“昨天的比赛不是平局。”</p>
<p><img src="Image00107.jpg" alt></p>
<p>图6-2 足球比赛</p>
<p>在没有比赛之前，我们只能对结果做一个猜测，仅仅凭借自己对足球比较规则的了解，结果无非3种：“巴西胜”、“巴西负”、“平局”。任何一种的可能性都是存在的，然而由于一些其他原因导致其中某一种可能性出现的几率更大（大家都懂的）。</p>
<p>当有人告诉我准确的结果后，如“巴西胜”，那么另外两种结果就不存在了，这个过程就是前面说的“消除随机不确定性”，这一句“巴西胜”就是信息。当随机不确定性被消除后，再被告知的这些消息里就没有消除随机不确定性的因素了，所以这些消息就不是信息。但是如果有人具体描述了由谁在什么时间进球得分或者犯规吃了红黄牌则又是信息，因为这个消息又消除了其他的不确定性。</p>
<h2 id="6-2-信息量"><a href="#6-2-信息量" class="headerlink" title="6.2 信息量"></a>6.2 信息量</h2><h3 id="6-2-1-信息量的计算"><a href="#6-2-1-信息量的计算" class="headerlink" title="6.2.1 信息量的计算"></a>6.2.1 信息量的计算</h3><p>信息量是一个重要的知识点。平时在读网文的时候会看到很多人开玩笑说某某消息信息量真大，其实言外之意是说这篇网文在字面意思的背后再琢磨一下能推断出很多其他信息来。而在信息论中，对信息量是有确定解释并且可以量化计算的，这里提到的信息量是一种信息数量化度量的规则，注意，信息是具体要数量化的而不再是前面开玩笑中的大小的说法。</p>
<p>用科学的公式性的方法量化一段说话的录音，一段文字有多少信息的想法最早还是在1928年由哈特莱（R.V.L.Hartley）首先提出，即信息定量化的初步设想，他将消息数的对数（log）定义为信息量。若信源有m种消息，且每个消息是以相等可能产生的，则该信源的信息量可表示如下：</p>
<p><img src="Image00108.jpg" alt></p>
<p>这段话不太好懂是吗？我们具体化看一下会有更加深刻的认识。</p>
<p>上述公式是一个以m为自变量的对数函数，即I（m）=log2 m。还记得对数函数的定义是什么吗？这里复习一下，例如，23 =8（即2×2×2，2的3次幂等于8），那么log2 8=3。</p>
<p>举个具体的例子，假设是中国乒乓球队和巴西乒乓球队的男子单打比赛，注意，它和足球比赛可大不一样，它不存在平局的问题。那么中国乒乓球队获胜，巴西乒乓球队获胜，这两个就已经是所有可能发生的情况了，只有2种情况，即上述公式中m=2，信源有2种（中国队获胜且巴西队失败，巴西队获胜且中国队失败，这2种信源消息穷举了所有的可能性）。I=log2 2=1，信息量为1，单位是比特（bit）。</p>
<p>再举一个例子，足球世界杯在淘汰赛阶段会有32支球队参赛，分为8组，每组4支球队，通过单循环赛每个组有两支球队会有出线权，之后就是各自捉对厮杀直到决出冠军。理论上讲，在32支球队没有开赛之前每支球队都是有获取冠军的可能性的。也就是说，对于谁获得冠军这件事来讲，有32种情况。那么最后在不知道比赛过程的情况下，突然被通知有一支球队获胜，这个信息量为多大呢？即m=32（也就是32支球队的任意一支获胜都是一种信源消息，32种信源消息穷举了所有的可能性），I=log2 32=5，信息量为5，单位是比特（bit）。</p>
<p>为了计算方便，这里使用的是m为2的整次幂来计算的。比对一下结果不难发现，后者有32个可能值的时候信息量为5，有2个可能值的时候信息量为1。极端情况是，只有1个可能值的时候信息量为0，因为log2 1=0，也就是无须告知也知道结果时，即便告知了结果，信息量也为0。</p>
<h3 id="6-2-2-信息量的理解"><a href="#6-2-2-信息量的理解" class="headerlink" title="6.2.2 信息量的理解"></a>6.2.2 信息量的理解</h3><p>看到这里，可能有一些读者已经看出一些矛盾来了。估计会有人有下面的质疑。</p>
<p>“我怎么觉得中国乒乓球队和巴西乒乓球队的比赛毫无悬念呢？基本可以确定中国获胜，那说中国获胜跟废话区别大吗？”</p>
<p>“我怎么觉得中国足球队和巴西足球队的比赛也毫无悬念呢？基本可以确定巴西获胜，那说巴西获胜信息量是0吗？”</p>
<p>“按照理论，消除的不确定程度越高，信息量越大；消除的不确定程度越低，信息量越小。那么，前面的计算方法似乎不成立啊！”</p>
<p>这种质疑确实有道理。</p>
<p>回过头来看看，刚才使用I=log2 m这个公式来计算信息量其实是有一个隐含前提的，就是m种情况产生的概率是均等的，没有任何一种信源比其他信源出现的可能性大。举一个极端的例子，虽然有两种情况发生，但是一种可能性实在大到接近100%，而另一种可能性接近0。</p>
<p>例如，现在的《中华人民共和国道路交通安全法》，也就是人们平时说的交通法规，其中规定“红灯停绿灯行”，在某一次进行交通法规修订的时候被修改为红灯行绿灯停的可能性有多大？理论上说这种可能性确实存在，至少没有办法证明它一定不存在，但是实际上从人类社会协调的角度看，从修法的社会效益来看，确实也是没有理由。所以在一次交通法规修订后，被告知“交通法规仍旧是红灯停绿灯行”这一信息的信息量如何呢？那显然跟0差不多，虽然这种情况m确实是2。那么这种概率不等的情况应该怎么计算比较合适呢？信息论里也有确切的解释和计算方法。</p>
<p>在日常生活中，极少发生的事件一旦发生是容易引起人们关注的，而司空见惯的事件不会引起注意，也就是说，极少见的事件所带来的信息量大。如果用统计学的术语来描述，就是出现概率小的事件信息量大。因此，事件出现的概率越小，信息量越大，即信息量的多少是与事件发生频繁程度大小（即概率大小）恰好相反的，这里不能称作成反比，因为它们不是倒数关系。</p>
<p>公式如下：</p>
<p><img src="Image00109.jpg" alt></p>
<p>Xi 表示一个发生的事件，P表示这个事件发生的先验概率。所谓先验概率，就是这个事件按照常理，按照一般性规律发生的概率。</p>
<p>还是用上述中国乒乓球队和巴西乒乓球队比赛的例子来说明。</p>
<p>假设中国乒乓球队和巴西乒乓球队历史交手共64次，其中中国获胜63次，63/64是赛前普遍认可的中国队获胜的概率，即先验概率。那么这次中国获胜的信息量有多大呢？</p>
<p><img src="Image00110.jpg" alt></p>
<p>巴西获胜的信息量有多大呢？</p>
<p><img src="Image00111.jpg" alt></p>
<p>单位都是bit。</p>
<p>同理得到，对于概率100%的事件，信息量为0。</p>
<p>而概率特别小的事件信息量是多少，是无穷大吗？再来看一个例子。</p>
<p>胡润研究院发布了2015年中国富豪榜，王健林2200亿元财富超过马云重回中国首富宝座，马云以1450亿元位于第二名。</p>
<p>马云身价1450亿元，假设一个人通过每次购买双色球中头奖而得到500万奖金（而且假设不需要交税），那就是最快的情况下需要连中29000次才能赶上马云（图6-3）。也就是说假设一星期开奖3次，3次全中500万，需要连中185.9年。前面算过，中头等奖的概率为1/17721088，那么连中29000次的概率是多少呢——<img src="Image00112.jpg" alt>。</p>
<p><img src="Image00113.jpg" alt></p>
<p>图6-3 连中双色球头奖金额累计（见彩插）</p>
<p><img src="Image00114.jpg" alt></p>
<p>信息量约为70万。</p>
<p>当然，这个例子其实非常不不严谨，因为185.9年通胀得有多少没有人知道，马云在185.9年内的情况也全无定数，我们这个例子完全是一种刻舟求剑的示意而已，切莫深究。我们看一下，这种已经不可能到极点或者荒谬到极点的信息也不是我们想象的天文数字，但是要知道在信息量里这种几十万的数字已经就是天文数字了，我们有个感性认识就可以了，在实际生产生活中我们计算这种事情的信息量的机会其实也不多。</p>
<h2 id="6-3-香农公式"><a href="#6-3-香农公式" class="headerlink" title="6.3 香农公式"></a>6.3 香农公式</h2><p>香农他老人家留给我们最经典不过的东西就是香农公式了——这个以他老人家的大名命名的公式。在数据挖掘和机器学习中我们没什么机会用到香农公式，不过他发明的这个公式无时无刻不在我们身边飘荡。不信吗？我们用的无线路由器的通信传输速度就是用香农公式来计算的，讲到信息论我觉得还是提一句这个经典公式比较好。我们先来看一下这个公式：</p>
<p><img src="Image00115.jpg" alt></p>
<p>单位bps。</p>
<p>其中：</p>
<p>·B是码元速率的极限值（奈奎斯特指出，B=2H，H为信道带宽，单位为Baud）；</p>
<p>·S是信号功率（瓦）；</p>
<p>·N是噪声功率（瓦）。</p>
<p>这个让人头大的公式确实晕得很，如果你不是搞通信工程的，那么真的很庆幸，你一辈子确实也没什么机会能真正用上它。我们在这里提到这个公式是为了拓展一下知识宽度和考虑问题的思路。</p>
<p>公式左边的C表示在一个信道里面信号传输的速度上限，单位为比特。</p>
<p>公式右面的B是和带宽大小H成正比的，带宽越大传输速度越快，不管是有线传输还是无线传输。</p>
<p>S/N是信噪比，就是要传输的信号功率和在这个信道里产生的各种信号噪声（基本都是电信号或者无线射频信号）的功率大小比值。</p>
<p>从公式定性分析来看，带宽越大传输速度越快。这里的带宽与人们平时说的“家里装了100Mbps的电信宽带”中的带宽不是一个概念，这里的带宽的单位是Baud（波特），家里装的宽带带宽单位是bit（比特）。</p>
<p>信噪比越大传输速度越快，这平时也有体会，就是WiFi满格时或者4G信号满格时看网页、下载电影速度就快，这个时候普遍的信噪比比较大；相反WiFi信号或者4G信号只有一格时刷网页效果会非常不好，能出来就不错了，还有的时候居然刷了半天没反应，这个时候其实主要就是因为信噪比比较小（图6-4）。</p>
<p><img src="Image00116.jpg" alt></p>
<p>图6-4 信噪比对传输速度的影响</p>
<p>信噪比小就影响传输，传过去的信号由于噪声信号干扰太大，传输有可能完全不成功。如果万幸信噪比的大小还能保证部分信号传输成功，那又希望保证传输的信息是完整的怎么办？只能多传几次或者在信息后面加入一些校验码来做信息冗余。</p>
<h2 id="6-4-熵"><a href="#6-4-熵" class="headerlink" title="6.4 熵"></a>6.4 熵</h2><h3 id="6-4-1-热力熵"><a href="#6-4-1-热力熵" class="headerlink" title="6.4.1 热力熵"></a>6.4.1 热力熵</h3><p>在本章的最后要介绍的是本章最重要的一个概念，即信息熵（shang），也是最抽象的一个概念。</p>
<p>“熵”这个词在热力学、生物学、信息学上都出现过，而且都有比较确切的科学解释。</p>
<p><img src="Image00117.jpg" alt></p>
<p>图6-5 克劳修斯</p>
<p>热力学中的“熵”的概念是1855年由德国物理学家兼数学家鲁道夫·尤利乌斯·埃马努埃尔·克劳修斯（图6-5 <a href="#ch1_back">[1]</a><br>）提出的。据史料称，1870年克劳修斯在普法战争中组织了一支救伤队奔赴前线，而他本人在战争中受了伤而且持久伤残，因此被授予铁十字勋章。</p>
<p>克劳修斯在物理学方面的贡献主要在热力学方面，别的我们都不提，只说“熵”，如果你对“熵”没什么概念，我们就举例说一个大家在学生时代常见的现象来做说明吧。</p>
<p>在一个“U”型槽里，放一个小铁球，在没有空气、没有摩擦的情况下，小铁球从一端高点落下会由于能量守恒定律在这个“U”型槽里反复运动，从左到右再从右到左。这是理想的无能量损失的状态。</p>
<p>而实际状况是什么呢？空气阻力以及槽壁与小铁球之间的摩擦使得一部分能量从重力势能向动能转化的过程中损耗掉，小铁球随之能够到达的高点也越来越低。在这个过程中，越来越多的机械能不可逆地变成了内能——人们观察到的就是整个系统发热，温度会升高一些，从而产生了熵的增加。这么看来“熵”好像不是个好东西……</p>
<p>在这个封闭系统里，随着熵的增加，机械能也将越来越少，最后以至于小铁球静止不动，此时所有的机械能都转化成了熵。而这个一直在增加的熵是什么？由于系统封闭，所以根据能量守恒定律，可以推断出，最后小铁球运动的机械能全部转化成了内能，也就是在整个系统中最后温度会上升。能量是怎么转化的呢？根据分子动理论，最开始系统中的各种分子不管是固体还是气体都以较低的速度进行布朗运动，也就是无规则的运动。而有相当质量的小铁球在系统里却是按照一个相对确定的方向往复运动，它是有机会由重力势能转化成动能，再从动能转化成重力势能的，当然也有机会转化成内能。</p>
<p>最开始（图6-6）：</p>
<p>系统里的总能量=小铁球的重力势能+小铁球的动能+内能</p>
<p>最开始小铁球的重力势能较大，小铁球的动能为零，内能较小。</p>
<p>在中间某一时刻小铁球运行到最低点时，小铁球的重力势能为零，小铁球的动能较大，内能比最开始时升高（图6-7）。</p>
<p><img src="Image00118.jpg" alt></p>
<p>图6-6 开始时（见彩插）</p>
<p><img src="Image00119.jpg" alt></p>
<p>图6-7 小球运动到最低点（见彩插）</p>
<p>最后小铁球静止时，小铁球的重力势能为零，动能也为零，内能升高到最高（图6-8）。</p>
<p>最后机械能全部不可逆地转化成内能，而这些内能是没有机会再变成机械能的，这个内能的增加量就是整个系统里熵的增量（图6-9）。</p>
<p><img src="Image00120.jpg" alt></p>
<p>图6-8 小铁球静止</p>
<p><img src="Image00121.jpg" alt></p>
<p>图6-9 熵的增量（见彩插）</p>
<p>其实从最微观的角度去看也是一样的，最开始大量铁原子的质量由于地球引力基本都是以一定的速度向一方面前进，再克服重力升高，再往复。到最后由于能量在摩擦中都给了周围的空气分子和其他分子，其他分子向各个不同方向前进的速度都变快了，温度升高了，但是再也没有能力像原来铁原子一样向一个方向运动了（图6-10）。</p>
<p>我们也可以粗略地认为，这种分子运动的杂乱程度变高了，伴随着这种杂乱程度的变高，熵增加了。</p>
<p><img src="Image00122.jpg" alt></p>
<p>图6-10 分子不能向一个方向运动（见彩插）</p>
<p>也有一些物理学家推广了热力熵的存在场景，放到整个宇宙里去，提出了热寂理论（Heat Death）这种猜想宇宙终极命运的假说——最早是由爱尔兰数学物理学家威廉·汤姆森（William Thomson）于1850年提出的。大意是，宇宙也是由原子之间的斥力引力所提供的能量来运动的，宇宙最终和上述“U”型槽命运差不多，也是向着熵增的方向发展，最后宇宙所有角落的机械能都没有了，宇宙内能增高，一片死寂。但是到目前为止还没有任何事实证据支持该学说的正确性。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<h3 id="6-4-2-信息熵"><a href="#6-4-2-信息熵" class="headerlink" title="6.4.2 信息熵"></a>6.4.2 信息熵</h3><p>前面介绍热力熵主要是为了引出信息熵并和信息熵进行概念的类比，热力熵的概念在数据挖掘和机器学习领域基本是用不上的，只是两者形式比较像。</p>
<p>信息熵如果要用平民语言说得尽可能直白的话，我觉得可以认为是信息的杂乱程度的量化描述。</p>
<p>公式如下：</p>
<p><img src="Image00123.jpg" alt></p>
<p>其中，x可以当成一个向量，就是若干个xi 产生的概率乘以该可能性的信息量，然后各项做加和。</p>
<p>也许有的读者在其他资料上会看到这里的log是取10的对数lg，或者自然常数e的ln自然对数。这里强调一下，在应用的过程中用任何一种值做底都是可以的，但是注意在某一次应用的整个过程中，参与本次应用的所有信息熵都必须采用同一个底，不能将不同底的对数求出的熵再做加和或者比较，这样完全没有意义（就好像3米和2英尺，虽然都是长度单位，但是3米+2英尺既得不到5米也得不到5英尺）。本书的例子大部分都是使用以2为底的对数进行计算，请注意这一点。</p>
<p>1.示例1：2选1“一边倒”</p>
<p>为了说得清楚还是具体举例吧，还是用中国乒乓球队和巴西乒乓球队比赛的例子说明。</p>
<p>假设中国乒乓球队和巴西乒乓球队历史交手共64次，其中中国获胜63次，63/64是赛前普遍认可的中国队获胜的概率——注意，这个是先验概率。那么这次“中国获胜”这个消息的信息量有多大呢？</p>
<p><img src="Image00124.jpg" alt></p>
<p>“巴西获胜”的信息量有多大呢？</p>
<p><img src="Image00125.jpg" alt></p>
<p>所以，中国乒乓球队和巴西乒乓球队比赛的结果的信息熵约为</p>
<p><img src="Image00126.jpg" alt></p>
<p>对于无限不循环小数只能根据需要取一个近似值，注意这是一个“2选1的情况，并且确定性相当高”的事件的熵。</p>
<p>2.示例2：2选1“差不多”</p>
<p>再看一个两者势均力敌的例子，假设德国乒乓球队和法国乒乓球队比赛，双方历史交手64次，交手胜负为32：32，那么1/2是赛前普遍认可的德国队的获胜概率，同时也是法国队的获胜概率。</p>
<p>德国获胜的信息量：</p>
<p><img src="Image00127.jpg" alt></p>
<p>法国获胜的信息量：</p>
<p><img src="Image00128.jpg" alt></p>
<p>则信息熵为</p>
<p><img src="Image00129.jpg" alt></p>
<p>注意这是一个“结果2选1且等概率”的熵。</p>
<p>3.示例3：32选1“差不多”</p>
<p>如果在足球世界杯决赛阶段，即假设32支球队获得冠军等概率的情况下进行信息熵的计算。</p>
<p>队伍1获胜的信息量：</p>
<p><img src="Image00130.jpg" alt></p>
<p>队伍2获胜的信息量：</p>
<p><img src="Image00131.jpg" alt></p>
<p>……</p>
<p>队伍32获胜的信息量：</p>
<p><img src="Image00132.jpg" alt></p>
<p>则信息熵为（一共32个）</p>
<p><img src="Image00133.jpg" alt></p>
<p>注意这是一个“32选1的情况，并且等概率”的熵。</p>
<p>4.示例4：32选1“一边倒”</p>
<p>假设队伍1获胜概率为99%，而其他31支队伍每一支队伍的获胜概率都为1%/31，求比赛结果的信息熵为多少。</p>
<p>队伍1获胜的信息量：</p>
<p><img src="Image00134.jpg" alt></p>
<p>队伍2获胜的信息量：</p>
<p><img src="Image00135.jpg" alt></p>
<p>……</p>
<p>队伍32获胜的信息量：</p>
<p><img src="Image00136.jpg" alt></p>
<p>则信息熵为（后面一共有31个（0.01/31）×11.60）</p>
<p><img src="Image00137.jpg" alt></p>
<p>5.结论</p>
<p>在信息可能有N种情况时，如果每种情况出现的概率相等，那么N越大，信息熵越大。</p>
<p>在信息可能有N种情况时，当N一定，那么其中所有情况概率相等时信息熵是最大的，而如果有一种情况的概率比其他情况的概率都大很多，那么信息熵就会越小。</p>
<p>具体的值在具体情况可以进行量化的计算比较。笼统地说就是：</p>
<p>信息越确定，越单一，信息熵越小；</p>
<p>信息越不确定，越混乱，信息熵越大。</p>
<p>信息熵的用途是比较广泛的，其实看到信息熵的定义就大概能够知道它用在哪里。既然它是用来度量信息混乱程度的，那么凡是关心信息混乱程度对系统的影响的地方都可以用信息熵来辅助调整或判断。</p>
<p>例如，后面将要介绍的判定树算法里就可以用信息熵来进行条件的优化；在文本挖掘中可以用信息熵来断定一个句子断句的方案和各自成句的可能性。</p>
<h2 id="6-5-小结"><a href="#6-5-小结" class="headerlink" title="6.5 小结"></a>6.5 小结</h2><p>本章需要了解的是信息量的定义、信息熵的定义和计算方法，尤其是信息熵的计算方法，这在后面很多算法中都有应用。如果觉得这些例子还是让你体会不深，那也没关系，就记住定性的结论就好了，只有好处没有坏处。</p>
<h1 id="第7章-多维向量空间"><a href="#第7章-多维向量空间" class="headerlink" title="第7章 多维向量空间"></a>第7章 多维向量空间</h1><h2 id="7-1-向量和维度"><a href="#7-1-向量和维度" class="headerlink" title="7.1 向量和维度"></a>7.1 向量和维度</h2><p>向量（Vector）这个词最初来源于几何学。</p>
<p>几何向量也称为欧几里得向量，通常简称向量、矢量，是指具有大小和方向的几何对象表示。为了形象表示，在平面几何和立体几何中通常把一个向量画成一个箭头。在中学同样学过用复数来表示的向量，如2+3i，其实相当于x和y两个向量维度空间里的向量，是从点（0，0）到（2，3）之间引出向量（图7-1）。</p>
<p><img src="Image00138.jpg" alt></p>
<p>图7-1 向量的表示</p>
<p>向量除了用箭头表示外，还有一种在数据计算领域更常用的方法，即用（a，b，c，d，…）来表示。其中，a、b、c、d等每个元素都是一个维度上的数据取值。在大数据领域的计算对象基本都是这种格式的向量，所以只要理解这种格式的含义就可以了。</p>
<p>下面通过具体的例子认识向量的用法。</p>
<p>例如，在一个企业的数据仓库中要描述一条销售信息，可以表示如下：</p>
<p>（’北京’，’电风扇’，’网上商城T’，400000）</p>
<p>括号里的向量各个维度分别表示如下：</p>
<p>（地区，产品类别，代理商，销售额）</p>
<p>上面的叫做向量实例，下面的叫做向量定义。这种向量在计算机里面可以用数组来实现，也可以在关系型数据库里用一行记录中的字段来实现，也可以用其他组件来实现。不管用哪种载体，客观上表示的数学含义是没有差别的，差别仅仅在于在处理这些数据时所使用的语言或者工具不同而已，所以完全不用担心哪些语言支持向量哪些语言不支持。下面就是定义这个向量（地区，产品类别，代理商，销售额）分别在Python、Java、SQL这3种语言中的写法。</p>
<p>（1）在Python中的写法：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#PYTHON CODING</span><br><span class="line">class sales：</span><br><span class="line">  zone = &apos;&apos;</span><br><span class="line">  type = &apos;&apos;</span><br><span class="line">  agency = &apos;&apos;</span><br><span class="line">  sales_amount = 0.00</span><br></pre></td></tr></table></figure>

</details>

<p>（2）在Java中的写法：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#JAVA CODING</span><br><span class="line">public class sales &#123;</span><br><span class="line">  public string zone；</span><br><span class="line">  public string type；</span><br><span class="line">  public string agency；</span><br><span class="line">  public BigDecimal sales_amount；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>

<p>（3）在SQL中的写法：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#SQL CODING</span><br><span class="line">CREATE TABLE SALES（</span><br><span class="line">  ZONE VARCHAR（20） COMMENT &apos;地区&apos;，</span><br><span class="line">  TYPE VARCHAR（50） COMMENT &apos;产品类别&apos;，</span><br><span class="line">  AGENCY VARCHAR（30） COMMENT &apos;代理商&apos;，</span><br><span class="line">  SALES_AMOUNT DECIMAL（18，3） COMMENT &apos;销售额&apos;）；</span><br></pre></td></tr></table></figure>

</details>

<p>这些语句都是在各种不同语言上的向量定义语法。在Python语言和Java语言里，向量的定义其实就可以理解成类的定义，向量上每个不同的属性就是不同的维度，一个实例化的对象就是一个向量的实例。在SQL语言里，一个表的定义就是向量的定义，一个向量的实例其实就是表里的一条数据。</p>
<h3 id="7-1-1-信息冗余"><a href="#7-1-1-信息冗余" class="headerlink" title="7.1.1 信息冗余"></a>7.1.1 信息冗余</h3><p>一般来说，向量的每个维度之间是不相关的，在设计一个向量时也是希望每个维度不相关。如果想知道维度如果相关会有什么问题，举一个向量设计上的反例，如在某系统里记录一个个人的信息。</p>
<p>向量定义如下：</p>
<p>（姓名，姓，名，出生日期，年龄，驾驶证类别，初领驾照时间，驾龄年限）</p>
<p>向量实例如下：</p>
<p>（’张三’，’张’，’三’，’1986-03-01’，30，’C1’，’2015-01-01’，1）</p>
<p>在本例中可以看到，从一般的逻辑来说，“姓名”这个维度是可以由“姓”、“名”两个维度推断出来的，“姓”和“名”收尾相接就能得到“姓名”这个维度。而“年龄”是可以从“出生日期”这个维度推断出来的，在任意时刻使用当前日期与“出生日期”相减就能得到“年龄”这个维度。“驾龄年限”也是可以通过“初领驾照时间”推断出来的，在任意时刻使用当前日期与“初领驾照时间”相减就能得到“驾龄年限”。所以这种记录方式其实是有冗余信息的，冗余在IT领域里一般是指一模一样的数据存储多于一份的情况。本例在关系型数据库上不满足设计第三范式（3NF，这个概念如果没学过可以忽略）。对于这种有冗余的信息，还是一分为二来看，不要武断地说一定不可行。</p>
<p>冗余的问题是，如果其中一个相关的字段发生变化，则另一个字段也必须相应地做出变化，否则就会出现信息矛盾或者不一致的现象。如现在描述的张三是30岁，明年他就31岁了，是否还需要一个程序功能自动或手动对这个字段进行更新？如果在系统运行过程中发现在登记的时候“初领驾照时间”这一字段填写有误需要更正，那么“驾龄年限”也需要同步修改。这对于保持数据一致性来说，维护成本显然是会提高的。</p>
<p>冗余也未必全是缺点没有一丝优点。那么它的优点是什么呢？再来看一个例子。</p>
<p>向量定义如下：</p>
<p>（用户ID，1月消费额，2月消费额，3月消费额，4月消费额，5月消费额，6月消费额，7月消费额，8月消费额，9月消费额，10月消费额，11月消费额，12月消费额，全年消费总额）</p>
<p>向量实例如下：</p>
<p>（’0001’，160，130，135，150，160，170，175，165，150，155，155，160，1865）</p>
<p>在这个例子里，有一个全年消费总额用来做前面12个月的消费额的加和。如果这个系统里有5000万用户，应该怎么统计这5000万用户一年的消费总额？在没有“全年消费总额”这个维度的情况下，需要让计算机做6亿个数值对象的加法（12次×5000万），在有了这个维度的情况下，只需对这5000万个向量的最后一个维度——“全年消费总额”做加和，即做5000万个数值对象的加法即可，这两者在计算效率上有11倍的速率差距。</p>
<p>这11倍速率的差距意味着什么呢？如果这5000万个向量的计算需要2小时，那么这种计算可以在凌晨进行，并且次日一早做成报告。而6亿个向量时间计算很可能需要22小时，那么这种报告就只能第三天才能送达。这不仅仅是一个计算效率的问题，甚至影响了一个业务的反馈和完整流程。</p>
<p>至于在具体的应用场景里是否使用冗余字段需要应用者根据系统设计的经验和自己的实际需求去判断，应选择在满足自己系统业务运转要求的前提下“成本”更低的方式。</p>
<p>冗余信息被作为加快数据访问速度的手段应用最多的情况一般不是在一个表里设置冗余字段，而是在很多海量数据的数据仓库里把很多小粒度的数据计算成为以一天、一周、一个月作为更大粒度统计单位的冗余信息表或者指标信息表，而直接访问这些大粒度的冗余数据，比直接访问最小粒度的数据进行统计效率可能快上几千倍。</p>
<h3 id="7-1-2-维度"><a href="#7-1-2-维度" class="headerlink" title="7.1.2 维度"></a>7.1.2 维度</h3><p>在解释向量的过程中其实多次提到了维度这个词。维度的英文是Demension，人们平时说的3D游戏、3D电影中的3D说的就是3个Demension，或者说3个维度的视觉效果，以区别过去玩的和观看的平面视觉效果的2D游戏和2D电影。就连80后们最熟悉的FC游戏90坦克大战也能从2D搬到3D空间（图7-2 <a href="#ch1_back">[1]</a> ）。</p>
<p><img src="Image00139.jpg" alt></p>
<p>图7-2 90坦克大战</p>
<p>维度指的是参照系，有多少个维度就有多少个参照系，2D就是有两个参照系，3D就是有3个参照系。这种说法如果还是觉得抽象，那么回过头来看刚才的例子。</p>
<p>这里有一个向量（’北京’，’电风扇’，’网上商城T’，400000），笼统地说是有4个维度（地区，产品类别，代理商，销售额）。但是仔细观察不难发现，销售额和其他3个维度好像略有不同，它的值所发生的变化更容易引起人们的研究兴趣，因而这类维度通常会被当做研究对象——虽然把它当做一般参考维度来进行数据分析也不一定没有意义。在这个例子里，研究对象是销售额，有3个参考维度：地区、产品类别、代理商。后面将会介绍这种情况下怎么去做相关的研究。</p>
<p>维度的设置一般都是具有“正交性”的。“正交性”是从几何学中借用来的术语。如果两条直线相交成直角，它们就是正交的。用向量术语来说，这两条直线互不“依赖”。一个点沿着某一条直线移动，该点投影到另一条直线上的位置始终不变（图7-3）。</p>
<p>以平面直角坐标系为例，其中X轴和Y轴是垂直的，也是正交的。如果有一个点沿着X轴移动，不论它怎么移动，它到Y轴上的投影都会在（0，0）坐标点上；同样，如果有一个点沿着Y轴移动，不论它怎么移动，它到X轴上的投影都会在（0，0）坐标点上。在这个坐标系里，X、Y两个维度就是正交的。</p>
<p><img src="Image00140.jpg" alt></p>
<p>图7-3 两条直线正交</p>
<p>如果觉得不是很好理解那就记住这样一个特点，正交向量的任何一个维度，值发生变化时都不会引起其他的维度的值变化。看前面举过的例子。</p>
<p>向量定义如下：</p>
<p>（姓名，姓，名，出生日期，年龄，驾驶证类别，初领驾照时间，驾龄年限）</p>
<p>向量实例如下：</p>
<p>（’张三’，’张’，’三’，’1986-03-01’，30，’C1’，’2015-01-01’，1）</p>
<p>在这个例子里，“姓”和“名”这两个维度相互之间是独立的，也就是正交的，“姓”不会因为“名”变化而变化，“名”也不会因为“姓”变化而变化。但是，“姓名”是依赖于“姓”和“名”这两个维度的，“姓”和“名”任何一个发生变化“姓名”这个维度的值都会有变化。</p>
<p>一般来说，向量的设计推荐采用维度正交的原则，主要原因也是为了避免两个非正交维度不一致时不知道该采信哪种更好。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<h2 id="7-2-矩阵和矩阵计算"><a href="#7-2-矩阵和矩阵计算" class="headerlink" title="7.2 矩阵和矩阵计算"></a>7.2 矩阵和矩阵计算</h2><p>《黑客帝国》（The Matrix，也有译作《骇客帝国》的）是一部非常精彩的科幻影片，其英文名字里的Matrix就是矩阵的意思。</p>
<p>之所以电影会取名为Matrix是取义未来大规模机器智能的世界都是以无处不在的大规模矩阵存储和大规模矩阵运算构成的，矩阵是其实现的数学基础。</p>
<p>矩阵的研究历史悠久，拉丁方阵和幻方在史前年代已有人研究。但是，在19世纪才真正成为一种比较完整的数学分学科概念。英国数学家凯利被公认为矩阵论的奠基人。他从1858年开始，发表了《矩阵论的研究报告》等一系列关于矩阵的专门论文，研究了矩阵的运算律、矩阵的逆以及转置和特征多项式方程。</p>
<p>矩阵作为一种独立的数据构建单元，有着自己约定的矩阵相加、矩阵相减、矩阵数乘、矩阵转置等完整计算逻辑。例如：</p>
<p><img src="Image00141.jpg" alt></p>
<p>这里的A就是一个矩阵，通常说一个矩阵是m×n矩阵，就是说有m行和n列。在这个例子里，A是一个4×3的矩阵。每个矩阵的元素anm 都是一个数字。像不像二维数组和数据库里的表？</p>
<p>矩阵是有着完整的计算定义的，常用的矩阵计算规则如下。</p>
<p>1.矩阵的加法</p>
<p>假设矩阵</p>
<p><img src="Image00142.jpg" alt></p>
<p>矩阵</p>
<p><img src="Image00143.jpg" alt></p>
<p>规则很简单，就是直接把对应“坐标”的值相加。请注意，只有这种m和n分别相同的两个矩阵才能做加法，这种m和n分别相等的两个矩阵叫做同型矩阵。</p>
<p>2.矩阵的减法</p>
<p>假设矩阵</p>
<p><img src="Image00144.jpg" alt></p>
<p>矩阵</p>
<p><img src="Image00145.jpg" alt></p>
<p>减法就是直接把对应“坐标”的值相减。矩阵减法也要求必须是同型矩阵。</p>
<p>3.矩阵的数乘</p>
<p>假设矩阵</p>
<p><img src="Image00146.jpg" alt></p>
<p>那么</p>
<p><img src="Image00147.jpg" alt></p>
<p>矩阵的数乘计算就是把矩阵的每个元素都乘以这个数乘倍数。</p>
<p>4.矩阵的转置</p>
<p>假设矩阵</p>
<p><img src="Image00148.jpg" alt></p>
<p>如果B是A矩阵的转置矩阵，那么</p>
<p><img src="Image00149.jpg" alt></p>
<p>矩阵的转置计算相当于把矩阵沿着对角线翻了个身，行变列，列变行。一般习惯把矩阵的转置记做AT 。在这个例子里，B=AT 。</p>
<p>5.矩阵的内积</p>
<p>只了解m=1和n=1这两个矩阵做内积的算法即可。</p>
<p>假设矩阵</p>
<p><img src="Image00150.jpg" alt></p>
<p>A和B的内积：</p>
<p><img src="Image00151.jpg" alt></p>
<p>在这个例子里</p>
<p><img src="Image00152.jpg" alt></p>
<p>如果A、B两个矩阵如下：</p>
<p><img src="Image00153.jpg" alt></p>
<p>那么可否将两个矩阵的对应元素两两相乘然后加和呢？也是可以的，这就可以用前面介绍的矩阵转置的形式来表示。</p>
<p>AT B和BT A这两种形式都会得到30这个结果，因为计算过程没有什么差别都是1×4+3×3+5×2+7×1=30。</p>
<p>在高级人工智能里的矩阵运算使用得极其复杂，在入门阶段只要了解基本的矩阵相加、矩阵相减、矩阵数乘、矩阵转置即可。在本书的后面神经网络的部分会有的地方有引用，做到心里有数就好，在公司日常指标运营里矩阵计算几乎用不到。</p>
<h2 id="7-3-数据立方体"><a href="#7-3-数据立方体" class="headerlink" title="7.3 数据立方体"></a>7.3 数据立方体</h2><p>常见的魔方是一个3×3×3大小的方块，也就是以27个小正方体为单位的组合体。</p>
<p>想象一下，有27个透明的立方体，对这27个方块每一个都做上标记，如图7-4所示。</p>
<p><img src="Image00154.jpg" alt></p>
<p>图7-4 标记27个方块</p>
<p>把向量（地区，产品类别，代理商，销售额）分解一下，每一条边都写上一个维度的值，每个立方体中间都写一个数字，再组合起来，发现原来的27个向量在立体空间里变得非常直观。</p>
<p>如果觉得数字不够给人眼睛产生刺激，那就试试另一种方式，把数字对应的值做一下颜色深度标识，数字大的颜色深，数字小的颜色浅，如图7-5所示。</p>
<p><img src="Image00155.jpg" alt></p>
<p>图7-5 用颜色标记</p>
<p>颜色深浅一般的通常不会引起人们的注意，最吸引人注意的其实是那些颜色特别深的和颜色特别浅的。其实也不难理解，在一群人里，比较引人注意的是什么样的人呢？个子特别高的，个子特别矮的，长得特别漂亮的，长得特别丑的，长得特别胖的，长得特别瘦的。也就是说，吸引人注意的一定是和众多样本相去较远的样本，而比较“平庸”的样本通常不太引人注意。</p>
<p>上述数据“魔方”就是数据立方体，是一种比较直观的大数据可视化技术，它能够帮助人们在一个研究对象和3个维度（及以下）的情况下快速找到让人感兴趣的那些小数据块，快速定位“问题”所在。从例子里容易看出来什么地区、什么产品类别、什么代理商的销售额度格外高或者格外低，在同一个地区同一个产品类别里，哪个代理商销售的额度格外高或者格外低。</p>
<p>但是，一个向量可能不止有3个维度而且每个维度里只有3个枚举值，有的维度可能有几十个甚至上百个枚举值，这时立方体会变得很复杂。要想看得比较细致就只能做切片处理，如图7-6所示。</p>
<p>需要强调的是，数据立方体这种视角在一些商业BI软件里提供了很好的互动查询工具，但是很多自研发的企业BI部门可能就需要自己再去摸索这种视图如何开发了。</p>
<p><img src="Image00156.jpg" alt></p>
<p>图7-6 切片</p>
<h2 id="7-4-上卷和下钻"><a href="#7-4-上卷和下钻" class="headerlink" title="7.4 上卷和下钻"></a>7.4 上卷和下钻</h2><p>数据立方体帮助人们快速定位“问题”。但是数据立方体还有一个问题，就是一下子展开这么多维度值会让人眼花缭乱。</p>
<p>目前人类通过研究发现，人类自身的机械记忆极限是5～9个对象，也就是说正常情况下人记住5个毫无关联的对象是没问题的，记忆好一些的可以记住9个，只有经过特殊训练的人可能用其他技巧能够记住更多。多于9个的对象常人就很难记得清楚了，也很难一下子掌握。那么能否用逐层递进的方式对数据立方体进行归纳式的管理呢？答案是肯定的。这就是将要介绍的上卷（Rollup）和下钻（Drilldown）。</p>
<p>在一个视图下，“向下钻入”一个立方体时可以看到更多的细节，而当人们对当前的视图不再有兴趣需要回退到上一级别的视图时，可以把当前视图“向上卷起”，如图7-7所示。这样在每个视图中都能在有限的研究对象里较快定位到“问题”所在，这就是上卷和下钻两种操作的应用。</p>
<p><img src="Image00157.jpg" alt></p>
<p>图7-7 上卷和下钻（见彩插）</p>
<p>当然，数据立方体的局限性也是很明显的，即如果向量更多——超过3个维度，就无法在空间里画出一个立方体进行呈现了，而数据挖掘很多时候研究的数据远远比3个维度要多，所以在高维度数据的研究中，虽然数据立方体的逻辑客观存在，但是在对其进行数据挖掘和机器学习的过程中基本用不到数据立方体的可视化技术。读者在实现的时候请酌情进行视图选择。</p>
<h2 id="7-5-小结"><a href="#7-5-小结" class="headerlink" title="7.5 小结"></a>7.5 小结</h2><p>关于多维向量空间，只要掌握向量的定义、维度的定义即可。至于正交维度，请读者注意在日常生产生活中的设计技巧。</p>
<h1 id="第8章-回归"><a href="#第8章-回归" class="headerlink" title="第8章 回归"></a>第8章 回归</h1><p>回归是一种解题方法，或者说“学习”方法，也是机器学习中比较重要的概念。没有学过回归的朋友可能看到回归这个词猜不出这是个什么概念，觉得很神秘。其实我平生第一次看到这个词的时候也猜不出是干什么用的，“回归？……回哪儿去？”。</p>
<p>回归的英文是Regression，单词原型的regress大概的意思是“回退，退化，倒退”。其实Regression——回归分析的意思借用了“倒退，倒推”的含义。简单说就是“由果索因”的过程，是一种归纳的思想——当看到大量的事实所呈现的样态，推断出原因是如何的；当看到大量的数字对（pair）是某种样态，推断出它们之间蕴含的关系是如何的。</p>
<h2 id="8-1-线性回归"><a href="#8-1-线性回归" class="headerlink" title="8.1 线性回归"></a>8.1 线性回归</h2><p>线性回归是利用数理统计学中的回归分析来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。其表达形式如下：</p>
<p><img src="Image00158.jpg" alt></p>
<p>e为误差服从均值为0的正态分布。</p>
<p>简单说大概是这样，通过统计或者实验，可能会得到两种值（两个系列的值）的对应关系，这两种值一种是y一种是x，每组y和x是成对出现的一一对应，最后可以用一种y=ax+b+e的表达式来表示它们的关系。</p>
<p>而这其中的e不是一个定值，它和y、x对应着出现（有一对y和x，就有一个e）。这个e的值满足正态分布，μ为0。</p>
<p>有人说还是y=ax+b看着比较舒服，a是斜率，b是截距，初中时大家都学过。但是y=ax+b+e看着别扭，我们后面来说说这个e是怎么出来的。</p>
<p>下面看一个完整的操作过程。</p>
<h2 id="8-2-拟合"><a href="#8-2-拟合" class="headerlink" title="8.2 拟合"></a>8.2 拟合</h2><p>回归这种方法我们在中学课本里是没有讲过的，但是他的思想我们可是早在高中一年级的时候物理课上就学过的，只是我们没意识到。</p>
<p>回忆一下，还记得高中的时候学过一个用打点计时器计算重力加速度g的大小的实验吗？</p>
<p>将一个小铁球用一根线拴在一个很轻的小车上，小车后面拉着一根长长的有毫米刻度的纸带，一个打点计时器一边电源接着220V 50Hz的市电，有一根圆珠笔尖悬在纸带上方，如图8-1 <a href="#ch1_back">[1]</a> 所示。</p>
<p><img src="Image00159.jpg" alt></p>
<p>图8-1 用打点计时器计算重力加速度</p>
<p>当接通电源后，小铁球由于重力作用开始做自由落体的运动，同时拉着小车越跑越快，后面纸带上圆珠笔尖会以每秒50次的速度不停打点，这样在纸带上留下许多的点，每个点之间的距离会越来越大，每个相邻点之间的打点时间差为0.02s，如图8-2 <a href="#ch2_back">[2]</a> 所示。</p>
<p><img src="Image00160.jpg" alt></p>
<p>图8-2 打出的点</p>
<p>这个实验的先决条件是牛顿第二定律：</p>
<p><img src="Image00161.jpg" alt></p>
<p>F是物体受到的外力大小，m是物体的质量，a是产生的加速度。由于在重力的作用下，所以</p>
<p><img src="Image00162.jpg" alt></p>
<p>G是物体所受重力，m是物体质量，g是重力加速度，也就是要求的值。</p>
<p>我们现在手里得到什么了？是一个位移的记录，即纸带。</p>
<p>在处理数据的时候我们用了一个技巧，即计算每个点之间的间隔</p>
<p><img src="Image00163.jpg" alt></p>
<p>s0 就是t0 和t1 两个时刻所打点的位移差距。</p>
<p>位移s和时间t的关系是什么呢？我们都知道：</p>
<p><img src="Image00164.jpg" alt></p>
<p>速度乘以时间就是位移，比如我骑自行车一秒钟跑10米，那10秒钟就跑100米，这个对于匀速运动肯定没错。</p>
<p>v=f（t）示意图如图8-3所示，横坐标轴为t，纵坐标轴为v。当然只有t大于0的部分是有意义的，如果想知道当t=30时，s为多少，只需要在t=30的位置把这个图形切开，求t为从0到30，v=10的长方形面积即可（s=vt）。</p>
<p><img src="Image00165.jpg" alt></p>
<p>图8-3 匀速直线运动</p>
<p>如果运动速度是不均匀的怎么办呢？可以用积分的方法：</p>
<p><img src="Image00166.jpg" alt></p>
<p>其中v（t）是一个v=f（t）的函数，是一个变化的v和对应时间t的关系，后面的dt是Δt的概念，也会有人习惯地写成这种形式：</p>
<p><img src="Image00167.jpg" alt></p>
<p>整个积分式的含义就是位移s是一个随着t变化的函数，而整个过程中s是由这一刻瞬间的速度v（t）和瞬间的时间长度dt（Δt）相乘而来的。听起来好像是所有的值都在变化很难把握，读者先别慌神，往下看。</p>
<p>在这里先讲一个小故事。</p>
<p>德国著名的教学家、物理学家波恩哈德·黎曼图8-4 <a href="#ch3_back">[3]</a><br>发明了一种很有名的丈量方法——“黎曼和”。这个思想很简单，大概如下。</p>
<p><img src="Image00168.jpg" alt></p>
<p>图8-4 波恩哈德·黎曼</p>
<p>要求一块不规则形状的土地的面积，怎么办？</p>
<p>把它划分成很多小长方形，每个小长方形的面积都可以比较容易用长乘以宽的方式求出来，然后再加到一起，就能算出整个不规则土地的面积了。这里有误差怎么办？可以尽量把长方形画窄一些，越窄，面积和越逼近“真实”的土地面积，只要这个误差到达允许的范围内，那这个面积和基本就是所要求的面积，如图8-5所示。这个方法可不土，人家实际上用的可是正经是微积分的思想哟。</p>
<p><img src="Image00169.jpg" alt></p>
<p>图8-5 黎曼和示意图</p>
<p>再回到刚刚的问题s=vt，这其实也是一个面积公式，t是宽度，v是高度，只是v这个高度是变化的。匀速的时候v是一个定值，算出来是一个长方形面积，这就是一个黎曼和求和的过程。</p>
<p>另外，有公式</p>
<p><img src="Image00170.jpg" alt></p>
<p>其中由于一开始小球和车是静止的，所以v0 =0是知道的。那么</p>
<p><img src="Image00171.jpg" alt></p>
<p>成立，而且有</p>
<p><img src="Image00172.jpg" alt></p>
<p>这样求位移的方法成了求三角形面积的方法了。如图8-6所示，10s、20s、30s、40s、50s时所产生的位移就是此时的t刻度纵线与t轴以及函数曲线v=v0<br>+gt所围成的面积。</p>
<p>图8-6所为v=v0 +gt函数图线，当然，也只是t大于零的部分才有效。</p>
<p><img src="Image00173.jpg" alt></p>
<p>其中Δt就是0.02s，指的是每次打点的间隔。</p>
<p><img src="Image00174.jpg" alt></p>
<p>图8-6 v=v0+gt函数图线</p>
<p>以此类推</p>
<p><img src="Image00175.jpg" alt></p>
<p>也就是说，每两个临近的点之间的距离都可以用这个公式来套算，因为这个公式本身就是计算位移差的公式。</p>
<p>如果把临近的位移差做减法会得到什么呢？</p>
<p><img src="Image00176.jpg" alt></p>
<p>所有临近的两个点之间的距离之差为一个确定的数字，就是gΔt2<br>，g和Δt都是定值，只是现在还假装不知道g的具体大小。那下面也就更好办了，把每个临近的距离差都列出来，平均一下就能得到gΔt的大小，再除以Δt，就可以得到g。</p>
<p>请注意，这个例子和教科书上讲的用“逐差法”求加速度的细节有点区别，教科书上是把临近的6个点的差作为一个单位求差的，这主要是为了尽可能减小误差。</p>
<p>还可以用v-t图法进行绘图。有瞬时速度公式</p>
<p><img src="Image00177.jpg" alt></p>
<p>其实也就是</p>
<p><img src="Image00178.jpg" alt></p>
<p>尝试一下能不能从最直观量取到的位移s的差值得到和v有关的公式描述：</p>
<p><img src="Image00179.jpg" alt></p>
<p>类推一下：</p>
<p><img src="Image00180.jpg" alt></p>
<p>这里的vn 也就表示第n个0.02s的瞬时速度可以直接通过测量位移差求出来，从vn 上是可以直接得到n和v的对应关系的。如图8-7 <a href="#ch4_back">[4]</a> 所示，在坐标纸上画一条直线穿过这些点。</p>
<p><img src="Image00181.jpg" alt></p>
<p>图8-7 v和t的关系示意图</p>
<p>图8-7中的散点用一条直线连接，就可以得到一个比较粗糙的v=gt的函数，斜率就是g，直接测量即可得出。</p>
<p>这种从大量的函数结果和自变量反推回函数表达式的过程就是回归。刚才用的是划线穿点的方法来求一个粗略的v=gt。这种思路本身就是使用线性回归的方法。</p>
<p>假设最后得到的数值如表8-1所示。</p>
<p>表8-1 n和vn 的数值</p>
<p><img src="Image00182.jpg" alt></p>
<p>试着用Python编程，用线性回归的方法实现这个例子：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">#原始数据</span><br><span class="line">x = [1， 2， 3， 4， 5， 6， 7， 8， 9]</span><br><span class="line">y = [0.199， 0.389， 0.580， 0.783， 0.980， 1.177， 1.380， 1.575， 1.771]</span><br><span class="line">A = np.vstack（[x， np.ones（len（x））]）.T</span><br><span class="line"># A：</span><br><span class="line">#[[ 1.  1.]</span><br><span class="line"># [ 2.  1.]</span><br><span class="line"># [ 3.  1.]</span><br><span class="line"># [ 4.  1.]</span><br><span class="line"># [ 5.  1.]</span><br><span class="line"># [ 6.  1.]</span><br><span class="line"># [ 7.  1.]</span><br><span class="line"># [ 8.  1.]</span><br><span class="line"># [ 9.  1.]]</span><br><span class="line">#调用最小二乘法函数</span><br><span class="line">a， b = np.linalg.lstsq（A， y）[0]</span><br><span class="line">#转换成numpy array</span><br><span class="line">x = np.array（x）</span><br><span class="line">y = np.array（y）</span><br><span class="line">#画图</span><br><span class="line">plt.plot（x， y， &apos;o&apos;， label=&apos;Original data&apos;， markersize=10）</span><br><span class="line">plt.plot（x， a * x + b， &apos;r&apos;， label=&apos;Fitted line&apos;）</span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>

</details>

<p>画出的图形如图8-8所示。</p>
<p>回归在数据挖掘算法里也有着举足轻重的地位，在R语言里，在Mahout等开源数据挖掘框架里都有比较好的支持。如果读者有兴趣，可以再找R语言的环境或者Mahout做一下实验。</p>
<p>刚刚在打点计时器的实验里用直线穿过众多散点的方式可以得到一个比较粗糙的v=gt的函数。说它粗糙的原因很简单，因为这条线是根据打点记录的位移来画的，小车轮子是否光滑，小车自重是不是比较大，市电是不是标准的50Hz，以及空气阻力大小如何等因素都会影响打点的准确性。这些因素也会导致n和vn 的比率在每个打点瞬间记录的不稳定，或比理想值大或比理想值小。而且在画这条直线穿过这些点时也可能会画不准，可能会有几种画法，每种画法在手法上略有偏差而画出来都能从中间穿过去，且都不能算错。不只在这个例子里，再换任何一个场景，通过统计或实验产生的这样一组对应关系同样都会有类似的问题。</p>
<p><img src="Image00183.jpg" alt></p>
<p>图8-8 画出的图形</p>
<p>这种把平面上一系列的点用一条光滑的曲线连接起来的过程就叫做拟合——刚刚用一条函数曲线v=gt来进行了拟合。而多种拟合方法究竟哪一种画法最科学呢？下面将介绍分析的方法，即残差分析。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<p><a href="#ch2">[2]</a> 图片来源于百度图库。</p>
<p><a href="#ch3">[3]</a> 图片来源于百度图库。</p>
<p><a href="#ch4">[4]</a> 图片来源于百度图库。</p>
<h2 id="8-3-残差分析"><a href="#8-3-残差分析" class="headerlink" title="8.3 残差分析"></a>8.3 残差分析</h2><p>刚刚已经尝试着做过拟合这个过程了，在前面提到过，在线性回归中希望最终得到一个函数y=ax+b+e来表示y和x的关系。</p>
<p>从前面的打点计时器的例子来看，从理论上推定v=gt，而在实验中产生的其实是一个不太准确的函数v=gt+e，这里面没有b（v0 ）这一项是因为v0 是0。那问题就转化为，g究竟取多少才能让e最小呢？这个过程就是残差分析，而最终得到的结果就是要计算出一个g，使得e为误差服从均值为0的正态分布。定性地说就是在拟合的过程中，每一个点所产生的误差e大部分在0附近，而越远离0误差的e越少越好。</p>
<p>有一种非常经典的用来进行线性回归中的系数猜测的方法——最小二乘法。推导过程对于没有学过微积分的读者可能会比较复杂，下面只简单介绍原理。</p>
<p>假设有多个x和y的样本值，同时尝试用y=ax+b+e来拟合，可以得到</p>
<p><img src="Image00184.jpg" alt></p>
<p>也就是说误差大小其实是猜想的ax+b的值和观测到的y值之间的差值。试着把所有的|e|都求和。构造一个函数：</p>
<p><img src="Image00185.jpg" alt></p>
<p>Q指根据每一组样本里的x拟合得到的y（也就是ax+b）和观察到的样本里的y都做一个差，把差平方后求和。Q就是每一个|e|2 的和，现在的问题转化为让a和b分别等于什么值时Q最小（也就是所有的|e|的加和最小）。</p>
<p>即</p>
<p><img src="Image00186.jpg" alt></p>
<p>且</p>
<p><img src="Image00187.jpg" alt></p>
<p>这两个表达式的数学含义是，Q是一个a和b作为自变量的二元函数，Q分别对a和b求偏微分，满足每个偏微分方程为0的a、b变量的值就是要找的值。</p>
<p>如二元一次方程组</p>
<p><img src="Image00188.jpg" alt></p>
<p>经过变换，</p>
<p><img src="Image00189.jpg" alt></p>
<p>y的极值为x=b/2a时的值。</p>
<p>这里面其实没有用到高等数学里求导数的概念，只是用了算数平方和大于等于零的性质来确定极值的位置，当平方项为0时，y一定是极值，有</p>
<p><img src="Image00190.jpg" alt></p>
<p><img src="Image00191.jpg" alt><br>表示函数Q对自变量a求导，它的结果又是一个函数，而这个新函数称为导函数，简称导数。一般来说，导数里面还是会由a作为自变量的，而导数的函数值就是Q这个函数在a点上的切线斜率（本例不考虑不可导的情况）。可以想象，Q是一个曲线函数，既然是曲线，在每个点上就有切线，让切线随着Q函数的曲线滑动，切线的斜率也会跟着变化，而Q函数的极值应该在切线斜率为0外，如<img src="Image00192.jpg" alt><br>这个例子中的极限值位置。切线应该是一条水平线。</p>
<p>如图8-9所示，AB直线从左向右紧贴着y=x2 曲线滑动，让切点一直沿着y=x2 曲线从小到大变化。其中切线斜率为0的地方恰好是y=0（x轴）的位置。二维空间的曲线是这样，三维空间的切线通常是沿着曲面方的x轴和y轴的方向作的，如图8-10所示为z=x2<br>+y2 的图像，先做一个剖面。剖面本身也会形成一个二维的曲线方程，再让切线沿着这个剖面上的二维曲线方程滑动，找到斜率为0的地方。这也就是<img src="Image00193.jpg" alt><br>和<img src="Image00194.jpg" alt> 各自的含义了。</p>
<p><img src="Image00195.jpg" alt></p>
<p>图8-9 求曲线上斜率为0的地方</p>
<p>如果<img src="Image00193.jpg" alt> =0，且<img src="Image00194.jpg" alt><br>=0，解出这两个方程之后就能分别找到a的取值能让Q产生极值的条件，以及b的取值能让Q产生极值的条件，即找到了a、b两个维度上类似<img src="Image00196.jpg" alt><br>中函数曲线的切线为0的情况。</p>
<p>分别求导后：</p>
<p><img src="Image00197.jpg" alt></p>
<p>找满足上式的a和b的取值。Q是一个误差的平方和，a和b是斜率和截距，现在要能找到一个极值必定是一个使Q最小的极值。证明略，读者可以想像用直线在纸上穿点的过程，谨小慎微地穿能找到一个误差最小的点，稍微偏一偏这个误差就很大，而且越偏越大，那么这个大的极值点是不存在的，小的极值点是存在的，也就是要求解的点。</p>
<p><img src="Image00198.jpg" alt></p>
<p>图8-10 z=x2 +y2 的图像（见彩插）</p>
<p>如果实在不知道为什么求偏微分会得到<img src="Image00199.jpg" alt> 和<img src="Image00200.jpg" alt><br>，那就先囫囵吞枣往下看，只要还记得这个是我们要求的结果，根据上述得到的求导之后得0的两个等式可以把公式展开推导出</p>
<p><img src="Image00201.jpg" alt></p>
<p>其实是<img src="Image00202.jpg" alt> 展开，把a和b作为系数提出去之后的结果。</p>
<p>同理</p>
<p><img src="Image00203.jpg" alt></p>
<p>这是<img src="Image00204.jpg" alt> 展开，把a和b作为系数提出去之后的结果。</p>
<p>这样得到一个方程组：</p>
<p><img src="Image00205.jpg" alt></p>
<p>把（2）式做如下处理：</p>
<p><img src="Image00206.jpg" alt></p>
<p>得到下面的等式：</p>
<p><img src="Image00207.jpg" alt></p>
<p>把a和b各自的表达式推导出来：</p>
<p><img src="Image00208.jpg" alt></p>
<p>其中，a就是斜率，b是截距。</p>
<p>上式中，把所有的观测值x和y都代入，就可以得出a；再把观测值和已经求出的a代入下式，就可以相应得出b。</p>
<p>在Python中，有一些框架可以直接使用最小二乘法进行线性拟合。例如：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">#原始数据</span><br><span class="line">x = [1，2，3，4，5，6，7，8，9]</span><br><span class="line">y = [0.199， 0.389， 0.580， 0.783， 0.980， 1.177， 1.380， 1.575， 1.771]</span><br><span class="line">t1 = t2 = t3 = t4 = 0</span><br><span class="line">n = len（x）</span><br><span class="line">for i in range（n）：</span><br><span class="line">    t1 += y[i]      #∑y</span><br><span class="line">    t2 += x[i]      #∑x</span><br><span class="line">    t3 += x[i]*y[i] #∑xy </span><br><span class="line">    t4 += x[i]**2   #∑x^2</span><br><span class="line">a = （t1*t2/n - t3） / （t2*t2/n - t4）</span><br><span class="line">b = （t1 - a*t2） / n</span><br><span class="line">x = np.array（x）</span><br><span class="line">y = np.array（y）</span><br><span class="line">#画图</span><br><span class="line">plt.plot（x， y， &apos;o&apos;， label=&apos;Original data&apos;， markersize=10） </span><br><span class="line">plt.plot（x， a*x + b， &apos;r&apos;， label=&apos;Fitted line&apos;）            </span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>

</details>

<h2 id="8-4-过拟合"><a href="#8-4-过拟合" class="headerlink" title="8.4 过拟合"></a>8.4 过拟合</h2><p>过拟合简称“过拟”，是在拟合过程中出现的一种“做过头”的情况。</p>
<p>怎么叫做过头呢？我们通过对数据样本的观察和抽象，最后归纳得到一个完整的数据映射模型。但是在归纳的过程中，可能为了迎合所有样本向量点甚至是噪声点而使得模型描述过于复杂。</p>
<p>过度拟合的危害有以下几点。</p>
<p>（1）描述复杂。所有的过度拟合的模型都有一个共同点，那就是模型的描述非常复杂——参数繁多，计算逻辑多。</p>
<p>（2）失去泛化能力。所谓泛化能力就是通过学习（或机器学习）得到的模型对未知数据的预测能力，即应用于其他非训练样本的向量时的分类能力。对于待分类样本向量分类正确度高，表示泛化能力比较好；反之，如果对于待分类样本向量分类正确度低，则表示泛化能力较差。</p>
<p>我们通常希望模型的泛化能力是比较好的，因为没有泛化能力的模型对于生产指导基本没有什么意义。</p>
<p>造成过拟合的原因比较多，最常见的是以下两种。</p>
<p>（1）训练样本太少。对于训练样本过少的情况，通常都会归纳出一个非常不准确的模型。例如，要通过样本统计的方式来进行疾病成因总结，只有一个病例时，这一个病例自身的个案特点很可能会被当成通用性的特点，这样总结出来的模型显然没有泛化能力。而样本多时就可以通过统计分析保留那些共性较多的特点，而共性较少的特点就是我们所说的噪声——就不会被当做分类参数。</p>
<p>（2）力求“完美”。对于所有的训练样本向量点都希望用拟合的模型覆盖，但是实际上的训练样本却有很多是带有噪声的。这里说的噪声不是指刺耳的破坏心情的声响，而是在收集到的训练数据中由于各种原因导致的与预期分类不一致的样本向量。</p>
<p>如上述打点过程中，小车轮子的摩擦不均匀，纸张粗糙程度不同，导致某些点打出来比理想时间要晚，或比理想时间要早，当偏差比我们预期大很多时就可以认为该点是噪声点了。</p>
<p>再如，如果想测量两地之间的平均行车时间，选100辆汽车加装计时设备进行测试，所有车辆从A地出发前往B地。其中97辆车计时设备读数都是20～22分钟，有1辆车由于超速行驶计时设备读数为15分钟，有1辆车由于车辆故障中途抛锚计时设备读数300分钟，有1辆车由于忘记开启计时器计时设备读数0分钟。在这个例子中，基本可以判断A地到B地行车时间为21分钟左右，而后面的3个数据：15分钟、300分钟和0分钟就是典型的噪声数据了，完全可以在建模的过程中舍弃。</p>
<p>如果穷尽所有的办法都没办法舍弃这些噪声点而又力求让模型描述覆盖这些点，那将是非常可怕的，归纳出来的模型会产生很大的偏差。</p>
<p>在上述残差分析中，尝试着将误差e这个部分做成一个以0为中心的正态分布。如果希望这个正态分布中σ小一些，甚至为0。在这个过程中会出现什么现象？</p>
<p>首先，在分析的过程中已经尝试着用最小二乘法确定系数，这种情况已经比胡乱确定任何一个系数要合理得多，因为大部分情况下误差都很小。</p>
<p>误差为0的这种极限是什么呢？其实是类似分段函数，以上述打点计时器为例：</p>
<p><img src="Image00209.jpg" alt></p>
<p>这种描述的啰嗦程度几乎没有任何回归后的简洁可言，况且，v=gt并不是一个离散函数，t是一个在非负实数上连续的自变量，难道有任何办法罗列出所有的值吗？当然不可能，甚至连实验得到的数据内容都是有限的。</p>
<p>在多元线性回归的过程中也会有类似的情况，在为了使得误差尽可能小的过程中，会同时使得函数的描述变得过于复杂，以至于大大提高使用成本，那么这种减小误差的行为其实是得不偿失的。</p>
<p>所有这种使得函数的描述变得过于复杂，或者参数过于繁多，或者由于训练样本的问题导致函数失去泛化特性的拟合过程都叫做过度拟合（Overfitting）。</p>
<h2 id="8-5-欠拟合"><a href="#8-5-欠拟合" class="headerlink" title="8.5 欠拟合"></a>8.5 欠拟合</h2><p>与过度拟合相反，还有一种现象叫做欠拟合，简称欠拟。</p>
<p>欠拟顾名思义，就是由于操作不当——也可以说建模不当产生的误差e分布太散或者太大的情况。这种情况下，通常体现出来的都是在线性回归中的因素考虑不足的情况，常见的原因有以下两种。</p>
<p>（1）参数过少。对于训练样本向量的维度提取太少会导致模型描述的不准确。</p>
<p>例如，要根据银行储户的信息来判断其信誉好或不好，通常需要综合考虑用户的年龄、流水总和、账户余额、借贷频次、借贷额度、归还准时程度等信息特征。这些因素考虑得越充分，通常对于用户的信誉好或不好，给予的信用额度多少为宜就会有比较可靠的预测程度。而如果参数太少，如只有账户余额一项，那么就不得不用账户余额一个参数和信誉好坏去建立一个模型映射关系。这个模型是很不科学的，通过一个余额的数字就能断言一个人信誉几何太过武断。</p>
<p>（2）拟合不当。拟合不当的原因比较复杂，通常是拟合方法不正确造成的。</p>
<p>例如，某个训练样本向量x与结果值y之间的关系如下：</p>
<p>（1，1）</p>
<p>（1.41，2）</p>
<p>（1.7，3）</p>
<p>（2.1，4）</p>
<p>形成这4组向量以后，用y=x2 在第一象限的曲线去拟合误差更小。而如果用图8-11所示的直线y=3.732x-2.932做拟合，显然都没有y=x2 的误差小。</p>
<p><img src="Image00210.jpg" alt></p>
<p>图8-11 用直线y=3.732x-2.932拟合</p>
<p>凡是在欠拟合时，都要重新考虑建模是不是有考虑欠缺的地方。因为误差太大以至于拟合函数没有泛化能力而失去指导意义，这与没有做拟合差不多。</p>
<h2 id="8-6-曲线拟合转化为线性拟合"><a href="#8-6-曲线拟合转化为线性拟合" class="headerlink" title="8.6 曲线拟合转化为线性拟合"></a>8.6 曲线拟合转化为线性拟合</h2><p>非线性回归的情况太过复杂，在生产实践中也尽量避免使用这种模型。好在分类算法有很多，而且更多的是为了处理半结构化数据，所以非线性回归相关的内容只做一般性了解即可。</p>
<p>非线性回归一般可以分为一元非线性回归和多元非线性回归。</p>
<p>一元非线性回归是指两个变量——一个自变量，一个因变量之间呈现非线性关系，如双曲线、二次曲线、三（多）次曲线、幂曲线、指数曲线、对数曲线等。在解决这些问题时通常建立的是非线性回归方程或者方程组。</p>
<p>多元非线性回归分析是指两个或两个以上自变量和因变量之间呈现的非线性关系建立非线性回归模型。对多元非线性回归模型求解的传统做法，仍然是想办法把它转化成标准的线性形式的多元回归模型来处理。有些非线性回归模型，经过适当的数学变换，便能得到它的线性化的表达形式，但对另外一些非线性回归模型，仅仅做变量变换根本无济于事。属于前一种情况的非线性回归模型一般称为内蕴的线性回归，而后者则称之为内蕴的非线性回归。</p>
<p>线性拟合里最简单的就是上述y=ax+b这种形式。除此之外，可以将等式两边转化为几个一次式加和的情况，不论是一元的还是多元的，都仍然是线性回归研究的范畴。再如：</p>
<p>20世纪60年代的世界人口状况如表8-2所示。</p>
<p>表8-2 20世纪60年代世界人口状况</p>
<p><img src="Image00211.jpg" alt></p>
<p>根据马尔萨斯人口模型：</p>
<p><img src="Image00212.jpg" alt></p>
<p>其中s是人口，t是年份，e是自然常数（约取2.71828），试着推导一下到2030年时世界人口的数量。这个问题是一个比较典型的多元线性回归的问题模型。求解如下。</p>
<p>对等式两边同时取ln（以e为底的log），得到</p>
<p><img src="Image00213.jpg" alt></p>
<p>在这里实际上用的还是线性回归模型，相当于</p>
<p><img src="Image00214.jpg" alt></p>
<p>且</p>
<p><img src="Image00215.jpg" alt></p>
<p>这种使用方式在Python中同样可以套用线性回归的方法，代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.optimize import curve_fit</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">#原始数据</span><br><span class="line">T = [1960， 1961， 1962， 1963， 1964， 1965， 1966， 1967， 1968]</span><br><span class="line">S = [29.72， 30.61， 31.51， 32.13， 32.34， 32.85， 33.56， 34.20， 34.83]</span><br><span class="line">xdata = np.array（T）</span><br><span class="line">ydata = np.log（np.array（S））</span><br><span class="line">def func（x， a， b）：</span><br><span class="line">    return a + b*x</span><br><span class="line">#使用非线性最小二乘法拟合函数</span><br><span class="line">popt， pcov = curve_fit（func， xdata， ydata）  </span><br><span class="line">#画图</span><br><span class="line">plt.plot（xdata， ydata， &apos;ko&apos;， label=&quot;Original Noised Data&quot;）     </span><br><span class="line">plt.plot（xdata， func（xdata， *popt）， &apos;r&apos;， label=&quot;Fitted Curve&quot;） </span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>

</details>

<p>如图8-12所示，横轴写着+1.96e3，这是用科学记数法来表示的，意思为1.96×103 ，也就是1960，横轴上的点就是1960～1968。纵轴是ln s的值，斜率是β的值，截距是lnα的值，计算完成后通过代换可以计算出β和α的值。</p>
<p><img src="Image00216.jpg" alt></p>
<p>图8-12 线性回归函数图形</p>
<p>在这个例子中，最后可以得到β=0.01859，α=4.48401395866716×10-15</p>
<p>代入公式s=α·eβt ，求得s的值，单位是亿。</p>
<p>在预测人口数量时，直接把年份代入即可，如2000年时，代入公式得到s约为62.9亿。</p>
<p>关于马尔萨斯人口模型公式s=α·eβt 有一个需要注意的地方，即预测近期的人口数据基本准确，如2000年的人口，使用这个模型预测出来是62.9亿，真实的统计数据为61.02亿，误差不到3.1%，还是一个相对比较精确的模型。但是代入更大的数字就会出现问题，如代入4000这个数字，算出来大约88263万亿，这个数字是不可信的，因为公元4000年的时候世界人口数量增大至现在的1200多万倍，按照地球陆地面积1.49亿平方千米计算，平均每平方米上有592个人而且不管是高山峡谷沼泽森林统统都算在内。这种现象可不是这一个例子里所特有的，所有的由统计而来的回归方程在自变量很大或者很小时都容易发生失真。所以回归这种模型只能用来预测和自变量统计的区间比较近的自变量对应的函数值。</p>
<h2 id="8-7-小结"><a href="#8-7-小结" class="headerlink" title="8.7 小结"></a>8.7 小结</h2><p>从机器学习的角度来说，回归算法应该算作“分类”算法。它更像是人们先给了计算机一些样本，然后让计算机根据样本计算出一种公式或者模型，而在公式或者模型成立后，人们再给这个模型新的样本，它就可以把这个样本猜测或者说推断为某一分类。</p>
<p>不同的是，在回归中研究的都是具体的数值（实数），而分类算法则不一定，它的样本除了可以是数值外，可能很多是一些枚举值或者文本。读者只需要从这个角度来做感性上的区分即可。</p>
<p>在使用回归的过程中，要注意尽量避免出现过拟和欠拟，让函数描述在简洁和精确之间找一个平衡，这才是众多从统计而来的回归过程最后落地所要考虑的事情。过拟和欠拟不仅出现在回归方法中，在其他基于样本向量的统计归纳的模型训练中都有这样的问题，请读者一定要注意。</p>
<h1 id="第9章-聚类"><a href="#第9章-聚类" class="headerlink" title="第9章 聚类"></a>第9章 聚类</h1><p>聚类（Clustering）指的是一种学习方式（操作方式），即把物理或抽象对象的集合分组为由彼此类似的对象组成的多个类的分析过程。</p>
<p>聚类这种行为我们不要觉得很神秘，也不要觉得这个东西是机器学习所独有的，恰恰相反，聚类的行为本源还是人自身。我们学习的所有数据挖掘或者机器学习的算法或者思想的来源都是人类自己的思考方式，只不过我们把它教给机器让它们代劳，让它们成为我们肢体和能力的延伸而不是让它们替我们创造和思考。</p>
<p>聚类是一种什么现象呢？我们在认识客观世界的过程中其实一直遇到容量性的问题，遇到的每一棵树、每一朵花、每一只昆虫、每一头动物、每一个人、每一栋建筑……每个个体之间其实都不同，有的还差距相当大。那么人在认知和记忆这些客观事物的过程中就会异常痛苦，因为量实在是大到无法承受的地步。</p>
<p>因此人类才会在“自底向上”的认识世界的过程中“偷懒”性地选择了归纳归类的方式，注意“偷懒”这种方式是人类与生俱来的。</p>
<p>我们在小时候被父母用看图说话的方式来教咿呀学语的时候就有过类似的体会了，图片上画了一只猴子，于是我们就认识了，这是一只猴子；图片上画了一辆汽车，于是我们就了解了，这是一辆汽车……我们上街或者去动物园的时候再看，猴子也不是画上的猴子，而且众多猴子也长得各式各样，每个都不同，我们会把他们当成一个个新事物去认识吗？我们看汽车也同样，大小、颜色、样式，甚至是喇叭的声音也是形形色色、五花八门，它们在我们眼里是一个个新的事物吗？不，它们都还是汽车。这些事物之间确实有所不同，但是它们对我们的认知带来了很大的困扰吗？并没有。我们无论如何是不会把猴子和汽车当成一类事物去认知的，猴子彼此之间是不同，但是体格、毛发、行为举止，种种形态让我们认为这些不同种类的猴子都还是属于猴子这一个大类的动物，更别说是和汽车混为一谈，就是跟狗、马、熊这些脊椎动物我们也能轻易地分开。</p>
<p>人类天生具备这种归纳和总结的能力，能够把相似的事物放到一起来作为一类事物进行认识，它们之间可以有彼此的不同，但是有一个“限度”，只要在这个限度内，特征稍有区别无关大碍，它们仍然是这一类事物（图9-1）。</p>
<p><img src="Image00217.jpg" alt></p>
<p>图9-1 人类的归纳和总结能力</p>
<p>在这一类事物的内部，同样有这种现象，一部分个体之间比较相近，而另一部分个体之间比较相近，这两部分个体彼此之间能够被明显认知到差别，那么这个部分的事物又会在大类别的内部重新划分成两个不同的部分进行认知。如汽车从样子上可以分成小轿车、卡车、面包车等种类，虫子也被从外形上区别为飞虫、爬虫、毛毛虫……</p>
<p>在没有人特意教授不同小种群的称谓与特性之前，人类自然具备这种主观的认知能力，以特征形态的相同或近似将它们划在一个概念下，以特征形态的不同划在不同的概念下，这本身就是聚类的思维方式。</p>
<h2 id="9-1-K-Means算法"><a href="#9-1-K-Means算法" class="headerlink" title="9.1 K-Means算法"></a>9.1 K-Means算法</h2><p>在聚类中K-Means算法是很常用的一个算法，也是基于向量距离来做聚类。算法步骤如下。</p>
<p>（1）从n个向量对象任意选择k个向量作为初始聚类中心。</p>
<p>（2）根据在步骤（1）中设置的k个向量（中心对象向量），计算每个对象与这k个中心对象各自的距离。</p>
<p>（3）对于步骤（2）中的计算，任何一个向量与这k个向量都有一个距离，有的远有的近，把这个向量和距离它最近的中心向量对象归在一个类簇中。</p>
<p>（4）重新计算每个类簇的中心对象向量位置。</p>
<p>（5）重复（3）（4）两个步骤，直到类簇聚类方案中的向量归类变化极少为止。例如，一次迭代后，只有少于1%的向量还在发生类簇之间的归类漂移，那么就可以认为分类完成。</p>
<p>这里要注意的是：</p>
<p>①需要事先指定类簇的数量。</p>
<p>②需要事先给定初始的类中心。</p>
<p>例如，先准备一个中国城市经纬度表，一共604个向量，这4个维度分别是（省，市，北纬，东经）：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">北京市,北京市,39.55,116.24</span><br><span class="line">福建省,福州,26.05,119.18</span><br><span class="line">福建省,长乐,25.58,119.31</span><br><span class="line">福建省,福安,27.06,119.39</span><br><span class="line">福建省,福清,25.42,119.23</span><br><span class="line">福建省,建瓯,27.03,118.20</span><br><span class="line">福建省,建阳,27.21,118.07</span><br><span class="line">福建省,晋江,24.49,118.35</span><br><span class="line">福建省,龙海,24.26,117.48</span><br><span class="line">福建省,龙岩,25.06,117.01</span><br><span class="line">福建省,南安,24.57,118.23</span><br><span class="line">福建省,南平,26.38,118.10</span><br><span class="line">福建省,宁德,26.39,119.31</span><br><span class="line">福建省,莆田,24.26,119.01</span><br><span class="line">福建省,泉州,24.56,118.36</span><br><span class="line">福建省,三明,26.13,117.36</span><br><span class="line">福建省,邵武,27.20,117.29</span><br><span class="line">福建省,石狮,24.44,118.38</span><br><span class="line">福建省,武夷山,27.46,118.02</span><br><span class="line">福建省,厦门,24.27,118.06</span><br><span class="line">福建省,永安,25.58,117.23</span><br><span class="line">福建省,漳平,25.17,117.24</span><br><span class="line">……</span><br><span class="line">浙江省,衢州,28.58,118.52</span><br><span class="line">浙江省,瑞安,27.48,120.38</span><br><span class="line">浙江省,上虞,30.01,120.52</span><br><span class="line">浙江省,绍兴,30.00,120.34</span><br><span class="line">浙江省,台州,28.41,121.27</span><br><span class="line">浙江省,桐乡,30.38,120.32</span><br><span class="line">浙江省,温岭,28.22,121.21</span><br><span class="line">浙江省,温州,28.01,120.39</span><br><span class="line">浙江省,萧山,30.09,120.16</span><br><span class="line">浙江省,义乌,29.18,120.04</span><br><span class="line">浙江省,乐清,28.08,120.58</span><br><span class="line">浙江省,余杭,30.26,120.18</span><br><span class="line">浙江省,余姚,30.02,121.10</span><br><span class="line">浙江省,永康,29.54,120.01</span><br><span class="line">浙江省,舟山,30.01,122.06</span><br><span class="line">浙江省,诸暨,29.43,120.14</span><br><span class="line">重庆市,重庆市,29.35,106.33</span><br><span class="line">重庆市,合川市,30.02,106.15</span><br><span class="line">重庆市,江津市,29.18,106.16</span><br><span class="line">重庆市,南川市,29.10,107.05</span><br><span class="line">重庆市,永川市,29.23,105.53</span><br></pre></td></tr></table></figure>

</details>

<p>把它保存成一个文本文件“city.txt”以备后面实验使用。</p>
<p>使用K-Means算法的代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#coding=utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">#从磁盘读取城市经纬度数据</span><br><span class="line">X = []</span><br><span class="line">f = open（&apos;city.txt&apos;）</span><br><span class="line">for v in f：</span><br><span class="line">    X.append（[float（v.split（&apos;，&apos;）[2]）， float（v.split（&apos;，&apos;）[3]）]）</span><br><span class="line">#转换成numpy array</span><br><span class="line">X = np.array（X）</span><br><span class="line">#类簇的数量</span><br><span class="line">n_clusters = 5</span><br><span class="line">#现在把数据和对应的分类数放入聚类函数中进行聚类</span><br><span class="line">cls = KMeans（n_clusters）.fit（X）</span><br><span class="line">#X中每项所属分类的一个列表</span><br><span class="line">cls.labels_</span><br><span class="line">#画图</span><br><span class="line">markers = [&apos;^&apos;， &apos;x&apos;， &apos;o&apos;， &apos;*&apos;， &apos;+&apos;]</span><br><span class="line">for i in range（n_clusters）：</span><br><span class="line">    members = cls.labels_ == i</span><br><span class="line">    plt.scatter（X[members， 0]， X[members， 1]， s=60， marker=markers[i]， c=&apos;b&apos;， alpha=0.5）</span><br><span class="line">plt.title（&apos;&apos;）</span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>

</details>

<p>绘出的图形如图9-2所示。</p>
<p>图9-2中的横坐标表示北纬维度，纵坐标表示东经维度。把中国的城市划分为5个大城市区域，在图上对应划分出东北、华中、华南、西部、西北5个区域。形状比较奇怪，像是沿着左下到右上的对角线“照了个镜子”，西北跑到了东南，东南跑到了西北。</p>
<p><img src="Image00218.jpg" alt></p>
<p>图9-2 绘出的图形</p>
<h2 id="9-2-有趣模式"><a href="#9-2-有趣模式" class="headerlink" title="9.2 有趣模式"></a>9.2 有趣模式</h2><p>有趣模式顾名思义，是指容易让我们产生兴趣的模式。</p>
<p>在数据挖掘和机器学习中，一次计算会产生大量的“模式”。所谓模式可以理解成一种数据规律。例如，上述K-<br>Means将研究的数据对象分成若干个组，这些划分方法就是模式。</p>
<p>在后面的章节中也会碰到其他的算法带来的模式。在一次算法中可能会产生很多模式，然而“有趣模式”可能并不多。</p>
<p>如果一个模式具备以下特点，那么它是有趣的（Interesting）。</p>
<p>（1）易于被人理解。</p>
<p>（2）在某种确信度上，对于新的或检验数据是有效的。</p>
<p>（3）是潜在有用的。</p>
<p>（4）是新颖的。</p>
<p>如果一个模式证实了某种假设，则它也是有趣的。有趣的模式代表知识。</p>
<p>怎么理解呢，我们来看几个例子体会一下。</p>
<p>第一，“易于被人理解”。易于理解的意思就是说人们看到这样一种数据关系、一种结果，比较容易从非数据层面进行理解，或者比较容易在实际的生产生活中解释其原理或关系。</p>
<p>第二，“在某种确信度上，对于新的或检验数据是有效的”。也就是说，这样一种总结出来的数据规律和特性对于新的样本是可以套用的，即有趣模式研究的绝非一个空前或绝后的孤本，因为这种东西不具备任何迁移性和继承性。</p>
<p>第三，“是潜在有用的”是指这些有趣模式一定要有实际意义或者价值，而那些虽然能被理解，而且可以用来检验新的数据样本的模式，如果没有实际意义或者价值，则同样不是有趣的。</p>
<p>第四，“是新颖的”。例如，不会有人对几十年前就已经非常熟识并为每个人所知晓的知识有兴趣。因为已知的东西已经“浮出水面”，不需要去“挖掘”了。</p>
<h2 id="9-3-孤立点"><a href="#9-3-孤立点" class="headerlink" title="9.3 孤立点"></a>9.3 孤立点</h2><p>在聚类的过程中，会常常碰到一些离主群或者离每个群都非常远的点，这种点就叫作孤立点，也叫离群点。孤立点在很多数据研究材料中是专门作为一类研究方法来研究的。</p>
<p>聚类算法通常不能直接解释孤立点产生的原因，但是孤立点通常也是有具体意义的。</p>
<p>第一，孤立点可能是由于数据清洗不当而产生的，这属于操作性的失误问题，不是要研究的内容。</p>
<p>第二，孤立点通常由一些和群里的个体特点差异极大的样本组成，它们的行为在真实世界里和在我们的生产生活中也极有可能和群里的样本有着巨大的差异。</p>
<p>在生产生活中，孤立点的应用和研究也很多。例如，在银行的信用卡诈骗识别中，研究孤立点就是很好的办法，通过对大量的信用卡用户信息和消费行为进行向量化建模和聚类，发现在聚类中远离大量样本的点显得非常可疑——因为他们和一般性的信用卡用户特性不同，他们的消费行为和一般性的信用卡消费行为也相去甚远。还有医学领域和刑侦领域的大数据研究中都大量应用了孤立点的研究技术。在某些大型网络商城里，为了防止一些商家恶意刷单，也采用了与刑侦手段相似的措施，对店铺的销售行为以及买家的购买行为进行聚类，看看有哪些对象是与一般性的店铺销售行为或者买家购买行为差异巨大的，从而做出规则上的预防。</p>
<h2 id="9-4-层次聚类"><a href="#9-4-层次聚类" class="headerlink" title="9.4 层次聚类"></a>9.4 层次聚类</h2><p>层次聚类这个说法很形象，与本章最开始介绍的人类“自底向上”认识事物的过程是一样的。</p>
<p>前面介绍的K-<br>Means算法是直接把样本分成若干个群，而现在讨论的层次聚类就是通过聚类算法把样本根据距离分成若干大群，大群之间相异，大群内部相似，而大群内部又当成一个全局的样本空间，再继续划分成若干小群，小群之间相异，小群内部相似。这就是层次聚类的思想。最后形成的是一棵树的结构。</p>
<p>图9-3所示为部分动物界物种分类层级的示意图，从大到小分别是门、纲、目、科、属、种6个层级，这里只列出了前3个层级。这就像公司内部的部门层级结构一样，从小到大看也是根据个体与个体、小类与小类之间的相似程度汇聚成一个囊括更多相似特征的大群体，而后层层如此。</p>
<p><img src="Image00219.jpg" alt></p>
<p>图9-3 部分动物界物种分类层次示图</p>
<p>从思考角度来说，有两种思路，一种是“凝聚的层次聚类方法”，一种是“分裂的层次聚类方法”。这两种说法也很形象，“凝聚的层次聚类方法”就是在大量的样本中自底向上找那些距离比较近的样本先聚合成小的群，聚合到一定程度再由小的群聚合成更大的群。“分裂的层次聚类方法”也好理解，就是先把所有的样本分成若干个大群，再在每个群里各自重新进行聚类划分。</p>
<p>“分裂的层次聚类方法”比较好做。其实可以尝试用前面介绍的K-Means，可以先用K-Means进行一次聚类，分成若干个类簇，再在每个类簇中使用K-<br>Means进行聚类，这种思路是很容易被想到的。这种方式大家尝试去用一下就可以了。</p>
<p>下面重点介绍“凝聚的层次聚类方法”。</p>
<p>凝聚的层次聚类方法是通过这样的算法思路进行工作的：在Scikit-<br>learn库中，提供了一种叫作AgglomerativeClustering的分类算法。首先，它把整个待分类样本看作一棵完整的树，树根是所有的训练样本向量，而众多树叶就是每一个单独的样本。然后，设计几个观察点，让它们散布在整个训练样本中。让这些观察点自下而上不断地进行类簇的合并。这种聚类合并也是遵循一定的原则的，即基于连接度的度量来判断是否要向上继续合并两个类簇。度量有以下3种不同的策略原则。</p>
<p>·Ward策略：让所有类簇中的方差最小化。</p>
<p>·Maximum策略：也叫completed linkage（全连接策略），力求将类簇之间的距离最大值最小化。</p>
<p>·Average linkage策略：力求将簇之间的距离的平均值最小化。</p>
<p>本例用的是Ward策略。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#coding=utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.cluster import AgglomerativeClustering</span><br><span class="line">#从磁盘读取城市经纬度数据</span><br><span class="line">X = []</span><br><span class="line">f = open（&apos;city.txt&apos;）</span><br><span class="line">for v in f：</span><br><span class="line">    X.append（[float（v.split（&apos;，&apos;）[2]）， float（v.split（&apos;，&apos;）[3]）]）</span><br><span class="line">#转换成numpy array</span><br><span class="line">X = np.array（X）</span><br><span class="line">#类簇的数量</span><br><span class="line">n_clusters = 5</span><br><span class="line">#现在把数据和对应的分类数放入聚类函数中进行聚类，使用方差最小化的方法&apos;ward&apos;</span><br><span class="line">cls = AgglomerativeClustering（linkage=&apos;ward&apos;， n_clusters=n_clusters）.fit（X）</span><br><span class="line">#X中每项所属分类的一个列表</span><br><span class="line">cls.labels_</span><br><span class="line">#画图</span><br><span class="line">markers = [&apos;^&apos;， &apos;x&apos;， &apos;o&apos;， &apos;*&apos;， &apos;+&apos;]</span><br><span class="line">for i in range（n_clusters）：</span><br><span class="line">    members = cls.labels_ == i</span><br><span class="line">    plt.scatter（X[members， 0]， X[members， 1]， s=60， marker=markers[i]， c=&apos;b&apos;， alpha=0.5）</span><br><span class="line">plt.title（&apos;&apos;）</span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>

</details>

<p>绘出的图形如图9-4所示。</p>
<p><img src="Image00220.jpg" alt></p>
<p>图9-4 绘出的图形</p>
<p>层次聚类的思路其实并没有什么很新奇的地方，它的优势更多的是为层次化的可视化提供支持，在我们认识比较陌生的数据层次时比较有帮助。那么什么是比较熟悉的？什么是比较陌生的？比较熟悉的就是人类主观上已经确定或者规定的层次方式，如员工等级、交通工具分类等，这种本来就是由人主观性规定的数据就是人类熟悉的数据。不熟悉的数据如生物的分类和进化等，是几千万年来客观留下的，在人类出现之前早就已经开始而且到现在还没有结束的，只能通过典型的穷举个体的方式来逐步“自底向上”认识它们。这个课题，可以通过对生物解剖学的特征进行向量建模，也可以根据DNA的特征进行向量建模然后挖掘。此外，层次聚类的思路也可以用于对人们社会活动中的一些现象进行总结，如一个做歌曲发布的网站，如果希望做推荐算法，可以考虑对一个人爱听的歌曲进行层次化的聚类。对每首爱听的歌曲进行向量建模，如对一首歌的各个信息维度进行建模，例如：</p>
<p>（’音域’，’调式’，’节拍’，’速度’，’配乐乐器’，’国家元素’，’滑音’，’长音’，’语言’，’歌手年龄’，’歌手性别’…）</p>
<p>对上述信息进行量化。那么可以尝试挖掘这个用户喜欢歌曲的大类别，以及其下的小类别。或者研究歌曲流行风格进化细化的趋势等。</p>
<h2 id="9-5-密度聚类"><a href="#9-5-密度聚类" class="headerlink" title="9.5 密度聚类"></a>9.5 密度聚类</h2><p>密度聚类很多时候用在聚类形状不规则的情形下，如图9-5所示，黑点和白点分别代表两种不同的聚类类型。</p>
<p><img src="Image00221.jpg" alt></p>
<p>图9-5 聚类形状不规则</p>
<p>从图9-5中可以看出，很显然类别是这两个不规则的形状。但是如果使用K-Means算法，无论如何是得不到这个结果的，因为K-<br>Means是用欧氏距离半径来进行类簇划分的，对于这种带拐弯的、狭长的、不规则形状的聚类效果就没有圆形类簇的效果好。</p>
<p>值得注意的是，K-Means的距离计算公式也是可以选取的，一般用欧氏距离比较简单，或者用曼哈顿距离。常用的距离度量方法包括欧氏距离和余弦相似度。两者都是评定个体间差异的大小的。欧氏距离度量会受指标不同单位刻度的影响，所以一般需要先进行标准化或者归一化，同时距离越大，个体间差异越大；空间向量余弦夹角的相似度度量不会受指标刻度的影响，余弦值落于区间[-1，1]上，值越大，差异越小。有关使用余弦相似度来度量个体向量距离的例子将在后面具体介绍。</p>
<p>在sklearn里也有专门用来做基于密度分类的算法库——sklearn.cluster.DBSCAN。</p>
<p>由于密度聚类在样本比较多时容易看出效果，所以这里使用一个向量比较多的例子。向量定义如下：</p>
<p>（位次，国家，面积km2，人口，GDP亿美元，人均GDP美元）</p>
<p>注意：向量实例一共有12个，为了和代码统一，这里不写两边的括号，但是意义是一样的。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">国家        面积km2        人口             GDP     人均GDP</span><br><span class="line">中国        9670250     1392358258      99960   7179</span><br><span class="line">印度        2980000     1247923065      18707   1505</span><br><span class="line">美国        9629091     317408015       167997  53101</span><br><span class="line">巴西        8514877     201032714       22429   11311</span><br><span class="line">日本        377873      127270000       49015   38491</span><br><span class="line">澳大利亚    7692024     23540517        15053   64863</span><br><span class="line">加拿大      9984670     34591000        18251   51990</span><br><span class="line">俄罗斯      171244422   143551289       21180   14819</span><br><span class="line">泰国        513115      67041000        3871.6  5674</span><br><span class="line">柬埔寨      181035      14805358        156.5   1016</span><br><span class="line">韩国        99600       50400000        12218   24329</span><br><span class="line">朝鲜        120538      24052231        355     1476</span><br></pre></td></tr></table></figure>

</details>


<p>对它们进行密度聚类的代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#coding=utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.cluster import DBSCAN</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">#国家面积和人口</span><br><span class="line">X = [</span><br><span class="line">    [9670250， 1392358258]， #中国</span><br><span class="line">    [2980000， 1247923065]， #印度</span><br><span class="line">    [9629091， 317408015]， #美国</span><br><span class="line">    [8514877， 201032714]， #巴西</span><br><span class="line">    [377873， 127270000]， #日本</span><br><span class="line">    [7692024， 23540517]， #澳大利亚</span><br><span class="line">    [9984670， 34591000]， #加拿大</span><br><span class="line">    [17075400， 143551289]， #俄罗斯</span><br><span class="line">    [513115， 67041000]， #泰国</span><br><span class="line">    [181035， 14805358]， #柬埔寨</span><br><span class="line">    [99600， 50400000]， #韩国</span><br><span class="line">    [120538， 24052231]] #朝鲜</span><br><span class="line">#转换成numpy array</span><br><span class="line">X = np.array（X）</span><br><span class="line">#做归一化</span><br><span class="line">a = X[：， ：1] / 17075400.0 * 10000</span><br><span class="line">b = X[：， 1：] / 1392358258.0 * 10000</span><br><span class="line">X = np.concatenate（（a， b）， axis=1）</span><br><span class="line">#现在把训练数据和对应的分类放入分类器中进行训练，这里没有出现噪点是因为把min_samples#设置成了1</span><br><span class="line">cls = DBSCAN（eps=2000， min_samples=1）.fit（X）</span><br><span class="line">#类簇的数量</span><br><span class="line">n_clusters = len（set（cls.labels_））</span><br><span class="line">#X中每项所属分类的一个列表</span><br><span class="line">cls.labels_</span><br><span class="line">#画图</span><br><span class="line">markers = [&apos;^&apos;， &apos;x&apos;， &apos;o&apos;， &apos;*&apos;， &apos;+&apos;]</span><br><span class="line">for i in range（n_clusters）：</span><br><span class="line">    my_members = cls.labels_ == i</span><br><span class="line">    plt.scatter（X[my_members， 0]， X[my_members， 1]， s=60， marker=markers[i]， c=&apos;b&apos;， alpha=0.5）</span><br><span class="line">plt.title（&apos;dbscan&apos;）</span><br><span class="line">plt.show（）</span><br></pre></td></tr></table></figure>

</details>

<p>绘出的图形如图9-6所示。</p>
<p><img src="Image00222.jpg" alt></p>
<p>图9-6 绘出的图形</p>
<p>在这个例子里，国家被分为了5个不同的类别，在图上分别由5种不同的图形元素来标识，我们也可以看出大体上分，就是人口和面积都多，人口和面积都少，人口多面积少，人口少面积多，还有一个在右下角的用“+”来表示的这个是俄罗斯，它在“地广人稀”方面是脱颖而出。</p>
<p>这里唯一要解释的是关于归一化的问题即以下两行程序代码：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = X[：， ：1] / 17075400.0 * 10000</span><br><span class="line">b = X[：， 1：] / 1392358258.0 * 10000</span><br></pre></td></tr></table></figure>

</details>

<p>归一化问题是为了解决由于维度量纲或单位不同所产生的距离计算问题而进行的权重调整——这几乎是数据挖掘必需的工作，一般放在数据准备阶段，目的是把两个不同维度的数据都投影（延展或压缩）到以10000为最大值的正方形区域里。如果不做归一化处理，在这个例子中，会由于人口的数字（万人）比面积的单位（万平方千米）大太多而导致由于人口差距产生的距离比重太大，失去了多个维度上聚类的意义。如果觉得不好理解，再举一个更极端的例子，如果这里人口单位用人，而面积单位用万平方千米，那么人口维度上会用几千万或者几亿来表示，产生的距离差距是几千万或者几亿；而国土面积用的是几十或者几百来表示，在距离估算的过程中几乎可以忽略国土面积参与计算的成分。归一化问题在聚类中会比较常遇到，后面第13章中同样有这个话题的讨论。</p>
<p>关于参数这里需要做一个补充说明：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cls = DBSCAN（eps=2000， min_samples=1）.fit（X）</span><br></pre></td></tr></table></figure>

</details>

<p>这条语句中有两个参数，一个是eps，一个是min_samples。</p>
<p>eps的含义是设置一个阈值，在根据密度向外扩展的过程中如果发现在这个阈值距离范围内找不到向量，那么就认为这个聚簇已经查找完毕。在这个例子中设置的是2000，因为归一化以后所有的变量都落在一个10000×10000的区间单位，设置一个2000的单位能够充分把各个聚簇隔开，如果归一化到1000×1000的区间单位则应该采用200作为参数值，或者设置其他参数值来改变聚簇的原则。</p>
<p>min_samples的含义是告诉算法聚簇最小应该拥有多少个向量。如果这个数字设置为3，那么算法会认为所有小于3个向量的聚类为噪声点，会在结果中直接丢弃。例如，在图9-6中，如果将min_samples设置为3，则右下角的俄罗斯、上面的中国和印度都将作为噪声点被消除而不会显示。</p>
<p>由于聚类是一个非监督学习的过程，所以在聚类的过程中免不了要多尝试几次，调整参数，以找出最合理的聚类方式。</p>
<h2 id="9-6-聚类评估"><a href="#9-6-聚类评估" class="headerlink" title="9.6 聚类评估"></a>9.6 聚类评估</h2><p>除了以上聚类的算法外还有很多聚类的算法，读者有兴趣可以查阅相关的资料，但是总体思路都非常相近，很多只是在不同场景下计算效率不同而已。</p>
<p>但是聚类的好坏怎么判断呢？聚类质量如何判断呢？下面进行讨论。</p>
<p>聚类的质量评估包括以下3个方面。</p>
<p>（1）估计聚类的趋势。对于给定的数据集，评估该数据集是否存在非随机结构，也就是分布不均匀的情况。如果直接使用各种算法套用在样本上，然后返回一些类簇，这些类簇的分布很可能是一种错误的分类，会对人们产生误导。数据中必须存在非随机结构，聚类分析才是有意义的。</p>
<p>（2）确定数据集中的簇数。上述K-<br>Means算法在一开始就需要确定类簇的数量，并作为参数传递给算法。这里容易让人觉得有点矛盾，即人主观来决定一个类簇的数量的方法是不是可取。</p>
<p>（3）测量聚类的质量。可以用量化的方法来测量聚类的质量。</p>
<h3 id="9-6-1-聚类趋势"><a href="#9-6-1-聚类趋势" class="headerlink" title="9.6.1 聚类趋势"></a>9.6.1 聚类趋势</h3><p>如果样本空间里的样本是随机出现的，本身没有聚类的趋势，那么使用聚类肯定是有问题的。</p>
<p>图9-7和图9-8都是在二维空间里的向量样本，图9-7所示为随机样本，图9-8所示为有聚类样本，两者的分布样态非常不同。我们常用霍普金斯统计量（Hopkins Statistics）来进行量化评估。</p>
<p><img src="Image00223.jpg" alt></p>
<p>图9-7 随机样本</p>
<p><img src="Image00224.jpg" alt></p>
<p>图9-8 有聚类样本</p>
<p>第一步，从所有的样本向量中随机找n个向量，把它们称为p向量，每一个向量分别是p1 、p2 、……、pn<br>。对每一个向量都在样本空间里找一个离其最近的向量，然后求距离（用欧氏距离即可），然后用x1 、x2 、……、xn 来表示这个距离。</p>
<p>第二步，在所有样本向量中随机找n个向量，把它们称为q向量，每一个向量分别是q1 、q2 、……、qn<br>。对每一个向量都在样本空间里找一个离其最近的向量，然后求距离（用欧氏距离即可），然后用y1 、y2 、……、yn 来表示这个距离。</p>
<p>第三步，求出霍普金斯统计量H：</p>
<p><img src="Image00225.jpg" alt></p>
<p>上式中，分子就是把所有第二次找出的q向量的每个向量的临近向量求距离然后加和。分母是两项的加和，一项是分子，一项是p向量的每个向量的临近向量的距离加和。</p>
<p>如果整个样本空间是一个均匀的，没有聚类趋势（聚簇不明显）的空间，那么H应该为0.5左右。反之，如果是有聚类趋势（聚簇明显）的空间，那么H应该接近于1。</p>
<p>霍普金斯统计量不是一个常用的统计指标，相关资料较少。在R语言里有一个叫作comato的包，其中clustering.r这个文件里会有源代码。</p>
<p>这里用Python实现霍普金斯统计量，下面先声明一段公用代码段。</p>
<p>霍普金斯统计量计算用代码：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">#coding=utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">X = [</span><br><span class="line">    [9670250， 1392358258]， #中国</span><br><span class="line">    [2980000， 1247923065]， #印度</span><br><span class="line">    [9629091， 317408015]， #美国</span><br><span class="line">    [8514877， 201032714]， #巴西</span><br><span class="line">    [377873， 127270000]， #日本</span><br><span class="line">    [7692024， 23540517]， #澳大利亚</span><br><span class="line">    [9984670， 34591000]， #加拿大</span><br><span class="line">    [17075400， 143551289]， #俄罗斯</span><br><span class="line">    [513115， 67041000]， #泰国</span><br><span class="line">    [181035， 14805358]， #柬埔寨</span><br><span class="line">    [99600， 50400000]， #韩国</span><br><span class="line">    [120538， 24052231]] #朝鲜</span><br><span class="line">#转换成numpy array</span><br><span class="line">X = np.array（X）</span><br><span class="line">#归一化</span><br><span class="line">a = X[：， ：1] / 17075400.0 * 10000</span><br><span class="line">b = X[：， 1：] / 1392358258.0 * 10000</span><br><span class="line">X = np.concatenate（（a， b）， axis=1）</span><br><span class="line">pn = X[np.random.choice（X.shape[0]， 3， replace=False）， ：]</span><br><span class="line">#随机选出3个</span><br><span class="line"># [[   221.29671926    914.06072589]</span><br><span class="line">#  [    70.59161132    172.74455667]</span><br><span class="line">#  [ 10000.           1030.99391392]]</span><br><span class="line">xn = []</span><br><span class="line">for i in pn：</span><br><span class="line">    distance_min = 1000000</span><br><span class="line">    for j in X：</span><br><span class="line">        if np.array_equal（j， i）：</span><br><span class="line">            continue</span><br><span class="line">        distance = np.linalg.norm（j - i）</span><br><span class="line">        if distance_min &gt; distance：</span><br><span class="line">            distance_min = distance</span><br><span class="line">    xn.append（distance_min）</span><br><span class="line">qn = X[np.random.choice（X.shape[0]， 3， replace=False）， ：]</span><br><span class="line">#随机选出3个</span><br><span class="line"># [[ 10000.           1030.99391392]</span><br><span class="line">#  [  4986.63398808   1443.82893444]</span><br><span class="line">#  [   221.29671926    914.06072589]]</span><br><span class="line">yn = []</span><br><span class="line">for i in qn：</span><br><span class="line">    distance_min = 1000000</span><br><span class="line">    for j in X：</span><br><span class="line">        if np.array_equal（j， i）：</span><br><span class="line">            continue</span><br><span class="line">        distance = np.linalg.norm（j - i）</span><br><span class="line">        if distance_min &gt; distance：</span><br><span class="line">            distance_min = distance</span><br><span class="line">    yn.append（distance_min）</span><br><span class="line">H = float（np.sum（yn）） / （np.sum（xn） + np.sum（yn））</span><br><span class="line">print H</span><br><span class="line">#结果为 0.547 059 223 781</span><br></pre></td></tr></table></figure>

</details>

<h3 id="9-6-2-簇数确定"><a href="#9-6-2-簇数确定" class="headerlink" title="9.6.2 簇数确定"></a>9.6.2 簇数确定</h3><p>确定一个样本空间里有多少簇数也是很重要的，尤其是在K-<br>Means这种算法里一开始就要求给定要被分成的簇数。况且，簇数的猜测也会影响聚类的结果，簇数太多，样本被分成很多小簇，簇数太少，样本基本没有被分开，都没有意义。</p>
<p>有的资料上讲有一种简单的经验法就是对于n个样本的空间，设置簇数p为<img src="Image00226.jpg" alt><br>，在期望状态下每个簇大约有<img src="Image00227.jpg" alt> 个点。但是这种说法没有找到太多的依据，只能作为参考。</p>
<p>还有一种方法叫“肘方法”（The Elbow Method），被认为是一种更为科学的方式。思路如下。</p>
<p>尝试把样本空间划分1个类、2个类、3个类、……、n个类，要确定哪种分法最为科学，在分成m个类簇的时候会有一个划分方法，在这种划分的方法下，每个类簇的内部都有若干个向量，计算这些向量的空间中心点，即计算这m个类簇各自的空间重心在哪里。再计算每个类簇中每个向量和该类簇重心的距离（大于等于0）的和。最后把m个类簇各自的距离和相加得到一个函数var（n），n就是类簇数。</p>
<p>可以想象，这个平方和最大的时刻应该是分1个类——也就是不分类的时候，所有的向量到重心的距离都非常大，这样的距离的和是最大的。那么试着划分成2个类、3个类、4个类……随着分类的增多，第m次划分时，每个向量到自己簇的重心的距离会比上一次（m-1次）临近的机会更大，那么这个距离和就会总体上缩小。极限情况是最后被分了n个类簇，n是整个空间向量的数量，也就是一个向量一个类簇，每个类簇一个成员。这种情况最后距离的和就变成了0，因为每个向量距离自己（自己就是重心）的距离都是0。</p>
<p>如图9-9所示，m从1次、2次、3次……逐步往上增加的过程中，整个曲线的斜率会逐步降低，而且一开始是快速下降的。下降过程中有一个拐点，这一个点会让人感觉曲线从立陡变成平滑，那么这个点就是要找的点。这个样本空间被分为m个类簇之后，再分成更多的类簇时，每次的“收获”没有前面每次“收获”那么大，此时的m的值就是被认为最为合适的类簇数量。这个点在曲线上给人的感觉像是人的胳膊肘一样，所以被形象地命名为“肘方法”。例如，图9-9中的A点或者B点都可以尝试作为这个拐点。</p>
<p><img src="Image00228.jpg" alt></p>
<p>图9-9 m与距离的关系</p>
<p>但是这个方法的时间复杂度可能会相当高，因为每一次尝试都要计算，可以用来做一次经验总结或者试探性的数据分析，但是在线计算时不推荐使用。而且有时候会发现这个点可能不是很好确定，这个拐点不清晰。这种时候适当地在计算效率和收益程度上做一个平衡即可，不必太纠结，毕竟聚类是一个无监督的学习。</p>
<p>我们给出Python的示例代码，如下所示：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line">#面积km2，人口</span><br><span class="line">X = [</span><br><span class="line">    [9670250， 1392358258]， #中国</span><br><span class="line">    [2980000， 1247923065]， #印度</span><br><span class="line">    [9629091， 317408015]， #美国</span><br><span class="line">    [8514877， 201032714]， #巴西</span><br><span class="line">    [377873， 127270000]， #日本</span><br><span class="line">    [7692024， 23540517]， #澳大利亚</span><br><span class="line">    [9984670， 34591000]， #加拿大</span><br><span class="line">    [17075400， 143551289]， #俄罗斯</span><br><span class="line">    [513115， 67041000]， #泰国</span><br><span class="line">    [181035， 14805358]， #柬埔寨</span><br><span class="line">    [99600， 50400000]， #韩国</span><br><span class="line">    [120538， 24052231]] #朝鲜</span><br><span class="line">#转换成numpy array</span><br><span class="line">X = np.array（X）</span><br><span class="line">#归一化</span><br><span class="line">a = X[：， ：1] / 17075400.0 * 10000</span><br><span class="line">b = X[：， 1：] / 1392358258.0 * 10000</span><br><span class="line">X = np.concatenate（（a， b）， axis=1）</span><br><span class="line">#类簇的数量</span><br><span class="line">n_clusters = 1</span><br><span class="line">cls = KMeans（n_clusters）.fit（X）</span><br><span class="line">#每个簇的中心点</span><br><span class="line">cls.cluster_centers_</span><br><span class="line">#X中每个点所属的簇</span><br><span class="line">cls.labels_</span><br><span class="line">#曼哈顿距离</span><br><span class="line">def manhattan_distance（x， y）：</span><br><span class="line">    return np.sum（abs（x-y））</span><br><span class="line">distance_sum = 0</span><br><span class="line">for i in range（n_clusters）：</span><br><span class="line">    group = cls.labels_ == i</span><br><span class="line">    members = X[group， ：]</span><br><span class="line">    for v in members：</span><br><span class="line">        distance_sum += manhattan_distance（np.array（v）， cls.cluster_centers_）</span><br><span class="line">print distance_sum</span><br><span class="line">#结果为63 538.244 390 5</span><br></pre></td></tr></table></figure>

</details>

<p>以上为使用K-Means算法，在m=1时所计算的肘方法的函数值，如果想计算m为其他值时的结果，则修改代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_clusters = 1</span><br></pre></td></tr></table></figure>
<p>即可得到对应的距离值。</p>
<h3 id="9-6-3-测定聚类质量"><a href="#9-6-3-测定聚类质量" class="headerlink" title="9.6.3 测定聚类质量"></a>9.6.3 测定聚类质量</h3><p>在使用前面介绍的方法确定了类簇的数量后，就可以进行聚类了。但是即便类簇数量一样，聚类也可能不止一种方案。</p>
<p>测定聚类质量的方法很多，一般分为“外在方法”和“内在方法”两种。</p>
<p>所谓外在方法是一种依靠类别基准的方法，即已经有比较严格的类别定义时再讨论聚类是不是足够准确。这里通常使用“BCubed精度”和“BCubed召回率”来进行衡量。但是外在方法适用于有明确的外在类别基准的情况，而聚类是一种无监督的学习，更多是在不知道基准的状况下进行的，所以我们更倾向于使用“内在方法”。</p>
<p>“内在方法”不会去参考类簇的标准，而是使用轮廓系数（Silhouette Coefficient）进行度量。</p>
<p>对于有n个向量的样本空间，假设它被划分成k个类簇，即C1 、C2 、……、Ck<br>。对于任何一个样本空间中的向量v来说，可以求一个v到本类簇中其他各点的距离的平均值a（v），还可以求一个v到其他所有各类簇的最小平均距离（即从每个类簇里挑选一个离v最近的向量，然后计算距离），求这些距离的平均值，得到b（v），轮廓系数定义为</p>
<p><img src="Image00229.jpg" alt></p>
<p>一般来说，这个函数的结果在-1和1之间。a（v）表示的是类簇内部的紧凑型，越小越紧凑，而b（v）表示该类簇和其他类簇之间的分离程度。如果函数值接近1，即a（v）比较小而b（v）比较大时，说明包含v的类簇非常紧凑，而且远离其他的类簇。相反，如果函数值为负数，则说明a（v）&gt;b（v），v距离其他的类簇比距离自己所在类簇的其他对象更近，那么这种情况质量就不太好，应该尽可能避免。</p>
<p>为了让聚类中的类簇划分更为合理，可以计算簇中所有对象的轮廓系数的平均值（但是这个计算量有可能会相当大，请谨慎使用），然后求平均值。在一种方案里，如果轮廓系数是负数那么可以直接淘汰，如果是正数则可以在多个方案里进行比较，选择一种轮廓系数接近1的方案。但是计算时占用较多内存，尤其是在数据量巨大时，在使用时请谨慎，或者使用抽样后的数据进行计算。</p>
<p>下面给出一个用K-Means做完分类后再做聚类质量评估的Python的示例代码。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line"># encoding=utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">#面积km2，人口</span><br><span class="line">X = [</span><br><span class="line">    [9670250， 1392358258]， #中国</span><br><span class="line">    [2980000， 1247923065]， #印度</span><br><span class="line">    [9629091， 317408015]， #美国</span><br><span class="line">    [8514877， 201032714]， #巴西</span><br><span class="line">    [377873， 127270000]， #日本</span><br><span class="line">    [7692024， 23540517]， #澳大利亚</span><br><span class="line">    [9984670， 34591000]， #加拿大</span><br><span class="line">    [17075400， 143551289]， #俄罗斯</span><br><span class="line">    [513115， 67041000]， #泰国</span><br><span class="line">    [181035， 14805358]， #柬埔寨</span><br><span class="line">    [99600， 50400000]， #韩国</span><br><span class="line">    [120538， 24052231]] #朝鲜</span><br><span class="line">#转换成numpy array</span><br><span class="line">X = np.array（X）</span><br><span class="line">#归一化</span><br><span class="line">a = X[：， ：1] / 17075400.0 * 10000</span><br><span class="line">b = X[：， 1：] / 1392358258.0 * 10000</span><br><span class="line">X = np.concatenate（（a， b）， axis=1）</span><br><span class="line">#类簇的数量</span><br><span class="line">n_clusters = 3</span><br><span class="line">cls = KMeans（n_clusters）.fit（X）</span><br><span class="line">#每个簇的中心点</span><br><span class="line">cls.cluster_centers_</span><br><span class="line">#X中每个点所属的簇</span><br><span class="line">cls.labels_</span><br><span class="line">#曼哈顿距离</span><br><span class="line">def manhattan_distance（x， y）：</span><br><span class="line">    return np.sum（abs（x-y））</span><br><span class="line">#a（v）， X[0]到其他点的距离的平均值</span><br><span class="line">distance_sum = 0</span><br><span class="line">for v in X[1：]：</span><br><span class="line">    distance_sum += manhattan_distance（np.array（X[0]）， np.array（v））</span><br><span class="line">av = distance_sum / len（X[1：]）</span><br><span class="line">print av</span><br><span class="line">#11971.5037823</span><br><span class="line">#b（v）， X[0]</span><br><span class="line">distance_min = 100000</span><br><span class="line">for i in range（n_clusters）：</span><br><span class="line">    group = cls.labels_ == i</span><br><span class="line">    members = X[group， ：]</span><br><span class="line">    for v in members：</span><br><span class="line">        if np.array_equal（v， X[0]）：</span><br><span class="line">            continue</span><br><span class="line">        distance = manhattan_distance（np.array（v）， cls.cluster_centers_）</span><br><span class="line">        if distance_min &gt; distance：</span><br><span class="line">            distance_min = distance</span><br><span class="line">bv = distance_sum / n_clusters</span><br><span class="line">print bv</span><br><span class="line">#43895.5138683</span><br><span class="line">sv = float（bv - av） / max（av， bv）</span><br><span class="line">print sv</span><br><span class="line">#0.727272727273</span><br></pre></td></tr></table></figure>

</details>

<p>在这种分类方案下，得到的轮廓系数约为0.727，如果找不到更好的方案，这个就已经是最优解了。如果不满意，则修改以下代码，换成其他类簇值，然后查看轮廓系数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_clusters = 3</span><br></pre></td></tr></table></figure>
<h2 id="9-7-小结"><a href="#9-7-小结" class="headerlink" title="9.7 小结"></a>9.7 小结</h2><p>聚类这一章的内容是机器学习中探索性较强的一章，是一类用归纳方式来进行认知和观察的方法体系。应该说聚类在我们发现和总结观察对象的共性和规律方面还是有很多应用场景的，例如在向量化相对完整的前提下找出忠诚客户的共性、找出流式客户的共性、找出疑似在业务场景中作弊的个案等，这些都可以尝试使用聚类的方法进行发掘和分析。请大家灵活运用。</p>
<h1 id="第10章-分类"><a href="#第10章-分类" class="headerlink" title="第10章 分类"></a>第10章 分类</h1><p>分类算法是机器学习中的一个重点，也是人们常说的“有监督的学习”。这是一种利用一系列已知类别的样本来对模型进行训练调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。</p>
<p>换句话说，首先知道大量的样本对象，并且知道这些样本对象的“特征”和所属类别，把这些数据告诉计算机，让计算机总结分类的原则，形成一个分类模型，再把新的待分类或者说未知分类的样本交给它，让它完成分类过程。</p>
<p>也就是说，先用一部分有种种特征的数据和每种数据归属的标识来训练分类模型，当训练完毕后（等于计算机学会了应该怎么分类），再让计算机用这个分类模型来区分新的“没见过”的只有“特征”、没有类别标识的样本，完成该样本的分类。</p>
<p>所以所有的分类算法不管怎么变，都是在解决：“某样本是某对象，某样本不是某对象”的概率问题。请注意，这里用的是概率问题的说法。因为从任何方面来看，目前都没有办法保证“零误判”，人自己尚且无法做到，就更别说由人教会的计算机了。</p>
<p>分类和回归看上去有一些相似之处，从直观感觉上去认识，可以这么感觉：因变量是定量型的归纳学习称为回归，或者说是连续变量预测；因变量是定性型的归纳学习称为分类，或者说是离散变量预测。</p>
<p>从实时收集的路况来预测某地段目前的行车速度为多少米每秒是典型的回归归纳过程，而预测这个路段的行车状态是“畅通”、“繁忙”、“拥堵”则是典型的分类归纳过程。</p>
<p>分类算法是一大类算法，都是用来解决这种离散变量预测的，举例如下。</p>
<p>在银行的信用卡审批这一环节会用到分类的例子，应不应该给一个人办理信用卡呢？应该给一个申请人分配多少金额呢？尤其在有大量的申请人及调额申请的情况下。在这里会比较密集地用到分类算法。在此只示意性地进行说明，毕竟这个过程非常复杂，而且不同银行的计算原则也不尽相同。</p>
<p>首先，办理信用卡时，银行会收集很多信息，除了身份证上的姓名、年龄外，婚姻状况、工作类别、年薪、是否有房产、是否有汽车、教育情况等都在收集之列。在大量的历史信息中，银行会根据统计规律得出一个诸如：</p>
<p>适合办卡=f（年龄，婚姻状况，工作类别，年薪，是否有房产，是否有汽车，教育情况）的函数。</p>
<p>“适合办卡”这个值就只有两个，一个是1（适合），一个是0（不适合）。而函数f的计算过程是为了保证这个办理与否的评判更加合理的。也就是说，是不是适合办理，是银行在用自己的账目风险和办理人的信用做一个权衡。在统计（训练）的过程中，把那些符合信誉良好的人群特征甄别出来，这样再有申请信用卡的人出现时，就按照统计规律判断，这个人的特征是更像那些讲信用的人还是那些不讲信用的人，然后做出是否办理的判断。</p>
<p>而在赋予额度时也有同样的问题，同样要对这些属性进行评估，甚至包括历史账务记录是否都为健康记录。</p>
<p>然后，计算机根据分类算法会得到一个诸如：</p>
<p>额度=f（年龄，婚姻状况，工作类别，年薪，是否有房产，是否有汽车，教育情况，账务记录）的函数。</p>
<p>这个函数看上去更像回归的学习方法，因为这个额度是一个具体的值，更像连续值域的函数，所以判断成一个回归分析也未尝不可，只要后面这些自变量都能数值化。</p>
<p>说到额度应该是多少，是多点好还是少点好，这还是一个平衡性的问题。对于每种不同的“人群”来说，他们对额度的需求实际上是不同的，这种额度应该是他消费需求的客观需要同时也是可承担的一个限制值。额度给得过低，会抑制信用卡的使用影响银行的业绩，毕竟有很多大额的消费无法支付；而额度给得过高，会让银行的风险变得过大，因为坏账变多也难以避免。</p>
<p>经过训练后的模型会得到一个相对比较合理的额度值，既符合该人群特征的消费额度需求，又能够兼顾风险的保障。从中得到的一个样本的特征和最后额度之间的关系，其实就是判定这个样本是否应该为某额度的概率最高。</p>
<p>分类应用的场景比聚类其实要多得多，那么分类的训练算法有多少种呢？非常多，而且很多算法有变种或者衍生算法，说有几百种也不夸张，我们在这里只介绍一些最为经典和普适的方法。</p>
<h2 id="10-1-朴素贝叶斯"><a href="#10-1-朴素贝叶斯" class="headerlink" title="10.1 朴素贝叶斯"></a>10.1 朴素贝叶斯</h2><p>托马斯·贝叶斯（Thomas Bayes，约1701～1761年），主业为牧师，副业为数学家（图10-1 <a href="#ch1_back">[1]</a><br>）。他在数学方面的主要贡献在概率论上，他首先将归纳推理法用于概率论基础理论，并创立了贝叶斯统计理论，在统计决策函数、统计推断、统计的估算等领域做出了卓越的贡献。1763年发表了名为《机会学说中一个问题的解》论著，对于现代概率论和数理统计都有很重要的作用。贝叶斯的另一著作《机会的学说概论》发表于1758年。</p>
<p><img src="Image00230.jpg" alt></p>
<p>图10-1 托马斯·贝叶斯</p>
<p>贝叶斯决策理论是主观贝叶斯派归纳理论的重要组成部分。</p>
<p>贝叶斯决策理论方法是统计模型决策中的一个基本方法，基本思想如下</p>
<p>（1）已知类条件概率密度参数表达式和先验概率。</p>
<p>（2）利用贝叶斯公式转换成后验概率。</p>
<p>（3）根据后验概率大小进行决策分类。</p>
<p>简单地说，朴素贝叶斯算法是利用统计中的“条件概率”来进行分类的一种算法。前面的章节介绍的古典概型的概率计算方法，就是扔硬币的那种，穷举出所有的情况，然后看看每种情况的占比，这都是基于排列组合的思路去做概率分析。</p>
<p>朴素贝叶斯分类的方式不太一样。贝叶斯概率研究的是条件概率，也就是研究的场景就是在带有某些前提条件下，或者在某些背景条件的约束下发生的概率问题。</p>
<p>我们先给出这个著名的贝叶斯公式：</p>
<p>设D1 、D2 、……、Dn 为样本空间S的一个划分，如果以P（Di ）表示Di 发生的概率，且P（Di<br>）＞0（i=1，2，…，n）。对于任何一个事件x，P（x）＞0，则有</p>
<p><img src="Image00231.jpg" alt></p>
<p>解释如下。</p>
<p>在一个样本空间里有很多事件发生，Di 就是指不同的事件划分，并且用Di 可以把整个空间划分完毕，在每个Di 事件发生的同时都记录事件x的发生，并记录Di 事件发生下x发生的概率。等式右侧的分母部分就是Di 发生的概率和Di 发生时x发生的概率的加和，所以分母这一项其实就是在整个样本空间里x发生的概率。P（Dj<br>|x）这一项是指x发生的情况下，Dj 发生的概率。不难看出，左侧和右侧分母项相乘得到的是在全样本空间里，在x发生的情况下又发生Dj 的情况的概率。右侧分子部分的含义是Dj 发生的概率乘以Dj 发生的情况下又发生x的概率。</p>
<p>所以最后等式两边就化简为</p>
<p><img src="Image00232.jpg" alt></p>
<p>也就是说，在全样本空间下，发生x的概率乘以在发生x的情况下发生Dj 的概率，等于发生Dj 的概率乘以在发生Dj 的情况下发生x的概率，如图10-2所示。</p>
<p><img src="Image00233.jpg" alt></p>
<p>图10-2 条件概率示意图</p>
<p>左侧的圈是x发生的概率，右侧的圈是Dj 发生的概率，中间交集的部分就是等号两边各自表示的内容。</p>
<p>贝叶斯分类器通常是基于这样一个假定：“给定目标值时属性之间相互条件独立”。</p>
<p>基于这种“朴素”的假定，贝叶斯公式一般简写为</p>
<p><img src="Image00234.jpg" alt></p>
<p>上式也称为朴素贝叶斯公式（算法、定理、分类模型）——Naive Bayesian。</p>
<p>在有的资料上，会看到如下说法。</p>
<p>P（A）叫做A事件的先验概率，就是一般情况下，认为A发生的概率。</p>
<p>P（B|A）叫做似然度，是A假设条件成立的情况下发生B的概率。</p>
<p>P（A|B）叫做后验概率，在B发生的情况下发生A的概率，也就是要计算的概率。</p>
<p>P（B）叫做标准化常量，和A的先验概率定义类似，就是一般情况下，B的发生概率。</p>
<p>朴素贝叶斯分类器是在机器学习中应用最广泛的一种分类器。与其说这是一个公式，不如说这是一种思想或者思维方式，在人们生产生活中使用朴素贝叶斯分类器的思维解决问题比直接套用公式的机会多得多。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<h3 id="10-1-1-天气的预测"><a href="#10-1-1-天气的预测" class="headerlink" title="10.1.1 天气的预测"></a>10.1.1 天气的预测</h3><p>天气预报一般是基于天气图、卫星云图和雷达图来做的，预报的内容很多，如降水情况、风力、风向等。</p>
<p>大气变化是混沌的（Chaotic）——这一瞬间的天气情况会由于之后一系列不可预测的扰动行为而变得不可预测。也可以说，即便两个完全一模一样的瞬时大气状况，由于之后一系列不可预测的行为不断进行干扰而产生完全两种截然不同的天气结果。所以有那么一句话：“亚马逊雨林一只蝴蝶翅膀偶尔振动，也许两周后就会引起美国得克萨斯州的一场龙卷风。”这是20世纪70年代，美国一个名叫洛伦兹的气象学家在解释空气系统理论时做的一个形象的比喻，后来这个说法被形象地称为“蝴蝶效应”。</p>
<p>天气的预测是非常困难的，变化快且计算量巨大，所以人们虽然总在埋怨气象台预报不准确却没有什么有效的办法做出实质性的改进。就好像在这一刻预测明天一天会打几个电话一样，问题是不知道明天会打几个电话，也不知道会不会需要打电话给别人，也同样不知道别人会不会打电话找自己，未知性在这个场景里是绝对性的。但是，在预测天气时还是有一些手段可以尝试的。</p>
<p>曾经有一段时间，在天气预报中有一种说法叫做“降水概率”，如明天天气阴转多云降水概率40%。很多人在听到这样的预报时不知所云，“降水概率40%，那明天到底是下雨还是不下雨？真是一种不负责任的说法。”其实气象台说的是一个确定的事情。</p>
<p>在长期研究天气变化的过程中，会发现极端的变化还是少数，大部分的天气变化有一定的规律可循。虽然确实不知道未来大气变化的具体情况，但是还是有一些经验可以借鉴的，如今天天气非常阴沉，那下雨的可能性就是比响晴白日的时候大很多。利用统计学知识和手段，从大量的历史数据中可以尝试做出一个贝叶斯分类模型，从而判断出降水概率——依据现在的大气状况，未来24小时降水概率为多少。</p>
<p>为了简化说明过程，示意性地只列出一个地区10天中每一天的天气情况，如表10-1所示。</p>
<p>表10-1 一个地区10天中每一天的天气情况</p>
<p><img src="Image00235.jpg" alt></p>
<p>根据这10天的天气情况来看，10天里面降水4次，也就是降水概率40%，这是从全部统计的数据来看的。但是如果当天是阴天呢？阴天的日期为2、4两天，它们的第二天都是降水，所以当天是阴天的情况下，降水概率为2÷2=100%。</p>
<p>代入公式：</p>
<p><img src="Image00236.jpg" alt></p>
<p>P（x）就是阴天的概率，为20%。</p>
<p>P（Dj ）就是降水的概率，为40%。</p>
<p>P（x|Dj ）指的是降水的前一天是阴天的概率，为50%。</p>
<p>P（Dj |x）是要求的当天为阴天，第二天降水的概率，算出来为100%。</p>
<p>如果当天是晴天呢？晴天的日期为1、6、7，而晴天的第二天的天气情况分别是阴、晴、多云，降水概率为0%。</p>
<p>代入公式：</p>
<p><img src="Image00237.jpg" alt></p>
<p>P（x）就是晴天的概率，为30%。</p>
<p>P（Dj ）就是降水的概率，为40%。</p>
<p>P（x|Dj ）指的是降水的前一天是晴天的概率，为0%。</p>
<p>P（Dj |x）是要求的当天为晴天，第二天降水的概率，算出来为0%。</p>
<p>同样，如果当天是晴天，第二天天气为多云的概率为33%，为晴天的概率为33%，为阴的概率同样为33%。</p>
<p>再回过来看气象台报的“明天降水概率40%”就能理解了吧，从历史角度来看真的就是有40%的天气会降水还有60%不会降水。</p>
<p>其实在天气预测中的参数复杂得多，不仅是当天的天气这么简单的一个变量，而且温度、湿度、季节、风速等因素，统计的日期也肯定不是10天这么少的样本，肯定是长期的天气样本信息，这些因素的多少直接决定了模型的复杂程度。下面将给出一段用Python进行实现的代码。</p>
<p>在给出代码之前要补充说明一下，在Python的Scikit-<br>learn库中虽然对朴素贝叶斯分类算法做了实现，但是对于建模针对性的问题，分别做了以下几种贝叶斯分类的变种模型封装。</p>
<p>（1）高斯朴素贝叶斯（Gaussian Naive Bayes）；</p>
<p>（2）多项式朴素贝叶斯（Multinomial Naive Bayes）；</p>
<p>（3）伯努利朴素贝叶斯（Bernoulli Naive Bayes）。</p>
<p>这3种训练的方式非常相近，引用时所写的代码也非常简短。其中，高斯朴素贝叶斯是利用高斯概率密度公式来进行分类拟合的。多项式朴素贝叶斯多用于高维度向量分类，最常用的场景是文章分类。伯努利朴素贝叶斯一般是针对布尔类型特征值的向量做分类的过程。</p>
<p>本例使用高斯朴素贝叶斯模型，代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">#0：晴 1：阴 2：降水 3：多云</span><br><span class="line">data_table = [[&quot;date&quot;， &quot;weather&quot;]，</span><br><span class="line">          [1， 0]，</span><br><span class="line">          [2， 1]，</span><br><span class="line">          [3， 2]，</span><br><span class="line">          [4， 1]，</span><br><span class="line">          [5， 2]，</span><br><span class="line">          [6， 0]，</span><br><span class="line">          [7， 0]，</span><br><span class="line">          [8， 3]，</span><br><span class="line">          [9， 1]，</span><br><span class="line">          [10， 1]]</span><br><span class="line">#当天的天气</span><br><span class="line">X = [[0]， [1]， [2]， [1]， [2]， [0]， [0]， [3]， [1]]</span><br><span class="line">#当天的天气对应后一天的天气</span><br><span class="line">y = [1， 2， 1， 2， 0， 0， 3， 1， 1]</span><br><span class="line">#现在把训练数据和对应的分类放入分类器中进行训练</span><br><span class="line">clf = GaussianNB（）.fit（X， y）</span><br><span class="line">p = [[1]]</span><br><span class="line">print clf.predict（p）</span><br></pre></td></tr></table></figure>

</details>

<h3 id="10-1-2-疾病的预测"><a href="#10-1-2-疾病的预测" class="headerlink" title="10.1.2 疾病的预测"></a>10.1.2 疾病的预测</h3><p>在百度中输入“基因测序”进行搜索，会发现很多公司承接基因测序的工作，其中也有一些面向一般个人的测序工作。测试完成以后，这些公司会给出一个比较完整的报告，里面记载着这些基因的各种相关信息。</p>
<p>其中，有一些关于性格、身高、血型等的数据，由于人们在成长的过程中已经比较了解自己的这些信息了，所以似乎不会太关心。但是有一些内容还是会比较关心的，如关于罹患疾病的预测——罹患冠心病的概率为20%，罹患阿尔兹海默综合征的概率为5%，罹患帕金森氏病的概率为12%等。这些就是通过基因测序得到的预测结论。</p>
<p>这个预测过程也是一个分类过程，训练样本是大量的个体基因信息和个人的疾病信息。然后通过建模分析，最后得到一个基因片段和罹患疾病之间的概率转换关系，这也是一个比较典型的朴素贝叶斯分类模型。</p>
<p>示意性地做一个训练过程，训练样本如表10-2所示。</p>
<p>表10-2 训练样本</p>
<p><img src="Image00238.jpg" alt></p>
<p>假设能够得到一个关于病患所得的疾病和基因片段之间的关系记录，这是一个客观统计的结果。需要注意的是，这种记录是忽略其他建模因素的记录，如这位病患所生活的城市、所从事的工作、是否有烟酒习惯等，这些因素是没有参与建模的，直接假设罹患这些疾病只和基因状况有关。</p>
<p>如果有一个用户来做基因测序，测试结果为基因片段A、B分别为1、0，那么他罹患高血压和胆结石两种疾病的概率分别为多少？</p>
<p>在这个例子里，需要明确以下公式：</p>
<p><img src="Image00239.jpg" alt></p>
<p>式中的每个值分别指代什么？以计算高血压疾病的罹患概率为例：</p>
<p>P（A）是先验概率，即全局性的高血压罹患概率，在10个人中有3个人患病，即概率为3/10。</p>
<p>P（B|A）是似然度，表示高血压患者中基因是“10”型的患者数量。在3位高血压患者中只有1位患者的基因是“10”型，即概率为1/3。</p>
<p>P（B）是标准化常量，是“10”型基因出现的概率，在10个人中出现3例，即概率为3/10。</p>
<p>P（A|B）可以代入求解，得到1/3。</p>
<p>交叉验证一下，先看全局中所有的“10”型基因的患者，有3位，即4号、6号和9号。得高血压的只有9号一人，所以从这个角度去计算也是1/3。</p>
<p>下面给出完整的训练和分类的代码示例：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#基因片段A  基因片段B 高血压胆结石</span><br><span class="line">#1： 是    0：否</span><br><span class="line">data_table = [</span><br><span class="line">    [1， 1， 1， 0]，</span><br><span class="line">    [0， 0， 0， 1]，</span><br><span class="line">    [0， 1， 0， 0]，</span><br><span class="line">    [1， 0， 0， 0]，</span><br><span class="line">    [1， 1， 0， 1]，</span><br><span class="line">    [1， 0， 0， 1]，</span><br><span class="line">    [0， 1， 1， 1]，</span><br><span class="line">    [0， 0， 0， 0]，</span><br><span class="line">    [1， 0， 1， 0]，</span><br><span class="line">    [0， 1， 0， 1]</span><br><span class="line">]</span><br><span class="line">#基因片段</span><br><span class="line">X = [[1， 1]， [0， 0]， [0， 1]， [1， 0]， [1， 1]， [1， 0]， [0， 1]， [0， 0]， [1， 0]， [0， 1]]</span><br><span class="line">#高血压</span><br><span class="line">y1 = [1， 0， 0， 0， 0， 0， 1， 0， 1， 0]</span><br><span class="line">#训练</span><br><span class="line">clf = GaussianNB（）.fit（X， y1）</span><br><span class="line">#预测</span><br><span class="line">p = [[1， 0]]</span><br><span class="line">print clf.predict（p）</span><br><span class="line">#结果为0</span><br><span class="line">#胆结石</span><br><span class="line">y2 = [0， 1， 0， 0， 1， 1， 1， 0， 0， 1]</span><br><span class="line">#训练</span><br><span class="line">clf = GaussianNB（）.fit（X， y2）</span><br><span class="line">#预测</span><br><span class="line">p = [[1， 0]]</span><br><span class="line">print clf.predict（p）</span><br><span class="line">#结果为0</span><br></pre></td></tr></table></figure>

</details>

<p>当模型中有更为丰富的信息，如加入生活的城市、从事的工作、是否有烟酒习惯，还可以加入是否多盐、是否多糖等饮食习惯等内容来丰富这个模型。</p>
<h3 id="10-1-3-小结"><a href="#10-1-3-小结" class="headerlink" title="10.1.3 小结"></a>10.1.3 小结</h3><p>在本章的最后，我们先不管算法，也不管各种公式，我们就看看贝叶斯理论体系是在干什么？</p>
<p>在我看来，贝叶斯的理论体系其实揭示的是一种非常典型的人类自身的推测逻辑行为。</p>
<p>例如，在黄昏的时候走在自己居住的小区里，光线很昏暗，前面突然闪过一个影子，从路一边的草丛蹿到另一边，速度较快体型较大，其他信息没捕捉到。这时候大概会猜测，这有可能是一只较大的家犬。而如果是在非洲大草原上，从越野车里同样看到昏暗的草原上蹿过一个速度较快体型较大的动物，也许会猜测那是一头狮子，或者一头猎豹。这两种猜测对于捕捉到的对象信息都是非常有限的，而且内容相近，但是得出两种不同的推测。原因很简单，就是因为当时的环境不同，导致的两种事件的概率不同，带有比较明确的倾向性。也就是说，正常人的逻辑推断不会和上述例子相反，不会在小区里推断出现狮子或者猎豹，也不会推断在非洲大草原上出现家犬。这种推断的思路或者方式本身就是贝叶斯理论体系的核心内容。</p>
<p>朴素贝叶斯是一种机器学习的思想，而不是一个简单的直接套用的公式。而且在用朴素贝叶斯方式进行分类机器学习时还经常需要使用其他一些辅助的建模手段。朴素贝叶斯在生产生活中作为机器学习手段的场景确实非常多，是一种使用很广泛的方式，所以也很重要。后续章节将会介绍用朴素贝叶斯算法进行文章分类，希望读者能够灵活掌握。</p>
<h2 id="10-2-决策树归纳"><a href="#10-2-决策树归纳" class="headerlink" title="10.2 决策树归纳"></a>10.2 决策树归纳</h2><p>决策树也是一种常用的方式，这种方式几乎是人们可以无师自通的。</p>
<p>在平时做决定的时候常常也会有一些原则尺度可以用一棵树来表示，下面举两个小例子。</p>
<p>假如一个男生安排休息日的活动时思路如图10-3所示。</p>
<p><img src="Image00240.jpg" alt></p>
<p>图10-3 安排活动的思路</p>
<p>按照优先程度：</p>
<p>如果能约会就去看电影，</p>
<p>若不能，如果能约到球友就去打球，</p>
<p>若不能，如果能约到饭友就去下馆子，</p>
<p>若不能，如果有好片子就自己看，</p>
<p>若不能，如果有好玩的游戏就自己玩，</p>
<p>若不能，这些都没有那就在家睡觉。</p>
<p>这其实就是一个比较典型的决策树，一个样本，如某一天具体的客观情况，从树根（树是倒着从上往下长的，但也可以画成从下往上、从左往右、从右往左，不影响逻辑的表达）开始一步一步最后落入决策结果。</p>
<p>再如某大龄女青年在相亲网站进行海选，因为资源太多而自己精力有限，所以肯定是要进行相亲决策的，如图10-4所示。</p>
<p><img src="Image00241.jpg" alt></p>
<p>图10-4 相亲决策</p>
<p>年龄35以上直接拉黑，年龄35以下可以考虑见面。</p>
<p>年收入20万元以上的，属于比较有能力的高质量男性，其他条件可以适当放宽。</p>
<p>如果学历为硕士以上，身高不够175cm也可以；如果学历为本科及以下，那么身高必须在175cm以上。</p>
<p>如果年收入20万元以下，要看是不是“潜力股”，或者颜值是否够高。</p>
<p>如果学历为硕士以上，不够180cm身高也可以；如果学历为本科及以下，那么身高必须在180cm以上。</p>
<p>本例同样是根据样本——对该大龄女青年打招呼的男性的情况从树根开始走决策树，最后决定是相亲还是不相亲。当然实际生活中相亲的条件还是很复杂的，尤其还要靠“眼缘”这种超级难量化的东西，哪是这么一张图这两三个条件就全都表达清楚。特别声明一下，本人对年龄、收入、学历及身高不够自信的男性绝没有歧视的意思，这里只是做个无厘头的比方而已。</p>
<p>上述两个例子实际上是一种比较“主观”的决策树构造方式，把决策树节点的分裂条件直接决定下来形成规则，但是下面重点讨论的是决策树归纳方式。</p>
<p>决策树归纳和上述过程看上去有些相反，是一个“自底向上”的认识过程。解释如下。</p>
<p>总结出第一棵决策树是根据某人长期以来进行休息日行为决策的条件和结果从大量样本中总结而来的，而不是由他自己经过深思熟虑进行总结和口述；那么第二棵决策树也不是某人自己直接口述而来的，而是她在网站上进行不断地互动和相亲，通过计算机学习归纳总结出来的。这种归纳总结的过程比从陈述而来的过程可能更为客观和准确——所谓察其言不如观其行。</p>
<p>况且更多时候确实是没有机会听别人陈述，更多的是在观察和总结中认识世界，自己总结出知识和规律。这才是整个分类算法体系要研究的内容。</p>
<h3 id="10-2-1-样本收集"><a href="#10-2-1-样本收集" class="headerlink" title="10.2.1 样本收集"></a>10.2.1 样本收集</h3><p>为了把整个归纳过程讲述清楚，还是沿用上述相亲的例子来做说明。可以想象，相亲网站的运营人员也没有可能去跟她做一个访谈来了解到她对相亲决策过程的描述，怎么办？如果能够拿到她相亲结果的反馈，如跟谁见过面等反馈，就很容易归纳出她的策略了。</p>
<p>假设相亲信息表如表10-3所示。</p>
<p>表10-3 相亲信息</p>
<p><img src="Image00242.jpg" alt></p>
<p>假设拿到真实的12个样本，由于网站ID这种信息对大龄女青年们做出相亲决策没有什么影响，所以直接忽略，下面来看后面的数据项。</p>
<p>图10-4所示的相亲决策树图以年龄与35岁相比作为树根。试想一下，其他的数据项能不能做树根？另外，是不是一定要用大于或小于35岁作为树根分裂的条件呢，不能是34岁或者36岁吗？是不是存在一种比较科学或者客观的方法能够找到这个描述最简洁的方式呢？这里需要用到一个重要的概念，即信息增益。</p>
<h3 id="10-2-2-信息增益"><a href="#10-2-2-信息增益" class="headerlink" title="10.2.2 信息增益"></a>10.2.2 信息增益</h3><p>在介绍信息增益之前先要介绍信息熵，在第6章中提到过信息熵的概念和计算方法。信息熵是用来描述信息混乱程度或者说确定程度的一个值。混乱程度越高，熵越大；混乱程度越低，熵越小。</p>
<p>整个样本集合的熵如下：</p>
<p><img src="Image00243.jpg" alt></p>
<p>这又是一个加和结果，m的数量就是最后分类（决策）的种类，这个例子里m就是2——要么见面要么不见面。pi 指的实际是这个决策项产生的概率。本例中有两个决策项，一个是N（不相亲），概率为5/12，一个是Y（相亲），概率为7/12，这个概率就是从拿到的完整的相亲记录里得到的结论了。熵为</p>
<p><img src="Image00244.jpg" alt></p>
<p>这个熵也有另外一个叫法，即期望信息。</p>
<p>现在要做的是挑出这个“树根”，挑出“树根”的原则是这一个点挑出来一刀切下去，要尽可能消除不确定性，最好一刀下去就把两个类分清楚，如果不行才会选择在下面的子节点再切一次，切的次数越少越好。从熵的定义来看，不难看出，熵越大说明信息混乱程度越高，做切割时越复杂，要切割若干次才能完成；熵越小说明信息混乱程度越低，做切割时越容易，切割次数也就越少。</p>
<p>所以试试看究竟用哪个字段做树根能够使得消除信息混杂的能力最强。</p>
<p>假设用某一个字段A来划分，在这种划分规则下的熵为</p>
<p><img src="Image00245.jpg" alt></p>
<p>式中，InfoA 是指要求的熵，右侧从1到v做加和，其中v表示一共划分为多少组，A字段有3个枚举值，表示划分成3组，如例子中“学历”字段就有3个枚举值，那么用“学历”字段划分就是v=3的情况。Pj 表示这种分组产生的概率，也可以认为是一种权重，即3种学历各自占的比例，这里大专是2/12，本科是5/12，硕士是5/12。Info（Aj<br>）是在当前分组状态下的期望信息值。</p>
<p>具体来看看用“学历”字段做分割的情况下，熵有什么变化。</p>
<p>把上面的公式展开：</p>
<p><img src="Image00246.jpg" alt></p>
<p><img src="Image00247.jpg" alt></p>
<p>信息增益如下：</p>
<p><img src="Image00248.jpg" alt></p>
<p>这就是用“学历”字段作为根的信息增益。如果希望挑选到的是增益最大的那种方式，那么还需要试试其他字段是否有更大的信息增益。</p>
<h3 id="10-2-3-连续型变量"><a href="#10-2-3-连续型变量" class="headerlink" title="10.2.3 连续型变量"></a>10.2.3 连续型变量</h3><p>试试用“年龄”字段看是否能取得最大的信息增益，但是“年龄”字段比较麻烦，它是一个连续型的字段，不像上述“学历”字段，就是3个枚举值。这种方法通常是在这个字段上找一个最佳的分裂点，然后一刀切下去，让它的信息增益最大。</p>
<p>在一个连续的字段上可以尝试用如下做法。</p>
<p>先把这个字段中的值做一个排序，从小到大。</p>
<p>年龄：25、25、28、28、29、30、33、34、35、36、40、46。</p>
<p>这一刀可以在任意两个数字之间切下去，切分点就是这两个数字加和再平均，如25和25之间就是25，30和33之间就是31.5。要用与用“学历”字段分割类似的方法去做切割。如果有n个数字，那么就有n-1种切法，究竟哪种好只能一个一个地试。但是也可以选择中位点，然后一个一个往两边去试。</p>
<p>如果猜测这个字段值大小确实对最终决策有比较大的影响，如确实年龄是一个很重要的问题，大于某个值就直接淘汰了，小于某个值就有很大机会，那么从中位点往两边试，第一次第v个点（中位点），第二次v-1个点，第三次v+1个点，第四次v-2个点，第五次v+2个点，以此类推。每一次切割都会产生一个信息熵，一共v-1个信息熵，当发现某一个点m比它左右两边的m-1和m+1点的信息熵都要小时，就认为找到了这个点。但是这个前提条件太强了，要求确实存在一个分水岭式的分割点。</p>
<p>下面我们还是老老实实把这n-1种方式都计算一次找到这个信息熵最小的点吧。</p>
<p>那我们下面给出两段Python代码来说明枚举类型的字段的期望信息和连续类型的字段的期望信息计算方法。</p>
<p>“学历”字段分割，代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#学历分类中大专、本科、硕士占比</span><br><span class="line">education = （2.0 / 12， 5.0 / 12， 5.0 / 12）</span><br><span class="line">#大专分类中相亲占比</span><br><span class="line">junior_college = （1.0 / 2， 1.0 / 2）</span><br><span class="line">#本科分类中相亲占比</span><br><span class="line">undergraduate = （3.0 / 5， 2.0 / 5）</span><br><span class="line">#硕士分类中相亲占比</span><br><span class="line">master = （4.0 / 5， 1.0 / 5）</span><br><span class="line">#学历各分类中相亲占比</span><br><span class="line">date_per = （junior_college， undergraduate， master）</span><br><span class="line">#“相亲”字段划分规则下的熵</span><br><span class="line">def info_date（p）：</span><br><span class="line">    info = 0</span><br><span class="line">    for v in p：</span><br><span class="line">        info += v * log（v， 2）</span><br><span class="line">    return info</span><br><span class="line">#使用“学历”字段划分规则下的熵</span><br><span class="line">def infoA（）：</span><br><span class="line">    info = 0</span><br><span class="line">    for i in range（len（education））：</span><br><span class="line">        info += -education[i] * info_date（date_per[i]）</span><br><span class="line">    return info</span><br><span class="line">print infoA（）</span><br><span class="line">#结果0.872032787226</span><br></pre></td></tr></table></figure>

</details>



<p>“年龄”字段分割：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#年龄</span><br><span class="line">age = [25， 25， 28， 28， 29， 30， 33， 34， 35， 36， 40， 46]</span><br><span class="line">#是否相亲 1：是   0：否</span><br><span class="line">date = [0， 1， 1， 0， 1， 1， 1， 1， 1， 0， 0， 0]</span><br><span class="line">#这里从年龄28、29中间切开</span><br><span class="line">#左、右分类中的数量占总数的百分比</span><br><span class="line">split_per = （4.0 / 12， 8.0 / 12）</span><br><span class="line">#左边分类中相亲占比</span><br><span class="line">date_left = （1.0 / 2， 1.0 / 2）</span><br><span class="line">#右边分类中相亲占比</span><br><span class="line">date_right = （5.0 / 8， 3.0 / 8）</span><br><span class="line">#左、右各分类中相亲占比</span><br><span class="line">date_per = （date_left， date_right）</span><br><span class="line">#“相亲”字段划分规则下的熵</span><br><span class="line">def info_date（p）：</span><br><span class="line">    info = 0</span><br><span class="line">    for v in p：</span><br><span class="line">        info += v * log（v， 2）</span><br><span class="line">    return info</span><br><span class="line">#左、右分类划分规则下的熵</span><br><span class="line">def infoA（）：</span><br><span class="line">    info = 0</span><br><span class="line">    for i in range（len（split_per））：</span><br><span class="line">        info += -split_per[i] * info_date（date_per[i]）</span><br><span class="line">    return info</span><br><span class="line">print infoA（）   </span><br><span class="line">#结果是0.969 622 668 617</span><br></pre></td></tr></table></figure>

</details>



<p>从“年龄”字段的28和29两个值中间分开的情况来看，熵非常大，几乎是0.97，很接近1了，这说明这一刀切得很没有效率，还要寻找其他切分位置。修改以下代码，就可以得到其他方案的熵值。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#左、右分类中的数量占总数的百分比</span><br><span class="line">split_per = （4.0 / 12， 8.0 / 12）</span><br><span class="line">#左边分类中相亲占比</span><br><span class="line">date_left = （1.0 / 2， 1.0 / 2）</span><br><span class="line">#右边分类中相亲占比</span><br><span class="line">date_right = （5.0 / 8， 3.0 / 8）</span><br></pre></td></tr></table></figure>

</details>




<p>最后归纳总结一下构造整棵树时的思路，应该是遵循下面这样的方式：</p>
<p>第一步，找到信息增益最大的字段A和信息增益最大的切分点v（不管是连续类型还是枚举类型）。</p>
<p>第二步，决定根节点的字段A和切分点v。</p>
<p>第三步，把字段A从所有待选的字段列表中拿走，再从第一步开始找。注意这时相当于决策已经走了一步了，如果在根节点上已经分裂成两个分支，那么每一个分支各自又形成一个完整的决策树的选择过程，注意不同点。不同的是：可选的字段不一样了，因为A字段被去掉了；此外，在这个分支上的样本也比原来少了，因为两个分支分割了整个样本，使得一个部分分支只拥有样本的一部分。</p>
<p>这个过程只是看上去比较啰嗦而已，用程序套算时还是比较快的。缺点是这个归纳出来的树可能会非常复杂，分支和层次极多，这样在可视化上也有问题，在实际用新样本来做分类时也会感觉操作麻烦。</p>
<p>还可以用“减枝法”进行树的修剪，有“前减枝”和“后剪枝”两种方法。“前剪枝”就是提前终止树的构造，如只用了2个字段，两层树就已经构造完整个树了，保持了树的精简性。“后剪枝”就是等树完全构造完，如建模一共使用7个字段，全都用上，这样就形成了一个7层的树，如果一个分支下分类已经比较“纯粹”了，就没必要再通过其他条件分支来进行细化，那么整个枝可以直接减掉变成一个叶。</p>
<p>剪枝这个动作其实是在分类精度上和算法繁琐的程度上做了一个妥协，这种思路几乎贯穿所有的分类算法的始终。在第8章中介绍过过拟和欠拟的问题，过拟的诱因之一是更重视精度的思路，欠拟的诱因之一是更重视简洁度的思路。因此，不能武断地说过拟不好或者欠拟不好，而是要在方案的收益和成本之间做一个权衡。这一点在具体方案落地的时候很重要。</p>
<h2 id="10-3-随机森林"><a href="#10-3-随机森林" class="headerlink" title="10.3 随机森林"></a>10.3 随机森林</h2><p>随机森林算法是一种并行性比较好的算法规则，一再强调的是，在数据挖掘中的很多算法实际是一种问题处理方式或者原则，而不是针对某一个具体的问题所书写的代码。这本身也是一个哲学上的矛盾，针对性越强，深度越大，适应度越窄；而反过来，针对性约弱，深度越小，适应度就越宽。在学习数据挖掘诸多算法时应该还是更多注重这些适应度较宽的算法思路。</p>
<p>看到“森林”这个词，很容易联想到前一节介绍的决策树，很多很多树就可以构成森林。确实，和前面的决策树归纳的过程类似，随机森林是一个构造决策树的过程，只是它不是要构造一棵树，而是构造许多棵树。</p>
<p>在决策树的构造中会遇到过拟和欠拟的问题，在随机森林算法中，通常在一棵树上是不会追求及其精确的拟合的，而相反，希望的是决策树的简洁和计算的快速。</p>
<p>步骤和原则如下。</p>
<p>（1）随机挑选一个字段构造树的第一层。</p>
<p>（2）随机挑选一个字段构造树的第二层。</p>
<p>……</p>
<p>（3）随机挑选一个字段构造树的第n层。</p>
<p>（4）在本棵树建造完毕后，还需要照这种方式建造m棵决策树。</p>
<p>补充原则如下。</p>
<p>（1）树的层级通常比较浅。</p>
<p>（2）每棵树的分类都不能保证分类精度很高。</p>
<p>（3）一个样本进行分类时同时对这m棵决策树做分类概率判断。</p>
<p>人们会为一个训练集构造若干棵决策树，通常可能是几十甚至上百棵，具体会根据样本属性的数量和杂乱程度来决定。当有新样本需要进行分类时，同时把这个样本给这几棵树，然后用类似民主投票表决的方式来决定新样本应该归属于哪类，哪一类“得票多”就归为哪一类。</p>
<p>下面把上述例子用随机森林的方式来实现，代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">#学历 0：大专 2：硕士 1：本科</span><br><span class="line">#&apos;年龄&apos;，&apos;身高&apos;，&apos;年收入&apos;，&apos;学历&apos;</span><br><span class="line">X = [</span><br><span class="line">[25， 179， 15， 0]，</span><br><span class="line">[33， 190， 19， 0]，</span><br><span class="line">[28， 180， 18， 2]，</span><br><span class="line">[25， 178， 18， 2]，</span><br><span class="line">[46， 100， 100， 2]，</span><br><span class="line">[40， 170， 170， 1]，</span><br><span class="line">[34， 174， 20， 2]，</span><br><span class="line">[36， 181， 55， 1]，</span><br><span class="line">[35， 170， 25， 2]，</span><br><span class="line">[30， 180， 35， 1]，</span><br><span class="line">[28， 174， 30， 1]，</span><br><span class="line">[29， 176， 36， 1]，</span><br><span class="line">]</span><br><span class="line">#有否相亲 0：N 1：Y</span><br><span class="line">y = [0， 1， 1， 1， 0， 0， 1， 0， 1， 1， 0， 1]</span><br><span class="line">#现在把训练数据和对应的分类放入分类器中进行训练</span><br><span class="line">clf = RandomForestClassifier（）.fit（X， y）</span><br><span class="line">#预测下面此人是否相亲</span><br><span class="line">p = [[28， 180， 18， 2]]</span><br><span class="line">clf.predict（p）</span><br><span class="line">#输出结果是[1] 是</span><br></pre></td></tr></table></figure>

</details>

<p>这里验证用的是训练样本中的第3个[28，180，18，2]，而第3个样本也确实是相亲的对象，验证成功。同样可以用更多的样本来进行训练，这样的预测结果会更加准确。</p>
<h2 id="10-4-隐马尔可夫模型"><a href="#10-4-隐马尔可夫模型" class="headerlink" title="10.4 隐马尔可夫模型"></a>10.4 隐马尔可夫模型</h2><p>隐马尔可夫模型（Hidden Markov Model，HMM）最初由L.E.Baum发表在20世纪70年代一系列的统计学论文中，随后在语言识别、自然语言处理以及生物信息等领域体现了很大的价值。</p>
<p>还有一个概念叫做“马尔可夫链”（也有写成马尔科夫链或者马尔科夫模型的）。这两者有什么关系呢？马尔可夫链是一个数学概念，因为它由俄罗斯物理学家兼数学家安德烈·马尔可夫（A.A.Markov）提出而得名。马尔可夫链的核心是，在给定当前知识或信息的情况下，观察对象过去的历史状态对于预测将来是无关的。也可以说，在观察一个系统变化的时候，它下一个状态（第n+1个状态）如何的概率只需要观察和统计当前状态（第n个状态）即可以正确得出。另外，在一些资料上会看到贝叶斯信念网络的分类模型概念。隐马尔可夫链和贝叶斯信念网络的模型思维方式有些接近，区别在于，隐马尔可夫链的模型更为简化，或者可以认为隐马尔可夫链就是贝叶斯信念网络的一种特例。而且隐马尔可夫链是一个双重的随机过程，不仅状态转移之间是一个随机事件，状态和输出之间也是一个随机过程，如图10-5所示。</p>
<p><img src="Image00249.jpg" alt></p>
<p>图10-5 隐马尔可夫链示意图</p>
<p>在一个完整的观察过程中有一些状态的转换，即图10-5中用虚线圈表示的X1 到XT 。在观察中X1 到XT 的状态存在一个客观的转化规律，但是没办法直接观测到，观测到的是每个X状态下的输出O，即O1 到OT 。需要通过O1 到OT 这些输出值来进行模型建立和计算状态转移的概率。</p>
<p>为了比较容易理解整个过程，下面举一个很有趣的例子 <a href="#ch1_back">[1]</a> 。</p>
<p>假设有3个不同的骰子。</p>
<p>第一个骰子是常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。</p>
<p>第二个骰子是一个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。</p>
<p>第三个骰子有8个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。</p>
<p>当然用其他点数的骰子原理是一样的。3种骰子和掷骰子可能产生的结果如图10-6所示。</p>
<p><img src="Image00250.jpg" alt></p>
<p>图10-6 3种骰子和掷骰子可能产生的结果</p>
<p>先随机选择一个骰子，然后再用它掷出一个数字，并记录下这个选择和数字。先从3个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后掷骰子，得到一个数字，1、2、3、4、5、6、7、8中的一个。不停地重复上述过程，会得到一串数字，每个数字都是1、2、3、4、5、6、7、8中的一个。</p>
<p>例如，可能得到这么一串数字（掷骰子10次）：1、6、3、5、2、7、3、5、2、4，这串数字叫做可见状态链，也就是记录的这组数字，也是前面介绍的On<br>。但是在隐马尔可夫模型中，不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是选出的骰子的序列。例如，隐含状态链有可能是D6、D8、D8、D6、D4、D8、D6、D6、D4、D8，如图10-7所示。如果继续选取和投掷还能得到这个状态链上更多的节点。一般来说，HMM中的马尔可夫链其实是指隐含状态链，因为实际是隐含状态（所选的骰子）之间存在转换概率（Transition Probability），如图10-8所示。</p>
<p><img src="Image00251.jpg" alt></p>
<p>图10-7 隐马尔可夫模型示意图</p>
<p>在这个例子中，D6的下一个状态是D4、D6、D8的概率都是1/3。D4、D8的下一个状态是D4、D6、D8的转换概率也都一样是1/3，虽然在示例中没有画出所有的情况，但是从古典概型的角度来分析，应该是这样的，而实际上也可以从大量的掷骰子实验中得到这样的转换概率的统计结果。这样设定是为了最开始容易说清楚，其实是可以随意设定转换概率的。例如，可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。也可以假设骰子不是3个，而是有10个，其中D4有2个，D6有9个，D8有1个，等等。这样就是一个新的HMM，因为转换概率肯定是与当前的例子不同的。而同样的，尽管可见状态之间没有直接的转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（Emission Probability）。</p>
<p><img src="Image00252.jpg" alt></p>
<p>图10-8 隐含状态转换关系示意图</p>
<p>就本例来说，六面骰子（D6）产生1的输出概率是1/6。产生2、3、4、5、6的概率也都是1/6。同样可以对输出概率进行其他定义。例如，有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2、3、4、5、6的概率是1/10。</p>
<p>其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，进行模拟是相当容易的。但是应用HMM模型时，往往缺失一部分信息，有时候知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。这些算法将会在后续章节详细介绍。</p>
<p>和HMM模型相关的算法主要分为3类，分别解决3种问题。</p>
<p>问题1： 知道骰子有几种（隐含状态数量）、每种骰子是什么（转换概率）、根据掷骰子掷出的结果（可见状态链），想知道每次掷出来的都是哪种骰子（隐含状态链）。</p>
<p>这个问题在语音识别领域叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。每个答案都正确，只是这些答案的意义不一样。第一种解法求最大似然状态路径，通俗地说，就是求一串骰子序列，这串骰子序列产生观测结果的概率最大。第二种解法不是求一组骰子序列，而是求每次掷出的骰子分别是某种骰子的概率。</p>
<p>例如，看到结果后，可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2。</p>
<p>问题2： 知道骰子有几种（隐含状态数量）、每种骰子是什么（转换概率）、根据掷骰子掷出的结果（可见状态链），想知道掷出这个结果的概率。</p>
<p>这个问题看似意义不大，因为掷出来的结果很多时候都对应了一个比较大的概率。这个问题的目的其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明已知的模型很有可能是错的，有人偷偷把骰子换了。</p>
<p>问题3：<br>知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），想反推出每种骰子是什么（转换概率）。这个问题很重要，因为这是最常见的情况。很多时候只有可见结果，不知道HMM模型中的参数，需要从可见结果估计出这些参数，这是建模的一个必要步骤。</p>
<p><a href="#ch1">[1]</a> 该例子来源于知乎，作者：Yang Eninala，链接：<a href="http://www.zhihu.com/question/20962240/answer/33438846，有部分删改。" target="_blank" rel="noopener">http://www.zhihu.com/question/20962240/answer/33438846，有部分删改。</a></p>
<h3 id="10-4-1-维特比算法"><a href="#10-4-1-维特比算法" class="headerlink" title="10.4.1 维特比算法"></a>10.4.1 维特比算法</h3><p>接着上述例子说问题1，解最大似然路径问题。</p>
<p>有3个骰子：六面骰、四面骰、八面骰。也知道掷了10次的结果（1、6、3、5、2、7、3、5、2、4），但是不知道每次用了哪种骰子，而想知道最有可能的骰子序列。其实最简单的方法就是穷举所有可能的骰子序列，然后根据古典概型的分布特点来计算每个序列对应的概率，再从中把对应最大概率的序列挑出来。如果马尔可夫链不长，这种方法是可行的。如果马尔可夫链长，穷举的数量太大，就很难完成了。另外一种很有名的算法叫做维特比算法（Viterbi algorithm）。要理解这个算法，先看几个简单的例子。</p>
<p>首先，如果只掷一次骰子，如图10-9所示。</p>
<p>结果为1，对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8。把这个情况拓展，掷两次骰子，如图10-10所示。</p>
<p><img src="Image00253.jpg" alt></p>
<p>图10-9 掷一次骰子</p>
<p><img src="Image00254.jpg" alt></p>
<p>图10-10 掷两次骰子</p>
<p>结果分别为1、6。这时问题变得复杂起来，要计算3个值，分别是第二个骰子是D6、D4、D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率如下：</p>
<p><img src="Image00255.jpg" alt></p>
<p>上面等式右侧表示的是第一个骰子选到D4的概率（1/3）乘以在选到D4的情况下掷出1点的概率（1/4），P（D6|D4）是指第一个骰子选到D4的情况下第二个骰子选到D6的概率（1/3），最后一项是指第二个骰子选到D6的情况下掷出6点的概率（1/6）。</p>
<p>同样的，可以计算第二个骰子是D4或D8时的最大概率。发现，第二个骰子取到D6的概率最大。而使这个概率最大时，第一个骰子为D4。所以最大概率骰子序列就是D4、D6。继续拓展，掷3次骰子，如图10-11所示。</p>
<p><img src="Image00256.jpg" alt></p>
<p>图10-11</p>
<p>同样，计算第三个骰子分别是D6、D4、D8的最大概率。再次发现，要取到最大概率，第二个骰子必须为D6。这时，第三个骰子取到D4的最大概率如下：</p>
<p><img src="Image00257.jpg" alt></p>
<p>和计算两个骰子序列概率的方法一样，还可以计算第三个骰子是D6或D8时的最大概率。可以发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4、D6、D4。既然掷骰子1到3次可以算，掷多少次都可以，以此类推。</p>
<p>可以发现，要求最大概率骰子序列时要做下面几件事情。</p>
<p>首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下取到每个骰子的最大概率都算过了，重新计算其实不难。当算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，要把对应这个最大概率的序列从后往前推出来。这就是在刚刚掷骰子的例子中展示出的完整维特比算法。</p>
<p>维特比算法的提出者叫安德鲁·维特比，美籍犹太人，高通首席科学家，同时也是高通公司创始人之一。维特比算法的目的也比较单纯，即找出可能性最大的隐藏序列。</p>
<p>这种算法研究是一种链的可能性问题。现在应用最广的领域是CDMA通信以及打字提示功能。</p>
<p>通信系统是一个非常复杂的系统，不管是人们用的手机的通信，还是家里无线路由器的WiFi通信，还是光纤里的光波通信，涉及一系列的问题。例如，在进行语音通话时要把声音信号的模拟信号进行抽样，再用傅里叶变换变成余弦波组成的频域信号，再把频域信号进行数字化传输，再用傅里叶逆变换变回时域信号，再由模拟放大电路变成声音信号放出来。而在一个小区域里大量的人都用的是一个手机基站，会不会出现把基站挤满了没办法打电话的情况呢？</p>
<p>相信大家都有体会，在没有微信的时候，手机基站最忙的时候就是除夕钟声敲响的前后了。打电话打不出去，别人也打不进来，短信发起来也是很困难，因为带宽被接通的这些电话占满了。每一台手机和基站之间通话都是要占用通信带宽的，带宽有限，所以在手机通话负荷已经超过手机基站的情况下，基站就没办法分配足够的频带（带宽）给新接入的手机用了，电话自然打不出去也打不进来。对于基站来说，通信频带肯定是有这样的局限性的，通信要保证每个接入的手机都要正常通话一般有以下两种办法。</p>
<p>（1）每个人用不同的频段。每个手机各用各的频率，当然就不会干扰了。</p>
<p>（2）大家轮流“说话”。把时间打碎，如一秒钟分成几十个小段，每个手机被分配只在这个小段和基站用某个频率通信，下次通信就得到下一秒去。这样所有的手机虽然用的是同一个频率，但是大家“说话”都很紧凑，1秒钟收集到的信息压缩在几十分之一秒发过去。时间上错开，也不会有问题。</p>
<p>当然，还有一种方法就是两种方案混搭，即分配不同的频段，又分配不同的时间。这样一个基站就能容纳几百个人同时使用了。</p>
<p>还有一种看上去非常高级的协议，叫做CDMA协议（Code Division Multiple Access），中文译作码分多址协议，国内目前用的3G或者4G都是CDMA协议族里的通信协议。</p>
<p>怎么理解这种协议呢？先想象一下，很多人聚在一起说话时有什么现象？即大家都在用3000～4000Hz的频率——常人耳朵能识别的语音频率来进行对话。大家都近距离小声说话，基本上还没问题。这属于多个点对点的对话，其他人之间对话的声音传过来就非常小了，基本不会干扰到和谈话对象之间的会话——从前面提到过的香农公式来说这属于信噪比比较大的时候。如果大家说话时声音都比较大，那就显得非常嘈杂，如果噪声压过了正常交谈的声音再想要跟谈话对象说清楚就需要离得近一些，声音大一些，这就是加大信号功率，还是加大信噪比的方案。</p>
<p>CDMA是一种与这种处理思路不同的方式，它相当于在整个房间里强迫每一对谈话者都用一种与其他谈话的人不一样的语言。如甲乙两个人用英语，丙丁两个人用中文，戊己两个人用韩国语，等等。最后整个房间里虽然听起来还是会比较嘈杂，但是每个对话者经历的是一种什么现象呢？如甲会在乱嗡嗡的背景噪音里，听到有一个乙在讲英语，乙也是同样的感觉——他会听到在嘈杂的噪声里有一个甲在讲英语。其他每一组对话的对象都会有类似的感觉，即能听到杂乱的背景噪声里有一个人在讲自己能听懂的语言——只要这种噪声不要大到完全听不到谈话对象在说什么即可。每对人物对话的过程却是听到了很多的声音，但是根据上下文关系是能够从众多的声音中滤出那些和自己语言一致的声音，甚至在语言种类一致的情况下能够滤出和自己对话内容一致的声音。这基本就是CDMA技术最为通俗的解释了，整个方案就是基站，不同的手机就是里面每个人，唯一不同的一点是，基站本身会说N种语言，它每次和一台手机开启一个会话都会指定让这次会话使用某种语言而且和其他手机不同，虽然频段和时间上不做区分但是手机和基站的通信在各自的“语言”下并行不悖。</p>
<p>人类为什么会具有这样一种能力，其实是根据这些语音各自具备一定的特性和上下文关系，所以人们轻易能够从噪声中提取到能理解的信息，而忽略那些认为不是信息的内容。</p>
<p>维特比算法整体的思路就是在寻找收到的上一段信息和它后面跟随的下一段信息的转移概率问题——在这段信息后最可能出现的是哪些前置内容。</p>
<p>再来看一个和生活更贴近的例子——打字软件猜测输入内容对应文字，如图10-12所示。</p>
<p><img src="Image00258.jpg" alt></p>
<p>图10-12 打字软件猜测输入内容对应文字</p>
<p>在Windows中能够使用的输入法有很多种，有全拼，即要把拼音输入完整；有双拼，即一种缩拼，用一个字符代表若干个字符；有五笔字型，即用一个字符代表一个偏旁；等等，但是猜测输入文字时，原理差不多。以全拼为例，在使用输入法时，输入的是英文字母。在这个应用中，隐藏的序列是真实要输入的中文字符和词汇，显示的部分是输入的英文字符。</p>
<p>在输入jin时，输入法软件会猜测想要输入“近”、“斤”、“今”等。而这种排序不是瞎猜的，通常是根据统计而来。也就是说“近”、“斤”、“今”这样的顺序一般是根据使用人的输入习惯形成的——在平时打字聊天的过程中“近”出现在词汇或句子（如果输入法知道这个是句子开始）输入开始的概率大于“斤”，而“斤”大于“今”。在使用输入法时也能感觉到，一个字如果被输入一次，下一次再输入的时候排名可能就比原来靠前很多，尤其是那些比较冷僻的字排名变化尤其明显。</p>
<p>但是又输入了tian时就不一样了。“今天”作为一个词汇，比其他任何一个被拼作jintian的词汇都使用得更为高频。也可以理解为，当输入tian时，由jin（今）到tian（天）这条路径的概率是最高的，这是把“今天”这个词汇放在第一个的原因。</p>
<p>后面输入了几个其他的完整词汇：“我们”、“应该”、“做些”、“什么”，输入法也会继续对这些词汇在句子中的路径概率进行计算，每次输入都会猜测一次到目前的输入状态为止最有可能的那条路径，那么看到的这个第一顺位的词汇，准确说是一个句子——“今天我们应该做些什么”就是猜测到的最优的结果，它比其他任何一种路径产生的概率都要高。</p>
<p>上述内容是一个在没有看该输入的源代码的情况下做的一个猜测，只是说这样做是可以的，具体操作起来还是会比较复杂，还有很多其他的因素应该考虑进去。如这个马尔可夫模型的训练（拼音串的输入与最终产生的汉字串的输出）是应该来源于本地的输入者的习惯呢，还是应该来源于更加有代表性的互联网呢，还是两者结合，如果结合又是怎么一种策略来调解其中矛盾的部分呢？这些都是值得探讨的问题。</p>
<p>下面模拟输入法的猜测方法给出一个算法示例，先给出各级转移矩阵，如表10-4～表10-7所示。</p>
<p>表10-4 jin概率矩阵</p>
<p><img src="Image00259.jpg" alt></p>
<p>表10-5 jin-tian转移矩阵</p>
<p><img src="Image00260.jpg" alt></p>
<p>表10-6 wo概率矩阵</p>
<p><img src="Image00261.jpg" alt></p>
<p>表10-7 wo-men转移矩阵</p>
<p><img src="Image00262.jpg" alt></p>
<p>简单做一下说明，输出一个完整拼音后，用户就会按空格或者数字把输入备选框中的汉字输出，当单字词输出时就会由统计产生“jin输入概率矩阵”和“wo概率矩阵”这样的统计结果，这个计算比较简单，计算次数即可。下次再度输入单字词拼音就会根据输入概率矩阵进行排序，概率大的单字词会列在前面。</p>
<p>而当用户输出的是一个“双字词”时就会由统计产生“jin-tian转移矩阵”和“wo-<br>men转移矩阵”这样的统计结果，同样是用计数的统计方法即可。而且每个双字词、三字词等的输入统计都用这种方法。在输入双字词汉字拼音时会根据转移概率表进行计算。多个词相连就是多个转移矩阵的概率相乘计算，从而得到概率最大的输入可能项。</p>
<p>在实际应用的过程中，这个概率矩阵会是一个系数矩阵，在磁盘或者内存上肯定不会像这4个表格这样直接列成一个方阵来存储的。而且转移概率足够小，如小于0.001时可以认为是输入统计中的噪声点，不进行词汇输入推荐，这样输入备选框的前面也只会出现高频输入词汇，这样备选框比较简洁。</p>
<p>代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">jin = [&apos;近&apos;， &apos;斤&apos;， &apos;今&apos;， &apos;金&apos;， &apos;尽&apos;]</span><br><span class="line">jin_per = [0.3， 0.2， 0.1， 0.06， 0.03]</span><br><span class="line">jintian = [&apos;天&apos;， &apos;填&apos;， &apos;田&apos;， &apos;甜&apos;， &apos;添&apos;]</span><br><span class="line">jintian_per = [</span><br><span class="line">    [0.001， 0.001， 0.001， 0.001， 0.001]，</span><br><span class="line">    [0.001， 0.001， 0.001， 0.001， 0.001]，</span><br><span class="line">    [0.990， 0.001， 0.001， 0.001， 0.001]，</span><br><span class="line">    [0.002， 0.001， 0.850， 0.001， 0.001]，</span><br><span class="line">    [0.001， 0.001， 0.001， 0.001， 0.001]]</span><br><span class="line">wo = [&apos;我&apos;， &apos;窝&apos;， &apos;喔&apos;， &apos;握&apos;， &apos;卧&apos;]</span><br><span class="line">wo_per = [0.400， 0.150， 0.090， 0.050， 0.030]</span><br><span class="line">women = [&apos;们&apos;， &apos;门&apos;， &apos;闷&apos;， &apos;焖&apos;， &apos;扪&apos;]</span><br><span class="line">women_per = [</span><br><span class="line">    [0.970， 0.001， 0.003， 0.001， 0.001]，</span><br><span class="line">    [0.001， 0.001， 0.001， 0.001， 0.001]，</span><br><span class="line">    [0.001， 0.001， 0.001， 0.001， 0.001]，</span><br><span class="line">    [0.001， 0.001， 0.001， 0.001， 0.001]，</span><br><span class="line">    [0.001， 0.001， 0.001， 0.001， 0.001]]</span><br><span class="line">N = 5</span><br><span class="line">def found_from_oneword（oneword_per）：</span><br><span class="line">    index = []</span><br><span class="line">    values = []</span><br><span class="line">    a = np.array（oneword_per）</span><br><span class="line">    for v in np.argsort（a）[：：-1][：N]：</span><br><span class="line">        index.append（v）</span><br><span class="line">        values.append（oneword_per[v]）</span><br><span class="line">    return index， values</span><br><span class="line">def found_from_twoword（oneword_per， twoword_per）：</span><br><span class="line">    last = 0</span><br><span class="line">    for i in range（len（jin_per））：</span><br><span class="line">        current = np.multiply（oneword_per[i]， twoword_per[i]）</span><br><span class="line">        if i == 0：</span><br><span class="line">            last = current</span><br><span class="line">    else：</span><br><span class="line">        last = np.concatenate（（last， current）， axis=0）</span><br><span class="line">    index = []</span><br><span class="line">    values = []</span><br><span class="line">    for v in np.argsort（last）[：：-1][：N]：</span><br><span class="line">        index.append（[v / 5， v % 5]）</span><br><span class="line">        values.append（last[v]）</span><br><span class="line">    return index， values</span><br><span class="line">def predict（word）：</span><br><span class="line">    if word == &apos;jin&apos;：</span><br><span class="line">        for i in found_from_oneword（jin_per）[0]：</span><br><span class="line">            print jin[i]</span><br><span class="line">    elif word == &apos;jintian&apos;：</span><br><span class="line">        for i in found_from_twoword（jin_per， jintian_per）[0]：</span><br><span class="line">            print jin[i[0]] + jintian[i[1]]</span><br><span class="line">    elif word == &apos;wo&apos;：</span><br><span class="line">        for i in found_from_oneword（wo_per）[0]：</span><br><span class="line">            print wo[i]</span><br><span class="line">    elif word == &apos;women&apos;：</span><br><span class="line">        for i in found_from_twoword（wo_per， women_per）[0]：</span><br><span class="line">            print wo[i[0]] + women[i[1]]</span><br><span class="line">    elif word == &apos;jintianwo&apos;：</span><br><span class="line">        index1， values1 = found_from_oneword（wo_per）</span><br><span class="line">        index2， values2 = found_from_twoword（jin_per， jintian_per）</span><br><span class="line">        last = np.multiply（values1， values1）</span><br><span class="line">        for i in np.argsort（last）[：：-1][：N]：</span><br><span class="line">            print jin[index2[i][0]]， jintian[index2[i][1]]， wo[i]</span><br><span class="line">    elif word == &apos;jintianwomen&apos;：</span><br><span class="line">        index1， values1 = found_from_twoword（jin_per， jintian_per）</span><br><span class="line">        index2， values2 = found_from_twoword（wo_per， women_per）</span><br><span class="line">        last = np.multiply（values1， values1）</span><br><span class="line">        for i in np.argsort（last）[：：-1][：N]：</span><br><span class="line">            print jin[index1[i][0]]， jintian[index1[i][1]]， wo[index2[i][0]]， women[index2[i][1]]</span><br><span class="line">    else：</span><br><span class="line">        pass</span><br><span class="line">if __name__ == &apos;__main__&apos;：</span><br><span class="line">    predict（&apos;jin&apos;）</span><br><span class="line">    # 近</span><br><span class="line">    # 斤</span><br><span class="line">    # 今</span><br><span class="line">    # 金</span><br><span class="line">    # 尽</span><br><span class="line">    predict（&apos;jintian&apos;）</span><br><span class="line">    # 今天</span><br><span class="line">    # 金田</span><br><span class="line">    # 近天</span><br><span class="line">    # 近填</span><br><span class="line">    # 近田</span><br><span class="line">    predict（&apos;wo&apos;）</span><br><span class="line">    # 近</span><br><span class="line">    # 斤</span><br><span class="line">    # 今</span><br><span class="line">    # 金</span><br><span class="line">    # 尽</span><br><span class="line">    predict（&apos;women&apos;）</span><br><span class="line">    # 我们</span><br><span class="line">    # 我闷</span><br><span class="line">    # 我门</span><br><span class="line">    # 我焖</span><br><span class="line">    # 我扪</span><br><span class="line">    predict（&apos;jintianwo&apos;）</span><br><span class="line">    # 今天我</span><br><span class="line">    # 金田窝</span><br><span class="line">    # 近天喔</span><br><span class="line">    # 近填握</span><br><span class="line">    # 近田卧</span><br><span class="line">    predict（&apos;jintianwomen&apos;）</span><br><span class="line">    # 今天我们</span><br><span class="line">    # 金田我闷</span><br><span class="line">    # 近田我扪</span><br><span class="line">    # 近填我焖</span><br><span class="line">    # 近天我门</span><br></pre></td></tr></table></figure>

</details>




<details>
  <summary>lang+源代码添加注释(非原书内容) </summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">#coding = utf-8</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#初始化数据</span><br><span class="line">#假设数组jin为&quot;jin&quot; 对应的汉字</span><br><span class="line">jin = [&apos;近&apos;, &apos;斤&apos;, &apos;今&apos;, &apos;金&apos;, &apos;尽&apos;]</span><br><span class="line">#假设jin_per 表示&quot;jin&quot;是对应jin数组对应位置汉字可能的概率</span><br><span class="line">jin_per = [0.3, 0.2, 0.1, 0.06, 0.03]</span><br><span class="line"></span><br><span class="line">#假设 jintian数组表示”tian“对应的汉字</span><br><span class="line">jintian = [&apos;天&apos;, &apos;填&apos;, &apos;田&apos;, &apos;甜&apos;, &apos;添&apos;]</span><br><span class="line">#假设 jintian_per数组表示&quot;jintian&quot;这两个拼音的对应汉字的条件概率</span><br><span class="line">jintian_per = [</span><br><span class="line">    [0.001, 0.001, 0.001, 0.001, 0.001],</span><br><span class="line">    [0.001, 0.001, 0.001, 0.001, 0.001],</span><br><span class="line">    [0.990, 0.001, 0.001, 0.001, 0.001],</span><br><span class="line">    [0.002, 0.001, 0.850, 0.001, 0.001],</span><br><span class="line">    [0.001, 0.001, 0.001, 0.001, 0.001]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">#同上</span><br><span class="line">wo = [&apos;我&apos;, &apos;窝&apos;, &apos;喔&apos;, &apos;握&apos;, &apos;卧&apos;]</span><br><span class="line">wo_per = [0.400, 0.150, 0.090, 0.050, 0.030]</span><br><span class="line"></span><br><span class="line">#同上</span><br><span class="line">women = [&apos;们&apos;, &apos;门&apos;, &apos;闷&apos;, &apos;焖&apos;, &apos;扪&apos;]</span><br><span class="line">women_per = [</span><br><span class="line">    [0.970, 0.001, 0.003, 0.001, 0.001],</span><br><span class="line">    [0.001, 0.001, 0.001, 0.001, 0.001],</span><br><span class="line">    [0.001, 0.001, 0.001, 0.001, 0.001],</span><br><span class="line">    [0.001, 0.001, 0.001, 0.001, 0.001],</span><br><span class="line">    [0.001, 0.001, 0.001, 0.001, 0.001]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#获取某个词拼音的前N个最有可能的输出</span><br><span class="line">def topn_from_ownword(oneword_per, N=5):</span><br><span class="line">    index = []</span><br><span class="line">    values = []</span><br><span class="line">    oneword_per = np.array(oneword_per)</span><br><span class="line">    #概率进行从小到大的升序排序， 并取最后N个</span><br><span class="line">    for v in np.argsort(oneword_per)[::-1][:N]:</span><br><span class="line">        index.append(v)</span><br><span class="line">        values.append(oneword_per[v])</span><br><span class="line">    return index, values</span><br><span class="line"></span><br><span class="line">#获取一个词组拼音的前N个最有可能的输出</span><br><span class="line">def topn_from_twoword(oneword_per, twoword_per, N=5):</span><br><span class="line">    last = 0</span><br><span class="line">    #前一个拼音对应汉字的个数</span><br><span class="line">    word_num = len(oneword_per)</span><br><span class="line"></span><br><span class="line">    for i in range(word_num):</span><br><span class="line">        #当一个词组，例如当&quot;jintian&quot;，这一对词组，第一个拼音”jin“取”今“, 第二个拼音”tian“取天时的条件概率</span><br><span class="line">        current = np.multiply(oneword_per[i], twoword_per[i])</span><br><span class="line">        if i == 0:</span><br><span class="line">            last = current</span><br><span class="line">        else:</span><br><span class="line">            #将计算出来的概率追加到结果列表中</span><br><span class="line">            last = np.concatenate((last, current), axis=0)</span><br><span class="line"></span><br><span class="line">    index = []</span><br><span class="line">    values = []</span><br><span class="line">    for v in np.argsort(last)[::-1][:N]:</span><br><span class="line">        #计算概率高的词组对应的位置，v/word_num代表第一个汉字在第一个汉字中的位置， v%word_numv代表第二个汉字在第二个汉字中的位置</span><br><span class="line">        index.append([int(v/word_num), v%word_num])</span><br><span class="line">        #last[v]表示概率</span><br><span class="line">        values.append(last[v])</span><br><span class="line">    return index, values</span><br><span class="line"></span><br><span class="line">#推理预测</span><br><span class="line">def predict(word):</span><br><span class="line">    N = 5</span><br><span class="line">    if word == &apos;jin&apos;:</span><br><span class="line">        indexs, _ = topn_from_ownword(jin_per, N)</span><br><span class="line">        for i in indexs:</span><br><span class="line">            print (jin[i])</span><br><span class="line">            </span><br><span class="line">    elif word == &apos;jintian&apos;:</span><br><span class="line">        indexs, _ = topn_from_twoword(jin_per, jintian_per, N)</span><br><span class="line">        for first_index, second_index in indexs:</span><br><span class="line">            print (jin[first_index] + jintian[second_index])</span><br><span class="line">            </span><br><span class="line">    elif word == &apos;wo&apos;:</span><br><span class="line">        indexs, _ = topn_from_ownword(wo_per)</span><br><span class="line">        for i in indexs:</span><br><span class="line">            print (wo[i])</span><br><span class="line">            </span><br><span class="line">    elif word == &apos;women&apos;:</span><br><span class="line">        indexs, _ = topn_from_twoword(wo_per, women_per, N)</span><br><span class="line">        for first_index, second_index in indexs:</span><br><span class="line">            print (wo[first_index] + women[second_index])</span><br><span class="line">            </span><br><span class="line">    elif word == &apos;jintianwo&apos;:</span><br><span class="line">        </span><br><span class="line">        index1, values1 = topn_from_ownword(wo_per, N)</span><br><span class="line">        index2, values2 = topn_from_twoword(jin_per, jintian_per, N)</span><br><span class="line">        last = np.multiply(values1, values2)</span><br><span class="line">        for i in np.argsort(last)[::-1][:N]:</span><br><span class="line">            print (jin[index2[i][0]] + jintian[index2[i][1]] + wo[i])</span><br><span class="line">            </span><br><span class="line">    elif word == &apos;jintianwomen&apos;:</span><br><span class="line">        index1, values1 = topn_from_twoword(jin_per, jintian_per, N)</span><br><span class="line">        index2, values2 = topn_from_twoword(wo_per, women_per, N)</span><br><span class="line">        last = np.multiply(values1, values2)</span><br><span class="line">        for i in np.argsort(last)[::-1][:N]:</span><br><span class="line">            print(jin[index1[i][0]] + jintian[index1[i][1]] + wo[index2[i][0]] + women[index2[i][1]])</span><br><span class="line">    else:</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__mian__&apos;:</span><br><span class="line">    predict(&apos;jin&apos;)</span><br><span class="line">    #近</span><br><span class="line">    #斤</span><br><span class="line">    #今</span><br><span class="line">    #金</span><br><span class="line">    #尽</span><br><span class="line">    predict(&apos;jintian&apos;)</span><br><span class="line">    #今天</span><br><span class="line">    #金田</span><br><span class="line">    #近天</span><br><span class="line">    #近填</span><br><span class="line">    #近田</span><br><span class="line">    predict(&apos;two&apos;)</span><br><span class="line">    #近</span><br><span class="line">    #斤</span><br><span class="line">    #今</span><br><span class="line">    #金</span><br><span class="line">    #尽</span><br><span class="line">    predict(&apos;women&apos;)</span><br><span class="line">    #我们</span><br><span class="line">    #我闷</span><br><span class="line">    #我门</span><br><span class="line">    #我焖</span><br><span class="line">    #我扪</span><br><span class="line">    predict(&apos;jintianwo&apos;)</span><br><span class="line">    #今天我</span><br><span class="line">    #金田窝</span><br><span class="line">    #近天喔</span><br><span class="line">    #近填握</span><br><span class="line">    #近田卧</span><br><span class="line">    predict(&apos;jintianwomen&apos;)</span><br><span class="line">    #今天我们</span><br><span class="line">    #金田我闷</span><br><span class="line">    #近田我扪</span><br><span class="line">    #近填我焖</span><br><span class="line">    #近天我门</span><br></pre></td></tr></table></figure>

</details>



<p>根据算法，示例输入会在输入“jin”、“jintian”、“jintianwo”、“jintianwomen”时分别排序输出：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">“jin”：近、斤、今、金、尽</span><br><span class="line">“jintian”：今天、金田</span><br><span class="line">“jintianwo”：今天我</span><br><span class="line">“jintianwomen”：今天我们</span><br></pre></td></tr></table></figure>

</details>

<h3 id="10-4-2-前向算法"><a href="#10-4-2-前向算法" class="headerlink" title="10.4.2 前向算法"></a>10.4.2 前向算法</h3><p>再来看问题2所涉及的问题，“知道骰子有几种（隐含状态数量）、每种骰子是什么（转换概率）、根据掷骰子掷出的结果（可见状态链），想知道掷出这个结果的概率”，解决这个问题的算法叫做前向算法（Forward Algorithm）。</p>
<p>还是来看这个例子：怀疑自己的六面骰子被赌场动过手脚了，有可能被换成另一种六面骰子，这种六面骰子掷出来是1的概率更大，是1/2，掷出来是2、3、4、5、6的概率是1/10。应该怎么办？答案很简单，算一算正常的3个骰子掷出一段序列的概率，再算一算不正常的六面骰子和另外两个正常骰子掷出这段序列的概率。如果前者比后者小，就要小心了。例如，掷骰子的结果如图10-13所示。</p>
<p>要算用正常的3个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算。同样，简单的方法就是穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。这里应用一个和前一个问题类似的解法，只是前一个问题关心的是概率最大值，这个问题关心的是概率之和。首先，如果只掷一次骰子，如图10-14所示。</p>
<p>结果为1。产生这个结果的总概率可以按照如表10-8所示计算，总概率为0.18（表格中取的都是约等值）。</p>
<p><img src="Image00263.jpg" alt></p>
<p>图10-13 掷骰子的结果</p>
<p><img src="Image00264.jpg" alt></p>
<p>图10-14 掷一次骰子</p>
<p>表10-8 概率</p>
<p><img src="Image00265.jpg" alt></p>
<p>把这个情况拓展，掷两次骰子，如图10-15所示。</p>
<p><img src="Image00266.jpg" alt></p>
<p>图10-15 掷两次骰子</p>
<p>概率如表10-9所示。</p>
<p>表10-9 概率</p>
<p><img src="Image00267.jpg" alt></p>
<p>继续拓展，计算掷骰子3次的情况，如表10-10所示。</p>
<p>同样，如果有更长的掷骰子序列，也能进行统计和计算。用一样的方法进行计算，可以算出正常的六面骰子和另外两个正常骰子掷出这个序列的概率，然后比较一下两个序列概率的大小就能知道骰子是否被人置换过。在这个例子里，如果发现使用的骰子掷出来的序列的出现概率比计算出来的“标准”概率低很多或者高很多，那就很可能是被置换过的。</p>
<p>表10-10 概率</p>
<p><img src="Image00268.jpg" alt></p>
<h2 id="10-5-支持向量机SVM"><a href="#10-5-支持向量机SVM" class="headerlink" title="10.5 支持向量机SVM"></a>10.5 支持向量机SVM</h2><p>支持向量机SVM是一种比较抽象的算法概念，全称是Support Vector Machine，它可以用来做模式识别、分类或者回归的机器学习。</p>
<p>前面介绍过机器学习是为了解决样本和具体分类映射的问题，构造一个算法，把已知样本的特征和分类情况做一个逻辑映射关系，这样碰到新样本时就能用这个算法把它进行分类了。</p>
<p>初高中数学里学的不少数学概念其实已经是分类的概念了，只是太稀松平常，从学术上不能把它们归为机器学习的算法，因为这简直无须学习。例如，大于零的实数叫正数，小于零的实数叫负数。这是一个定义，但同时也是一个算法。数学表达式如下：</p>
<p><img src="Image00269.jpg" alt></p>
<p>所以，如果有一个实数x，判断x属于正数（Positive）还是负数（Negative），只要在这个算法里一套，立刻就能进行分类，这简直是太棒了。</p>
<p>这个过程其实不是一个机器学习过程，细心的读者应该能注意到，这里似乎看不到有“学习”这个过程存在。没错，这个方法和我们在“机器学习”之前学习的算法书写方式几乎是一样的，都是人来告诉计算机判定的定义如何，然后计算机根据这个判定的定义来处理每一个待判定的对象。这里面计算机确实没有这个学习的过程。</p>
<p>什么情况下就算开始学习的过程了呢？下面把上述例子“进化”。</p>
<h3 id="10-5-1-年龄和好坏"><a href="#10-5-1-年龄和好坏" class="headerlink" title="10.5.1 年龄和好坏"></a>10.5.1 年龄和好坏</h3><p>如果到了某公司工作，领导交代任务：“来，把这些客户给我分分类。看看什么样的用户质量比较高，什么样的用户质量比较低。以后业务部门去拓展就能提高客户发展的效率。”客户信息列表如表10-11所示。</p>
<p>表10-11 客户信息</p>
<p><img src="Image00270.jpg" alt></p>
<p>在这个列表中，只能看到“客户年龄”和“客户质量”两个列，这个例子已经是比较极端的例子了，因为在这里从一开始就认为只有客户年龄有可能会跟客户质量有关系。客户质量是由具体的业务部门根据他们的评价标准来做的衡量，衡量标准是不是合理暂且不管——因为不管他们内部用什么衡量标准，肯定是有他们自己的评价体系去做，总之客户质量是能够这样描述就行了。再来看“客户年龄”这一列，这个列表在真实情况下有可能会非常长，如果有10000个客户，那就可能有10000行，这里示意性地找出10行来演示，10000行也是同样的原理。那在这个有限信息的模型里只能通过年龄来对客户质量好不好进行评价了，如图10-16所示。</p>
<p><img src="Image00271.jpg" alt></p>
<p>图10-16 客户信息的数轴表示</p>
<p>从图10-16中可以看出，30以上的都好，25以下的都不好。也就是说可以考虑在“客户年龄”字段的30和25中间切一刀，一边是好一边是不好，那么切在哪儿好呢？直观上看，似乎应该是切在（30+25）/2的位置，也就是27.5，这基本上也是从目前的情况能够得到的最为合理的方案。例如，来了一个27岁的新客户，那么他更有可能是“好”的客户还是“不好”的客户呢？因为看起来离“不好”客户这边更近，所以算“不好”的客户应该更合理，如图10-17所示。</p>
<p><img src="Image00272.jpg" alt></p>
<p>图10-17 以27.5为分界</p>
<p>如果这种例子怎么办？例如，客户信息列表如表10-12所示。</p>
<p>表10-12 客户信息</p>
<p><img src="Image00273.jpg" alt></p>
<p>数轴表示如图10-18所示。</p>
<p><img src="Image00274.jpg" alt></p>
<p>图10-18 客户信息的数轴表示</p>
<p>这个就比较麻烦了，因为发现没有办法做到一刀切，确实做不到，怎么切都至少有一方“成分不纯”。那么现在有以下两种选择。</p>
<p>（1）把这两类全部标出来：</p>
<p><img src="Image00275.jpg" alt></p>
<p>这又是一个分段函数一样的标记，准确，但是标记起来相当啰嗦。别忘了，你可能是在10000个甚至更多的客户对象里去做这个事情，最后函数描述会不会一张A4纸都印不下可说不准了，毕竟这个关系是从得到的数据里归纳而来的，实际有多复杂只有看实际情况了。所以这种标记方法容易产生的问题就是过拟。过拟在第8章已经接触过了，即为了逼近“事实”而让描述变得过于复杂的情况。</p>
<p>（2）一刀切。反正只要一刀切会非常简洁，如果这一刀切下去虽然两边的类都不纯或者一边的类不纯，但是只要不纯的程度在能容忍的范围内就可以。</p>
<p><img src="Image00276.jpg" alt></p>
<p>这次还是试着从27.5切，大于等于27.5的就算是“好”，小于27.5的算是“不好”。那么这个分类中“好”类的“不纯度”为1/6，约为16.7%；“不好”类的“不纯度”为1/4，即25%。这个比率反正在我看来还是挺高的，一般来说，“不纯度”肯定是越低越好，在这个场景里不确定16.7%的不纯度是不是能够满足领导的需要。如果说原来在不做这种数据分析的情况下，发展10个客户，有6个是“好”客户，4个是“不好”的客户；现在改进后虽然有一定的误判率——由于分类不纯的问题，但是直接过滤掉一些对象，发展10个客户有8.3个是“好”的客户，还有大概1.7个是“不好”的客户。从数值上看，方案策略的提升应该是有改进的。</p>
<p>总结一下，上述例子里有以下几个关键性的信息。</p>
<p>关键点1：<br>切下去点在SVM算法体系里叫“超平面”。这个“超平面”是一个抽象的面概念，在一维空间里就是一个点，用x+A=0来表示；二维空间里就是一条线，用Ax+By+C=0的直线来表示；三维空间里就是一个面，用Ax+By+Cz+D=0的平面来表示；四维空间就想象这个面的存在吧，但是可以推断出应该是用Ax+By+Cz+Dα+E=0来表示，以此类推……上述4个方程都可以变形为</p>
<p><img src="Image00277.jpg" alt></p>
<p>的这种形式。</p>
<p>在上述例子中，相当于是在一个一维空间里在x=27.5的位置画了一个“超平面”（零维的点）。即x-27.5=0这个方程就是超平面。即x-27.5＞0的都是分类为“好”的样本，x-27.5＜0的则都不是分类为“好”的样本。</p>
<p>关键点2：<br>过拟问题，一般来说，设计分类器都是要尽量避免过拟的。过拟会给归纳过程带来很大的麻烦，而且在应用的过程中也非常不方便，只要精确度达到标准就足够了。什么是精确度，就是要说的最后一点。</p>
<p>关键点3：<br>不纯度问题。不纯度和精确度是一对矛盾，精确度越高那么不纯度就越低，反之，不纯度越高精确度就越低。分类器的研究和调整的过程是一个精度和成本平衡的过程，所以并不是不纯度越低越好，而是在实际生产中操作成本一样的情况下，不纯度越低越好。</p>
<h3 id="10-5-2-“下刀”不容易"><a href="#10-5-2-“下刀”不容易" class="headerlink" title="10.5.2 “下刀”不容易"></a>10.5.2 “下刀”不容易</h3><p>在上一个例子里发现样本在维度单纯（只有一个年龄维度），而且“分界线”比较清晰的情况下，几乎没有用到任何超过高中及以上学历所学的知识内容就能轻易解决。</p>
<p>如果x的分布复杂一些，情况会怎么样？</p>
<p>下面举一个三角函数作为分界线的例子，如图10-19所示。</p>
<p><img src="Image00278.jpg" alt></p>
<p>图10-19 sinx的图形</p>
<p>图10-19中有两类样本，一类是用圆圈标识出来的A类别，一类是用三角标识出来的B类别。如果碰巧遇到这种情况，在一个（x，y）二维向量组成的空间上有大量的样本（这比在只有一个空间维度年龄x要复杂一些），但是恰好两类样本能用一个函数巧妙地分开，能保证简洁（不过拟）和精确（不纯度极低）。如果y=sin（x）这样的曲线能够恰好把它们分开，将是一条非常简洁的分割线（虽然不是直线）。</p>
<p>但是，机器学习的方法基础都是统计和归纳，从大量样本空间来的样本可没有那么简单，如果找不到一个像y=sin（x）这么简洁的曲线来分割怎么办？是不是只能面临过拟的情况？如果下一个是y=cos（x），或者y=sin（x）+log（x）怎么办，会每次那么幸运都能轻易找到一条简洁的曲线来解决问题吗？</p>
<p>人们研究机器学习的目的是使用具备广谱性的算法思路来解决多样的事情，如果解决每一个问题都是不同的解法，甚至思路迥异，那这种Case by Case的方式完全不能被称为一类解决方法。而现在讨论的支持向量机SVM就是要解决这类问题，它也号称是万能分类器。</p>
<h3 id="10-5-3-距离有多远"><a href="#10-5-3-距离有多远" class="headerlink" title="10.5.3 距离有多远"></a>10.5.3 距离有多远</h3><p>在一个平面直角坐标系中，有一些样本点作为训练点，一些被标记为类别X，一些标记为非类别X。也就是样本训练中，一些样本明确标出是这个分类，而另一些样本标出不是这个分类。</p>
<p>最朴素的想法是，如果能找到一条直线Ax+By+C=0能够恰好把它们区分成两个部分就最好了。那么这条线应该怎么来找呢？</p>
<p>如果真的像图10-19所画这样，那么确实会有一条直线Ax+By+C=0，问题是就像用逐差法求解重力加速度g的过程一样，这个Ax+By+C=0有很多种画法。在求解重力加速度g的那一节讨论过，有一种叫做最小二乘法的办法可以求出一个使得回归后误差最小的解。那么在求解这个Ax+By+C=0的过程中也是为了求解出适当的A和B，让这两个组分开最为恰当。恰当的标准是让类别X中与该直线最近的样本点的距离和非类别X中的样本点中与该直线的距离最大。</p>
<p>这和回归中希望拟合出来的曲线和得到的样本尽可能贴近这一观点接近，这里画出的这条直线是希望让被分开的两个类别（类别X和非类别X）尽可能远，也就是相差越远分得越开。相反，如果划了一条线几乎要同时经过一个类别X的样本点和一个非类别X的样本点，那出现一个新的和X很“像”的样本点则很容易被画到非类别X中去，这和期望的形态是不一样的。</p>
<p>在平面直角坐标系中，如果有一条直线，方程是Ax+By+C=0，那么点（x0 ，y0 ）到该直线的距离如下：</p>
<p><img src="Image00279.jpg" alt></p>
<p>如果数轴上也需要用类似：“x-27.5＞0的都是分类为‘好’的样本，x-27.5＜0的则都不是分类为‘好’的样本。”这种方式来做分割，有了Ax+By+C=0这样一个方程等于有了一个工具，可以发现，所有X类别中的样本点都满足Ax+By+C＞0，而反之所有非X类别中的样本点都满足Ax+By+C＜0，当然这个结果也是最开始构造Ax+By+C=0这条线的目的。</p>
<p>推广一下，三维空间上的超平面划分后两个分类（一个是目标分类，一个不是目标分类）表示就是一个满足Ax+By+Cz+D＞0另一个满足Ax+By+Cz+D＜0。</p>
<p>四维空间上则是Ax+By+Cz+Dα+E＞0和Ax+By+Cz+Dα+E＜0。</p>
<p>其实样子看上去相当类似，都是各维度坐标点前面乘以一个系数之后再加一个常数。可以把这个超平面的公式简写成</p>
<p><img src="Image00280.jpg" alt></p>
<p>v是样本向量，b是常数。</p>
<p>在二维空间里，v就是（x，y），在三维空间里v就是（x，y，z），其余情况以此类推；而w也是一个向量，在二维空间里w就是（A，B），在四维空间里w就是（A，B，C），其余情况以此类推。而wt在二维空间里就是Ax+By，在三维空间里就是Ax+By+Cz。如果看到</p>
<p><img src="Image00281.jpg" alt></p>
<p>或者</p>
<p><img src="Image00282.jpg" alt></p>
<p>或者</p>
<p><img src="Image00283.jpg" alt></p>
<p>这几种形式意思是一样的，其中的x都不是一个实数值而是在不同维度空间下的向量。</p>
<h3 id="10-5-4-N维度空间中的距离"><a href="#10-5-4-N维度空间中的距离" class="headerlink" title="10.5.4 N维度空间中的距离"></a>10.5.4 N维度空间中的距离</h3><p>在g（v）=wv+b这种定义方式中，w是一个1×n的矩阵，v是一个n×1的矩阵，wv就是两个矩阵做了一个内积，也就是<img src="Image00284.jpg" alt><br>，n是维度的数量，wi 就是每个维度变量前面的系数，vi 就是每个维度变量。</p>
<p>在二维空间内，一个点（x0 ，y0 ）到一条直线Ax+By+C=0的距离如下：</p>
<p><img src="Image00285.jpg" alt></p>
<p>在三维空间内，一个点（x0 ，y0 ，z0 ）到一个平面Ax+By+Cz+D=0的距离如下：</p>
<p><img src="Image00286.jpg" alt></p>
<p>在一维空间内，一个点（x0 ）到一个点Ax+B=0的距离如下：</p>
<p><img src="Image00287.jpg" alt></p>
<p>但同时</p>
<p><img src="Image00288.jpg" alt></p>
<p>规律性已经非常明显了。</p>
<p>在有些书上会介绍一个叫做范数的概念，上述<img src="Image00289.jpg" alt> 、<img src="Image00290.jpg" alt><br>、<img src="Image00291.jpg" alt><br>等分别就是一维空间、二维空间、三维空间中的范数，写作||w||。在不同的维度下，这个范数||w||的具体值是不一样的，是超平面方程的各维度系数的平方和再开方，是一个和超平面描述方程系数相关的定值。</p>
<p>这样一来，距离公式就可以简写成</p>
<p><img src="Image00292.jpg" alt></p>
<p>维度确定时这个公式中所有的值都是确定的。要判断一个点到这个平面的距离，只要把空间坐标代入v即可。</p>
<h3 id="10-5-4-N维度空间中的距离-1"><a href="#10-5-4-N维度空间中的距离-1" class="headerlink" title="10.5.4 N维度空间中的距离"></a>10.5.4 N维度空间中的距离</h3><p>在g（v）=wv+b这种定义方式中，w是一个1×n的矩阵，v是一个n×1的矩阵，wv就是两个矩阵做了一个内积，也就是<img src="Image00284.jpg" alt><br>，n是维度的数量，wi 就是每个维度变量前面的系数，vi 就是每个维度变量。</p>
<p>在二维空间内，一个点（x0 ，y0 ）到一条直线Ax+By+C=0的距离如下：</p>
<p><img src="Image00285.jpg" alt></p>
<p>在三维空间内，一个点（x0 ，y0 ，z0 ）到一个平面Ax+By+Cz+D=0的距离如下：</p>
<p><img src="Image00286.jpg" alt></p>
<p>在一维空间内，一个点（x0 ）到一个点Ax+B=0的距离如下：</p>
<p><img src="Image00287.jpg" alt></p>
<p>但同时</p>
<p><img src="Image00288.jpg" alt></p>
<p>规律性已经非常明显了。</p>
<p>在有些书上会介绍一个叫做范数的概念，上述<img src="Image00289.jpg" alt> 、<img src="Image00290.jpg" alt><br>、<img src="Image00291.jpg" alt><br>等分别就是一维空间、二维空间、三维空间中的范数，写作||w||。在不同的维度下，这个范数||w||的具体值是不一样的，是超平面方程的各维度系数的平方和再开方，是一个和超平面描述方程系数相关的定值。</p>
<p>这样一来，距离公式就可以简写成</p>
<p><img src="Image00292.jpg" alt></p>
<p>维度确定时这个公式中所有的值都是确定的。要判断一个点到这个平面的距离，只要把空间坐标代入v即可。</p>
<h3 id="10-5-6-分不开怎么办"><a href="#10-5-6-分不开怎么办" class="headerlink" title="10.5.6 分不开怎么办"></a>10.5.6 分不开怎么办</h3><p>由于得到的众多的（v，y）样本分类是不确定的——v为多维空间上的样本，y为分类只有0和1两种状态。所以确实没有办法保证在当前的空间上一定能有一个超平面把它们清晰地隔开，或者说不管怎么画，这个超平面都会产生让人不能忍受的误判率，如图10-20所示。</p>
<p><img src="Image00293.jpg" alt></p>
<p>图10-20 两种分类</p>
<p>图10-20所示为由白点和黑点表示的两种分类，不管怎么画这个超平面都会有非常大的误判率，这种情况也叫做线性不可分。在3个维度以下，用肉眼观察有限的几个样本还是能够一目了然地做判断的，可以判断是不是线性可分，如果维度超过3个或者样本数量太多就不好观察了，这时要使用计算机进行计算。可以转化一种思路来解决这个问题了，即升维——这才是SVM算法最为吸引人的部分。</p>
<p>先来看一个一维数据的例子，来说明升维的概念。</p>
<p>假设在数轴上给出一些数据，其中[-2，2]区间内的被标记成了分类1，其余的都是分类0，能够给出一个分段函数吗？似乎不能。怎么去定义这个切开的点都会产生大量的误分类。但是换一个办法可能就能顺利地解决。例如，在这个例子里可以构造一下，在[-2，2]这个区间里让一个函数大于0，而在其他部分小于0。例如，把分类函数写成以下形式：</p>
<p><img src="Image00294.jpg" alt></p>
<p>或者可以认为</p>
<p><img src="Image00295.jpg" alt></p>
<p>且</p>
<p><img src="Image00296.jpg" alt></p>
<p>而</p>
<p><img src="Image00297.jpg" alt></p>
<p>y=-x2 +4图形如图10-21所示，这实际上是y=-x2 +4这个函数在y=0（x轴）这条直线上的投影。</p>
<p><img src="Image00298.jpg" alt></p>
<p>图10-21 y=-x2+4的图形</p>
<p>同样，在二维空间中也有类似的方式，例如，如果样本向量v距离原点（0，0）的距离为1以内分类被标记为0，其余都是1。同样是线性不可分，但是可以构造一个这样的函数：</p>
<p><img src="Image00299.jpg" alt></p>
<p>或者可以这么认为：</p>
<p><img src="Image00300.jpg" alt></p>
<p>而</p>
<p><img src="Image00301.jpg" alt></p>
<p>x2 +y2 -1的图形如图10-22所示。</p>
<p><img src="Image00302.jpg" alt></p>
<p>图10-22 x2 +y2 -1的图形</p>
<p>同样用非常简洁的方式解决了这个分类问题。不过可能有的朋友要说了，这个也太凑巧了吧，我们不可能每次都这么幸运找到这种函数通过观察就很快构造出分类函数（超平面方程）。没错，我们平时生产生活中遇到的例子基本都是线性不可分的，SVM就是要解决这个问题。</p>
<p>可以看到，在一维空间上解决线性不可分问题是把函数映射到二维空间，使得一维空间上的分类边界是二维空间上的分类函数在一维空间上的投影；而在二维空间上解决线性不可分问题是把函数映射到三维空间，使得二维空间上的分类边界是三维空间上的分类函数在二维空间上的投影。那么所有的n维空间上的线性不可分的问题都可以考虑映射到n+1维上去构造分类函数，使得它在n维空间上的投影能够将两个类别分开。</p>
<p>这个构造过程SVM是有通用的方法可以解决的，就是使用核函数（Kernel）进行构造。而且，有几个常用的核函数是可以拿来直接使用的，如线性核函数、多项式核函数、径向基核函数（RBF核函数）、高斯核函数等，能够查到的核函数有二三十种之多。</p>
<p>核函数的目的很单纯，即只要在当前维度空间的样本是线性不可分的，就一律映射到更高的维度上去，在更高的维度上找到超平面，得到超平面方程。而在更高维度上的超平面方程实际并没有增加更多的维度变量，更高的这个维度只是像在解几何题里使用的辅助线而已，最后得到的方程不会增加其他维度。例如，研究二维空间上的向量分类问题，那么经过核函数映射，最后得到的超平面变成了二维空间上的曲线（但同时也是三维空间上的一次方程）；研究三维空间上的向量分类问题，那么经过核函数映射，最后得到的超平面变成了三维空间上的曲面（但同时也是四维空间上的一次方程）。函数表示只是一个变量代换关系。</p>
<h3 id="10-5-7-示例"><a href="#10-5-7-示例" class="headerlink" title="10.5.7 示例"></a>10.5.7 示例</h3><p>为了方便说明，这里还是用熟悉的例子给出一个分类用法的示例，客户信息列表如表10-13所示。</p>
<p>表10-13 客户信息</p>
<p><img src="Image00303.jpg" alt></p>
<p>这个例子很极端了，因为客户年龄和客户质量之间已经是没办法做线性分割了。这时可以用SVM来做分类。</p>
<p>在Python的Scikit-learn库中，用到的是一个叫做SVC的类，SVC是Support Vector Classification的缩写，即支持向量分类机。SVC所支持的核函数包括linear（线性核函数）、poly（多项式核函数）、rbf（径向基核函数）、sigmoid（神经元激活核函数）、precomputed（自定义核函数），默认使用rbf核函数。</p>
<p>在这个例子中，将使用rbf核函数，这也是最适合做非线性关系分类标准的首选核函数。如果这几种核函数实在不知道该用哪个，那就在实际场景中多做对比测试，看看哪一种的正确率更高，切莫纠结于学术层面的推导困难而不敢实践。代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import svm</span><br><span class="line">#年龄</span><br><span class="line">X = [[34]， [33]， [32]， [31]， [30]， [30]， [25]， [23]， [22]， [18]]</span><br><span class="line">#质量</span><br><span class="line">y = [1， 0， 1， 0， 1， 1， 0， 1， 0， 1]</span><br><span class="line">#现在把训练数据和对应的分类放入分类器中进行训练</span><br><span class="line">#这里使用rbf</span><br><span class="line">clf = svm.SVC（kernel=&apos;rbf&apos;）.fit（X， y）</span><br><span class="line">#预测年龄30的人的质量</span><br><span class="line">p = [[30]]</span><br><span class="line">print clf.predict（p）</span><br><span class="line">#结果是[1]</span><br></pre></td></tr></table></figure>

</details>

<h3 id="10-5-8-小结"><a href="#10-5-8-小结" class="headerlink" title="10.5.8 小结"></a>10.5.8 小结</h3><p>SVM解决问题的方法描述起来大概有以下几步。</p>
<p>（1）把所有的样本和其对应的分类标记交给算法进行训练。</p>
<p>（2）如果发现线性可分，那就直接找出超平面。</p>
<p>（3）如果发现线性不可分，那就映射到n+1维空间，找出超平面。</p>
<p>（4）最后得到超平面的表达式，也就是分类函数。</p>
<p>过程比较简单，只是实现起来的算法确实比较复杂。</p>
<h2 id="10-6-遗传算法"><a href="#10-6-遗传算法" class="headerlink" title="10.6 遗传算法"></a>10.6 遗传算法</h2><p>在本章的最后介绍一下遗传算法（Genetic Algorithm）。</p>
<p>与其说遗传算法是一个算法，不如说是一种处理问题的思想方式更为恰当，因为遗传算法整个体系都是在说对于一种问题处理的思路和原则，而不是一个具体的代码编写过程。</p>
<p>遗传算法是一类借鉴生物界的进化规律（适者生存，优胜劣汰遗传机制）演化而来的随机化搜索方法。它是由美国的J.Holland教授于1975年首先提出的，但是它借鉴的是进化论的理论依据。在这个体系里，思维方式远比编写代码重要，所以先介绍一下著名的英国生物学家查尔斯·罗伯特·达尔文——进化论的奠基人。</p>
<p>进化论不是一本书，而是一个有关物种发源与发展的逻辑体系。达尔文在1859年出版了《物种起源》，由此开创的“物竞天择，适者生存”的进化论体系被科学界公认为19世纪自然科学的三大发现之一（另外两个是细胞学说和能量守恒与转化定律）。达尔文于1882年4月病逝，为了表示对这位科学家的崇敬，大家把他安葬在牛顿墓的旁边，位于英国伦敦的威斯敏斯特大教堂（也叫西敏寺）。</p>
<h3 id="10-6-1-进化过程"><a href="#10-6-1-进化过程" class="headerlink" title="10.6.1 进化过程"></a>10.6.1 进化过程</h3><p>在物种进化中的例子中，最开始地球上只有海洋没有陆地，从单细胞动物到鱼不知道进化了多少代。这里以从陆地出现以后，第一批从海里想上岸去的鱼。</p>
<p>估计是一开始有一群鱼，它们由于种种原因跟其他的鱼有了差别，如胆儿比较大，有一个能直接呼吸空气的鳃——能在空气里还能存活，有格外强壮的鳍——能跑得快一些。再后来这些鱼里就有技能更强的，一些在陆上格外倚重的器官如果变异强大了就会继续支持它们在陆上生活，如粗壮的鳍、空气摄入能力更强的肺、更好的眼神等。适应的种群才会在相应的环境里生存下来。</p>
<p>根据进化论的观点“物竞天择，适者生存”，生物自己是会进行一代一代变化的。这个变化本来是没有什么方向的，生物自己也控制不了。变化由什么而来？第一，父母的基因进行交换重组；第二，基因突变。</p>
<p>一代一代在整个种群的不同个体里有无数次重组的机会，也有一定的基因突变的机会，导致了若干代以后，同一个种群之间的样态可能会非常不一样。例如，人有23对染色体，每一代的重组和突变使得人的多样性特点非常明显。现在人的长相彼此有很大差别，以及体态、性格、思维方式、疾病抵御……这些方面也都是千差万别。</p>
<p>尤其是人类历史上经历过若干次大的瘟疫——14世纪欧洲的黑死病造成全世界超过7000万人死亡；17世纪在欧洲肆虐的天花病毒也杀死超过4000万的欧洲人。14世纪得了黑死病（是一种鼠疫杆菌）基本就是没救，防治天花的牛痘疫苗也是到了18世纪才研发出来，能够存活下来的人群其实也并没有接受像样治疗，科学家分析基本只能解释为基因层面对抗的胜利——这些存活的人的基因比那些患病死去的人有着对抗这种疾病更强有力的成分。</p>
<p>不论是由于基因自身发生的突变，还是由于组合产生的新特性，这些都是不确定性的变化。而客观世界上有很多变化对人类种群做出这种选择的裁剪，出现疾病就是裁剪那些对疾病耐受力弱的人，出现饥荒就是裁剪那些对饥饿耐受力弱的人，出现严寒就是裁剪那些对严寒耐受力弱的人。这种“进化”可以说谈不到“进化”，而是被动地演化而后被动地被选择。</p>
<h3 id="10-6-2-算法过程"><a href="#10-6-2-算法过程" class="headerlink" title="10.6.2 算法过程"></a>10.6.2 算法过程</h3><p>在了解了进化基因层面的过程后，是要落实到算法过程上去的。那么怎么来构建这个算法过程呢？关键步骤如下。</p>
<p>（1）基因编码。在这个过程中，尝试着对一些个体的基因做一个描述，构造这些基因的结构，有点像确定函数自变量的过程。</p>
<p>（2）设计初始群体。在这个环节，需要造一个种群出来，这些种群有很多生物个体但是基因都不相同。</p>
<p>（3）适应度计算（剪枝）。在这个环节，要造一些“上帝的剪刀”对那些不太适应的个体进行裁剪，不让他们产生后代。和最终遴选规则差异大的个体肯定不适合作为备选对象，该减掉一定要减掉，否则它产生的后代只会让计算量更大而距离逼近目标没有增益。</p>
<p>（4）产生下一代。产生下一代这个部分有3种办法：直接选择，基因重组，基因突变。</p>
<p>而后再回到步骤3进行循环，适应度计算，产生下一代，这样一代一代找下去，直到找到最优解为止。</p>
<p>遗传算法在解决很多领域的问题时都体现出很好的特性，如TSP问题（Traveling Salesman Problem，即旅行商问题，也叫货郎担问题）、九宫问题（八数码问题）、生产调度问题（Job Shop Scheduling）、背包问题（Knapsack Problem，即NP问题）等。</p>
<p>下面介绍一个具体的解题实例。</p>
<h3 id="10-6-3-背包问题"><a href="#10-6-3-背包问题" class="headerlink" title="10.6.3 背包问题"></a>10.6.3 背包问题</h3><p>背包问题是一种组合优化的NP（即多项式复杂程度的非确定性问题）完全问题，这类NP问题的特点很明显，即“生成问题的一个解通常比验证一个给定的解需要花费更多的时间”。</p>
<p>先来看一个例子。</p>
<p>第一个例子是合数分解质因数问题，如果现在有一个命题，分解合数698975355227为几个质数相乘的结果。那么怎么算？只能把质数从2穷举到<img src="Image00304.jpg" alt><br>，2个质数一起、3个质数一起、4个质数一起……尝试着做各种组合相乘看是否等于这个结果，这是一个非常耗时的工作，即便用计算机，这个过程也非常耗时。但是如果反过来说，698975355227是否是809、887、977、997这4个素数的乘积，那么验证起来会非常快，只要用一个普通的计算器，用不了多长时间就能验算出来。这就是所谓的“生成问题的一个解通常比验证一个给定的解需要花费更多的时间”。</p>
<p>背包问题的大意是，有N件物品和一个容量为V的背包，第i件物品的重量是w[i]，价值是v[i]，求解将哪些物品装入背包可使这些物品的重量总和不超过背包容量，且价值总和最大。</p>
<p>这种问题就是典型的NP问题，验证一个猜想的解比算出一组解要快得多。</p>
<p>我们来具体看一些数字可能会更直观一些。</p>
<p>假设有一个背包，可以放置80公斤的物品。此外，还有如表10-14所示的6件物品。</p>
<p>表10-14 6件物品</p>
<p><img src="Image00305.jpg" alt></p>
<p>怎么放置才能让背包里的物品价值总和尽可能多呢？</p>
<p>有几种思路，第一种就是穷举法。每种物品只有存在（1）和不存在（0）两种状态，那么一共可能产生的方案最多是26个也就是64个。把每种组合具体的重量和价值都算出来，超过80公斤的方案就直接舍去，少于80公斤的就留下并且记下货品总价值，到最后比较每种方案的总价值大小即可。</p>
<p>但是如果有128种物品这个方案还可行吗？如果穷举大约有3.4×1038 种方案，如果计算机一秒能够验证一亿种情况，大概需要1.08×1023 年才能算出来，这是不可行的。</p>
<p>这种情况下，遗传算法就显示出优势来了。下面介绍在6个物品的情况下的求解方法。</p>
<p>1.基因编码</p>
<p>一共6种物品，每种物品的有无都可以作为独立的一个基因片段，如表10-15所示。</p>
<p>表10-15 基因片段</p>
<p><img src="Image00306.jpg" alt></p>
<p>一共是6位。假如只有物品2、物品3和物品6，则染色体是011001。</p>
<p>2.设计初始群体</p>
<p>为了计算方便设置初始群体为4个初始生物个体，随机产生。</p>
<p>100100，对应物品1、物品4存在。</p>
<p>101010，对应物品1、物品3、物品5存在。</p>
<p>010101，对应物品2、物品4、物品6存在。</p>
<p>101011，对应物品1、物品3、物品5、物品6存在。</p>
<p>3.适应度计算</p>
<p>适应度计算首先要用一个适应度的函数来做标尺。</p>
<p>设计适应度的函数为物品总价值，那么4个个体各自适应度结果如下。</p>
<p>基因100100=15+45=60。</p>
<p>基因101010=15+35+55=105。</p>
<p>基因010101=25+45+70=140。</p>
<p>基因101011=15+35+55+70=175。</p>
<p>在这里不要忘记物品本身还有重量，所以重量也要作为判断标准之一，各自的重量分别如下。</p>
<p>基因100100=10+25=35。</p>
<p>基因101010=10+20+30=60。</p>
<p>基因010101=15+25+35=75。</p>
<p>基因101011=10+20+30+35=95。</p>
<p>基因101011的重量显然已经超过了要求的80公斤而直接被淘汰。</p>
<p>这样还剩下：</p>
<p>基因100100，适应函数为60。</p>
<p>基因101010，适应函数为105。</p>
<p>基因010101，适应函数为140。</p>
<p>先把适应函数求和，60+105+140=305。下面进行一个用类似轮盘赌来进行遴选的过程。</p>
<p><img src="Image00307.jpg" alt></p>
<p>图10-23 轮盘赌</p>
<p>轮盘赌是一种赌博游戏，如图10-23所示，整个游戏道具是一个大木盘，可以进行旋转。木盘上刻着很多小格子，每个小格子上有数字，在轮盘开始旋转之后，放入一个小球沿盘面滚动与轮盘旋转方向相反。待轮盘静止后，小球掉入的各自所对应的标号即为获胜号码。从古典概型来看，每个格子的胜率应该是一样的。</p>
<p>现在想象一下，这个轮盘上有305个格子，其中基因100100作为一个玩家选取了其中的60个小格子，基因101010作为一个玩家选用了105个小格子，基因010101作为一个玩家选择了140个小格子，分别作为自己押注的赌点。</p>
<p>转动4次——这个推荐为每一代个体的数量。那么每旋转一次，“中奖”的这个基因组就允许繁殖一次，如果一次都没“中奖”那么这个基因将无法得到延续。在这个例子里，基因100100每次被遴选的概率为60/305，基因101010每次被遴选的概率为105/305，基因010101每次被遴选的概率为140/305。</p>
<p>这里计算的结果为，基因101010和基因010101各繁殖两次</p>
<p>4.生产下一代</p>
<p>基因101010和基因010101在成功被遴选后，需要进行基因重组来产生下一代。计算过程如表10-16所示。</p>
<p>表10-16 计算过程</p>
<p><img src="Image00308.jpg" alt></p>
<p>两个被遴选后的基因进行了基因重组，其中一对从第三位后面断开，尾部进行了交换，另一对从第四位后面断开，尾部进行了交换。这样又产生了4个不同的基因。一般来说交叉点位置是可以随机选取的。如图10-24 <a href="#ch1_back">[1]</a><br>所示，两段不同的基因从中间断开后进行结合，上段的前半部和下段的后半部结合成为新的基因，而下段的前半部和上段的后半部结合成为新的基因。</p>
<p><img src="Image00309.jpg" alt></p>
<p>图10-24 基因重组</p>
<p>在基因重组之后是可以有一个基因突变的过程的，就是随机把一定比例的基因里的某一位或者某几位做变化——1变成0，0变成1。这个过程建议还是取法一般的生物繁殖过程，让变异的基因比率低一些比较好，在这个例子里没有做变异。</p>
<p>5.迭代计算</p>
<p>下面就是一代一代用这种准则做下去了，直接求重量和价值。</p>
<p>基因101101，重量为90，价值（适应函数）为165，直接淘汰。</p>
<p>基因010010，重量为45，价值（适应函数）为80。</p>
<p>基因101001，重量为65，价值（适应函数）为120。</p>
<p>基因010110，重量为70，价值（适应函数）为125。</p>
<p>在这里可以看到一个现象，总体的适应函数和为80+120+125=325，比上一代的60+105+140=305适应性更好，貌似是进化了，但是上一代是有一个适应函数140的“超强基因个体”的，这一代却没有一个能够超越。</p>
<p>在一次完整的计算中，迭代过程可能会经历几十代甚至更久，如果发现出现了连续几代适应函数基本不增加或者甚至反而减少的情况，那就说明函数已经收敛了。</p>
<p>“收敛”这个词如果没有在算法学习中接触过，这里以一个形象的例子来说明，在体重秤上称量时，当人站上去时，指针就开始抖动，抖动幅度越来越小，最后基本稳定在一个值。稳定后，读取这个数字即可。假设体重秤称量是有算法控制的，那么这个摆动几下很快就能稳定在一个值的就是收敛性比较快（比较好）的算法；要摆动很久才能稳定的就是收敛性比较慢（比较差）的算法；如果摆幅随着时间的推移反而越来越大，那收敛性就非常不好，通常就没有解。</p>
<p>在上述例子中，可以就此结束迭代操作，也可以再观察一代到两代的变化。收敛的速度会因很多因素而变化，如基因位的长度、基因重组时的方案、基因变异的程度、每一代产生个体的数量等。一般发生适应函数收敛时就是迭代结束时。而在迭代结束前找到的最优的解就是要的解。</p>
<p>6.注意事项</p>
<p>在使用遗传算法的时候请注意以下几个地方，这几个地方是可以进行调整的。</p>
<p>（1）初始群体。初始群体的数量是可以调整的，可以想象，上述6个物品的背包问题的极限是直接生成所有的情况，26 也就是64个个体全部列出作为初始群体。但是这毫无意义，也不是要使用基因算法的目的。或许可以考虑初始群体的数量设置为N个，N为当前计算机最大可并行计算的数量，例如，是8核心的计算机，那就可以设置为8个个体作为初始群体。在每次产生基因后把不同的计算放到不同的线程中去。当然，这也要视并行对算法效率的改善程度而定。此外，可以定性考虑，初始数量太少可能会导致在向量空间中覆盖面积过小而导致收敛到了非最优解就终止了算法。</p>
<p>（2）适应度函数。适应度函数中的轮盘赌算法只是其中一种算法，也可以考虑使用其他算法进行遴选。注意遴选原则是从生物多样化中进行挑选。所以淘汰比较弱的基因是可以的，但是不建议淘汰的比例太大。</p>
<p>（3）基因重组。基因重组这个环节是变数比较大的。断开的位置几乎是可以随意进行的，如上述例子，一个6位长度的基因，1-5断开，2-4断开，3-3断开，4-2断开，5-1断开，都是可选的方案。其实在一次产生后代的过程中是可以允许以不同的方案产生多个后代的，如两个配对的基因是可以用2-4方案做两个后代，同时再用4-2方案做两个后代的，产生4个后代是可以的。这样会带来更大的基因丰富性，但是同时也要注意计算量如果发生增长，在若干代以后恐怕会严重影响计算性能。</p>
<p>另外相信读者也能意识到，不要一个基因自身和自身去做重组，没有意义，因为怎么重组还是自己，没有任何变化。</p>
<p>（4）迭代结束。这个算法迭代结束的判断标准因人而异，但是总体的原则是，如果连续观察几代都没有明显的适应函数的增长，那就说明进化到这几代基本“到头”了。在结束迭代时，在这之前找到的最优解就是能找到的最优解。</p>
<p>对于背包问题的解，在这里也同样给出一段Python代码供读者参考：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line">import random</span><br><span class="line">#背包问题</span><br><span class="line"># 物品重量价格</span><br><span class="line">X = &#123;</span><br><span class="line">    1： [10， 15]，</span><br><span class="line">    2： [15， 25]，</span><br><span class="line">    3： [20， 35]，</span><br><span class="line">    4： [25， 45]，</span><br><span class="line">    5： [30， 55]，</span><br><span class="line">    6： [35， 70]&#125;</span><br><span class="line">#终止界限</span><br><span class="line">FINISHED_LIMIT = 5</span><br><span class="line">#重量界限</span><br><span class="line">WEIGHT_LIMIT = 80</span><br><span class="line">#染色体长度</span><br><span class="line">CHROMOSOME_SIZE = 6</span><br><span class="line">#遴选次数</span><br><span class="line">SELECT_NUMBER = 4</span><br><span class="line">max_last = 0</span><br><span class="line">diff_last = 10000</span><br><span class="line">#收敛条件、判断退出</span><br><span class="line">def is_finished（fitnesses）：</span><br><span class="line">    global max_last</span><br><span class="line">    global diff_last</span><br><span class="line">    max_current = 0</span><br><span class="line">    for v in fitnesses：</span><br><span class="line">        if v[1] &gt; max_current：</span><br><span class="line">            max_current = v[1]</span><br><span class="line">    diff = max_current - max_last</span><br><span class="line">    if diff &lt; FINISHED_LIMIT and diff_last &lt; FINISHED_LIMIT：</span><br><span class="line">        return True</span><br><span class="line">    else：</span><br><span class="line">        diff_last = diff</span><br><span class="line">        max_last = max_current</span><br><span class="line">        return False</span><br><span class="line">#初始染色体样态</span><br><span class="line">def init（）：</span><br><span class="line">    chromosome_state1 = &apos;100100&apos;</span><br><span class="line">    chromosome_state2 = &apos;101010&apos;</span><br><span class="line">    chromosome_state3 = &apos;010101&apos;</span><br><span class="line">    chromosome_state4 = &apos;101011&apos;</span><br><span class="line">    chromosome_states = [chromosome_state1，</span><br><span class="line">                        chromosome_state2，</span><br><span class="line">                        chromosome_state3，</span><br><span class="line">                        chromosome_state4]</span><br><span class="line">    return chromosome_states</span><br><span class="line">#计算适应度</span><br><span class="line">def fitness（chromosome_states）：</span><br><span class="line">    fitnesses = []</span><br><span class="line">    for chromosome_state in chromosome_states：</span><br><span class="line">        value_sum = 0</span><br><span class="line">        weight_sum = 0</span><br><span class="line">        for i， v in enumerate（chromosome_state）：</span><br><span class="line">            if int（v） == 1：</span><br><span class="line">                weight_sum += X[i + 1][0]</span><br><span class="line">                value_sum += X[i + 1][1]</span><br><span class="line">        fitnesses.append（[value_sum， weight_sum]）</span><br><span class="line">    return fitnesses</span><br><span class="line">#筛选</span><br><span class="line">def filter（chromosome_states， fitnesses）：</span><br><span class="line">    #重量大于80的被淘汰</span><br><span class="line">    index = len（fitnesses） - 1</span><br><span class="line">    while index &gt;= 0：</span><br><span class="line">        index -= 1</span><br><span class="line">        if fitnesses[index][1] &gt; WEIGHT_LIMIT：</span><br><span class="line">            chromosome_states.pop（index）</span><br><span class="line">            fitnesses.pop（index）</span><br><span class="line">    #遴选</span><br><span class="line">    selected_index = [0] * len（chromosome_states）</span><br><span class="line">    for i in range（SELECT_NUMBER）：</span><br><span class="line">        j = chromosome_states.index（random.choice（chromosome_states））</span><br><span class="line">        selected_index[j] += 1</span><br><span class="line">    return selected_index</span><br><span class="line">#产生下一代</span><br><span class="line">def crossover（chromosome_states， selected_index）：</span><br><span class="line">    chromosome_states_new = []</span><br><span class="line">    index = len（chromosome_states） - 1</span><br><span class="line">    while index &gt;= 0：</span><br><span class="line">        index -= 1</span><br><span class="line">        chromosome_state = chromosome_states.pop（index）</span><br><span class="line">        for i in range（selected_index[index]）：</span><br><span class="line">            chromosome_state_x = random.choice（chromosome_states）</span><br><span class="line">            pos = random.choice（range（1， CHROMOSOME_SIZE - 1））</span><br><span class="line">            chromosome_states_new.append（chromosome_state[：pos] + chromosome_state_x[pos：]）</span><br><span class="line">        chromosome_states.insert（index， chromosome_state）</span><br><span class="line">    return chromosome_states_new</span><br><span class="line">if __name__ == &apos;__main__&apos;：</span><br><span class="line">    #初始群体</span><br><span class="line">    chromosome_states = init（）</span><br><span class="line">    n = 100</span><br><span class="line">    while n &gt; 0：</span><br><span class="line">        n -= 1</span><br><span class="line">        #适应度计算</span><br><span class="line">        fitnesses = fitness（chromosome_states）</span><br><span class="line">        if is_finished（fitnesses）：</span><br><span class="line">            break</span><br><span class="line">        #遴选</span><br><span class="line">        selected_index = filter（chromosome_states， fitnesses）</span><br><span class="line">        #产生下一代</span><br><span class="line">        chromosome_states = crossover（chromosome_states， selected_index）</span><br><span class="line"># 1： [[60， 35]， [105， 60]， [140， 75]， [175， 95]]</span><br><span class="line"># 2： [[60， 35]， [105， 60]， [80， 45]， [90， 50]]</span><br><span class="line"># 3： [[95， 55]， [115， 65]， [70， 40]， [90， 50]]</span><br><span class="line"># 4： [[70， 40]， [70， 40]， [150， 85]， [115， 65]]</span><br><span class="line"># 5： [[115， 65]， [115， 65]， [115， 65]， [70， 40]]</span><br><span class="line"># [&apos;100110&apos;， &apos;100110&apos;， &apos;100110&apos;， &apos;100110&apos;]</span><br></pre></td></tr></table></figure>

</details>




<p>求出的[115，65]就是要求的解，对应的物品是1：[10，15]、4：[25，45]、5：[30，55]。  </p>
<p>这里用的收敛条件是连续两代的适应函数最大值都不再增加，出现这种情况则判断为收敛。  </p>
<p><a href="#ch1">[1]</a><br> 图片来源于百度图库。</p>
<h3 id="10-6-4-极大值问题"><a href="#10-6-4-极大值问题" class="headerlink" title="10.6.4 极大值问题"></a>10.6.4 极大值问题</h3><p>在遗传算法中再举一个求极大值的例子。这种例子也是比较多见的，只要把一些数据关系描述成函数之后就会有一些求极大值或者极小值的问题。</p>
<p>其实极大值和极小值是一类问题，即极值问题，解题思路也是一样的。</p>
<p>假设在空间里有一个函数z=y sin（x）+x cos（y），图形如图10-25所示。</p>
<p><img src="Image00310.jpg" alt></p>
<p>图10-25 z=y sin（x）+x cos（y）的图形</p>
<p>假设这是一个地区中的地形图，而且地形每个点的（x，y，z）三维坐标是可以用z=y sin（x）+x cos（y）这个函数来进行描述的，如果想求这个函数在x位于[-10，10]和y位于[10，10]之间的最大值，怎么求呢？</p>
<p>可以使用微积分，把z对y求偏导数，把z对x求偏导数，这样能够求出满足两个偏导数同时为零的x、y的解，很可能有多个，把所有的x、y对代入z=y sin（x）+x cos（y），求出的（x，y，z）就是驻点。每个驻点的z值比较大小，最大的就是要求的解了。没学过微积分的朋友也先别着急，我们今天介绍的不是这种微积分领域常用的办法，还是考虑用遗传算法的思路来做。</p>
<p>可以想象，在这个地区无规律地放置很多人，有的在谷底，有的在半山腰，有的可能已经在山顶或者山顶附近。那么下面让他们一代一代生生不息地繁殖，凡是能爬得更高的就留下。有了这个思路就可以按部就班地求解。</p>
<p>1.基因编码</p>
<p>首先这里的基因编码问题就和背包问题不一样。背包问题属于离散型的自变量，一个物品在背包里要么有要么没有，即便是128个物品，也知道一共是有2128 种情况，基因编码最多用128bit也就够了。</p>
<p>但是这个例子中，有两个自变量x和y，这两个自变量都是实数。根据实数稠密性原理，在[-10，10]之间是有无穷多个实数，没有办法用类似背包问题中的基因编码的办法穷举所有的实数可能性。</p>
<p>但是，电子计算机在做所有实数运算时其实是没有那么精确的，因为本身用电子计算机来计算实数就是一个用离散解决连续，用有穷解决无穷的方案，本身就有天生的不可逾越的局限性。不管用64bit还是用128bit描述一个实数都太有限了，所以需要一个精度限制，也就是有效数字。问题是应该取多少位有效数字？</p>
<p>理想地，对于有效数字应该是尽可能多取，有多少取多少，但是在实际生活中却不这么做。原因也很简单，多取有效数字本来是为了提高精确度，降低误差与成本。然而多取有效数字同样需要更多的成本，而多出的有效数字的增长对提高收益如果没有明显的好处，那显然取太多有效数字反而是不划算的。</p>
<p>例如，在平时购物时取到元小数点后2位就够了，因为再小的货币单位也不发行，再往后标记更小的标价单位毫无意义。一些大宗交易的物品单价通常会取得比较多，如小米手机、iPhone手机中的电子元件，很多出厂单价才0.005元一个甚至更低。但是由于购买量巨大，一批就是以千万个原件作为单位的购买量，1000万个电子元件就是50000元人民币了，这种情况下取4位小数就有意义。再如生产CPU用的高纯硅通常是99.999999999%的，纯度低了无法支持22nm级别的生产工艺。太阳能板硅板就不需要这么精细了，一般99.99%就能满足，如果超过99.9999%只会增加成本。</p>
<p>所以对于那些无法感知精确的，感知了也无法把控的，把控了也对提高效益无意义的，这些场合下的精度提升就没必要了，在生产生活中可酌情进行取舍。</p>
<p>再回到上述例子，如果允许一定的误差，只要误差足够小其实就已经能够满足模型的需求了。</p>
<p>假设从-10到10之间指的是一个20公里的地带，那么取精度为1米可以理解为[-10.000，10.000]这个区间范围，取精度0.1米就可以理解为[-10.0000，10.0000]的范围，这里假设1米的精度已经能够满足误差要求，那么取[-10.000，10.000]作为取值范围，这样无限就变成有限了，这里实际是20001种取值。</p>
<p>如果还是要用二进制来进行基因编码应该选用多少位呢？214 是16384，215 是32768，因为这里有20001种情形，所以取15bit作为基因描述，x和y都用15bit的基因——这已经能够覆盖所有的设想样本。</p>
<p>那么下一个问题就是把一个15bit的二进制数字映射到[-10.000，10.000]这个区间里去。用第一位作为正负数的标识，剩下14bit作为具体数字的标识，这样这个15bit的数字就成为了[-16383，0]和[0，16383]两个区间取并集了，注意集合里的元素都是整数。再把每个数字除以16383除以1000，取值范围就被压缩在[-10.000，10.000]之间了。</p>
<p>注意在这里为了让区间包括两边的边界点，做一个小小的变换。</p>
<p>定义F（x）的内容如下：x是自变量，为二进制数字，首位为0代表正数，首位为1代表负数，函数值为x对应的十进制数字。那么就完成了编码到整个定义区间上的映射F（x），最后表示二进制到实数值的函数H（x）定义如下：</p>
<p><img src="Image00311.jpg" alt></p>
<p>但是这里有两个问题。</p>
<p>问题1：<br>F（x）≥0和F（x）≤0是有重叠部分的，F（x）=0究竟算谁的。补充说明一下，这个写法确实是不够严谨，具体意义是指x的首位是0还是1的情况，是0那就算F（x）≥0，是1那就算F（x）≤0，因为确实后面14bit都是0的情况下，不管首位是0还是1代表值都是0。</p>
<p>问题2： <img src="Image00312.jpg" alt> 和<img src="Image00313.jpg" alt><br>分别构成了x正负两个部分，但是区间却不是原来说的[-10.000，10.000]，变成了[-10.000，0）和（0，10.000]，0从自变量范围里被拿掉了。这是为了计算方便，但是由于确信0不是要的解，不管是x=0还是y=0，所以从解里面把所有x=0的情况以及所有y=0的情况都去除了，这个变换不影响最终求解。如果觉得不放心，那么可以尝试重新构造这个映射关系，把[-10.000，10.000]所有的点都覆盖到。</p>
<p>最后计算映射关系的具体值：</p>
<p><img src="Image00314.jpg" alt></p>
<p>怎么样，容易吧，用现在流行的词儿来说那就是——“完美”。</p>
<p>2.设计初始群体</p>
<p>设计初始群体是第二个要解决的大问题。</p>
<p>倾斜一下看这个小世界，如图10-26所示，要设置一些初始化的人让他们一代一代繁衍后代，能爬得更高的就继续观察，爬不高的就不管了。在刚刚设计的基因里其实有两条基因，一条是x，一条是y，这两条基因各有15个基因信息点，也就是215 个可能值，随机产生8组基因，如表10-17所示。</p>
<p>3.适应度计算</p>
<p>适应度在这个场景里不难设计，用z=y sin（x）+x cos（y）即可，z就是适应度。</p>
<p>4.产生下一代</p>
<p>在这个场景里，在每一代都可以让1个染色体中的基因X之间和基因Y之间进行组合，如表10-18所示。</p>
<p><img src="Image00315.jpg" alt></p>
<p>图10-26 z=y.sin（x）+x.cos（y）的图形</p>
<p>表10-17 8组基因</p>
<p><img src="Image00316.jpg" alt></p>
<p>表10-18 基因组合</p>
<p><img src="Image00317.jpg" alt></p>
<p>如果染色体1和2进行结合，那么：</p>
<p>1染色体的X基因的前7位和2染色体的X基因的后8位将结合。</p>
<p>1染色体的Y基因的前7位和2染色体的Y基因的后8位将结合。</p>
<p>2染色体的X基因的前7位和1染色体的X基因的后8位将结合。</p>
<p>2染色体的Y基因的前7位和1染色体的Y基因的后8位将结合。</p>
<p>由此形成XA、YA、XB、YB 4个后代基因，XA和YA将成为新的一组染色体，XB和YB将成为新的一组染色体，形成两个完整的后代基因染色体组。</p>
<p>如果是8组作为初始种群的大小，就有<img src="Image00318.jpg" alt> 种组合方式，而每一种组合产生2个后代，那么实际上第一代以后就产生56个个体。</p>
<p>这56个个体的适应度可以进行排序，只取出排名前8的个体。</p>
<p>这里同样可以允许一定的基因突变性，在8个已遴选的个体中，随机找到两个个体，让这两个个体其中一个x染色体发生变异而让另一个y染色体发生变异（这个例子由x和y两个自变量构成的染色体碰巧跟人类的性染色体同名）。变异也是随机改变x染色体中的某一位，或随机改变y染色体中的某一位。</p>
<p>这之后再进行两两重组的计算，产生下一代。</p>
<p>这里注意以下两点。</p>
<p>（1）断开点的位置。理论上，断开点的位置是任意的，但是断开点靠左对数值影响变化大，自变量“跳跃”范围也就大；断开点靠右对数值影响变化小，自变量“跳跃”范围也就小。</p>
<p>（2）基因变异的位置。和断开点位置的影响是完全一样的，同样是变异点靠左自变量“跳跃”范围大，变异点靠右自变量“跳跃”范围小。</p>
<p>5.迭代计算</p>
<p>这里直接看迭代计算的代码和执行过程。请注意，由于其中有很多随机的因素，所以计算的过程结果可能会不一致，迭代的代数也可能不一致，但是最终结果应该都是一样的。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">#coding=utf-8</span><br><span class="line">import random</span><br><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">#染色体长度</span><br><span class="line">CHROMOSOME_SIZE = 15</span><br><span class="line">#判断退出</span><br><span class="line">def is_finished（last_three）：</span><br><span class="line">    s = sorted（last_three）</span><br><span class="line">    if s[0] and s[2] - s[0] &lt; 0.01 * s[0]：</span><br><span class="line">        return True</span><br><span class="line">    else：</span><br><span class="line">        return False</span><br><span class="line">#初始染色体样态</span><br><span class="line">def init（）：</span><br><span class="line">    chromosome_state1 = [&apos;000000100101001&apos;， &apos;101010101010101&apos;]</span><br><span class="line">    chromosome_state2 = [&apos;011000100101100&apos;， &apos;001100110011001&apos;]</span><br><span class="line">    chromosome_state3 = [&apos;001000100100101&apos;， &apos;101010101010101&apos;]</span><br><span class="line">    chromosome_state4 = [&apos;000110100100100&apos;， &apos;110011001100110&apos;]</span><br><span class="line">    chromosome_state5 = [&apos;100000100100101&apos;， &apos;101010101010101&apos;]</span><br><span class="line">    chromosome_state6 = [&apos;101000100100100&apos;， &apos;111100001111000&apos;]</span><br><span class="line">    chromosome_state7 = [&apos;101010100110100&apos;， &apos;101010101010101&apos;]</span><br><span class="line">    chromosome_state8 = [&apos;100110101101000&apos;， &apos;000011110000111&apos;]</span><br><span class="line">    chromosome_states = [chromosome_state1，</span><br><span class="line">                     chromosome_state2，</span><br><span class="line">                     chromosome_state3，</span><br><span class="line">                     chromosome_state4，</span><br><span class="line">                     chromosome_state5，</span><br><span class="line">                     chromosome_state6，</span><br><span class="line">                     chromosome_state7，</span><br><span class="line">                     chromosome_state8]</span><br><span class="line">    return chromosome_states</span><br><span class="line">#计算适应度</span><br><span class="line">def fitness（chromosome_states）：</span><br><span class="line">    fitnesses = []</span><br><span class="line">    for chromosome_state in chromosome_states：</span><br><span class="line">        if chromosome_state[0][0] == &apos;1&apos;：</span><br><span class="line">            x = 10 * （-float（int（chromosome_state[0][1：]， 2） - 1）/16384）</span><br><span class="line">        else：</span><br><span class="line">            x = 10 * （float（int（chromosome_state[0]， 2） + 1）/16384）</span><br><span class="line">        if chromosome_state[1][0] == &apos;1&apos;：</span><br><span class="line">            y = 10 * （-float（int（chromosome_state[1][1：]， 2） - 1）/16384）</span><br><span class="line">        else：</span><br><span class="line">            y = 10 * （float（int（chromosome_state[1]， 2） + 1）/16384）</span><br><span class="line">        z = y * math.sin（x） + x * math.cos（y）</span><br><span class="line">        fitnesses.append（z）</span><br><span class="line">    return fitnesses</span><br><span class="line">#筛选</span><br><span class="line">def filter（chromosome_states， fitnesses）：</span><br><span class="line">    #top 8 对应的索引值</span><br><span class="line">    chromosome_states_new = []</span><br><span class="line">    top1_fitness_index = 0</span><br><span class="line">    for i in np.argsort（fitnesses）[：：-1][：8].tolist（）：</span><br><span class="line">        chromosome_states_new.append（chromosome_states[i]）</span><br><span class="line">        top1_fitness_index = i</span><br><span class="line">    return chromosome_states_new， top1_fitness_index</span><br><span class="line">#产生下一代</span><br><span class="line">def crossover（chromosome_states）：</span><br><span class="line">    chromosome_states_new = []</span><br><span class="line">    while chromosome_states：</span><br><span class="line">        chromosome_state = chromosome_states.pop（0）</span><br><span class="line">        for v in chromosome_states：</span><br><span class="line">            pos = random.choice（range（8， CHROMOSOME_SIZE - 1））</span><br><span class="line">            chromosome_states_new.append（[chromosome_state[0][：pos] + v[0][pos：]， chromosome_state[1][：pos] + v[1][pos：]]）</span><br><span class="line">            chromosome_states_new.append（[v[0][：pos] + chromosome_state[1][pos：]， v[0][：pos] + chromosome_state[1][pos：]]）</span><br><span class="line">    return chromosome_states_new</span><br><span class="line">#基因突变</span><br><span class="line">def mutation（chromosome_states）：</span><br><span class="line">    n = int（5.0 / 100 * len（chromosome_states））</span><br><span class="line">    while n &gt; 0：</span><br><span class="line">        n -= 1</span><br><span class="line">        chromosome_state = random.choice（chromosome_states）</span><br><span class="line">        index = chromosome_states.index（chromosome_state）</span><br><span class="line">        pos = random.choice（range（len（chromosome_state）））</span><br><span class="line">        x = chromosome_state[0][：pos] + str（int（not int（chromosome_state[0][pos]）））+ </span><br><span class="line">chromosome_state[0][pos+1：]</span><br><span class="line">        y = chromosome_state[1][：pos] + str（int（not int（chromosome_state[1][pos]））） +</span><br><span class="line">chromosome_state[1][pos+1：]</span><br><span class="line">        chromosome_states[index] = [x， y]</span><br><span class="line">if __name__ == &apos;__main__&apos;：</span><br><span class="line">    chromosome_states = init（）</span><br><span class="line">    last_three = [0] * 3</span><br><span class="line">    last_num = 0</span><br><span class="line">    n = 100</span><br><span class="line">    while n &gt; 0：</span><br><span class="line">        n -= 1</span><br><span class="line">        chromosome_states = crossover（chromosome_states）</span><br><span class="line">        mutation（chromosome_states）</span><br><span class="line">        fitnesses = fitness（chromosome_states）</span><br><span class="line">        chromosome_states， top1_fitness_index = filter（chromosome_states， fitnesses）</span><br><span class="line">        last_three[last_num] = fitnesses[top1_fitness_index]</span><br><span class="line">        if is_finished（last_three）：</span><br><span class="line">            break</span><br><span class="line">        if last_num &gt;= 2：</span><br><span class="line">            last_num = 0</span><br><span class="line">        else：</span><br><span class="line">            last_num += 1</span><br><span class="line">#x： 7.698 974 609 38  y：7.698 974 609 38  z：8.795 289 238 25</span><br><span class="line">#x： -8.356 933 593 75  y：-8.356 933 593 75 z：11.350 199 424 9</span><br></pre></td></tr></table></figure>

</details>




<p>这里最后的收敛条件是计算每一代的适应函数最大值并记录，判断最近3代的最大值如何变化。如果最近3代的适应函数最大值相比较，第一大的（最大的）比第三大的（最小的）增益小于1%，那么就判断为收敛。这种收敛的速度非常快，大概3到4代就可以收敛完毕求出最优解。当然，这个部分同样可以再讨论采用更为考究的计算方式，有兴趣的读者可以继续研究。  </p>
<p>请注意，在执行这段代码中可能会发现这么一个问题：有的时候z值会收敛到8.8附近，此时x和y都在7.7附近，就像例子中给出的这个数字；有的时候z值会收敛到11.35附近，此时x和y都在-8.35附近。从这两种不同的解来看，可以知道后者应该是正确的而前者是不正确的。  </p>
<p>从图10-27中可以看出，圆圈圈住的位置就是（7.7，7.7）附近的点，而实际上正确的解在方框圈住的位置（-8.35，-8.35）。为什么会出现这种现象呢？其实也不难理解，就是在繁殖下一代时又出现一次播撒的情况，但是播撒不够均匀，导致部分爬到“圆圈山”位置的种群个体表现格外良好，其他种群如在“方框山”半山腰的就没能被遴选——凡是第9名开外的都被淘汰。这样是有相当的概率会收敛到7.7附近的“圆圈山”的。这一类的问题可能以后在写遗传算法中也同样会遇到，请读者注意。</p>
<p><img src="Image00319.jpg" alt></p>
<p>图10-27　z=y.sin（x）+x.cos（y）的图形  </p>
<p>这一类问题可以考虑以下两个解决方法。  </p>
<p>方法一：<br> 初始种群扩大化。  </p>
<p>初始种群可以不止8个，可以是16个、32个，或者更多，总之就是一开始就使得第一代有足够的机会爬到“方框山”的比较好的位置去。  </p>
<p>方法二：<br> 每一代遴选增加名额。  </p>
<p>每一代现在只是留下8个种群个体，同样的，可以留下16个、32个，也一样会让更多爬到“方框山”较高位置的对象存活下来。  </p>
<p>这两种方法都采用之后是可以让找到最优解的概率大大增加的。</p>
<h2 id="10-7-小结"><a href="#10-7-小结" class="headerlink" title="10.7 小结"></a>10.7 小结</h2><p>本章是整本书中比重比较大的一章，也是因为分类算法在生产生活中使用得也最为广泛。</p>
<p>应该注意到，大部分的分类算法都是基于统计概率的分类算法，而凡是基于统计概率的分类算法究其本质仍然是贝叶斯概率体系下的分类原则。以SVM算法为例，要找超平面来做类别的区分，但是类别区分的原则仍旧是根据已知样本的特征情况，也就是抽象后的多维空间向量信息特征来做分类标准。在超平面确定后，对新的待分类样本仍然是根据一个向量的特征值来判断其属于某分类或不属于某分类的概率为多少，究竟是哪一种更高。SVM本质上仍旧是根据特征向量在空间的分布来拟合分类概率。在判断新的待分类样本时，如果待分类样本处在超平面附近，那就仍然是一个模棱两可的样本，是一个归属或不归属一个分类概率相当的情况。</p>
<p>此外，有误判的问题几乎是没办法避免的，虽然这个结论多少让人觉得有点沮丧。但是，只要算法本身的成本和误判带来的损失在一个可接受的范围内即可，千万不要过于纠结高精度而裹足不前。</p>
<p>遗传算法在数学上其实是采用梯度下降的方法来求解问题的。所谓梯度在第8章介绍最小二乘法时已经涉及，只是当时没有这么提。在最小二乘法时设计的函数Q（a，b）中，误差Q是一个用a和b表示的函数，其实也可以看成z（x，y），z是用x和y来表示的。在讨论Q（a，b）的极值时，说到了偏微分的概念，也就是求沿着a轴方向和b轴方向的多组剖面上的切线斜率问题，最后找到两个方向上的斜率为0的位置作为候选点。这个结果是通过数学上求偏导数的方式推导计算出来的。而梯度下降的方法与此不同，它的思路是，不求偏导数，但是沿着整个曲面“行进”，当行进同样单位距离时函数值变化大，那就说明斜率大；而当行进同样单位距离时函数值变化小，那就说明斜率小。当行进一次函数值变化趋近0时，那就说明到了驻点附近。这也是一种很巧妙的思路。在极大值问题求解的过程中已经展示过这种方式的思路了。</p>
<p>这种方式在地理地形图上的表现形式如图10-28所示。</p>
<p>在地理课本上会有地形图，而地形图中最容易让人产生画面感的莫过于等高线图。图10-28上用方框圈出的部分等高线稠密，表示地形高度变化快，坡度陡峭；用椭圆圈出的部分等高线系数，表示地形高度变化慢，坡度平缓。梯度下降法实际上是在找等高线稀疏的地方，那一定是一个趋于平缓的地带，要么在谷底，要么在峰顶。这就是梯度下降法的意义。</p>
<p>分类属于有监督的学习过程，这个过程中使用者可以根据经验以及数学推导等辅助方法给机器一些指导，帮助机器剪枝、收敛、去噪等，让计算变得更加快捷，更加准确。</p>
<p><img src="Image00320.jpg" alt></p>
<p>图10-28 地形图</p>
<h1 id="第11章-关联分析"><a href="#第11章-关联分析" class="headerlink" title="第11章 关联分析"></a>第11章 关联分析</h1><p>在学习完聚类和分类之后，补充性地讨论一下关联分析。</p>
<p>关联规则也是人类在认识客观事物中形成的一种认知模式。这种关联规则在人的认知里与反射类似。如在小时候不小心被针扎到，会有痛感，这样针刺和痛感就在大脑里有了这种关联。再如，小时候在不懂任何电学原理时也可以发现按下电灯开关，电灯就会点亮，再按一次，电灯就会关闭。这个对于电灯开关的动作就和电灯亮灭的事件形成了关联关系。这就是人在认识事物的过程中在认知中所建立的关联规则，即通过与客观世界互动，发现事物之间存在的依赖或者因果关系。</p>
<p>本章讨论的关联分析应该说是数据挖掘教程体系里最为经典的部分之一，也是尝试在数据中发现依赖或者因果关系的方法。这其中还有一个更为经典的案例，即几乎在每本BI（商业智能）教材或者数据挖掘教材里都会讲到的“啤酒和尿布”案例。</p>
<p>据说事情发生在20世纪80年代的美国沃尔玛，销售经理经过研究零售记录发现啤酒和尿布会同时出现在很多购物记录里，令人百思不得其解。经过观察发现，在这个地区有很多年轻的父亲会时常光顾超市，在超市里为孩子购买尿布的同时也会为自己买上一些啤酒。从此，超市经理借题发挥，又专门把啤酒和尿布的销售货架放在一起，进而让啤酒和尿布的销量进一步提高。</p>
<p>虽然后来也有一些专家说这个案例是假的，根本没那么回事，完全是BI产品的销售人员杜撰出来的。不论该案例的真假如何，这种研究方式却是在零售行业得到了比较广泛的应用，这种分析也称作“购物篮”分析。我们一起来看一下，这个分析过程是怎么实现的。</p>
<h2 id="11-1-频繁模式和Apriori算法"><a href="#11-1-频繁模式和Apriori算法" class="headerlink" title="11.1 频繁模式和Apriori算法"></a>11.1 频繁模式和Apriori算法</h2><p>一个超市里每天有很多的购物记录，很多大卖场每天甚至可能有数万笔交易，为了示例的方便假设一共有5个购物记录。</p>
<p>记录1：</p>
<p>（啤酒，香烟，白菜，鸡蛋，酸奶，卫生纸）</p>
<p>记录2：</p>
<p>（红酒，香烟，巧克力糖，酸奶）</p>
<p>记录3：</p>
<p>（牙刷，奶糖，食盐，冷冻鸡肉，卫生纸）</p>
<p>记录4：</p>
<p>（啤酒，一次性酒杯，香烟，瓜子，花生，油炸薯片）</p>
<p>记录5：</p>
<p>（酸奶，巧克力糖，味精）</p>
<p>每个记录相当于在超市发生的一笔结算，一个购物者把篮子里的商品都一起扫码付账。</p>
<h3 id="11-1-1-频繁模式"><a href="#11-1-1-频繁模式" class="headerlink" title="11.1.1 频繁模式"></a>11.1.1 频繁模式</h3><p>首先要明确一点，就是在每个购买记录中出现的各种单品其实体现的是一种组合的性质。也就是说，消费者在购买了一种单品的同时又购买了另一种单品。而且，这些单品的组合在记录中是无序的，也就是无法知道在记录1中究竟是先“购买”了啤酒然后诱使他又“购买”了香烟，还是先“购买”了香烟后来又购买了啤酒。因此只能研究一个无序的组合，这种组合就叫做“模式”。</p>
<p>这些模式里，有的出现频率很低，有的出现频率很高，一般认为频率较高的通常更有指导意义，这种高频率的模式就被称作“频繁模式”。</p>
<p>先尝试着把刚刚这些记录内容放入一个关系型数据库的表格进行存储以备实验。也可以尝试使用数组、内存向量等其他形式，这些并不影响Apriori算法的结果。购物记录表Buy_list如表11-1所示。</p>
<p>表11-1 购物记录表Buy-list</p>
<p><img src="Image00321.jpg" alt></p>
<p>假设已经把这个信息变成了一张表Buy_list，表至少有两个字段Serial和Type。注意，在数据仓库里，通常Type这个字段是一个类别编码，而不是一个具体的汉字品名描述。</p>
<h3 id="11-1-2-支持度和置信度"><a href="#11-1-2-支持度和置信度" class="headerlink" title="11.1.2 支持度和置信度"></a>11.1.2 支持度和置信度</h3><p>刚刚介绍过一个频繁模式的概念，即一般认为频率较高的模式叫做频繁模式。衡量频率的指标有两个：一个是支持度，一个是置信度 。</p>
<p>这两个指标分别指的是这种模式的有用性和确定性。设置门限“最小支持度”和“最小置信度”，支持度和置信度同时高于这两个门限就可以认为是频繁模式了。</p>
<p>例如，购买（啤酒，香烟）模式的支持度为40%，那就是说所有的购买记录里（例子里是5个），有40%的购买记录都包含这种模式。</p>
<p>置信度是有“方向性”的，如果说购买啤酒的记录里有100%的记录都购买了香烟，那么就说购买啤酒后购买香烟的置信度为100%；反向地看，如果购买了香烟的记录有67%的记录都购买了啤酒，那么就说购买香烟后购买啤酒的置信度为67%。</p>
<p>为了表达简便，也常记录如下：</p>
<p>啤酒=&gt;香烟[support=40%；confidence=100%]</p>
<p>香烟=&gt;啤酒[support=40%；confidence=67%]</p>
<p>用SQL语句能够很快地算出一个模式的支持度和置信度。</p>
<p>啤酒=&gt;香烟，支持度计算的代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">TRANSACTION_COUNT.COUNT_NUMBER/ TRANSACTION_ALL.COUNT_NUMBER</span><br><span class="line">FROM（</span><br><span class="line">    SELECT COUNT（*） COUNT_NUMBER</span><br><span class="line">    FROM</span><br><span class="line">    （SELECT Serial FROM Buy_list WHERE Type=&apos;啤酒&apos;） R1</span><br><span class="line">    INNER JOIN</span><br><span class="line">    （SELECT Serial FROM Buy_list WHERE Type=&apos;香烟&apos;） R2</span><br><span class="line">    ON</span><br><span class="line">    R1.Serial=R2.Serial） TRANSACTION_COUNT</span><br><span class="line">INNER JOIN（</span><br><span class="line">    SELECT COUNT（DISTINCT Serial） COUNT_NUMBER</span><br><span class="line">    FROM</span><br><span class="line">    Buy_list） TRANSACTION_ALL</span><br><span class="line">ON 1=1；</span><br></pre></td></tr></table></figure>

</details>

<p>看上去内容比较多，但逻辑很简单。</p>
<p>R1是找出所有有啤酒的Serial，R2是找出所有有香烟的Serial，这样TRANSACTION_COUNT.COUNT_NUMBER就是同时既有啤酒又有香烟的Serial的数量了。后面TRANSACTION_ALL.COUNT_NUMBER是所有购买记录的数量。两者相除得到支持度。</p>
<p>啤酒=&gt;香烟，置信度计算的代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE TEMP（R1_Serial VARCHAR（64）， R2_Serial VARCHAR（64）） AS</span><br><span class="line">SELECT *</span><br><span class="line">FROM（SELECT Serial R1_Serial  FROM Buy_list WHERE Type=&apos;啤酒&apos;） R1 </span><br><span class="line">LEFT JOIN（SELECT Serial R2_Serial FROM Buy_list WHERE Type=&apos;香烟&apos;） R2</span><br><span class="line">ON R1.R1_Serial=R2.R2_Serial；</span><br><span class="line">SELECT R2.R2_COUNT/R1.R1_COUNT</span><br><span class="line">FROM（SELECT COUNT（*） R2_COUNT</span><br><span class="line">FROM</span><br><span class="line">TEMP</span><br><span class="line">WHERE R2_Serial IS NOT NULL） R2</span><br><span class="line">INNER JOIN（SELECT COUNT（*） R1_COUNT</span><br><span class="line">FROM</span><br><span class="line">TEMP） R1</span><br><span class="line">ON 1=1；</span><br></pre></td></tr></table></figure>

</details>

<p>简单解释一下，前面的部分是建造一个只有两列的数据表，第一列R1是所有有啤酒的Serial，第二列R2是所有有香烟的Serial，LEFT JOIN后会得到一个R2有可能产生NULL的数据视图。第二个SQL里前半部分的R2.R2_COUNT是为了求出有啤酒且有香烟的Serial数量，后半部分的R1.R1_COUNT是为了求出所有含有啤酒的Serial数量。两者相除得到置信度。</p>
<p>按照定义一步一步去求看上去非常啰嗦，有兴趣的读者可以尝试优化SQL的写法。</p>
<p>在这里请注意，在生活中各超市各卖场的购物篮分析场景里，支持度和置信度都远没有上述例子这么高。很大数量的支持度和置信度可能都只有百分之零点几或者百分之零点零几。现在再来回答刚刚的那个问题“支持度和置信度多高才算高呢？”在比较成熟的行业，或者有行业专家可以请教的时候，可以寻求行业专家的帮助，让他们设置一个合适的值。如果实在没有行业专家可以咨询，可以尝试在所有商品中找出所有的模式，会发现有一些模式的支持度和置信度同时都比其他高很多，这时可以考虑用所有模式的支持度的平均值和置信度的平均值作为参考，适当提高一些作为阈值做过滤。这样过滤下来的模式就可以作为频繁模式进行进一步研究。</p>
<p>如果单纯支持度高或者置信度高能否直接被认为是频繁模式呢？</p>
<p>如果支持度高置信度低，说明这两种情况确实同时出现，但是“转化率”可能比较低。而如果支持度比较低，但是转化率比较高，说明这种模式在所有的模式里很平常，甚至可能不能算“频繁”。所以基于这样的原因，通常还是会选择支持度和置信度都高于阈值的门限的模式作为频繁模式。</p>
<h3 id="11-1-3-经典的Apriori算法"><a href="#11-1-3-经典的Apriori算法" class="headerlink" title="11.1.3 经典的Apriori算法"></a>11.1.3 经典的Apriori算法</h3><p>刚才讲过，找出频繁项集实际是找出同时满足最小支持度和最小置信度的模式。在Apriori算法的观点中，这个求解过程要分以下几个步骤。</p>
<p>步骤1：<br>先设置一个最小支持度作为阈值门限值进行扫描，因为对同时过滤最小支持度和最小置信度这两个操作来说，最小支持度的查找更为简单一些。假设设置的最小支持度为40%（也可以尝试设置为其他值）。</p>
<p>步骤2：<br>扫描所有满足最小支持度的单品。这个步骤的逻辑很简单，假设有一个单品和另一个单品组合的模式满足最小支持度40%，那该单品首先必须在所有购买记录中出现的概率大于等于40%才有可能，这是一个必要条件。所以扫描所有满足最小支持度的单品，找出大于等于40%的。</p>
<p>在本例中，单品支持度，如表11-2所示。</p>
<p>表11-2 单品支持度</p>
<p><img src="Image00322.jpg" alt></p>
<p>过滤出候选单品，即满足最小支持度40%的单品，如表11-3所示。</p>
<p>表11-3 支持度大于等于40%的单品</p>
<p><img src="Image00323.jpg" alt></p>
<p>在这个过程中可以发现，大量小于40%的单品已经被过滤掉了，这个过程在算法中叫剪枝。再逐级组合查找模式时，有很多的单品可以完全置之不理了。“剪枝法”是算法科学中一个比较重要的思路，在很多算法中有广泛的应用，即利用数学特性或一些其他技巧过滤掉那些没必要计算的情况，用来降低算法的时间复杂度。</p>
<p>步骤3： 查找满足条件的2项模式。根据已经过滤出的单品，组合一下看候选的2项模式有哪些，然后在前面的数据里具体比对一下，是否存在以及看有几个。</p>
<p>啤酒和香烟，出现过2次，2/5=40%。</p>
<p>啤酒和酸奶，出现过1次，1/5=20%。</p>
<p>……</p>
<p>得到表11-4（候选的2项模式）。</p>
<p>表11-4 满足条件的2项模式</p>
<p><img src="Image00324.jpg" alt></p>
<p>在所有已经滤出的2项模式中，找出满足最小支持度的2项模式（本例中为满足最小支持度为40%的2项模式），如表11-5所示。</p>
<p>目前只剩下3个2项模式了。</p>
<p>步骤4：<br>查找满足条件的3项模式。这个过程与步骤3类似，注意现在还剩下哪些单品是有可能的。假设有一个单品和另外两个单品组合的模式满足最小支持度40%，那么这两个单品组合而成的2项模式同样必须满足支持度40%。</p>
<p>表11-5 满足最小支持度的2项模式</p>
<p><img src="Image00325.jpg" alt></p>
<p>只剩下表11-5中的啤酒，香烟，酸奶，巧克力糖这4个单品了。</p>
<p>候选的2项模式如表11-6所示。</p>
<p>表11-6 候选的2项模式</p>
<p><img src="Image00326.jpg" alt></p>
<p>和前面一样做一个笛卡儿乘积，候选的3项模式如表11-7所示。</p>
<p>表11-7 候选的3项模式</p>
<p><img src="Image00327.jpg" alt></p>
<p>观察发现，到目前为止，所有候选的3项模式都“阵亡”了，全都不满足40%的门限要求。OK，那就结束，算到这里为止，因为我们也不可能找到满足条件的3项、4项以及以后任何多种符合条件的频繁模式了。如果大家对这个算法的学术性表达感兴趣，可以阅读《数据挖掘概念与技术（原书第3版）》（Jiawei Han、Micheline Kamber、Jian Pei著）的第161页到164页。</p>
<p>回过头来介绍上述笛卡儿积是怎么做的。如果没学过笛卡儿积的朋友请往下看，学过的请自动跳过。</p>
<p>我们从1项（单品）到2项的时候是怎么做的呢？</p>
<p>先从所有1项中罗列出所有的单品：</p>
<p>1项集合=（啤酒，香烟，酸奶，卫生纸，巧克力糖）</p>
<p>2项集合=1项集合×1项集合</p>
<p>注意：笛卡儿积就是用×号两边的两个集合中的元素去做一对一的循环匹配，匹配结果列表就是笛卡儿积的乘积（结果）。</p>
<p>2项集合=1项集合×1项集合</p>
<p>=（啤酒，香烟，酸奶，卫生纸，巧克力糖）×（啤酒，香烟，酸奶，卫生纸，巧克力糖）</p>
<p>=（（啤酒，啤酒），（啤酒，香烟），（啤酒，酸奶），（啤酒，卫生纸），（啤酒，巧克力糖），（香烟，啤酒），（香烟，香烟），（香烟，酸奶），（香烟，卫生纸），（香烟，巧克力糖），（酸奶，啤酒），（酸奶，香烟），（酸奶，酸奶），（酸奶，卫生纸），（酸奶，巧克力糖），（卫生纸，啤酒），（卫生纸，香烟），（卫生纸，酸奶），（卫生纸，卫生纸），（卫生纸，巧克力糖），（巧克力糖，啤酒），（巧克力糖，香烟），（巧克力糖，酸奶），（巧克力糖，卫生纸），（巧克力糖，巧克力糖））</p>
<p>从标准的笛卡儿乘积中，得到了上述25个2项模式。但是并非所有的结果都有意义，（啤酒，啤酒）这样的结果虽然在笛卡儿乘积上是有意义的，但是在业务解释中是没有意义的，因为谁都不会去研究买了“啤酒”又买了“啤酒”的情况。此外，（啤酒，香烟）和（香烟，啤酒）只需要保留一个即可，因为组合不关心方向问题。这样就留下了候选的2项集合：</p>
<p>（（啤酒，香烟），（啤酒，酸奶），（啤酒，卫生纸），（啤酒，巧克力糖），</p>
<p>（香烟，酸奶），（香烟，卫生纸），（香烟，巧克力糖），（酸奶，卫生纸），</p>
<p>（酸奶，巧克力糖），（卫生纸，巧克力糖））</p>
<p>一共10个。</p>
<p>最后这个3项集合的算法和2项的算法略显不同，但是本质是一样的：</p>
<p>3项集合=2项集合×2项集合=2项集合×1项集合×1项集合</p>
<p>=（（啤酒，香烟），（啤酒，酸奶），（啤酒，卫生纸），（啤酒，巧克力糖），（香烟，酸奶），（香烟，卫生纸），（香烟，巧克力糖），（酸奶，卫生纸），（酸奶，巧克力糖），（卫生纸，巧克力糖））</p>
<p>×（（啤酒，香烟），（啤酒，酸奶），（啤酒，卫生纸），（啤酒，巧克力糖），（香烟，酸奶），（香烟，卫生纸），（香烟，巧克力糖），（酸奶，卫生纸），（酸奶，巧克力糖），（卫生纸，巧克力糖））</p>
<p>=（啤酒，香烟）×（啤酒，香烟），（啤酒，香烟）×（啤酒，酸奶），（啤酒，香烟）×（啤酒，卫生纸），…</p>
<p>=（啤酒，香烟，啤酒），（啤酒，香烟，香烟），（啤酒，香烟，啤酒），（啤酒，香烟，酸奶），…</p>
<p>这种方式还是写成笛卡儿乘积更容易让人接受，展开后表述的内容太多。</p>
<p>4项集合以此类推。</p>
<p>这样实际上求出的是所有的满足支持度和置信度的频繁项集，2项、3项……一直到N项，只要它们满足设置的支持度和置信度，就都能被计算出来。</p>
<h3 id="11-1-4-求出所有频繁模式"><a href="#11-1-4-求出所有频繁模式" class="headerlink" title="11.1.4 求出所有频繁模式"></a>11.1.4 求出所有频繁模式</h3><p>刚刚的例子里写了两段SQL语句，主要是为了验算“啤酒=&gt;香烟”的支持度和置信度。剩下的整个算法实际是使用了SQL语言进行完整支持度和置信度的计算算法的实现。</p>
<p>如果要计算所有模式的支持度和置信度，显然是不能用这样的方式的，确实不会列出很多的一个一个的单品，然后一个一个地代入去求它们的支持度。可以试着用其他的SQL语句来求出所有模式的支持度和置信度，其实并不难。</p>
<p>首先希望有一个表能够构造出穷举所有模式的方式，代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE PATTERNS（Serial VARCHAR（64）， Type1 VARCHAR（64）， Type2 VARCHAR（64）） AS</span><br><span class="line">SELECT R1.Serial Serial， R1.Type1 Type1， R2.Type2 Type2</span><br><span class="line">FROM（SELECT Serial， Type Type1 FROM Buy_list） R1</span><br><span class="line">INNER JOIN（SELECT Serial， Type Type2 FROM Buy_list） R2</span><br><span class="line">ON R1.Serial=R2.Serial AND R1.Type1&lt;&gt;R2.Type2；</span><br></pre></td></tr></table></figure>

</details>

<p>现在得到一个PATTERNS表（表11-8）了，这个表有两列，里面是所有的Type组合的结果，而且绝对不包含模式的两个元素为同一元素的情况。这个表构造完毕后会得到一个Serial，Type1，Type2的穷举组合，穷举了在所有购买记录中的各次购买清单里的组合，而且正反向组合各存在一次。以Serial为0001为例：</p>
<p>表11-8 PATTERNS表</p>
<p><img src="Image00328.jpg" alt></p>
<p>这里的Type1和Type2是有方向的，指代的是Type1=&gt;Type2，代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT（*） Support， Type1， Type2</span><br><span class="line">FROM</span><br><span class="line">PATTERNS</span><br><span class="line">GROUP BY Type1， Type2；</span><br></pre></td></tr></table></figure>

</details>

<p>也许有的读者朋友会认为COUNT（*）需要除以2，其实是没有必要的，因为这里认为Type1和Type2是有方向的，也就是说Type1和Type2分别代表啤酒和香烟，和分别代表香烟和啤酒是两种不同的情况。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT（DISTINCT Serial） TRANSACTION_COUNT</span><br><span class="line">FROM</span><br><span class="line">PATTERNS；</span><br></pre></td></tr></table></figure>

</details>

<p>用每行的Support除以TRANSACTION_COUNT，就能得到所有单品对应Type1=&gt;Type2的支持度了。两个SQL语句可以写在一起，就不需要两次请求数据库了。如果要用阈值门限过滤支持度，可以用WHERE语句进行谓词限制。</p>
<p>求置信度相对麻烦一些，但是也不难。思路是，求Type1=&gt;Type2的置信度，即用同时有Type2和Type1的购买记录数除以所有含有Type1的购买记录数。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE TYPE_COUNT_VIEW（Type_count INT， Type VARCHAR（64）） AS</span><br><span class="line">SELECT COUNT（*） Type_count， Type</span><br><span class="line">FROM</span><br><span class="line">Buy_list</span><br><span class="line">GROUP BY Type；</span><br></pre></td></tr></table></figure>

</details>

<p>临时表为TYPE_COUNT_VIEW，它只放置了Type和它出现的数量Type_count两个数据列。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT R1.Type1， R1.Type2， R1.Type_count/TYPE_COUNT_VIEW.Type_count Confidence</span><br><span class="line">FROM（SELECT COUNT（*） Type_count， Type1， Type2</span><br><span class="line">FROM</span><br><span class="line">PATTERNS</span><br><span class="line">GROUP BY Type1， Type2） R1</span><br><span class="line">INNER JOIN</span><br><span class="line">TYPE_COUNT_VIEW</span><br><span class="line">ON R1.Type1=TYPE_COUNT_VIEW.Type；</span><br></pre></td></tr></table></figure>

</details>

<p>最后得到R1.Type1、R1.Type2、Confidence，每一条记录的这3个字段的含义就是Type1=&gt;Type2以及它们的置信度Confidence。</p>
<p>TYPE_COUNT_VIEW构造的是每个单品出现的次数。</p>
<p>R1是所有Type1=&gt;Type2的组合，每行记录表示Type1=&gt;Type2出现的次数，正反两个方向都有，也就是啤酒=&gt;香烟，香烟=&gt;啤酒分别进行了统计。</p>
<p>注意连接条件PARTTERNS.Type1=TYPE_COUNT_VIEW.Type，这个构造出来的方向性实际上只对Type1=&gt;Type2敏感。最后得到的记录结果就是想要的结果。记得在最后这段SQL语句上加上对于置信度Confidence的过滤条件限制，过滤出那些满足最小置信度的模式。</p>
<p>这里需要强调的是，刚刚用SQL求解的过程是一个对于最小频繁2项集的求解过程，也就是求解那些“哪两个”单品的组合的支持度和置信度能够满足要求。Apriori算法的求解过程是求解所有最小频繁项集，2项、3项……一直到N项，只要它们满足设置的支持度和置信度，这就是区别。</p>
<h2 id="11-2-关联分析与相关性分析"><a href="#11-2-关联分析与相关性分析" class="headerlink" title="11.2 关联分析与相关性分析"></a>11.2 关联分析与相关性分析</h2><p>在使用Apriori算法计算出有较高支持度和较高置信度的频繁模式之后，要对这些频繁模式进行一些甄别或者分析。但是，是不是所有的频繁模式都是有趣的？</p>
<p>关于有趣模式，在第10章做过一些讨论。如果一个模式具备以下特点，则它是有趣的（Interesting）。</p>
<p>（1）易于被人理解。</p>
<p>（2）在某种确信度上，对于新的或检验数据是有效的。</p>
<p>（3）是潜在有用的。</p>
<p>（4）是新颖的。</p>
<p>这些特点带有很浓郁的主观色彩，如新颖、是否有用，这些观点本身就是因人而异的，因此所有找到的频繁模式未必都是有趣模式。</p>
<p>Apriori能够过滤出关联度较高的模式，但是还不能对相关性做出解释。</p>
<p>这里需要引入一个有关相关规则的分析。</p>
<p>在前面见过这样一种记述方式，啤酒=&gt;香烟[support=40%；confidence=100%]，也就是Type1=&gt;Type2[支持度，置信度]的这种对频繁项集（关联规则）的记述方式，现在增补一种新的方式，Type1=&gt;Type2[支持度，置信度，关联度]，其中关联度记作correlation。</p>
<p>提升度（Lift）是一种简单的关联度度量，也是一种比较容易实现的统计方法。</p>
<p><img src="Image00329.jpg" alt></p>
<p>上式与朴素贝叶斯公式类似。等号左边是A和B的相关性定义，右边分子是发生A的情况下发生B的概率，分母是发生B的概率。</p>
<p>当相关性是1时，P（B|A）与P（B）相等，也就是说在全样本空间内，B发生的概率和在发生A的情况下发生B的概率是一样的，那么它们就是毫无关系。</p>
<p>当相关性大于1时，P（B|A）大于P（B），也就是说在全样本空间内，发生A的情况下发生B的概率要比单独统计B发生的概率要大，那么B和A是正相关的。换句话说，A的发生促进了B的发生。</p>
<p>相反，当相关性小于1时，P（B|A）小于P（B），也就是说在全样本空间内，发生A的情况下发生B的概率要比单独统计B发生的概率要小，那么B和A是负相关的。换句话说，A的发生抑制了B的发生。</p>
<p>还是以刚刚做好的PATTERNS表为例，SQL代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SELECT FRACTION.Type1,FRACTION.Type2,FRACTION.PBA/NUMERATOR.PBPossibility</span><br><span class="line">FROM</span><br><span class="line">(</span><br><span class="line">  SELECT R1.Support/R2.TRANSACTION_COUNT PBA, R1.Type1, R1.Type2</span><br><span class="line">  FROM</span><br><span class="line">  (</span><br><span class="line">    SELECT COUNT(*) Support, Type1, Type2</span><br><span class="line">    FROM</span><br><span class="line">    PATTERNS</span><br><span class="line">    GROUP BY Type1, Type2</span><br><span class="line">  ) R1</span><br><span class="line">  INNER JOIN</span><br><span class="line">  (</span><br><span class="line">    SELECT COUNT(DISTINCT Serial) TRANSACTION_COUNT, Type1</span><br><span class="line">    FROM</span><br><span class="line">    PATTERNS</span><br><span class="line">    GROUP BY Type1</span><br><span class="line">  ) R2</span><br><span class="line">  ON R1.Type1=R2.Type1</span><br><span class="line">) FRACTION</span><br><span class="line">LEFT OUTER JOIN</span><br><span class="line">(</span><br><span class="line">  SELECT R3.counts/R4.TRANSACTION_COUNT PB, Type2</span><br><span class="line">  FROM</span><br><span class="line">  (</span><br><span class="line">     SELECT COUNT(DISTINCT Serial) counts, Type2</span><br><span class="line">     FROM PATTERNS</span><br><span class="line">     GROUP BY Type2</span><br><span class="line">  ) R3</span><br><span class="line">  INNER JOIN</span><br><span class="line">  (</span><br><span class="line">     SELECT COUNT(DISTINCT Serial) TRANSACTION_COUNT</span><br><span class="line">     FROM</span><br><span class="line">     PATTERNS</span><br><span class="line">  ) R4</span><br><span class="line">  ON 1=1</span><br><span class="line">) NUMERATOR</span><br><span class="line">ON FRACTION.Type2=NUMERATOR.Type2;</span><br></pre></td></tr></table></figure>

</details>

<p>其中Possibility就是要求的每个单品Type对应的要求的提升度，Type1和Type2分别对应公式中的A和B两个项目。条件概率P（B|A）的求法和求支持度的思路是一样的，这里不再赘述。</p>
<p>可以发现在这个求相关性分析的过程中没有新的内容，仍然是对支持度的求解，只是要对单品独立的购买行为也要求解，看看这个支持度起的作用是促进购买概率上升还是下降，仅此而已。</p>
<h2 id="11-3-稀有模式和负模式"><a href="#11-3-稀有模式和负模式" class="headerlink" title="11.3 稀有模式和负模式"></a>11.3 稀有模式和负模式</h2><p>在关联分析的过程中，还有一类研究和前面研究的内容不同，前面研究的都是频繁模式，但也有一些情况下反而更关心那些格外“不频繁”的模式，那就是稀有模式和负模式。</p>
<p>所谓稀有模式，是支持度远低于设定的支持度的模式。这里请注意一点，在前面研究频繁模式设定支持度时，设置的这个门限阈值要么是行业内的一个经验值要么是挑选的大量支持度统计的平均值，大于这个数字，就认为是一个频繁模式。然而并不是小于这个数字就是稀有模式，而是要设置一个比这个数字小得多的值作为过滤条件，比这个值小很多的值才是稀有模式。至于设置为多少，很难用一个具体的数字划分，在实际生产生活中可以考虑用支持度倒排序的功能去找那些支持度极低的模式。</p>
<p>而对于负相关，从名称来考虑，基本可以认为，一种事物的增加同时就对应另一种事物的减少，这种直观感觉下的两种事物就是负相关的。这样的事物在日常生活中同样会遇到很多，例如，这个月购买数码产品的预算一共是3000元，那么买了数码照相机，购买PS4游戏机的预算就不足，导致不能购买PS4游戏机；相反，如果买了PS4游戏机，则很可能由于预算不足不能购买数码照相机。在大量的购物宏观统计中，这就不是观察一个人的行为了，而是观察很多人在大量购买行为中出现的这种取舍性的负相关行为。</p>
<p>一般来说，如果X和Y都是频繁的，但是很少或者不一起出现，那么就说X和Y是负相关的，X和Y组成的模式是负相关模式。如果X和Y组成的模式支持度远远小于X的支持度与Y的支持度的乘积，那么就说X和Y是强负相关的。</p>
<p>要计算是否为强负相关模式，还是要在整个样本空间里找到这种此消彼长，至少是很少一起出现的频繁模式。在实际生产中，挖掘负相关模式的场景还是很多的。如在购物篮分析中，当发现有一些物品X和Y都比较频繁，但是却通常只出现其中一个时，可以来进行统计分析，看看这些负相关的物品是否满足了人们在某一大领域的需求，进而估算人们在这个领域的预算规模和购物规律。在病症治疗或者基因分析中同样可以用负模式挖掘的方法来发现那些有关联的疾病或抗体。例如，在得过A疾病的患者中，罹患B疾病的病人比例比普通人小很多，那么就可以推断很可能A疾病会让病人同时具备抵抗B疾病的抗体。</p>
<h2 id="11-4-小结"><a href="#11-4-小结" class="headerlink" title="11.4 小结"></a>11.4 小结</h2><p>关联分析是数据挖掘中比较重要的一环，尤其是关于频繁项集的分析问题。</p>
<p>在计算机辅助进行的数据处理中，所有的频繁项集的问题都能用基于关系型数据库的统计方法进行分析，如果规模巨大则可以用分布式关系型数据库或者抽样数据进行分析。</p>
<p>关联分析在农业、军事、刑侦、医学等很多领域都有着广泛的应用，是帮助人们认识事物之间的关联关系的重要手段，在建立专家系统或者知识库的过程中，有着不可替代的作用，请读者多练习与思考。</p>
<h1 id="第12章-用户画像"><a href="#第12章-用户画像" class="headerlink" title="第12章 用户画像"></a>第12章 用户画像</h1><p>“用户画像”这个说法现在在数据分析和数据挖掘领域很流行。</p>
<p>这个说法比较形象，它是指在数据库或数据仓库里使用用户信息的记录，对这些信息逐渐丰富以后完成对用户的描述。整个描述的过程就像给用户画像一样，一笔一笔照着模特画，最后完成对模特样子的描述。</p>
<p>希望对用户做“画像”的目的也是比较明确的，就是希望通过某些手段对用户做甄别，把他们分成彼此相同或不同的人群或个体，进而区别化提供服务和进行观察分析——这通常是做用户画像的核心目的所在。</p>
<p>在数据库或者数据仓库中怎么对用户进行画像呢？最常用的办法是用标签来对用户进行画像——描述。</p>
<h2 id="12-1-标签"><a href="#12-1-标签" class="headerlink" title="12.1 标签"></a>12.1 标签</h2><p>标签的英文常用的有Tag和Label，怎么用词不重要，关键看标签怎么用（图12-1）。</p>
<p><img src="Image00330.jpg" alt></p>
<p>图12-1 标签</p>
<p>这些标签是从哪里来的？其实是从很多收集到的和用户相关的线索中来的。什么是线索？就是用户以他的身份标识所留下的各种行为的记录，这些记录基本是从各种各样的日志中来。</p>
<h2 id="12-2-画像的方法"><a href="#12-2-画像的方法" class="headerlink" title="12.2 画像的方法"></a>12.2 画像的方法</h2><p>打标签这件事情在我们发明“用户画像”这个词之前就已经有了。</p>
<p>例如，有很多人性格鲜明，当大家提到他的名字时，脑海里很容易联想起一些形象，或者一些词汇。如一说起医生，有的人脑海里就会浮现出“白衣天使”“救死扶伤”这样的词汇；但是有的人脑海里就会浮现出“勒索钱财”“态度恶劣”这样的词汇。同样的一个或一类对象在两个不同的人看来可能是完全不同的标签内容，原因可能就是这两个人平时关注的新闻热点不一样或者经历不一样。又如听到会计这个职业，就会想到“精细”“准确”，听到拳击运动员就会联想到“强壮”“矫健”。这就是最初的“打标签”动作，和IT技术无关，只是根据自己的认知所留的印象而已。</p>
<p>我们回过来说说生产中用到的例子，先看看数据库吧。</p>
<h3 id="12-2-1-结构化标签"><a href="#12-2-1-结构化标签" class="headerlink" title="12.2.1 结构化标签"></a>12.2.1 结构化标签</h3><p>其实在学习关系型数据库时，一张表里就融入了这种“画像”的方法。这里以数据库中的用户信息登记表Table_User为例介绍，如表12-1所示。</p>
<p>表12-1 用户信息登记表</p>
<p><img src="Image00331.jpg" alt></p>
<p>表12-1中记录的信息其实本身就是用于描述用户的，它们都是画像的标签。虽然这些信息比较有限，但是已经足够对用户做一定程度的区分。根据性别可以区分男女属性，根据所在地区（国家，省，市）可以区分地域，根据生日可以区分年龄段，根据手机号可以区分手机号所在地区。这已经比完全无法判别一个用户ID的拥有者具备什么属性强了不少——用户形象清晰了不少。</p>
<p>但是不足之处我们很快就发现了，我们往下看。</p>
<p>1.信息的丰富性</p>
<p>上述Table_User表中的信息实在是太有限了，能够只根据所在城市、生日（年龄段）、手机号（所在地）、性别就把用户区分开吗？我们也不要过于悲观。</p>
<p>如果你所处的行业，所提供的服务是比较粗犷的，那是否做这些区分恐怕不重要，因为即便这些信息不相同，也没有提供区别化服务的余地。这样的行业不胜枚举，如电力行业、煤炭行业、钢铁行业等。这些行业的用户数量（一般用户都是企业）和产品种类是相对单一的，和对大数据技术热衷的互联网行业、零售业、保险业、广告业等产业大相径庭，几乎没有可比性。所以用户画像对这类行业的吸引力不大，这应该不难理解。</p>
<p>如果你所处的行业是对服务和用户体验敏感的，那么这些信息的价值就太有限了。而且从未来趋势来看，区别化服务对传统粗犷型行业的冲击也在不断增强。只要区别化服务给企业带来的效益比带来的成本高，区别化服务就是一个竞争的利器。这也是很多公司愿意花大力气想办法去做用户画像的比较重要的原因。</p>
<p>对于用户结构化标签比较好的补充是采用一些其他的补充信息表Table_User_Other_Information来做补充描述，如表12-2所示。</p>
<p>表12-2 补充信息表</p>
<p><img src="Image00332.jpg" alt></p>
<p>对于购物类网站可以再建立类似Table_User_Other_Information的表来记录用户的消费行为，这张表的记录内容对用户画像的帮助是更一步逼近了用户的“真实模样”，在对用户品质做区分时有更大的提示作用。那么这些值怎么来？显然是从各次的购买记录中统计而来，这些统计的方法对于会SQL语言的开发人员来说还是很简单的。</p>
<p>知道了用户的性别、手机归属地、生日（年龄段）、所在城市，以及登录频繁程度、经常购买的商品种类、平均消费额度等信息，是不是已经比“未画像”之前有了更清晰的样貌呢？那么还能把用户画得更清晰一些吗？还是可以的。</p>
<p>如果希望把画像做到极致，还可以进行更深一步的描述。例如，服装类Table_User_Buy_Clothes，如表12-3所示。</p>
<p>表12-3 服装类</p>
<p><img src="Image00333.jpg" alt></p>
<p><img src="Image00334.jpg" alt></p>
<p>这里可以继续增补类似Table_User_Buy_Clothes的细类描述，一个用户购买衣服，那么他“喜欢”（至少是经常）买的颜色是什么，红色、黄色、蓝色还是绿色？经常买的衣服风格是什么？日版、韩版、英版还是中国版？不只是衣服，针对每个细类都可以做这种描述，而有的用户肯定是某些东西买得多而其他东西几乎不买。</p>
<p>这类的统计同样可以利用SQL语言轻松完成。</p>
<p>这些表的字段都是标签，而且是结构化的标签，这种结构化的标签很好用，不论是做推荐算法，还是细类分类的个性化研究，都非常方便。</p>
<p>2.信息的正确性</p>
<p>关于信息正确性的问题在Table_User表中就比较凸显。例如，所在地区填写是不是准确，QQ号填写是不是准确。</p>
<p>对于所有用户自己填写的信息来说，有的可以靠验证信息来进行确认，如EMAIL和MOBILE，这些信息可以通过信息回执码的方式来进行确认是否为有效。所在地这种字段可以考虑用外带的IP库来进行辅助确认，但是IP库也不是很准确，至少实时性也是有限的，所以也会有对照查询结果同样不准确的情况。</p>
<p>对于无法清晰界定填写信息是否准确无误的情况不要悲观，客观世界本身就是在不停变化的，只是一个动态中的瞬间平衡状态而已，完美的准确本身就是一种理想，只要在一定合理的范围内逼近准确就够了。</p>
<p>与其一味地纠结用户填写的信息是不是准确，不如把目光和精力更多地投入到更“客观”的数据上去，如购买记录和浏览记录，分析这些真实存在的行为要比停留在用户“主观”填写的一些数据字段有意义得多。</p>
<h3 id="12-2-2-非结构化标签"><a href="#12-2-2-非结构化标签" class="headerlink" title="12.2.2 非结构化标签"></a>12.2.2 非结构化标签</h3><p>1.免费软件真的免费吗</p>
<p>除了结构化的标签，还有很多互联网公司对用户画像尝试使用非结构化标签的画像方法。</p>
<p>非结构化的标签通常不对标签的属性进行明确区分，前面的例子里的每个表的字段有确切含义，这种就是对标签的含义做明确的种类区分。</p>
<p>然而互联网产品本身的特殊性造成很多时候无法对用户做这种属性确定的画像标签。</p>
<p>举个最典型的例子，我们在PC或者手机上都在用着各种免费软件。破解的软件就不提了，就说真的免费软件吧，如QQ、暴风影音、360安全卫士……其中有不少免费软件做得还是相当出色的。</p>
<p>公司不是慈善机构，公司的收入从何而来呢？</p>
<p>熟悉互联网经济模式的人都知道，这种模式就是俗称“羊毛出在猪身上”的互联网经济模式——处在上游的广告主投资，处在中游的软件做载体，处在下游的用户做接受对象。广告主花钱做广告，免费软件的开发商进行广告投放，而免费使用软件的用户则必须忍受或多或少让人抓狂的广告（图12-2）。</p>
<p><img src="Image00335.jpg" alt></p>
<p>图12-2 网络广告</p>
<p>细说来，这个行业的环节不只有3个角色，而是4个角色。</p>
<p>广告主： 花钱为自己的品牌或者产品做广告的人。</p>
<p>媒体： 广告位提供商，如电视台、网站、杂志，在刚刚的例子里，具有弹窗能力的软件都是媒体。</p>
<p>广告商： 本质上就是中介，帮广告主找媒体广告位，帮媒体找广告主。</p>
<p>受众： PC和手机终端用户。</p>
<p>这里的广告商作用是很大的，因为让广告主直接面对每个免费软件商去投放广告显然对于他们来说成本过高了，倒不如交给这些对投放更为专业的广告商，由他们根据广告主的预算和投放意愿进行有效的投放。</p>
<p>而广告商们收取了大量的广告素材（图片、视频、链接）和经费以后会怎么做呢？如何去帮广告主采购合适的广告位呢？现在比较流行的是使用RTB（Real Time Bidding，实时广告竞价）系统进行广告竞价，如图12-3所示。</p>
<p>这看上去有点像淘宝的秒杀，只是淘宝的秒杀是店主挂出有限的货品，在某一个具体的时间开放，然后买家蜂拥而上抢购先到先得。然而RTB的店主是SSP（Supply Side Platform，供应方平台），也就是广告位提供者，他们卖的是展示的广告位，这些广告位通过自动竞价的复杂策略，最后决定投放哪一家广告商的广告资源。抢购者是广告主，只是这个过程是广告商代劳的，广告商帮助他们做了竞价策略并根据策略进行广告投放。</p>
<p><img src="Image00336.jpg" alt></p>
<p>图12-3 广告的运作流程</p>
<p>这种广告的展示和点击收费是不一样的，点击的收费要比单纯展示的收费高很多。很多免费软件其实一直在绞尽脑汁试图根据用户的终端使用习惯做一个画像——尽管这个用户画像画的可能不是用户的个人而是用户的计算机。</p>
<p>“暴力弹窗”是一种在技术手段上显得毫无建树的投放方式，它太直接，太暴力，更让人感觉不爽的是它里面承载的广告内容很多是无法催化一次点击的。广告在暴力弹窗上无区别地投放会让人感觉像在每次进到家门口，都能发现在门口夹了一张硕大的小广告，上面赫然写着“得了脚气怎么办？找××老军医，一针就灵。”而且天天如此。或者是在一个使用管道天然气的小区，门口天天有人举个大牌子，挡住一半的路，上面写着“煤气罐便宜换”，你不得不每次都跟他说一句“闪开，让我过去”（图12-4）。</p>
<p><img src="Image00337.jpg" alt></p>
<p>图12-4 暴力弹窗</p>
<p>这两种方式显然会让人觉得做广告的人欠考虑，如果这种广告是按被用户看了多少次来收费的话，我是广告主也肯定不会找他们做广告的。最起码的，做这种广告还是要有一定的针对性，尤其是这个换煤气罐的，起码应该去一个非天然气管道的小区才会有点效果。</p>
<p>2.不犯法就画画看</p>
<p>好在各种软件在用户PC和手机里还做一些“画画看”的动作，画什么？当然是给用户画像。</p>
<p>软件在用户的PC和手机里都能做什么？从技术角度来看，如果它在PC或手机启动后就被加载入内存则几乎是什么都可以做的，一切行为都可以。还好，有很多信息安全类的软件，如金山毒霸、卡巴斯基等，会监控这些软件的行为，如果这些软件轻易打开本地的文件翻看里面的内容，或者记录在键盘上录入的信息，那么动辄是直接告警，严重的就会直接删除软件内容。此外，在法律层面，对用户敏感信息的收集以及对此产生的损失，软件的制作人是负有连带责任的。况且，这些软件本身在合法商业领域的应用价值的长远性远比做这些偷偷摸摸的下三滥扫描赚钱得多（图12-5）。</p>
<p><img src="Image00338.jpg" alt></p>
<p>图12-5 画像软件</p>
<p>有哪些东西是可以用来给一个PC或者手机做画像的呢？PC和手机上所有带描述作用的信息都可以。软件列表、软件使用记录、浏览器访问记录等都是比较典型的用户画像素材。</p>
<p>注意这里有一个信息差异化的问题。</p>
<p>就用户画像本身来说，只是在做用户的描述，如果每台PC的软件列表、软件使用记录、浏览器访问记录都一样，那么用户画像还是可以做的，只是“画”出来的用户之间“长得”差不多。</p>
<p>可以尝试把PC上的软件列表、软件使用记录、浏览器访问记录都抓取下来，然后做一个关键词的分词工作。或许会分析出很多类似的记录片段，用NoSQL的Key-<br>Value形式记录，如表12-4所示。</p>
<p>表12-4 记录片段</p>
<p><img src="Image00339.jpg" alt></p>
<p>这是对MAC为“37-25-CD-C1-2E-72”的一台终端设备的这些记录做了抓取，并提取关键词得到的。软件列表越长，浏览器访问记录越多，这样的关键词留下的也就越多，这就是最为“粗糙”的一种用户画像方式。与其说是画像，不如说是用户感兴趣的主题的一些提示。当“37-25-CD-C1-2E-72”的广告位（弹窗）再次请求广告资源时，这些事先收集的记录很有可能帮助和这些关键词有关的产品广告获得这个广告位，谁掌握了这些数据谁就有可能对这个MAC地址进行一些辅助性的描述，即提供画像。因此广告请求算法会有一些倾向性的弹窗，如弹出一个冲锋衣的广告，弹出一部高级玛瑙象棋的广告，或者弹出一个高端汽车模型的广告等。所以，这些标签谁会有兴趣购买呢？DMP（Data Management Platform，数据管理平台）会有可能帮助DSP提高一定的转化率，当然后面的匹配算法有可能会非常复杂。</p>
<p>3.标签的权重</p>
<p>这些标签的打法原理大致如此，但是并不是所有的免费软件都一定会无差别地搜集这些数据，因为搜集这些数据并进行分析同样是有成本的。</p>
<p>除了这些免费软件的提供商，还有其他商家会做用户画像操作吗？有的，而且有的商家做这些事情还是手到擒来非常方便，如电信运营商。</p>
<p>由于在我国电信是属于国家管制的领域——实名制注册，电信运营商会天然获得一些用户的信息，如身份证号、手机号，这些信息就天然且准确地存在。除此之外，电信运营商还有很多优势是普通的软件所不具备的，电信运营商有链路，有基站，有核心路由交换设备。所以，家里的电脑或者手机一旦连上网，那么所有的访问记录其实都能够被运营商获得，只要运营商愿意，基站会记录手机所在的位置以及此刻的时间。</p>
<p>你会经常在什么时间出现在哪个基站附近，你喜欢上什么网站，每天上网时间多长，你喜欢浏览哪些淘宝、天猫或者京东的货品专栏，你手机里有哪些APP应用……这些信息，除了HTTPS加密的访问以外，运营商几乎可以做到了如指掌。曾经和我同学聊天——他现在是某电信运营商的大数据部门主管，据说该运营商在做用户画像的时候做得十分细致，包括基本属性、位置属性、交往属性、家庭属性、账户属性、终端属性、行为属性、消费属性、接触属性、应用使用属性、流量属性、内容属性等，非常丰富，最多的情况下曾经给有的用户打标签多达3000多个！先不管打得是不是合理，起码是有了这么多不同的标签描述。</p>
<p>标签和标签之间应该有区别地看待吗？我的观点是，应该做以区别，让它更准确地描述用户。</p>
<p>回头再看刚刚的例子，MAC“37-25-CD-C1-2E-72”，后面的标签有暴风影音，游戏，棋牌，汽车模型，冲锋衣，iPhone 6s Plus，小米Note 3，芈月传，UGG，淘宝。从中能看到浏览顺序吗？能看到对这些不同主题的关心程度吗？（关于怎么来做分词后面在第14章里会介绍）不大能看出来。这其实好比给一个人画像，画了一个鼻子很像，画了一只耳朵很像，画了两只眼睛很像，但是没有放在一起，松散地放了一片。</p>
<p>这样的画像其实还是有很多改进的余地。</p>
<p>其一，想想画像这种东西。我自己3岁时的画像，和我上个月的画像，哪个和现在更像一些？显然是后者。所以，距离现在时间越近发现的一些特质应该和久远时间发现的特质区别对待，更强调近距离时间的特质而模糊远期的特质。</p>
<p>其二，每个人的画像中都有不同的形象，有的人臂膀硕壮，有的人眼大有神，有的人长发飘飘，这些与众不同的特点即便不加以夸张也应该在画像中有所体现。</p>
<p>其三，数字化等级标注。在前两点里面所说的程度的区别是要用数字做标记区分的。还是那句话，没办法做到数字化的东西是不能计算也不能比较的。</p>
<p>改良后的用户标签Table_User_Tag如表12-5所示。</p>
<p>表12-5 改良后的用户标签</p>
<p><img src="Image00340.jpg" alt></p>
<p>用0～9这10个数字来标记程度，0代表很低，9代表很高。这样的一个标记会比原先没有做数字标记更有层次感。这种标记的依据可以有很多，最简单的标记方式可以用以下策略。</p>
<p>每天对用户当天的软件列表和浏览器访问记录进行一次扫描，整理出一堆新的关键词列表，把它和库里的关键词列表比对。新发现的关键词标记为5，已有的关键词的标记数字+1，原来有而当天没有更新的关键词-1。这种策略可以作为一种备选的方式。也就是，头一次发现的关键词就是5，一旦5天没发现这个关键词就标记为0或者干脆遗忘掉（删除），因为这是一个偶然性的关键词；新词连续4天被发现，热度被升为9……这些数值是可以调整的，调整之后的结果无非是遗忘的周期会变化，或者热词升温的节奏会变化而已。</p>
<p>权重标记的策略是一种比较主观的标记方法，因为权重标记策略说到底是一种人写的算法，既然是算法，那就是加工的规则，是人告诉机器要怎么做，这显然是主观的做法。其实不只是权重标记带有主观因素，就连画像本身都带有主观因素。扫描用户访问记录时分词是否恰当，是否要对浏览中的中英文或同义词进行对照转译，是否要对每个浏览的页面的性质进行区分，是否要对众多浏览页面之间的关联做挖掘来判断新的关键词。这些方法的取舍都是在考量成本和收益而已，并无确切的对错可言，所以仍然是主观性占主导的方式。</p>
<h2 id="12-3-利用用户画像"><a href="#12-3-利用用户画像" class="headerlink" title="12.3 利用用户画像"></a>12.3 利用用户画像</h2><h3 id="12-3-1-割裂型用户画像"><a href="#12-3-1-割裂型用户画像" class="headerlink" title="12.3.1 割裂型用户画像"></a>12.3.1 割裂型用户画像</h3><p>前面谈论了用户画像怎么来做。在前面的场景里其实是割裂地进行用户画像——设想自己在一种“公平”、“无偏向”、“无目的”的环境中对用户进行描述，希望对用户的描述尽量做到根据客观观察到的情况做客观性的描述。这种画像就是割裂的用户画像。</p>
<p>这种画像的好处是计算相对比较简单，而且因为画像内容和未来的应用目的无关，所以画像的应用场景也会更加丰富。就拿刚刚说的电信公司画像的例子来说，这样的画像是要比同一个用户在某个垂直电商网站里的画像应用场景更广的（所谓垂直电商就是指只经营某一方面产品的窄范围电商，如只经营母婴产品，只经营各种书籍，或者只经营数码产品等。而与垂直电商相对的就是京东、淘宝这种综合性的电商了）。</p>
<h3 id="12-3-2-紧密型用户画像"><a href="#12-3-2-紧密型用户画像" class="headerlink" title="12.3.2 紧密型用户画像"></a>12.3.2 紧密型用户画像</h3><p>本章最前面举的例子里，在一个购物网站通过购物记录的分析来给用户打标签画像是紧密的用户画像——这个“紧密”是相对前面“割裂”而言的。画像直接来自于营业中的行为，画像的结果也直接应用于营业内容。尤其是对于刚刚提到的这种垂直电商，它的画像紧密程度会更高。</p>
<h3 id="12-3-3-到底“像不像”"><a href="#12-3-3-到底“像不像”" class="headerlink" title="12.3.3 到底“像不像”"></a>12.3.3 到底“像不像”</h3><p>“我的画像到底像不像？”这个问题是所有初试用户画像的技术人员都会问的一个问题。我们可能都曾经历这样的纠结，做了画像之后，怀疑自己画得不够“像”。如果你还在纠结，下面就说说我的看法。</p>
<p>1.“像不像”跟谁比</p>
<p>谈论像和不像时是有比较对象的。</p>
<p>真实在纸上画一个人的肖像，拿去和照片比，肉眼直观是能看出像不像的。在用户信息库里的用户画像也要有对比的对象，问题是，这个对象能捕捉到吗？未必能。</p>
<p>能够收集到的用户相关信息实际是非常片面和有限的，即便可以购买到其他第三方的用户画像库进行对比也不一定能比对出结果。因为收集的标签维度未必一致，维度一致内容不一致也同样不能断言是自己的库错误还是对比库错误。所以这种对比本身就可能是没标准的比较。</p>
<p>2.信息反馈</p>
<p>信息反馈对于紧密型用户画像来说通常比较有效，因为反馈会很直接而且及时，针对性强。也就是说，“画得不准”没关系，再观察再画就是了。</p>
<p>这种情况一般出现在系统“冷启动”时，用户画像库里没有任何可以参考的凭据，标签打出来也是片面的。</p>
<p>在这种紧密型的用户画像系统中，通常会非常依赖信息反馈，因为画像本身也就是为了提高产能转化率。在电商网站上，用户画像就是为了最终的用户购物引导更为有效，而引导是不是有效的验证周期是极短的，甚至一两天就能验证完毕——画得不准大不了过两天再重新试着画一次。这种迭代的思想方式比花费很长时间只为画一个完美的用户画像容易实现得多。</p>
<p>还记得原来我在做公开课的时候，曾经做过一个比喻。</p>
<p>德国二战时期最大的战列舰是俾斯麦号战列舰，满载排水量5万吨左右，主炮口径381mm，最大射程36.5km。战舰安装了“FuMo 23”火控雷达，有效探测距离25km，远不及其火炮的最大射程。人的肉眼在晴好的海面上最远也不过能看到30km左右的地方有目标而已。如果想准确知道25～36.6km这个距离上的目标有没有被炮火击中，就只能出动其搭载的Ar196侦察机了。一轮齐射以后，侦察机报告射击偏差，重新调整射击角度。如此反复。这种战术在很多陆军战斗中的炮兵+侦察兵的配合中也屡试不爽。</p>
<p>这种方式就和我们刚才说的多次画像类似，因为我们的感知精度就是这么有限。</p>
<p>3.0.1%已经很多了</p>
<p>有很多比较成功的用户画像系统，能够把转化率提高0.3%左右，或者也能到0.1%——大概就是这种感觉：原来没有用用户画像系统时，广告推荐系统转化率大概是1%，使用了用户画像系统后，转化率变成了1.1%。听起来这个数字好像挺让人失望的——废了这么大力气才提高这么一点点。</p>
<p>但是别忘了整个市场的容积是很大的，2014年度，淘宝的全年成交额为1.172万亿元人民币，如果能够提高仅仅0.1%的销售额是多少呢，大约11.72亿元人民币。</p>
<p>iiMedia Research（艾媒咨询）在2015年8月发布了《2014—2015年中国DSP行业发展研究报告》。报告显示，2015年中国网络广告市场规模将达到2123.4亿元。2123.4亿元人民币，提高0.1%也有2.1234亿元人民币，这也不是一个小数字！</p>
<h2 id="12-4-小结"><a href="#12-4-小结" class="headerlink" title="12.4 小结"></a>12.4 小结</h2><p>用户画像这个概念只需理解即可。每个公司有不同的用户画像的画法，只要掌握基本的方法，不怕试错，用户画像库是完全有可能收集成为一个对业务有足够帮助的参考系统的。</p>
<p>用这些标签和这些标签对应的用户行为，可以通过逻辑回归或者归纳树算法进行用户行为的预测，也可以由紧密型用户画像直接成为协同过滤的参考对象。大胆尝试，不要怕试错，用户画像不难。</p>
<h1 id="第13章-推荐算法"><a href="#第13章-推荐算法" class="headerlink" title="第13章 推荐算法"></a>第13章 推荐算法</h1><p>推荐算法在现在很多电子商务网站中普遍应用。</p>
<p>推荐系统作为现在众多电商系统、内容分发系统等网站的必要子系统，越来越受到运营者的重视。推荐系统核心要解决的问题是提高转化率，也就是经过分析，要猜测某一个用户更喜欢什么商品，更可能购买什么商品，或者更喜欢哪些歌曲、文章，在系统中要进行适当形式的推荐，如页面飘窗、营销邮件、短信息等。</p>
<h2 id="13-1-推荐思路"><a href="#13-1-推荐思路" class="headerlink" title="13.1 推荐思路"></a>13.1 推荐思路</h2><p>在电子商务网站中有这样的场景，就是系统期望通过分析（或者说猜测）给用户推荐一些商品，从而提高购物的转化率。这种推荐的现象当然首先还是出现在实体店中，会经常在超市或者商场里看到那些卖酸奶的小妹妹会老远就招呼小女孩过去尝尝她们的新品酸奶，或者卖肉的会招呼路过的大爷大妈过来买新鲜的肉。这本身就是人自己在利用推荐系统的思路，起码卖肉的小贩会认为把鲜肉推荐给大爷大妈会比推荐给小朋友有着更高的转化率。</p>
<p>总体来说，推荐系统作为一个确定目的的系统——提高转化率，有很多的线索可以去做。因为只要有片段性的信息，就有提高转化率的可能性，只要转化率比推荐系统不参与的时候高，推荐系统应该说就是有效的，只是不同方式的有效程度有可能会非常悬殊。</p>
<p>其实推荐系统中流派比较多，每个公司每个项目具体落实起来也是千差万别。在没有介绍协同过滤个思路之前，先想想看，利用已经学过的什么方式可以尝试做这种推荐呢？</p>
<h3 id="13-1-1-贝叶斯分类"><a href="#13-1-1-贝叶斯分类" class="headerlink" title="13.1.1 贝叶斯分类"></a>13.1.1 贝叶斯分类</h3><p>可以尝试使用朴素贝叶斯分类的思路来做一下推荐。通过统计某用户所有购买物品的分布特性，统计该用户购买物品的分布情况。</p>
<p>如某用户，他在某网站一共购买过100件商品，其中70件是数码产品或小家电，20件是登山用品，10件是其他各类用品。</p>
<p>在70件数码产品中，有40件是各类音像制品，5件是手机，5件是笔记本电脑，还有30件是U盘、鼠标、MP3、iPad、音箱、USB充电器等各种外设。</p>
<p>20件登山用品里有3件帐篷、3件登山服、3双登山靴、3个登山包、3支登山杖、3顶登山帽、还有2只专用手电。</p>
<p>10件各类用品就非常杂了，有零食、箱包、内衣裤、日化用品等。</p>
<p>这种情况下，其实拿来一个新的商品，判断要不要推荐给某人，就看他购买的记录的偏重，可以说购买音像制品的概率是最高的，在全部的100件货物里占了40%，其他类别的分析也是同理。</p>
<p>所以这种情况下，有理由相信，为这个用户推荐一个音像制品要比什么都不推荐是有更高的转化率的，刚刚的统计结果就是根据。这种做法从思路上应该是没有问题的，但是同样地这些转化率是要进行量化的，只需要记录推荐的商品有多少确实被购买了就知道这种方法的可靠性有多高了。</p>
<h3 id="13-1-2-利用搜索记录"><a href="#13-1-2-利用搜索记录" class="headerlink" title="13.1.2 利用搜索记录"></a>13.1.2 利用搜索记录</h3><p>不管是在一些网站的广告位上，还是在京东商城、淘宝等网店上，都有过类似的经历，就是在浏览器中搜索过的东西，会从广告位上显示出来。在网店里，网店会根据用户搜索过的关键词猜测用户想要买某些产品，所以在列出产品列表的同时也会在右侧给用户推荐一些商品（图13-1）。</p>
<p><img src="Image00341.jpg" alt></p>
<p>图13-1 淘宝网</p>
<p>即便在什么都没有输入的情况下单击“搜索”按钮，淘宝网还是在右侧为用户推荐了一些商品，下面也列出了一个推荐的商品列表，并且还注明是根据“黄钻爱买店铺”和“回头客爱买店铺”这些店铺的热卖商品来推荐的（图13-2）。</p>
<p><img src="Image00342.jpg" alt></p>
<p>图13-2 商品推荐</p>
<p>在其他网站的广告位上也会发现一些有趣的事情，就是曾经在百度或者其他搜索引擎上搜索过的东西会出现在这些网站的广告位上，或者这个网站的广告位上会出现一些与当前页面内容的关键词相关的内容推荐。这是网站广告位的JavaScript代码读取了浏览器的本地Cookies（通常可以用来存储浏览器上的表单信息、用户名、搜索关键词等信息）和当前页面的文本信息，并做了相应的关键词提取，最后根据这些关键词来猜测用户可能感兴趣的内容再推荐到广告位上。</p>
<p>应该说这两种方法的猜测方式都不能算错，只有哪个性价比更好一说。</p>
<p>但是有时候也会看到一些让人感觉推荐系统很傻的地方，就是搜索并购买了一些产品之后它还在不停地进行最类似产品的推荐。就是我们平时在网上购物的时候经常看到现象，买了鞋子还在推荐鞋子，这道可以理解，因为鞋子这种很可能一个人会买不止一双。有的推荐就显得很不智能，比如我买了电视还推荐电视，买了电冰箱还推荐电冰箱。即便是在网站能够得知我购买电视和空调的成交记录的情况下仍然这样做，确实让我们觉得系统很弱智。但是这也还是可以理解，毕竟系统智能的程度是一个改进的过程，这种根据购买产品对象的保有率去对推荐内容做调优的尝试应该不是它最优先改进的内容。</p>
<p>目前协同过滤公认的应该是两种思路，第一种是利用早期大家研究比较多的邻居方法。而邻居方法中也有两种视角，我们分别来看一下。</p>
<p>第一种，基于用户。 也就是说，系统通过分析一个用户和哪些用户的特征比较像，然后看看这些用户喜欢买哪类的商品，再从这些商品里挑出一些推荐给该用户。</p>
<p>第二种，基于商品。 也就是说，系统通过分析用户的购买行为来判断用户喜欢的商品类型，然后从那些用户喜欢的商品类型里挑出一些推荐给用户。</p>
<p>前者称为User-based CF（User-based Collaborative Filtering），或者叫基于用户的协同过滤；后者称为Item-<br>based CF（Item-based Collaborative Filtering），或者叫基于商品的协同过滤。</p>
<p>除了邻居方法外，目前研究得比较多是另外一类方法，也是第二种思路——基于模型的推荐算法，如果读者有兴趣可以在网上搜索阅读Matirx factorization techniques for recommender systems这篇论文。</p>
<p>本章重点介绍邻居方法作为思路的协同过滤算法，这种算法也被称作基于邻域的推荐算法，是一种非常经典的推荐算法思路。</p>
<h2 id="13-2-User-based-CF"><a href="#13-2-User-based-CF" class="headerlink" title="13.2 User-based CF"></a>13.2 User-based CF</h2><p>当一个用户进入一个网店时，作为网店系统找到那些和该用户兴趣（喜好）类似的人，然后看看他们喜欢什么，就给该用户推荐什么也许是一种不错的选择。</p>
<p>第一步，看能不能找到这样的用户。用户与商品偏好如表13-1所示。</p>
<p>表13-1 用户与商品偏好</p>
<p><img src="Image00343.jpg" alt></p>
<p>假设能够得到这样一个用户偏好的列表，请注意两点。</p>
<p>第一，这个列表系统中本身是不存在的，是需要对用户的行为进行分析后量化得到的。其中的某个小格子里的数字是这个用户对该商品“兴趣程度”的一个量化值，0为没兴趣，10为非常有兴趣。量化的方法有很多种，而且没有所谓最正确的算法。得到某个用户对某类别产品的兴趣可以从他给产品的反馈打分上去看，也可以从他购买的频繁程度上去看，也可以从他浏览的频繁程度上去看。在度量这个值时可以只用其中的一种方法，或者对这些方法都进行量化然后加权平均得到。例如，可以设计这样的算法：当一个用户购买了一类商品，这个商品的兴趣程度就+1，当他浏览一次这个商品，兴趣程度就+0.1，当他给商品反馈以10分制打分一次，这个商品的兴趣程度就对应加这个分数的20%，如购买白酒一瓶并打10分，那就在白酒一栏+2，加到10后则不再累加。这可以作为一个策略。</p>
<p>第二，商品的分类是可以做调整的。究竟是使用白酒、红酒、女装这样的大类，还是具体使用某一个更小的分类，甚至精确到单品是值得权衡的。在使用算法的过程中一直在做权衡，计算的效率、占用的空间和推荐的质量的获得，这些其实是要在实践中不断比对来做调整的。这种调整，包括刚才的策略调整，是可以用AB测试来进行对比的，关于AB测试后面将会具体介绍。</p>
<p>假设确实通过一定的策略得到了这样一张表，这张表里是有一些空项的，这代表这个项目系统还没有任何依据来判断兴趣如何，因为系统也不知道是用户没有兴趣所以才不浏览，还是没来得及浏览，这两个概念的区分实在是太暧昧了。</p>
<p>以用户00001为研究对象，要找到和他兴趣最接近的人，怎么做比较好呢？这里需要引入一个概念，叫做余弦相似性。</p>
<p>说到余弦可能大家不陌生，就是高中时候学的cosine函数cos（x）。初中的时候学余弦是在直角三角形（图13-3）里，一个角的余弦值就是对边长度与斜边长度的比，x的取值范围是0°～90°。</p>
<p><img src="Image00344.jpg" alt></p>
<p>图13-3 直角三角形</p>
<p>到了高中就有了新的定义，并且x的取值范围可以是在正负无穷之间了，而且单位换成了弧度。这里用的也是cos这个余弦函数的定义，即两个向量在空间的夹角。</p>
<p><img src="Image00345.jpg" alt></p>
<p>式中，a和b都是向量，如果听着想象不出来，那我们来个图说明一下。</p>
<p><img src="Image00346.jpg" alt></p>
<p>图13-4 余弦</p>
<p>如图13-4所示，有两个三角形，这两个三角形都是直角三角形，a向量和b向量分别是两个三角形的斜边。假设下面的三角形左侧底角为30°，上面的三角形左侧底角为60°，那么是可以知道a向量的长度为2，而a表示为<img src="Image00347.jpg" alt><br>，b向量的长度也是2，表示为<img src="Image00348.jpg" alt> ，这也就是两个向量的顶点在平面直角坐标系里的坐标位置。</p>
<p>用a和b的向量坐标求a和b夹角大小，代入上面的公式：</p>
<p><img src="Image00349.jpg" alt></p>
<p>查反三角函数表，可以知道<img src="Image00350.jpg" alt><br>是30°角的余弦值，所以a和b向量的夹角为30°。这个例子在图上也能直接看出来是30°，印证起来方便一些。</p>
<p><img src="Image00351.jpg" alt><br>这个公式里，a·b就是两个向量的x、y维度各自做了乘积再加和，下面的|a|·|b|算式其实是两个向量线段的长度，即勾股定理，x2 +y2 。</p>
<p>a和b两个向量在空间上只要方向一致，大小不论是否相同，都会得到cos（a，b）=1，而方向相反，则cos（a，b）=-1，有兴趣的话我们可以用别的值来验算一下也无妨。</p>
<p>再看刚刚的例子，把用户在一些不相干的商品类别的爱好当做一个空间向量，把每个商品类别作为一个维度，就像刚才的x和y坐标那样。我们试着求一下00001这个用户和00002这个用户已知部分的爱好相似程度。</p>
<p><img src="Image00352.jpg" alt></p>
<p>因为知道最相似的是1，最不相似的是-1，所以这个相似度还是很高的。</p>
<p>同理也能够求出00001用户和其他任何一个用户的兴趣相似程度。</p>
<p>之后设置一个相似的阈值，如0.8、0.85……或者其他任何一个值，看看相似度超过这个阈值的用户都有什么购物喜好，把他们喜好购买的东西推荐给00001用户作为推荐方案即可。这就是一种思路最为朴素的基于用户的协同过滤算法思路。</p>
<p>扩展一下，除此之外还可以考虑这个用户和用户之间相似的向量还能怎么设计。用户属性表如表13-2所示。</p>
<p>表13-2 用户属性表</p>
<p><img src="Image00353.jpg" alt></p>
<p>或许在一些银行或者理财产品售卖的机构会有这样的一种列表，利用这种列表同样也能够去观察哪些用户之间更相似，然后找到相似的用户，再把这些用户比较喜好的产品推荐给他。方法有很多，后面将会介绍。</p>
<h2 id="13-3-Item-based-CF"><a href="#13-3-Item-based-CF" class="headerlink" title="13.3 Item-based CF"></a>13.3 Item-based CF</h2><p>除了刚才介绍的基于用户的协同过滤以外，再来看一下基于商品的协同过滤。</p>
<p>基于物品的协同过滤算法最早是由著名的电商公司亚马逊提出的。这种算法给用户推荐那些和他们之前喜欢的商品相似的商品。但是，这种算法和前面的基于用户的协同过滤算法不一样——它并不是要建立一个商品属性的矩阵来计算物品之间的相似度。其实想想也知道这种方式很可能行不通，一来是由于商品之间的属性相差较大，做起来可能会比较困难，二来是由于计算量太大难以实现。所以这个算法主要通过分析用户的行为来计算物品之间的相似度。</p>
<p>一句话概括就是这样：“有很多人喜欢商品A，同时他们也喜欢商品B，所以A和B应该是比较类似的商品。”这就是整个算法的核心思路。</p>
<p>计算起来可以分成以下两个步骤。</p>
<p>（1）计算商品之间的相似度。</p>
<p>（2）根据物品的相似度和用户的偏好来给用户生成推荐列表。</p>
<p>这里同样用到了余弦相似性的概念，但是公式略有不同：</p>
<p><img src="Image00354.jpg" alt></p>
<p>解释一下，如果计算商品A和商品B的相似性，那么就计算这个商值，分子是同时喜欢A和B两个商品的用户数量，分母是喜欢A的用户数量和喜欢B的用户数量的乘积的平方根。</p>
<p>如果要得到产品和用户喜好数量的关系，计算过程如下。</p>
<p>用户00001：象棋、扑克牌、篮球</p>
<p>用户00002：扑克牌、乒乓球、乒乓球拍</p>
<p>用户00003：乒乓球、乒乓球拍</p>
<p>用户00004：围棋、扑克牌</p>
<p>用户00005：象棋、围棋、足球、扑克牌</p>
<p>这是一个文娱用品商店的销售记录，记录了每一个用户购买的产品内容，这里只用5个用户来做一个演示。</p>
<p>首先要分别得到每个用户购买物品的邻接矩阵，如用户00001购物邻接矩阵如表13-3所示。</p>
<p>表13-3 用户00001购物邻接矩阵</p>
<p><img src="Image00355.jpg" alt></p>
<p>这个矩阵就是根据刚刚看到的用户00001的购买记录得到的，由于象棋、扑克牌和篮球同时出现在他的购物列表里，所以“象棋和扑克牌”、“象棋和篮球”、“扑克牌和篮球”两两“邻接”，也就是说这些标注1的小格子代表这两种一起在一个人的购物记录里出现过一次——注意买过就算，可不是必须出现在同一次购物篮里，这一点和关联分析时所用的Apriori算法的场景是不同的。</p>
<p>同样能够得到其他几个人的购物邻接矩阵。</p>
<p>用户00002购物邻接矩阵如表13-4所示。</p>
<p>表13-4 用户00002购物邻接矩阵</p>
<p><img src="Image00356.jpg" alt></p>
<p>用户00003购物邻接矩阵如表13-5所示。</p>
<p>表13-5 用户00003购物邻接矩阵</p>
<p><img src="Image00357.jpg" alt></p>
<p>用户00004购物邻接矩阵如表13-6所示。</p>
<p>表13-6 用户00004购物邻接矩阵</p>
<p><img src="Image00358.jpg" alt></p>
<p>用户00005购物邻接矩阵如表13-7所示。</p>
<p>表13-7 用户00005购物邻接矩阵</p>
<p><img src="Image00359.jpg" alt></p>
<p><img src="Image00360.jpg" alt></p>
<p>所有的这种邻接矩阵都是沿对角线对称的。</p>
<p>下一步把这些矩阵“叠加”在一起，即将每一个矩阵的每个对应的方格数字相加，最后得到如表13-8所示的中间矩阵C。</p>
<p>表13-8 中间矩阵C</p>
<p><img src="Image00361.jpg" alt></p>
<p>从这个中间矩阵里，可以看到同时喜欢象棋和扑克牌的有2个人，同时喜欢乒乓球拍和乒乓球的有2个人，同时喜欢围棋和象棋的有1个人……由于矩阵是对称的，所以读右上方的三角形就足够了。</p>
<p>这时如果对任意两个商品的相似度做评估，如计算象棋和围棋的相似程度，套用刚才的公式：</p>
<p><img src="Image00362.jpg" alt></p>
<p>分子是同时喜欢围棋和象棋的人，下面两个值N（A）和N（B）就是喜欢围棋和喜欢象棋的人——这两个值要从前面的购物记录里得到。</p>
<p><img src="Image00363.jpg" alt></p>
<p>象棋和围棋的相似度约为0.5。</p>
<p>再试算一下乒乓球拍和乒乓球的相似度：</p>
<p><img src="Image00364.jpg" alt></p>
<p>说明相似度极高，买乒乓球的人必买乒乓球拍，买乒乓球拍的人必买乒乓球。</p>
<p>全表（商品相似度）如表13-9所示。</p>
<p>表13-9 商品相似度</p>
<p><img src="Image00365.jpg" alt></p>
<p>具体在做推荐的时候可以这样使用。</p>
<p>计算完中间矩阵C之后，当要对一个用户做推荐时，先把这个用户的历史购买记录都列出来，假设有n个购买记录。然后对这个列表里每一个产品都用查表的方法查一次相似度，这样会得到n个列表，每个列表里都是一个产品和其对应的相似度的关系。把这n个列表做一个排序，相似度高的在前，相似度低的在后。如果要推荐3个商品就取前3个，如果要推荐5个商品就取前5个。</p>
<h2 id="13-4-优化问题"><a href="#13-4-优化问题" class="headerlink" title="13.4 优化问题"></a>13.4 优化问题</h2><p>1.规模和效率</p>
<p>早在2012年时京东商城的商品就已经超过100万种单品了，日均PV超过5000万。在这么大规模的环境下，使用Item-<br>basedCF算法会出现一些显而易见的问题。</p>
<p>如果有100万种商品，每种商品都被人买过至少一次，那么会产生一个100万×100万的矩阵，也就是10000亿个单位的表格。如果每个单位都用4字节的整数来计数，光这个表格就至少要使用3.64TB的数据——别说内存了，硬盘放都困难。</p>
<p>其实不妨想想看，这些商品也许确实都被人买过，但是什么时候买的，买的人是经常来买还是偶尔买了一次其实是没有做任何区分的。然而在一个购物网站中，应该更重视那些在网站经常购买商品的人，因为这些人才是真正的网购习惯者，另外就是近期被人购买的商品，远远比那些老的过时的或者淘汰的商品有价值。那么真的应该挑出100万种商品做这个邻接矩阵的计算吗？未必，而且我也不推荐那样做。可以尝试着只从活跃用户的购物列表去找候选产品，也可以从最近半年被人购买的产品中找候选产品，也可以两者结合来进行。</p>
<p>2.覆盖率</p>
<p>在Item-basedCF算法中还会存在一种问题，那就是关于覆盖率和多样性的问题。下面来具体看一个例子。</p>
<p>假设经过计算某用户喜欢的物品里有3本不同的书，3件不同的衣服，3盒不同品牌型号的乒乓球。很可能由于购买乒乓球时就会买乒乓球拍，导致进行商品相似度计算时，给该用户推荐的商品里乒乓球拍的相似度总是最高的，该用户也只会收到系统关于乒乓球拍的商品推荐，而这其实不是我们期望得到的。其次，那些热门商品之间的相似度也会非常高，因为大家在一个时间段内都买这些热门商品的概率比较高，所以这些商品之间的相似度计算出来自然就比较高。</p>
<p>我们期望的不是一个高度收敛的推荐算法，而是商品种类要丰富，也就是商品的覆盖率要高，要保证它的多样性。这里需要用到一个物品相似度的归一化算法。</p>
<p>所谓归一化就是把商品相似度矩阵做如下变化：</p>
<p><img src="Image00366.jpg" alt></p>
<p>也就是对每一行的相似度值和当前行的最大值计算一个比值，把这个比值当作新的结果放在矩阵里，变换之后的归一化商品相似度矩阵如表13-10所示（商品相似度-<br>Norm）。</p>
<p>表13-10 归一化商品相似度矩阵</p>
<p><img src="Image00367.jpg" alt></p>
<p>这样直观的感觉就是所有原来相似度看上去比较低的值都被拉高了，缩小了差距，这其实是对刚刚的忧虑在算法上做出了一些补偿。</p>
<p>这种补偿的思路如果没有理解清晰，可以看如下补充的例子。</p>
<p>在大学里有很多科目的考试，而对于科目考题难度的设计通常比较难把握。每一年会由于招生政策的变化以及提档线的变化导致生源质量不同；教材体系改革会导致教材难度也有不规律的波动；大学教师的教学水平和风格也会有差异等。这么多不同的因素组合在一起，就有可能引发一些奇怪的现象，如某一年由于考题设计太难，导致整个年级的学生最高分才59分，即便可以让他们每个人都算挂科一次，但是考试的选拔特性变得不太好，考生之间的档次也看不出来。</p>
<p>这里可以进行一次核算，和刚才的公式一样，让这个最高分的59分换算为100分，其他各位学生的分数同样做这种换算，那么就是套用如下公式：</p>
<p><img src="Image00368.jpg" alt></p>
<p>这样做的好处不是为了挽救一些人让他们不要被列入不及格的范畴，而是把原本分布很窄的一个分数区间拉开了，让分数和分数、人和人之间的距离感更好，便于进一步遴选和分类。这种思路在算法中会有很多地方有体现。归一化在第9章中也有使用，作用是很相近的。</p>
<h2 id="13-5-小结"><a href="#13-5-小结" class="headerlink" title="13.5 小结"></a>13.5 小结</h2><p>推荐系统是一个综合的生产过程，几乎所有用来提高转化率的方法都可以用来作为推荐系统的一部分。可以采用在本章中提到的协同过滤算法，可以采用基于用户画像的逻辑回归，也可以使用关键分析中的频繁项集去寻找可推荐的商品。</p>
<p>本章接触到一个新的度量距离的手段，就是使用余弦相似度来进行度量，这和以前介绍的用欧氏距离或曼哈顿距离的方法是大不一样的。余弦相似度用的是夹角概念，例如，张三买了5双皮鞋，又买了5双球鞋，这样在（皮鞋，球鞋）这两个维度的向量空间中可以用（5，5）来表示；李四买了3双皮鞋，又买了3双球鞋，就用（3，3）来表示。在这种情况下，可以认为他们对鞋类的喜爱程度虽然有所差别，但是体现出来的还都是没有什么疑义的喜爱，这种态度是明确的。所以在这个空间中向量（5，5）和向量（3，3）实际上夹角是0，也就是余弦相似度为1——非常相似。这种解释要比求出的欧氏距离<img src="Image00369.jpg" alt><br>更有意义，因为在欧氏距离上同为<img src="Image00369.jpg" alt><br>的距离，其向量上所体现出来的对不同物品维度的倾向恐怕会有很多不同甚至是相反的。如图13-5所示，AB线段用来表示<img src="Image00369.jpg" alt><br>的距离，一象限的两个向量之间的距离是<img src="Image00369.jpg" alt> ，二、三象限的两个向量之间的距离仍然是<img src="Image00369.jpg" alt><br>，但是从直观上看，一象限的两个向量的方向一致性要比二、三象限这两个向量的方向一致性好很多。在这种情况下是不应该做出距离相等的判断的。</p>
<p><img src="Image00370.jpg" alt></p>
<p>图13-5 欧氏距离不能体现向量的相似程度</p>
<p>在实际应用中，要注意多进行摸索，评估方法的有效性和对比测试，并作出不断的调整，这样才能使得算法准确程度不断进化。</p>
<h1 id="第14章-文本挖掘"><a href="#第14章-文本挖掘" class="headerlink" title="第14章 文本挖掘"></a>第14章 文本挖掘</h1><p>文本挖掘是近几年来越来越火的数据挖掘方向。</p>
<p>对于传统的结构化数据挖掘来说，文本挖掘更多的是对自然语言的分析，模糊性强，结构性弱，难度大，一直都是挑战的方向。</p>
<p>文本挖掘一般来说是从大量文本数据中抽取事先位置的、可理解的、最终可用的知识的过程，同时运用这些只是更好地组织信息以便将来参考。这是一个从非结构化的文本信息中寻找知识的过程。</p>
<h2 id="14-1-文本挖掘的领域"><a href="#14-1-文本挖掘的领域" class="headerlink" title="14.1 文本挖掘的领域"></a>14.1 文本挖掘的领域</h2><p>文本挖掘一般来说有以下7个主要的领域。</p>
<p>（1）搜索和信息检索（Information Search，IR）：存储和文本文档的检索，包括搜索引擎和关键字搜索。</p>
<p>（2）文本聚类：使用聚类方法，对词汇、片段、段落或文件进行分组和归类。</p>
<p>（3）文本分类：对片段、段落或文件进行分组和归类，在使用数据挖掘分类方法的基础上，通过训练来标记示例模型。</p>
<p>（4）Web挖掘：在互联网上进行数据和文本挖掘，并特别关注网络的规模和相互联系。</p>
<p>（5）信息抽取（Information Extraction，IE）：从非结构化文本中识别与提取有关的事实和关系；从非结构化或半结构化文本中抽取出结构化数据的过程。</p>
<p>（6）自然语言处理（Natural Language Processing，NLP）：将语言作为一种有意义、有规则的符号系统，在底层解析和理解语言的任务（如词性标注）；目前的技术主要从语法、语义的角度发现语言最本质的结构和所表达的意义。</p>
<p>（7）概念提取：把单词和短语按语义分成意义相似的组。</p>
<p>以上每一个概念内容都非常多，每一个概念都能写一本书甚至几大本书，这里主要对文本分类及相关的问题进行讨论。</p>
<h2 id="14-2-文本分类"><a href="#14-2-文本分类" class="headerlink" title="14.2 文本分类"></a>14.2 文本分类</h2><p>和其他分类学习的思路相似，拿到文本的样本后也是要对文本进行样本类别标记，然后把这些样本交给计算机进行学习。</p>
<p>文本分类中训练的主要工作步骤如下。</p>
<p>1.分词</p>
<p>由于文本本身的非结构化或者半结构化特性，没办法对一篇文章直接做向量标记。所以，拿到一篇文章后，通常第一步是做分词。而后通过词义以及词与词的逻辑衔接来判断语言的意义或情绪。</p>
<p>分词是在中文文本处理中遇到的第一个问题，因为和英文不一样，中文是没办法用空格进行分词的。所以最早在中文中尝试做分词就是使用类似查字典的办法，用标点把文章分成多个子句，在每个字句中推进式地“查词典”，在词库字典中查到的完整词就从字句中拿掉，然后继续向下搜索。这种方式已经淘汰了，因为它的查询非常机械而且不准确。</p>
<p>如句子“北京大学是所老牌大学。”这个句子如果按照这么机械的划分方法就会被划分成“北京”“大学”“是”“所”“老牌”“大学”，但是“北京大学”其实是一个专有名词，不应该这么划分。</p>
<p>而随着计算机技术的发展与进步，包括软件和硬件方面的进步，更为科学和智能的分词方式也逐步开发出来，即基于统计语言模型（Statistical Language Model，SLM）的方式。其中比较有影响力的是中国科学院计算所开发的汉语词法分析系统NLPIR汉语分词系统，也叫做ICTCLAS2013，现已公开发布供中文文本分类的研究使用。这是一个开源的软件系统，读者有兴趣可以在网上搜索并下载。据称这款产品的分词精度能够超过98%，中国人人名的识别召回率也接近98%。NLPIR在网上是有开放的在线文章分析平台以及分词系统下载的。</p>
<p>补充介绍一下，召回率（Recall）和精度（Precise）是广泛用于信息检索和统计学分类领域的两个指标，用来评价结果的质量，同样这两个指标在推荐系统也得到了广泛应用。其中召回率是检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。精度是检索出的相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率。</p>
<p>除此之外还有庖丁解牛分词器，这是一款基于Lucene的中文分词器开源软件系统，也是一个开源的分词器系统，不少熟悉Java和Lucene的程序员应该都对此比较熟悉，在此不做重点介绍，读者有兴趣可以自己去研究一下。</p>
<p>2.文本表示</p>
<p>文本表示其实就是文本的向量化问题，因为就文本本身来说，计算机是无法理解其含义的。现在用得比较多的模型是由Gerard Salton和McGill于1969年提出的向量空间模型（Vector Space Model，VSM）。</p>
<p>向量空间模型的基本思想是把文档简化为特征项的权重为分量的向量表示：（w1 ，w2 ，…，wn ），其中wi 为第i个特征项的权重，一般选取词作为特征项，权重用词频表示。词频分为绝对词频和相对词频。</p>
<p>绝对词频，即用词在文本中出现的频率表示文本。</p>
<p>相对词频，即为归一化的词频，其计算方法主要运用TF-IDF公式（Term Frequency-Inverse Document Frequency）。</p>
<p>什么是归一化呢？在第13章中也提到过归一化的问题，就是从绝对数量转化成比例的一种思路。在TF-<br>IDF算法中，归一化是为了避免长文档比短文档拥有过多数量的词频而采取的方式。举例如下。</p>
<p>词频（TF）是一个词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“汽车”出现了5次，那么“汽车”一词在该文件中的词频就是5/100=0.05或者说5%。一个计算文件频率（IDF）的方法是测定有多少份文件出现过“汽车”一词，然后除以文件集里包含的文件总数。所以，如果“汽车”一词在100份文件出现过，而文件总数是10000份，其逆向文件频率就是lg（10000/100）=2。最后的TF-<br>IDF的分数为0.05×2=0.1。其中TF-IDF分数和词频和逆向文件频率成正比。</p>
<p>也就是说，如果某个词汇在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为这个词汇有很好的类别区分能力，适合用来分类；TF表示该词汇在文档中出现的频率，而IDF则表示，含有该词汇的文档比例越低IDF越大，则说明该词汇具有越好的类别区分能力。</p>
<p>3.分类标记</p>
<p>分词和分词权重最后要和分类的标签之间产生一个映射关系。而描述这种映射的过程是需要算法来实现的，可以用概率来实现，也可以用基于向量空间的回归来实现。常用的算法有Rocchio算法、朴素贝叶斯分类算法、K-近邻算法、决策树算法、神经网络算法和支持向量机算法等，下面就对每种算法的实现原理做一个说明。</p>
<h3 id="14-2-1-Rocchio算法"><a href="#14-2-1-Rocchio算法" class="headerlink" title="14.2.1 Rocchio算法"></a>14.2.1 Rocchio算法</h3><p>Rocchio算法是一种从感性上非常直观的文本分类的算法。</p>
<p>Rocchio算法的核心思路是给每一个文档的类别都做一个标准向量——也有的地方称为原型向量，然后用待分类的文档的向量和这个标准向量比一下余弦相似度，相似度越高越可能属于该分类，反之则不然。</p>
<p>例如，在某新闻网站中，希望构造一个自动的文章分类系统，那么先收集10000个样本，然后由人给每篇文章划分类别。例如，有“军事类”、“体育类”、“经济类”、“娱乐类”、“科技类”等，每一篇文章都有至少一个所属的类别。如“军事类”，把里面每一篇文章逐个拿出来做分词和向量化，这样最后“军事类”里面的每一篇文章都有一个非常长的向量模型。</p>
<p>文章1：（’坦克，0.05’，’侵略，0.03’，’反击，0.01’，’战争，0.03’，’爆发，0.01’，’动员，0.01’，…）</p>
<p>文章2：（’战机，0.03’，’临空，0.01’，’雷达，0.02’，’抗议，0.01’，’和平，0.03’，’呼吁，0.02’，…）</p>
<p>文章3：（’和谈，0.02’，’斡旋，0.02’，’冲突，0.02’，’中东，0.01’，’和平，0.02’，’外交，0.02’，…）</p>
<p>后面还可以有很多的文章样本。</p>
<p>在这里为了表示方便，维度和它的权重直接标在一起，如’坦克，0.05’表示“坦克”这个词的词频为0.05。</p>
<p>把“军事类”所有的文章进行各个维度的平均，也就是对每篇文章中的“坦克”、“侵略”、“战机”、“雷达”等词汇的词频进行平均，会得到一个诸如这样的向量：军事类原型向量：（’坦克，0.010’，’侵略，0.003’，’战机，0.003’，’临空，0.002’，’雷达，0.003’，…），这个向量非常长，可能有几千或者几万维，可以把这个原型向量形象地称为“质心”。读者应该也注意到，在文本挖掘中使用的向量和前面在多维向量空间一章所介绍的向量有所不同。在第7章中的向量通常维度比较少，而且维度的值或为实数或为枚举。而在文本挖掘中用来描述文章的向量通常因文章的不同而不同，小短文也有几百个维度，长文章可能会有上万的维度。这里面一个词就是一个维度，如“坦克”、“侵略”、“战机”等就是维度的名称，相当于三维空间的x、y、z。后面的0.010、0.003、0.002等值是向量维度的值，在0和1区间内。</p>
<p>当有一篇新的文章要进行分类时，同样进行分词和向量化，也标记成向量和词频的形式。之后就顺理成章了，就是向量和向量求余弦相似性的计算了。“军事类”、“体育类”、“经济类”、“娱乐类”等这些文章类别各自都有一个原型向量，新的文章和它们逐个比较，和谁相似性越高，就属于谁。</p>
<p>余弦相似性怎么计算呢？来看以下公式：</p>
<p><img src="Image00371.jpg" alt></p>
<p>这个公式和在第13章里接触到的余弦相似性的计算规则非常相似，公式中s代表原型向量，c代表待分类向量，s向量各自的维度是s1 ，s2 ，…，sn<br>，c向量各自的维度是c1 ，c2 ，…，cn 。最后cos（θ）越接近1就说明越相似，越接近0就说明越不相似，注意这里面没有小于0的情况。</p>
<p>Rocchio算法还有一种改进的版本，在这个版本里，某一类文章不仅有正样本计算而来的原型向量（正向量），还有根据负样本（非本类文章）计算而来的负向量。在原型向量计算的过程中，希望它尽量靠近正样本而远离负样本。</p>
<p>在做所有的文本分类实验之前，先要做一个操作，就是从网上下载一些文本作为训练样本，在这里选用20Newsgroup提供的文本信息。它的官方网站网址是<a href="http://qwone.com/～jason/20Newsgroups/" target="_blank" rel="noopener">http://qwone.com/～jason/20Newsgroups/</a>，读者有兴趣可以再尝试访问该网站阅读更多的相关信息。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">from pprint import pprint</span><br><span class="line"></span><br><span class="line">newsgroups_train = fetch_20newsgroups（subset=&apos;train&apos;）</span><br><span class="line">pprint（list（newsgroups_train.target_names））</span><br><span class="line">#20个主题</span><br><span class="line">[&apos;alt.atheism&apos;，</span><br><span class="line">&apos;comp.graphics&apos;，</span><br><span class="line">&apos;comp.os.ms-windows.misc&apos;，</span><br><span class="line">&apos;comp.sys.ibm.pc.hardware&apos;，</span><br><span class="line">&apos;comp.sys.mac.hardware&apos;，</span><br><span class="line">&apos;comp.windows.x&apos;，</span><br><span class="line">&apos;misc.forsale&apos;，</span><br><span class="line">&apos;rec.autos&apos;，</span><br><span class="line">&apos;rec.motorcycles&apos;，</span><br><span class="line">&apos;rec.sport.baseball&apos;，</span><br><span class="line">&apos;rec.sport.hockey&apos;，</span><br><span class="line">&apos;sci.crypt&apos;，</span><br><span class="line">&apos;sci.electronics&apos;，</span><br><span class="line">&apos;sci.med&apos;，</span><br><span class="line">&apos;sci.space&apos;，</span><br><span class="line">&apos;soc.religion.christian&apos;，</span><br><span class="line">&apos;talk.politics.guns&apos;，</span><br><span class="line">&apos;talk.politics.mideast&apos;，</span><br><span class="line">&apos;talk.politics.misc&apos;，</span><br><span class="line">&apos;talk.religion.misc&apos;]</span><br><span class="line"></span><br><span class="line">#这里选取4个主题</span><br><span class="line">categories = [&apos;alt.atheism&apos;， &apos;comp.graphics&apos;， &apos;sci.med&apos;， &apos;soc.religion.christian&apos;]</span><br><span class="line"></span><br><span class="line">#下载这4个主题里的文件</span><br><span class="line">twenty_train = fetch_20newsgroups（subset=&apos;train&apos;， categories=categories）</span><br><span class="line"></span><br><span class="line">#文件内容在twenty_train.data这个变量里，现在对内容进行分词和向量化操作</span><br><span class="line">count_vect = CountVectorizer（）</span><br><span class="line">X_train_counts = count_vect.fit_transform（twenty_train.data）</span><br><span class="line"></span><br><span class="line">#接着对向量化之后的结果做TF-IDF转换</span><br><span class="line">tfidf_transformer = TfidfTransformer（）</span><br><span class="line">X_train_tfidf = tfidf_transformer.fit_transform（X_train_counts）</span><br></pre></td></tr></table></figure>

</details>

<p>Rocchio的示例代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors.nearest_centroid import NearestCentroid</span><br><span class="line"></span><br><span class="line">#现在把TF-IDF转换后的结果和每条结果对应的主题编号twenty_train.target放入分类器中进行训练</span><br><span class="line">clf = NearestCentroid（）.fit（X_train_tfidf， twenty_train.target）</span><br><span class="line"></span><br><span class="line">#创建测试集合，这里有2条数据，每条数据一行内容，进行向量化和TF-IDF转换</span><br><span class="line">docs_new = [&apos;God is love&apos;， &apos;OpenGL on the GPU is fast&apos;]</span><br><span class="line">X_new_counts = count_vect.transform（docs_new）</span><br><span class="line">X_new_tfidf = tfidf_transformer.transform（X_new_counts）</span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">predicted = clf.predict（X_new_tfidf）</span><br><span class="line"></span><br><span class="line">#打印结果</span><br><span class="line">for doc， category in zip（docs_new， predicted）：</span><br><span class="line">    print（&apos;%r =&gt; %s&apos; %（doc， twenty_train.target_names[category]））</span><br></pre></td></tr></table></figure>

</details>

<p>Rocchio算法的缺陷是很明显的，它做了两个假设，使得它的分类能力打了很大的折扣。</p>
<p>假设一： 一个类别的文档仅仅聚集在一个质心的周围，实际情况往往不是如此。</p>
<p>假设二： 训练数据是绝对正确的，因为它没有任何定量衡量样本是否含有噪声的机制，错误的分类数据会影响质心的位置。</p>
<h3 id="14-2-2-朴素贝叶斯算法"><a href="#14-2-2-朴素贝叶斯算法" class="headerlink" title="14.2.2 朴素贝叶斯算法"></a>14.2.2 朴素贝叶斯算法</h3><p>朴素贝叶斯算法关注的是文档属于某类别的概率。文档属于某个类别的概率等于文档中每个词属于该类别的概率的综合表达式。而每个词属于该类别的概率又在一定程度上可以用这个词在该类别训练文档中出现的次数（词频信息）来粗略估计，因而使得整个计算过程可行。使用朴素贝叶斯算法时，在训练阶段的主要任务就是估计这些值。</p>
<p>所以前两步仍然是分词和向量化。</p>
<p>贝叶斯概率公式如下：</p>
<p><img src="Image00372.jpg" alt></p>
<p>上式是完整的贝叶斯概率公式，简写的朴素贝叶斯概率公式如下：</p>
<p><img src="Image00373.jpg" alt></p>
<p>上式指的是在全样本空间里，独立事件B发生的概率乘以在B发生的情况下发生A的概率，等于独立事件发生A的概率乘以在A发生的情况下发生B的概率。</p>
<p>如果研究两个事件的条件概率关系用这个简化版的公式肯定是一目了然的，那么在文章分类里怎么用呢？</p>
<p>首先，如果能够统计某个词在某类别文章中出现的概率，就用P（Dj |x）来表示，如x是“军事类”（肯定还有y分类“体育类”，z分类“经济类”等），Dj 表示某一个词的词频，如D1 表示“雷达”的词频，那么P（D1<br>|x）就表示“军事类”文章中出现“雷达”的概率。除了“雷达”以外，肯定还有很多其他词向量的词频统计，所以就有很多的P（Dj |y）、P（Dj<br>|z）来分别表示它们在“体育类”或“经济类”文章中的每个词的词频。</p>
<p>反过来再看，还是以“雷达”这个词为例，恐怕不止出现在“军事类”的文章中，可能也出现在其他小说或者广告、科普读物里。那么“雷达”出现在“军事类”、“体育类”、“经济类”文章中的概率用P（x|D1<br>）、P（y|D1 ）、P（z|D1 ）来表示，如有10000篇“军事类”文章，其中有500篇提到了雷达，那P（x|D1<br>）就是0.05；除此之外，也可以求出每一个P（x|Dj ）、P（y|Dj ）、P（z|Dj ）等。</p>
<p>总结来说，步骤如下。</p>
<p>（1）对训练文章进行分词和向量化。</p>
<p>（2）对所有文章类别计算P（Dj |x）、P（Dj |y）、P（Dj |z）等。</p>
<p>（3）对待分类的文章进行分词和向量化。</p>
<p>（4）用待分类文章的词向量中的每个词计算P（x|Dj ）、P（y|Dj ）、P（z|Dj ）等。</p>
<p>需要强调的是，P（x|Dj ）可不是计算一个值，而是计算整个词向量中所有的词，如果词向量有1000个元素，那么Dj 就是D1 到D1000<br>，这1000个词都要进行计算。</p>
<p>（5）计算概率，看看待分类文章属于哪个类型的文章概率最大。</p>
<p>这个部分计算的时候要注意计算技巧，如计算一个完整的词向量D1 到D1000 属于x类的概率，公式如下：</p>
<p><img src="Image00374.jpg" alt></p>
<p>解释一下，P（Dj |x）中出现完整词向量Dj 所有元素的概率就是出现每个词的概率相乘，这个思考方式跟扔硬币没区别。</p>
<p>对P（Dj |x）P（x）=P（x|Dj ）P（Dj ）的含义也就好理解了吧，我们把它做个变形：</p>
<p><img src="Image00375.jpg" alt></p>
<p>P（x|Dj ）就是要求的，完整的词向量Dj 最终属于x文章分类的概率。</p>
<p>P（Dj ）可以设为1，因为对于已经拿到的待分类文本，所有的词频发生概率就已经是1了。</p>
<p>P（Dj |x）的含义我们刚刚解释过。</p>
<p>P（x）是所有训练文章中x类文章出现的概率。如果每个类别的文章都用一样多的数量来训练，如“军事类”、“体育类”、“经济类”各100篇，那所有的P（x）、P（y）、P（z）值都一样，都是<img src="Image00376.jpg" alt>。在这个公式里，所有的类别最后都是互相比P（x|Dj ），一个分类的P（x|Dj<br>）越大就说明这篇文章属于这个分类的概率越大，每个分类P（x）都一样大的情况下，那就可以都不乘了，直接化简成P（x|Dj ）=P（Dj<br>|x）。要记住，必须是所有类别的文章训练样本的数量一样多的时候才能做这个简化。</p>
<p>在前面介绍朴素贝叶斯分类的时候提到过，在Python Scikit-<br>learn库中支持高斯朴素贝叶斯、多项式朴素贝叶斯和伯努利朴素贝叶斯3种朴素贝叶斯分类算法。在这里使用多项式朴素贝叶斯来做文章分类，完整的例子如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#在Scikit-learn里提供了几种贝叶斯分类，其中多项式贝叶斯最适合做文本分类</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line"></span><br><span class="line">#现在把TF-IDF转换后的结果和每条结果对应的主题编号twenty_train.target放入分类器中进行训练</span><br><span class="line">clf = MultinomialNB（）.fit（X_train_tfidf， twenty_train.target）</span><br><span class="line"></span><br><span class="line">#创建测试集合，这里有2条数据，每条数据一行内容，然后进行向量化和TF-IDF转换</span><br><span class="line">docs_new = [&apos;God is love&apos;， &apos;OpenGL on the GPU is fast&apos;]</span><br><span class="line">X_new_counts = count_vect.transform（docs_new）</span><br><span class="line">X_new_tfidf = tfidf_transformer.transform（X_new_counts）</span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">predicted = clf.predict（X_new_tfidf）</span><br><span class="line"></span><br><span class="line">#打印结果</span><br><span class="line">for doc， category in zip（docs_new， predicted）：</span><br><span class="line">    print（&apos;%r =&gt; %s&apos; %（doc， twenty_train.target_names[category]））</span><br></pre></td></tr></table></figure>

</details>

<p>关于分类的问题可以扩展一下，每篇文章的分类可能不是只有一个“军事类”、“体育类”等类别，而可能是多个标签组合描述的主题性或情绪性分类说明，例如，在一篇文章里可以同时标出“军事”、“科技”、“颂扬”等标签，也可以标出“经济”、“局势”、“悲观”等标签。对于主题性或情绪性的分类，利用朴素贝叶斯算法同样可以进行分类标识，这样在文章分类时文章所赋予的分类也更加丰富。</p>
<h3 id="14-2-3-K-近邻算法"><a href="#14-2-3-K-近邻算法" class="headerlink" title="14.2.3 K-近邻算法"></a>14.2.3 K-近邻算法</h3><p>K-近邻算法英文全称为K-Nearest Neigbours，也有的资料上会写作KNN算法。</p>
<p>K-近邻算法的思路是，没有必要去总结原型向量，只需原始的训练样本，这些样本具有最基础最原始而且准确的向量信息，因此此算法产生的分类器也叫做“基于实例”的分类器。</p>
<p>流程如下，拿到训练的文章样本之后，对每个样本都进行分词和向量化。然后在给定新的待判定文章后，算法对该文档也进行分词和向量化，不同的地方在后面的操作上。这个待判定的文章的向量会和所有训练的样本进行向量特征比对，也就是相似度比对，这样会得到它与所有训练样本的相似度排名列表。把K定为一个变量，从这个列表中找出相似度最高的K篇文章，根据这K篇文章的类别分布投票决定这篇待判定的文章更像哪种分类。</p>
<p>这个方法看上去很简单，其实根本就没有模型训练的成分在里面，但是还是一分为二地来看这种方法。</p>
<p>它的优点：因为这种方法可以克服Rocchio算法中无法处理线性的缺陷，同时“训练成本”也非常低——其他分类方法进行算法和模型调整时要重新对所有的分类文本进行全局计算，而这种算法只需要对某个已有的训练文档进行删除，或者加入新的训练文档，这个训练的分类规则就同时发生了变化。</p>
<p>它的缺点：也是非常致命的，就是计算成本比较高。如果要计算一篇待分类的文章，就要将它和所有的训练样本进行比较。如果有100个文章分类，每个文章分类有100篇训练文章，那就是10000次计算，而且每分类一篇文章就要进行10000次计算，计算成本非常高。</p>
<p>下面给出应用的代码以供参考：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">#找出相似度最高的15篇文章</span><br><span class="line">#现在把TF-IDF转换后的结果和每条结果对应的主题编号twenty_train.target放入分类器中进行训练</span><br><span class="line">clf = KNeighborsClassifier（15）.fit（X_train_tfidf， twenty_train.target）</span><br><span class="line"></span><br><span class="line">#创建测试集合，这里有2条数据，每条数据一行内容，然后进行向量化和TF-IDF转换</span><br><span class="line">docs_new = [&apos;God is love&apos;， &apos;OpenGL on the GPU is fast&apos;]</span><br><span class="line">X_new_counts = count_vect.transform（docs_new）</span><br><span class="line">X_new_tfidf = tfidf_transformer.transform（X_new_counts）</span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">predicted = clf.predict（X_new_tfidf）</span><br><span class="line"></span><br><span class="line">#打印结果</span><br><span class="line">for doc， category in zip（docs_new， predicted）：</span><br><span class="line">    print（&apos;%r =&gt; %s&apos; %（doc， twenty_train.target_names[category]））</span><br></pre></td></tr></table></figure>

</details>

<h3 id="14-2-4-支持向量机SVM算法"><a href="#14-2-4-支持向量机SVM算法" class="headerlink" title="14.2.4 支持向量机SVM算法"></a>14.2.4 支持向量机SVM算法</h3><p>支持向量机在第10章已经介绍过。在该章节中讨论的是一种通用性的线性分类器构造原则，不管有多少维的数据，只要发现线性不可分，就可以映射到高一维度的空间去构造一个超平面。</p>
<p>在文章分类里，同样可以应用这样的方式，只要已经理解了SVM的解题思路，理解它在文章分类中的应用就不会困难。</p>
<p>前面说过，拿到文章以后，要进行分词和向量化，向量化之后，一篇文章就会变成一个几千维或者几万维的向量。这些向量在空间上的划分和g（v）=wv+b这样的超平面用法一样，几乎没有任何区别，只是v的维度会非常多而已。</p>
<p>总体来说，SVM分类器的文本分类效果很好，可以认为是最好的分类器之一。它有很多优点，如通用性较好，分类精度高，分类速度快，分类速度与训练样本个数无关，在查准和查全率（精度和召回率）方面都优于KNN及朴素贝叶斯方法。但是它也有缺点，如SVM训练速度很大程度上受到训练集规模的影响，计算开销比较大，针对SVM的训练速度问题，研究者提出了很多改进方法，包括Chunking方法、Osuna算法、SMO算法和交互SVM等。</p>
<p>前面在介绍SVM一节中提过，SVC所支持的核函数包括linear（线性核函数）、poly（多项式核函数）、rbf（径向基核函数）、sigmoid（神经元激活核函数）、precomputed（自定义核函数）。其中，径向基核函数rbf和线性核函数linear是人们在生产生活中用得最频繁的两种核函数。对于文章分类，一般推荐使用线性核函数linear，这种核函数计算效率极高，对文章分类的准确性也非常高。</p>
<p>这里也给出一个用SVM算法进行文章分类的例子：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import svm</span><br><span class="line">#现在把TF-IDF转换后的结果和每条结果对应的主题编号twenty_train.target放入分类器中进行训练</span><br><span class="line">#这里使用线性支持向量分类linear，对文章分类效果比较好</span><br><span class="line">clf = svm.SVC（kernel=&apos;linear&apos;）.fit（X_train_tfidf， twenty_train.target）</span><br><span class="line">#创建测试集合，这里有2条数据，每条数据一行内容，然后进行向量化和TF-IDF转换</span><br><span class="line">docs_new = [&apos;God is love&apos;， &apos;OpenGL on the GPU is fast&apos;]</span><br><span class="line">X_new_counts = count_vect.transform（docs_new）</span><br><span class="line">X_new_tfidf = tfidf_transformer.transform（X_new_counts）</span><br><span class="line">#预测</span><br><span class="line">predicted = clf.predict（X_new_tfidf）</span><br><span class="line">#打印结果</span><br><span class="line">for doc， category in zip（docs_new， predicted）：</span><br><span class="line">    print（&apos;%r =&gt; %s&apos; %（doc， twenty_train.target_names[category]））</span><br></pre></td></tr></table></figure>

</details>

<h2 id="14-3-小结"><a href="#14-3-小结" class="headerlink" title="14.3 小结"></a>14.3 小结</h2><p>由于篇幅有限，本章只介绍了文本分类方面的内容，文本分类是网站进行舆情分析、偏好猜测等行为的重要手段，读者掌握基本方法即可。</p>
<p>请注意，一般来说，文章越短分类的难度越大，准确性越差。这凭直觉也能感觉出来，一句很短的话肯定是能够在很多类型的文章中都有机会读到的，那么这样的句子是几乎没有办法去判断主旨内容的，要想知道主旨内容还是要通过大量的上下文。</p>
<h1 id="第15章-人工神经网络"><a href="#第15章-人工神经网络" class="headerlink" title="第15章 人工神经网络"></a>第15章 人工神经网络</h1><p>人类对于人工智能领域的研究随着其他各领域尤其是信息科学的进步而快速进步。现在在工厂生产线上看到的机器人其实是比较“弱智”的机器人，因为它基本只能根据人给它的固定指令去做固定的动作组合，所以称它为“机械手臂”也许更为贴切。而要想进行比较深层的突破，旧的算法科学恐怕是乏善可陈。</p>
<p>当人们意识到人脑的工作方式与数字计算机有着极大不同时，人们就逐渐开始研究“人工神经网络”（Artificial Neural Network，ANN），希望能够从仿生学的角度给这种研究带来新的动力。所谓“人法地，地法天，天法道，道法自然 <a href="#ch1_back">[1]</a> ”。从自然中学习和寻求规律有时比冥想要更有智慧。</p>
<p>人们从研究蝙蝠获得了发明雷达的灵感，通过研究鱼鳔获得了发明潜水艇的灵感。人们同样期望从研究人的神经网络中获得更多高级、智能化的数据处理思路和经验。</p>
<p>人脑是人们到目前为止发现的，最令人叹为观止的信息处理系统了。人脑在加减乘除方面的计算速度上可能赶不上很多单片机，但是在很多特殊场景的运算中却是高级计算机无法比拟的，如模式识别（声音识别、图像识别等）。</p>
<p><a href="#ch1">[1]</a> 出自《道德经·道经第二十五章》。</p>
<h2 id="15-1-人的神经网络"><a href="#15-1-人的神经网络" class="headerlink" title="15.1 人的神经网络"></a>15.1 人的神经网络</h2><p>大脑是人的神经最为关键和核心的部分，也同样是所有脊椎动物门的动物最宝贵的“财产”。人类豢养各种宠物，如狗或猫，条件比较富裕的人还会豢养马、鹿等，主要原因也是由于这类动物更加通人性、聪明、互动感好，容易与人的交流形成配合。也有一些养鸟、养鱼的，主要是为了欣赏鸟婉转的鸣叫声和鱼奇幻的外形。猎奇的宠物主可能会去豢养蜥蜴、蚂蚁等，但十有八九不是因为它们聪明。这些无一例外，都是脊椎动物门的动物，都有大脑。</p>
<p>通过大量的解剖研究，人们逐渐对神经网络的组成原理有了更多的认识，并开始尝试着解读神经网络的奥秘所在。</p>
<p>现在流通的资料里，关于人体神经元数量的估算还是莫衷一是，但是有一个数字大家基本认可，即人的大脑皮层里有神经元约140亿个，人的全身有数百亿到上千亿个神经元。</p>
<p>神经网络有以下几个非常优秀的特点。</p>
<p>（1）大规模并行分布式结构。</p>
<p>（2）神经网络的学习能力以及由此而来的泛化能力。</p>
<p>泛化这一点很重要，它是指在遇到一些没有在训练中遇到的数据时仍然可以得到合理的输出，这一点在第8章等章节同样提到过。</p>
<p>具备这两种强大的信息处理能力让神经网络可以为一些当前难以处理的复杂问题找到一些好的近似解。</p>
<h3 id="15-1-1-神经网络结构"><a href="#15-1-1-神经网络结构" class="headerlink" title="15.1.1 神经网络结构"></a>15.1.1 神经网络结构</h3><p>人自身是怎么感受到“快乐”这种情绪的？可以认为，这就是一种极复杂的计算过程。</p>
<p>研究表示，人类感受到的快乐是由脑神经元之间通过多巴胺进行信息传递来获得的。</p>
<p>瑞典人阿尔维德·卡尔森（Arvid Carlsson）由于确定多巴胺为脑内信息传递者的角色获得了2000年诺贝尔医学奖。</p>
<p>大量的多巴胺能够让人兴奋、快乐，而缺少多巴胺的人通常情绪低落，甚至会令人失去控制肌肉的能力，严重会令病人的手脚不自主地震颤或导致帕金森氏症。</p>
<p>让人产生多巴胺的行为有很多，如吃甜食、适度运动等。而令人谈虎色变的毒品通常有着极强的多巴胺“制造”能力——其实是让细胞里的多巴胺存量大量释放出来，让人在短时间内产生极大的快感。</p>
<p><img src="Image00377.jpg" alt></p>
<p>图15-1 多巴胺</p>
<p>多巴胺的分子结构如图15-1 <a href="#ch1_back">[1]</a> 所示。多巴胺的化学式如下：</p>
<p><img src="Image00378.jpg" alt></p>
<p>这个化学式很复杂，在计算机二进制系统里找不到能对其模拟得惟妙惟肖的传递介质，二进制系统里只有0和1这种存储和传输介质，更别说在神经元细胞内部的“计算”是怎么样的计算了，更让人捉摸不透的是让我们产生快感的计算是怎么做的，我们只能在有限的认识内对这种算法进行模拟。</p>
<p>人的神经细胞如图15-2所示，枝枝杈杈很多，远远看上去一边比较粗大一边比较纤细。最上端粗大的一边就是细胞体的所在，细胞体上有一些小枝杈叫做树突，细长的一条像尾巴一样的东西叫做轴突。不同细胞之间通过树突和轴突相互传递信息，它们的接触点叫突触，准确地说是由一个细胞的轴突通过突触将信号传递给另一个细胞的树突。</p>
<p><img src="Image00379.jpg" alt></p>
<p>图15-2 人的神经细胞（见彩插）</p>
<p>神经细胞用化学信号传递信息，如多巴胺，其过程非常复杂。这里用二进制的电子计算机模拟，为了模拟起来比较方便，我们只认为大脑的神经细胞有两种状态：兴奋和抑制，1为兴奋，0为抑制。</p>
<p>一个神经元兴奋和抑制的信号是由这个神经元的树突来影响的，因为树突是接受外界刺激的，刺激强烈神经元就兴奋，刺激不强烈神经元就抑制。一个神经元有很多的树突，这些树突其实可以同时接受多个其他神经元送过来的刺激信号，这些信号经过一系列复杂的计算，最终产生一个确定的兴奋或者抑制的结果，这些突触之间有的是通过化学信号进行信号传递的，有的是通过电信号进行信号传递的，但是这并不重要，反正建模时只能用类似0和1的方式来模拟。</p>
<p>人脑的神经细胞工作频率非常低，大概只有100Hz，这比20世纪80年代末流行的286计算机的20MHz主频还差了20万倍。但是神经细胞数量巨大，并且并行进行独立的处理工作，这让大脑具备很惊人的特点，如能够进行高效的无监督学习，模式识别等。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度地图。</p>
<h3 id="15-1-2-结构模拟"><a href="#15-1-2-结构模拟" class="headerlink" title="15.1.2 结构模拟"></a>15.1.2 结构模拟</h3><p>既然已经了解到神经元的工作方式，不妨设计一个和它的工作方式近似的电子信号处理系统，如图15-3所示。</p>
<p><img src="Image00380.jpg" alt></p>
<p>图15-3 由神经元模拟的电子信号处理系统</p>
<p>它有5个输入的树突（也可以是3、4个树突，也可以是6个或者更多，这里设计成5个没有特别的目的），有一个轴突，而中间的部分负责进行计算。当5个树突输入的内容合适时，让轴突输出1——兴奋，反之让轴突输出0——抑制。可以让每个树突上的输入数字做加和来模拟这个计算兴奋或者抑制的过程，每个刺激都是0或1，也就是来自其他神经元细胞的刺激，刺激的值为xi<br>。为了让树突上的刺激有所区别，可以考虑给每个树突输入的值乘以一个权值，这个权值同样是一个[-1，1]之间的小数，权值标注为wi 。后面如果不做说明，wi 的取值就是在[-1，1]区间，xi 就是0或1。</p>
<p>设置激励函数为<img src="Image00381.jpg" alt> ，作为整个过程的模拟。</p>
<p>从函数的定义域上看，f（x）的范围应该在[-5，5]之间。可以记作前面写的连加和的形式，也可以记作后面写的矩阵内积的形式，含义是完全一样的，看过第10章的读者尤其不会陌生。</p>
<p>整个函数产生的结果最终还会继续去刺激其他函数——毕竟要连成网络，所以还要让轴突产生的输出函数为1或0。所以在最终输出之前还需要一个环节：</p>
<p><img src="Image00382.jpg" alt></p>
<p>对f（x）进行一下加工，如果相加的和大于等于0就认为是兴奋状态，如果小于0就认为是抑制状态。</p>
<p>好了，如果到这里你还是觉得一头雾水，那么我只能说“嗯，真的很正常，想当年都是这么过来的。”我们现在手里有了一个世界上最弱的神经网络细胞，注意，真的只是一个细胞而已。下面来试一下，看这个细胞能不能完成机器学习的工作。</p>
<h3 id="15-1-3-训练与工作"><a href="#15-1-3-训练与工作" class="headerlink" title="15.1.3 训练与工作"></a>15.1.3 训练与工作</h3><p>和一般的分类算法一样，使用人工神经网络进行分类时同样需要先进行训练，即便是一个神经元也是要训练的。</p>
<p>训练过程大致如下。</p>
<p>（1）初始化权重。随机设置wi 的大小，即设置初始的每个树突上的权重。先给定一个任意值，如都是1。</p>
<p>（2）训练。给定输入各xi 的大小和对应的output输出值，这是一组样本。需要给定很多组样本，这就是学习的过程。</p>
<p>仔细看一下这个过程：</p>
<p><img src="Image00383.jpg" alt></p>
<p>有没有觉得特别眼熟。非常像回归对不对？知道自变量，知道函数值，用计算的方法算出这些待定的系数。没错，这个跟回归真的就是如出一辙。在8.1节曾经讨论过用最小二乘法来计算待定的系数，那么在现在这个例子里能不能用同样或者类似的办法来解决问题呢？答案是肯定的，只是这一类回归不是线性关系的，它在回归分析中是有确切的名词定义的，这种二项分布形的回归叫作逻辑回归（Logistic Regression）。逻辑回归判断的方式比较单纯，就如同我们在刚刚的例子说的这样，有一些输入的自变量，分类结果只有两类，即针对要分类的目标，要么是这一类，要么不是这一类。</p>
<p>除此之外还像什么？像不像在支持向量机SVM里看到的超平面的定义？</p>
<p>SVM算法是为了寻找一个g（v）=wv+b的超平面，其中v就是一个n维的向量，n可以是1到无穷的任何一个整数，其实当n等于5时，就与y=w1 x1 +w2 x2 +w3 x3 +w4 x4 +w5 x5 的形式很相近，只是这个g（v）展开以后是y=w1 x1 +w2 x2 +w3 x3 +w4 x4 +w5 x5 +b，变量具体用x还是用y、z来表示并不重要，关键是它们在处理问题上的逻辑，仔细看看，思路都是尝试找一个“超平面”。</p>
<p>以二维空间上的超平面2x+5y+4=0为例，经过变形就成了y=-0.4x-0.8。其实也就是g（v）=2x1 +5x2 +4，如图15-4所示。</p>
<p><img src="Image00384.jpg" alt></p>
<p>图15-4 y=-0.4x-0.8的图形</p>
<p>三维空间上的超平面4x+y-2z+6=0经过变形就成了z=2x+0.5y+3。其实也就是g（v）=4x1 +x2 -x3 +6，如图15-5所示。</p>
<p><img src="Image00385.jpg" alt></p>
<p>图15-5 z=2x+0.5y+3的图形</p>
<p>y=w1 x1 +w2 x2 +w3 x3 +w4 x4 +w5 x5<br>+b公式里的b这一项，可以这么理解，完全是在超平面斜率定好后为了调整截距而设置的，b恒等于0的情况下就是让超平面永远通过原点。所以在不同的资料上会看到两种表达方式，而且后者更常见：</p>
<p><img src="Image00386.jpg" alt></p>
<p><img src="Image00387.jpg" alt></p>
<p>这种单细胞人工神经网络一般用来处理手写识别、垃圾邮件分类、金融欺诈行为、网络注册用户是否真实等问题。</p>
<p>具体的方式在10.5节已经进行过比较详细的讨论了，用SVM算法去解决甚至更好，因为它最厉害的地方是能够映射到高维去解决线性不可分的问题。</p>
<p>神经网络和SVM解决问题的思路的不同之处在于，在线性不可分时，SVM会映射到高维去划分超平面；而神经网络是增加输入的变量、网络的层次、输出层。</p>
<h2 id="15-2-FANN库简介"><a href="#15-2-FANN库简介" class="headerlink" title="15.2 FANN库简介"></a>15.2 FANN库简介</h2><p>在人工神经网络算法的编写过程中，可以考虑自己一行一行去做代码实现，也可以使用开源的人工神经网络算法库，以把精力集中在建模工作上。FANN库就是众多人工神经网络库之一。</p>
<p>FANN是一个开源的神经网络模型库，全称是Fast Artificial Neural Network Library。它是用C语言编写的，运行高效，而且可以支持模拟单层、多层的各种全连接和半连接网络。2.1版本以上的FANN支持包括C#、Java、PHP、Python等超过20种计算机语言的黑盒式调用，使用非常方便。它最强大的特性之一是支持多种GUI，这种用户界面的友好型也让它颇受欢迎。</p>
<p>图15-6 <a href="#ch1_back">[1]</a> 用来做网络搭建的FANN Tool 1.2的界面。</p>
<p><img src="Image00388.jpg" alt></p>
<p>图15-6 FANN Tool 1.2</p>
<p>另一种FANN官网推荐的专门做神经网络搭建的可视化工具是Agiel Neural Network，如图15-7 <a href="#ch2_back">[2]</a> 所示。</p>
<p><img src="Image00389.jpg" alt></p>
<p>图15-7 Agiel Neural Network</p>
<p>FANN的中文资料少得可怜，要想做更多的了解还是推荐前往其官方网站阅读英文原文。网站地址：<a href="http://leenissen.dk/fann/" target="_blank" rel="noopener">http://leenissen.dk/fann/</a> 。</p>
<p>FANN的特性如下，后面将会有基于FANN库做算法实现的例子。</p>
<p>·使用C语言编写的多层人工神经网络库。</p>
<p>·反向传播训练。</p>
<p>·进化型拓扑训练，可动态建立并训练人工神经网络。</p>
<p>·极易使用。</p>
<p>·极其快速（据称比其他的人工神经网络框架快150倍）。</p>
<p>·极强的通用性，可覆盖与兼容极多的参数维度场景。</p>
<p>·文档完备。</p>
<p>·跨平台性良好，支持Linux和UNIX以及Windows平台使用的dll库。</p>
<p>·实现多种不同动态函数。</p>
<p>·可以便捷地存取完整人工神经网络模型。</p>
<p>·有多种易用的例子。</p>
<p>·可以在模型中兼容浮点型和整型变量。</p>
<p>·具有极好的缓存优化。</p>
<p>·开源并遵从LGPL协议。</p>
<p>·具有易用性的训练数据框架。</p>
<p>·具有图形界面。</p>
<p>·支持多种语言调用。</p>
<p>·应用范围广泛。</p>
<p><a href="#ch1">[1]</a> 来自FANN 官方网站。</p>
<p><a href="#ch2">[2]</a> 图片来源于sourceforge.net。</p>
<h2 id="15-3-常见的神经网络"><a href="#15-3-常见的神经网络" class="headerlink" title="15.3 常见的神经网络"></a>15.3 常见的神经网络</h2><p>神经网络模型发展到今天有了很多变种，据说已有不少于70种的不同神经网络模型。</p>
<p>常见的人工神经网络模型有以下几种。</p>
<p>·感知器网络（Perceptron），在前面接触到的“单细胞”的神经网络（应该叫神经元更合适）就是感知器网络最简单的形式。</p>
<p>·按误差逆传播算法训练的多层前馈网络（Back Propagation，BP神经网络）。</p>
<p>·自组织特征映射神经网络（Self-Organizing Feature Map，SOM）。</p>
<p>·Hopfield网络。</p>
<p>·玻尔兹曼机（Boltzmann Machine）。</p>
<p>·卷积神经网络（Convolutional Neural Network，CNN）。</p>
<p>还有很多其他的网络模型，形式各异，偏重也不同，适应场景也不尽相同。</p>
<h2 id="15-4-BP神经网络"><a href="#15-4-BP神经网络" class="headerlink" title="15.4 BP神经网络"></a>15.4 BP神经网络</h2><p>BP神经网络是指误差逆传播算法训练的多层前馈网络，听起来名字很啰嗦，但是构建的思路很简单。</p>
<p>如图15-8所示为两层的BP神经网络（只有隐含层和输出层是参与计算和权值调整的节点层），当然可以有更多的层次、更多的节点，这里只是示意性介绍构建思路。</p>
<p><img src="Image00390.jpg" alt></p>
<p>图15-8 两层的BP神经网络</p>
<h3 id="15-4-1-结构和原理"><a href="#15-4-1-结构和原理" class="headerlink" title="15.4.1 结构和原理"></a>15.4.1 结构和原理</h3><p>在“单细胞”的神经网络里，实际上只有一层，即最后的输出层。在图15-8中有两层，第一层每个节点的输入都是一样的，都是x1 、x2 、x3 、x4 、x5<br>。每个节点的超平面都可以用g（v）=wv+b来表示，也就是y=w1 x1 +w2 x2 +w3 x3 +w4 x4 +w5 x5<br>+b。但是要注意，所有的节点最后输出的函数只有1或者0两个状态，所以在很多资料上会看到以下写法。</p>
<p>激活函数为Logistic函数：</p>
<p><img src="Image00391.jpg" alt></p>
<p>即便变量名字不一样，但是实质内容是一样的。</p>
<p><img src="Image00392.jpg" alt> 这个函数其实是<img src="Image00393.jpg" alt> 和<img src="Image00394.jpg" alt><br>这两个函数组合变量代换形成的，把t代换进去，就变成了f（v）的形式，虽然变量名不同但是函数值和自变量的关系没有发生任何变化。</p>
<p><img src="Image00394.jpg" alt> 在SVM里已经介绍过。<img src="Image00395.jpg" alt> 的图形如图15-9所示。</p>
<p><img src="Image00396.jpg" alt></p>
<p>图15-9 <img src="Image00395.jpg" alt> 的图形</p>
<p>比0.5大时函数就是1，比0.5小时函数就是0。</p>
<p>也有地方写成以下形式：</p>
<p><img src="Image00397.jpg" alt></p>
<p>其中m是可以调整的正整数，m越小曲线越平缓；m越大曲线越立陡，分类边界越明显。具体在每个应用里怎么去取m值可以依情况而定，如果需要边界区分非常明显，那就把m设大一些。</p>
<p><img src="Image00398.jpg" alt> 函数图形如图15-10所示。</p>
<p><img src="Image00399.jpg" alt></p>
<p>图15-10 <img src="Image00398.jpg" alt> 的图形</p>
<p><img src="Image00400.jpg" alt> 函数图形如图15-11所示。</p>
<p><img src="Image00401.jpg" alt></p>
<p>图15-11 <img src="Image00400.jpg" alt> 的图形</p>
<p>所以对于<img src="Image00402.jpg" alt><br>函数来说，函数会根据v的输入对应产生1和0两种函数值，而这里面待定的（或者说没确定的）就是w这个矩阵，这就是在网络训练中需要决定的。要知道最后每个节点都有函数<img src="Image00403.jpg" alt><br>，而且每个节点之间都可能完全不一样。而且从前面网络拓扑图的结构可以想象，前面一层的输出结果作为后面一层节点的输入，最后一层的输出是5个不同的1或0，也就是说这个网络最多标识32种不同的分类——因为2的5次幂就是32，它没办法标识更多的分类，除非增加节点的数量。</p>
<h3 id="15-4-2-训练过程"><a href="#15-4-2-训练过程" class="headerlink" title="15.4.2 训练过程"></a>15.4.2 训练过程</h3><p>前面讨论了结构，现在讨论的是调整这个网络上各个节点上各个“树突”上的权值。</p>
<p>这里还是希望找到一种方法，能够让设置好的各个权值能够匹配尽可能多的训练样本的分类情况，让误判的情况尽可能少，这是大原则，和线性回归中希望残差尽量小的思路是完全一样的。当时用的是最小二乘法的方案，而最小二乘法的思路是，如果把误差表示成样本做自变量的函数，然后用求极值的方法来推导就可以找出这个误差最小情况下的各种系数值了。很幸运，在BP神经网络的w权值确定中同样可以用这种思路来完成。</p>
<p>对于完整的训练过程，看上去像在曲线上找驻点的过程，这里试着解释一下这个过程。下面这个推导过程看上去比较复杂，如果看不明白直接记结论即可，推导细节可以忽略。</p>
<p>（1）误差计算。</p>
<p>隐含层节点输入为hi =wih xi +bh 。</p>
<p>隐含层节点的输出为ho =f（hi ）。</p>
<p>输出层节点的输入为yi =who ho +bo 。</p>
<p>输出层节点的输出为yo =f（yi ）。</p>
<p>变量统一化，这样的表示可以把两层的输入输出串起来，里面的xi 、wih 、who 、ho 、yo 都是向量。</p>
<p><img src="Image00404.jpg" alt></p>
<p>整个网络误差函数为</p>
<p><img src="Image00405.jpg" alt></p>
<p>也就是说，给定n个训练样本，误差就是训练中给出的分类和学习后分类函数输出的分类的差值，然后平方再加和。这个过程与求方差差不多，反正最后的原则很清楚，方差尽可能小，最好是零。如果在设置了隐含层和输出层的权值后，这个等式的值很小，在期望的误差范围之内即可，如果超过了误差范围就需要调整——通常不太可能一次就把值都设置正确。这个函数前面本身是没有1/2的，这里特意配了一个1/2在里面，第一不会影响函数的单调性，第二，在求导时能够去掉，看上去简洁一些。</p>
<p>（2）反向传播。然后对这个误差函数进行求导求偏微分，这里只给出简单的推导过程。</p>
<p>输出层误差偏微分</p>
<p><img src="Image00406.jpg" alt></p>
<p>其中</p>
<p><img src="Image00407.jpg" alt></p>
<p>那么</p>
<p><img src="Image00408.jpg" alt></p>
<p>误差的梯度</p>
<p><img src="Image00409.jpg" alt></p>
<p>隐含层误差偏微分</p>
<p><img src="Image00410.jpg" alt></p>
<p>方法和输出层的推导完全一样：</p>
<p><img src="Image00411.jpg" alt></p>
<p>那么</p>
<p><img src="Image00412.jpg" alt></p>
<p>误差的梯度</p>
<p><img src="Image00413.jpg" alt></p>
<p>（3）权值修正。</p>
<p>隐含层更新：</p>
<p><img src="Image00414.jpg" alt></p>
<p>输出层更新：</p>
<p><img src="Image00415.jpg" alt></p>
<p>其中N表示第N次迭代，N+1表示第N+1次迭代。</p>
<h3 id="15-4-3-过程解释"><a href="#15-4-3-过程解释" class="headerlink" title="15.4.3 过程解释"></a>15.4.3 过程解释</h3><p>整个BP网络的训练过程我们用通俗的话解释一下。</p>
<p>首先先设置两套w作为两层网络各自的“超平面”的系数，然后输入一次完整的训练过程，就会有一个误差值出现，因为给出的w很可能不合适。</p>
<p>接着就是一次一次地进行w的调整。调整方法是，首先用类似最小二乘法的方式，找到一个误差和自变量的关系，然后求误差极值。误差极值存在的点就是误差最小的点，这和回归分析的最小二乘法思路几乎一样。</p>
<p>只是在最后的<img src="Image00416.jpg" alt> 和<img src="Image00417.jpg" alt><br>两个公式里，用的是试探性的方法。<img src="Image00418.jpg" alt> 和<img src="Image00419.jpg" alt><br>是一种试探的“步长”，第一次分别设置了who 和wih<br>，如果有误差，就试着往误差小的一边“走一步”，如果还不够小就再“走一步”，这就是一次一次迭代的目的。最后找到一个误差满足要求的点，把这一点的who 和wih 都记录下来，网络就训练完毕了。</p>
<p>这种用步长试探的思路在10.6节也介绍过，当时把这种方式称为梯度下降法。</p>
<h3 id="15-4-4-示例"><a href="#15-4-4-示例" class="headerlink" title="15.4.4 示例"></a>15.4.4 示例</h3><p>这里给出一个具体的BP神经网络训练和使用的示例。</p>
<p>在一些经典的有关BP神经网络的应用中都会看到关于手写识别的算法。这里用它入手来感受一下BP神经网络工作的过程。实验流程如下。</p>
<p>（1）准备训练图片。</p>
<p>（2）模型训练。</p>
<p>（3）判定验证。</p>
<p>具体操作如下。</p>
<p>（1）准备训练图片。</p>
<p>准备如图15-12所示的6组不同字体、不同颜色、不同大小的数字。在这里我是用EXCEL来做的，大家自己在实验的过程中也同样可以寻求其他的方法。</p>
<p>（2）把这些数字所在的方格，一共60个，都保存成BMP位图格式的图片文件。文件命名时，从上到下用A、B、C、D、E、F来表示组别，用1、2、3、4、5、6、7、8、9、0来表示数字，即</p>
<p><img src="Image00420.jpg" alt></p>
<p>图15-12 训练图片</p>
<p>A1.bmp、A2.bmp、A3.bmp、……、A9.bmp、A0.bmp。</p>
<p>B1.bmp、B2.bmp、B3.bmp、……、B9.bmp、B0.bmp。</p>
<p>……</p>
<p>F1.bmp、F2.bmp、F3.bmp、……、F9.bmp、F0.bmp。</p>
<p>还可以考虑用英文字符“A”、“B”、“C”作为识别对象，还可以考虑用中文的“甲”、“乙”、“丙”等汉字作为识别对象，原理是完全一样的。</p>
<p>（3）对图像进行二值化处理。图像本身是由很多像素点构成的，在本例中，使用的是32×32的像素矩阵。每一个像素点都有一个灰度值，灰度值是[0，255]区间的一个整数值。一个点的灰度越靠近255，则说明这个点颜色越深，反之如果一个点的灰度越靠近0，则说明这个点颜色越浅。</p>
<p>需要通过图像二值化处理把所有的图片变成二值化后的图片，也就是每个像素点做一个“非黑即白”的归类设置，要么设置成黑色，要么设置成白色。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def convert_to_blackwhite（image， name）</span><br><span class="line">image = image.convert（&quot;L&quot;）</span><br><span class="line">image.save（&quot;new_&quot;+name）</span><br><span class="line">image = image.point（lambda x：WHITE if x&gt;128 else BLACK）</span><br><span class="line">image = image.convert（&apos;1&apos;）</span><br><span class="line">image.save（</span><br></pre></td></tr></table></figure>

</details>

<p>经过上面这段代码的处理，所有的图片文件都已经变成了二值化后的图片，每张图片中的每个像素都是非黑即白，如图15-13所示。</p>
<p><img src="Image00421.jpg" alt></p>
<p>图15-13 二值化后的图片</p>
<p>注意上面代码中x＞128这个部分。虽然可以选择0到255中的任何值作为判断黑白的分水岭，但是选择时要注意，如果这个值设置得太高，那么很多颜色浅的训练样本就会丢失信息，导致模型失真。如果设置得太低，那么如果图片背景的颜色不是白色，而是有一些浅颜色的背景噪声，那么这些点就会被当成有效信息来训练模型，同样会误导模型。具体选多少合适，需要根据实际生产生活中的具体情况去分析，这里仅作参考。</p>
<p>然后就可以把图片一个一个传递给训练算法做训练了，传递时还是一个向量v和分类f（v）之间的关系。</p>
<p>以new_F5.bmp这个图片为例，如图15-14所示，这个图片在32×32像素的位图模式下放大来看如图15-14（a）所示，看上去很模糊，因为边缘上的一些为了产生视觉过渡效果的中间色也被二值化成了黑色。</p>
<p><img src="Image00422.jpg" alt></p>
<p>图15-14 new_F5.bmp图片</p>
<p>将32×32像素的图片的每个2×2像素作为1个基本输入单位，这样一来，就有16×16个基本输入单位了，也就是256个基本输入单位。</p>
<p>那么输入向量就可以确定格式了，整个图从左上到右下每一个单位都作为一个v的维度，这样输入的训练向量v就是一个拥有256个维度的向量了。每个维度的值的确定如图15-15所示。</p>
<p><img src="Image00423.jpg" alt></p>
<p>图15-15 确定每个维度的值</p>
<p>这4个像素中的黑色像素数作为维度值就可以了，例如上面这4个输入单位的像素颜色分别就对应着1、2、2、0这4种输入，以此类推，256个维度的值都能确定下来。刚刚这个5就可以转化成这样一个256维的向量了：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 0 4 4 4 4 4 4 4 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 0 4 4 4 4 4 4 4 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 0 4 2 0 0 0 0 0 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 0 4 2 0 0 0 0 0 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 2 4 2 2 2 2 0 0 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 2 4 4 4 4 4 4 1 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 2 4 4 1 0 3 4 4 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 1 2 1 0 0 1 4 4 1 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 1 2 2 0 0 0 4 4 2 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 3 4 4 0 0 1 4 4 1 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 4 4 3 0 0 2 4 4 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 1 4 3 2 2 4 4 1 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 0 1 3 4 4 3 1 0 0 0 0 0</span><br><span class="line"></span><br><span class="line">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br></pre></td></tr></table></figure>

</details>


<p>而5对应的分类向量可以设置为</p>
<p>0000100000</p>
<p>以此类推，0对应的是</p>
<p>0000000001</p>
<p>7对应的是</p>
<p>0000001000</p>
<p>这样一来，所有的图片向量和所有的图片对应的数字分类都能够一一对应起来，把这些训练样本按照FANN的要求存入文件，用下面的代码（注意不是把图片直接传递给FANN去做运算，是数字向量文本，FANN的安装请见附录E）：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#make_training_data.py</span><br><span class="line">f = open（&apos;train.data&apos;， &apos;wt&apos;） </span><br><span class="line">print &gt;&gt;f， len（result）， 256， 10  </span><br><span class="line">for input， output in result：  </span><br><span class="line">    print &gt;&gt;f， input  </span><br><span class="line">print &gt;&gt;f， output</span><br></pre></td></tr></table></figure>

</details>

<p>开始训练：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#train_ann.py</span><br><span class="line">connectionRate = 1  #神经层连接率为1（100%）</span><br><span class="line">learningRate = 0.008  #学习率为0.008</span><br><span class="line">desiredError = 0.001  #期望错误率为0.001</span><br><span class="line">maxIterations = 10000  #最多进行10 000次迭代</span><br><span class="line">iterationsBetweenReports = 100  #每间隔100次报告一下学习结果</span><br><span class="line">inNum= 256  #256个神经元</span><br><span class="line">hideNum = 64  #隐藏层为64个神经元</span><br><span class="line">outNum=10  #输出层有10个神经元</span><br><span class="line">class NeuNet（neural_net）：  </span><br><span class="line">    def __init__（self）：  </span><br><span class="line">        neural_net.__init__（self） </span><br><span class="line">        neural_net.create_standard_array（self，（inNum， hideNum， outNum）） </span><br><span class="line">    def train_on_file（self，fileName）： neural_net.train_on_file（self，fileName，maxIter-ations，iterationsBetweenReports，desiredError）</span><br></pre></td></tr></table></figure>

</details>

<p>主程序部分：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;：  </span><br><span class="line">    ann = NeuNet（） </span><br><span class="line">    ann.train_on_file（&quot;train.data&quot;） #读取训练向量文件</span><br><span class="line">ann.save（&quot;number_recognize.net&quot;） #将模型存储到文件</span><br></pre></td></tr></table></figure>

</details>

<p>一般不会直到10000次才满足条件退出，几百次就已经让误差率低于期望误差的0.001了，这时训练自动会结束。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#test_recognize.py</span><br><span class="line">if __name__ == &quot;__main__&quot;：  </span><br><span class="line">    ann = NeuNet（） </span><br><span class="line">    ann.create_from_file（&quot;number_recognize.net&quot;） #读取模型文件创建分类模型（网络）</span><br><span class="line">    data = read_test_data（） #读取测试数据</span><br><span class="line">    for k， v in data.iteritems（）：  </span><br><span class="line">        k = string_to_list（k） </span><br><span class="line">        v = string_to_list（v） </span><br><span class="line">        result = ann.run（k） </span><br><span class="line">        print euclidean_distance（v， result）</span><br></pre></td></tr></table></figure>

</details>

<p>最后使用test_recognize.py读入一段待分类的向量文本文件，打印出结果。这个文件就是由一个或多个待分类的图片文件各自经过向量化变成的256维向量组。输出的结果也是和待分类向量一一对应的一个或多个待分类的结果。</p>
<p>一般来说，训练的样本越多，数字书写的样式越丰富，分类就越准确。我们在这个环节可以自己和神经网络做个小游戏，看看歪歪扭扭的阿拉伯数字小网络买不买账，如果不买账那就继续拿这些歪歪扭扭的数字继续训练它，学会了为止。</p>
<h2 id="15-5-玻尔兹曼机"><a href="#15-5-玻尔兹曼机" class="headerlink" title="15.5 玻尔兹曼机"></a>15.5 玻尔兹曼机</h2><p>玻尔兹曼机（Boltzmann Machine）最经典的应用是用来解决货郎担问题。在前面遗传算法的部分介绍过TSP问题，Traveling Salesman Problem，即旅行商问题，也叫货郎担问题。货郎担问题用遗传算法是可以解决的，某些介绍遗传算法的材料中也对玻尔兹曼机和模拟退火算法做了介绍。玻尔兹曼机的网络模型与BP神经网络模型的结构没有什么区别，也可以有两层或者多层，只是训练的方式不太一样。</p>
<h3 id="15-5-1-退火模型"><a href="#15-5-1-退火模型" class="headerlink" title="15.5.1 退火模型"></a>15.5.1 退火模型</h3><p>模拟退火算法（Simulated Annealing，SA）最早的思想是由梅特罗波利斯（N.Metropolis）等人于1953年提出。到了1983年，柯克帕特里克（S.Kirkpatrick）等人在研究优化组合问题时发现，组合优化求解极小值与金属热处理工艺中的退火过程具有很大的相似性，于是他们创造了模拟退火算法。</p>
<p>退火是金属材料热处理中使用的一个名词，表示把钢铁缓慢烧热到一定温度后保持一段时间，然后再慢慢以一定的速度把温度降下来。</p>
<p>退火这个过程在物理学上是有原理解释的，大概过程是，一开始，金属内部原子和原子之间是有一定的斥力和引力的，这样形成了一定的势能（有的材料上又叫内能或者应力），这是一种相对不稳定的状态。通过加热的过程，让大量的原子开始运动离开原来的位置，然后随着温度的降低，原子的运动速度也没有原来那么快了，最后达到稳定的状态，而这个稳定的状态通常都是比一开始的状态势能要低的。退火之后的金属通常更柔韧，不易开裂或变形，切削性能更好。为了获得更好的退火后的金属性能，通常退火会进行几次。</p>
<p>如果觉得从炼钢的角度不好理解，那就理解为在一个底部不规则的大笸箩里有很多形状不规则的积木。在不做特殊处理的情况下，积木中间其实是有活动余地的，在上面放任何东西也都会因为积木有可能发生移动而变得不稳固。希望更稳固怎么办？最直接的想法是把笸箩笸一笸，让这些积木跑到更稳固的状态上去，通常跑到稳固状态上的积木也不容易再出来。这样最后停止时笸箩里的积木堆就比原来的积木堆要更稳定，更抗压。要想让积木堆尽可能稳定，多笸几次即可，直到看上去积木堆的体积不再变化了（图15-16）。</p>
<p><img src="Image00424.jpg" alt></p>
<p>图15-16 积木从不稳定到稳定</p>
<p>从算法上来看，还是迭代多次，然后让算法收敛到某个解附近，这个解就是全局最优解。这种思路我们在遗传算法里也介绍过。</p>
<h3 id="15-5-2-玻尔兹曼机"><a href="#15-5-2-玻尔兹曼机" class="headerlink" title="15.5.2 玻尔兹曼机"></a>15.5.2 玻尔兹曼机</h3><p>1868年，奥地利物理学家路德维希·玻尔兹曼（Ludwig Edward Boltzmann）在研究气体热平衡统计力学中给出一个玻尔兹曼因子：</p>
<p><img src="Image00425.jpg" alt></p>
<p>其中，e是自然常数，Ei 是状态i的能量，kB 是玻尔兹曼常数，T是当时的绝对温度（绝对零度是零下273℃）。波尔兹曼因子在使用时通常是用来比较同一温度之下，系统所处两个状态概率的比值。</p>
<p><img src="Image00426.jpg" alt></p>
<p>把这个时候的温度T、常数e和kB 代进去，这个比值和这两个能量的差做e的指数的幂函数值成正比。可以肯定的是，在T一定时，两个能量的差越大，这个比值就越大，也就是在两个状态下的概率的比率越大，状态为state2的概率越大。</p>
<p>还是从激活函数<img src="Image00427.jpg" alt><br>来看，这个函数能很好地把多维超平面和二项分布结合起来。对于分类也确实只有两种结果，要么属于（输出1），要么不属于（输出0）。</p>
<p>输出1时，概率函数</p>
<p><img src="Image00428.jpg" alt></p>
<p>输出0时，概率函数</p>
<p><img src="Image00429.jpg" alt></p>
<p>所以两个状态的比值</p>
<p><img src="Image00430.jpg" alt></p>
<p>也就是说要训练这个玻尔兹曼机，让它具备分类的能力，而应该决定的是wT 为多少，以及T为多少才能满足尽量少地误判。</p>
<p>根据函数<img src="Image00431.jpg" alt><br>的模型，t前面的系数绝对值越大，曲线越立陡；系数绝对值越小曲线越平缓，所以T这个温度的选择也是要进行相应考虑的，不同的是，由于取了倒数，T越大曲线越平缓，越小曲线越立陡：</p>
<p><img src="Image00432.jpg" alt></p>
<p>实际的训练过程如下。</p>
<p>（1）先设置一组wT 、b和T，这个初始的T叫T0 ，T0 设置得大一些。</p>
<p>（2）然后每一个v都能计算出其对应的分类结果的概率比率情况。如果是1，则表示这个v被判断为“属于该分类”的概率和“不属于该分类”的概率一样大；大于1且越大，则说明v被判断为“属于该分类”的概率越大；反之，小于1且越接近0，则说明v被判断为“不属于该分类”的概率越大。</p>
<p>（3）通过调整T，以<img src="Image00433.jpg" alt> 的方式进行迭代性下降模拟退火的过程。有的资料上写成lg（k+1），性质差不多。</p>
<p>（4）当发现连续迭代一两次后T的变化没有引起明显的误判率的减小，就可以结束。</p>
<h2 id="15-6-卷积神经网络"><a href="#15-6-卷积神经网络" class="headerlink" title="15.6 卷积神经网络"></a>15.6 卷积神经网络</h2><p>很多研究者喜欢把卷积神经网络放到“深度学习”（Deep Learning）的范畴来介绍。这里将卷积神经网络放在神经网络中进行介绍，后面专门还有一节对深度学习进行讨论。</p>
<p>卷积神经网络（Convolutional Neural Network，CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大规模的模式识别都是有着非常好的性能表现的，尤其是对大规模图形图像处理效率极高，这也是大家热衷研究这类网络的重要原因。说到底，卷积神经网络还是一个分类器，是一种有监督机器学习的工具。</p>
<p>在20世纪60年代，美国神经生物学家Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而提出了卷积神经网络。现在，CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用。日本人福岛邦彦（Kunihiko Fukushima）在20世纪90年代提出的新识别机是卷积神经网络的第一个实现网络。随后，更多的科研工作者对该网络进行了改进。</p>
<p>一般来说，CNN的基本结构包括两层。</p>
<p>第一层为特征提取层，每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其他特征间的位置关系也随之确定下来。</p>
<p>第二层为特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。</p>
<p>此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。</p>
<p>CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，所以在使用CNN时，避免了显式的特征抽取，而隐式地从训练数据中进行学习；再者由于同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。</p>
<p>为了了解卷积神经网络的实质，下面简单介绍一下卷积。</p>
<h3 id="15-6-1-卷积"><a href="#15-6-1-卷积" class="headerlink" title="15.6.1 卷积"></a>15.6.1 卷积</h3><p>在泛函分析中，有卷积、旋积或摺积（Convolution）的定义——一般的中文材料里还是习惯用卷积这个名词。它是通过两个函数f和g生成第三个函数的一种数学算子，表征函数f与g经过翻转和平移的重叠部分的面积。</p>
<p>在通信工程领域，卷积是非常常用的一种计算方法，通常用到卷积也是和傅里叶变换有关系。大家在知乎上去找到的话可以找到很多对卷积的解释，有不少形象的类比。</p>
<p>卷积的数学定义如下：</p>
<p><img src="Image00434.jpg" alt></p>
<p>这个等式可以从面积的角度去尝试解释一下。</p>
<p>在第8章曾经介绍过关于逐差法求重力加速度g的过程，其中有位移<img src="Image00435.jpg" alt><br>，整个积分式的含义就是位移s是一个随着t变化的函数，而整个过程中s是由这一瞬间的速度v（t）和瞬间的时间长度dt（Δt）相乘而来的。这里面有一个非常重要的概念，就是积分表示的是面积——函数曲线v（t）和s=0（轴）围成的面积。在上述卷积式中，就是h（x）和y=0（x轴）围成的面积。</p>
<p>那么这个积分式中的f（t）g（x-t）是什么呢？其实是，f（t）先不动，g（-t）相当于g（t）函数的图像沿着y轴（t=0）做了一次翻转。g（x-t）相当于g（-t）的整个图像沿着t轴进行了平移，向右平移了x个单位。</p>
<p>做了这个变换之后，可以想象这一共是有两个函数，一个是固定的函数，一个是滑动的函数，求它们相乘之后围起来的面积，滑动的变量就是x。如图15-17所示，f（t）就是一个三角形，在第二象限是一条过（-1，0）和（0，1）点的线段，在第一象限是一条过（0，1）和（1，0）点的线段。函数g（t）是一个正方的脉冲波，t在[1，2]上有定义，在这段区间里g（t）=t。函数g（x-t）是左侧的这个做过翻转的图形，图15-18中还分别有x=-2，x=-1，x=0，x=1，x=2时的图像。</p>
<p>可以观察到，在这个不定积分完成后，会形成两个函数叠加的部分，其中x是一个变量。假设x为0，或者当x不存在，那么就是f（t）和g（-t）这两个函数相乘后和y=0（t轴）围成的面积。当x出现后，x是帮着g（-t）图像左右平移的，刚刚也看到这个图像的变化过程了，那么会变成什么样？简单地说，这个函数h（x）的值就是求一个面积和x的关系，而这个面积就是函数f（t）和g（x-t）相乘后的曲线和y=0（t轴）围成的面积，其中自变量是x。在随着x变化的移动过程中，由于g（x-t）移动产生的h（x）的对应变化就是整个卷积公式的意义了——一个移动中用x进行取样的过程。</p>
<p><img src="Image00436.jpg" alt></p>
<p>图15-17 f（t）和g（t）</p>
<p><img src="Image00437.jpg" alt></p>
<p>图15-18 两个函数叠加的图形</p>
<p>对于工学层面的应用来说，卷积本身的含义以及推导在生产生活中通常没有机会接触，读者在脑海里形成一个移动过程中做乘积的印象就足够了。</p>
<h3 id="15-6-2-图像识别"><a href="#15-6-2-图像识别" class="headerlink" title="15.6.2 图像识别"></a>15.6.2 图像识别</h3><p>介绍了卷积的数学意义，下面介绍卷积神经网络的应用过程。</p>
<p>前面介绍了BP神经网络和玻尔兹曼机，其中BP神经网络通常是一个多层的神经网络系统。在这样一个神经网络中，每层的神经节点都是共享前一层所有的输入项的，也就是每层各个节点的g（v）=wv+b的v都是一样的。训练也是一次一次进行迭代来调整各个w的值。</p>
<p>这种网络有一个问题，就是一旦输入的v维度比较大，训练的时间都是几何级数增长的，只有三五个输入时训练很快，输入多达三五百个时基本就没办法在能够忍受的时间内训练出来了，而在图像识别时，一个图像如果是2048×1536，也就是3145728个像素时，已经完全没办法胜任了，这还不是高清图片，现在的数码相机动辄就1600万像素以上，这就更没可能训练了。</p>
<p><img src="Image00438.jpg" alt></p>
<p>图15-19 青苹果的图片</p>
<p>可是如果真的想这么做，怎么办？下面将讨论如果想识别一个2048×1536像素的图片将面临什么问题，以及怎么用卷积神经网络解决。</p>
<p>我们以青苹果的图片为例（如图15-19所示），如果想要用神经网络对这个2048×1536像素的图片进行建模处理，那起码要用一个3145728维度的向量来进行建模。好在受到猫脑皮层研究中的一些启发，有了局部感知学习的一些尝试。</p>
<p>1.设计卷积层</p>
<p>卷积神经网络有两种很特殊的功能与前面接触的神经网络不同，可以降低参数数目。</p>
<p>第一种功能：局部感知。</p>
<p>一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。视觉皮层的神经元就是局部接受信息的，即这些神经元只响应某些特定区域的刺激。如图15-20所示，图15-20（a）为全连接，图15-20（b）为局部连接。</p>
<p>为了不让图上的元素太密而让大家产生不快的感觉，在全连接情况下只是示意性地画出了两个神经元，每一个神经元都连接到前端2048×1536个像素输入点上，在纵横两个维度上都做连接；图15-20（b）画出的是整个图形的一部分。</p>
<p>在这个2048×1536像素的图片处理过程中，本来需要建立一个3145728个输入项的多节点网络（每个节点都是3145728个输入项），也就是2048×1536个神经元，每个神经元的输入又有参数2048×1536个。</p>
<p>但是在做过局部感知处理后，就可以把节点的数量减少，同时也可以把节点的参数数量减少。由于2048是211 ，1536是3×29<br>，因此设计一个节点，这个节点可以接收16×12，也就是192个参数。为了看得清楚，将局部放大，临近的16×12个像素点连接到一个神经元，每个神经元的接收范围都和左右两侧的神经元各有1列是不同的，其他15列是相同的；而和上下两侧的神经元各有1行是不同的，其余11行是相同的。</p>
<p><img src="Image00439.jpg" alt></p>
<p>图15-20 感知方式</p>
<p>在这种情况下，16×12个像素单位的图片需要1×1=1个16×12的神经元来进行连接；17×13个像素单位的图片需要2×2=4个16×12的神经元来进行连接；18×14个像素单位的图片需要3×3=9个16×12的神经元来进行连接；以此类推，2048×1536个像素单位需要2033×1525=3100325个16×12的神经元来进行连接。</p>
<p>仍然使用3100325，也就是2033×1525个神经节点。对比一下，这种方法的改进在权值训练上有怎样的提高。</p>
<p>按照BP神经网络，3145728个输入项就需要3145728个节点，每个节点需要3145728个权值，也就是需要3145728×3145728=9895604649984，也就是9万8千多亿个输入项输入到第一层（也叫隐含层）上。</p>
<p>按照刚刚的方案进行改进，有2033×1525个神经节点，每个节点能够处理16×12个参数，而现在实际上用了2033×1525×16×12，也就是595262400个输入项就解决了，对比原来的9万8千多亿个数量减少为0.006%，理论上训练速度提高至少5个数量级。</p>
<p>卷积层指的就是刚刚设计的这一层，而这16×12个参数被一个神经节点进行处理和特征值提取的过程操作就叫做卷积操作。一会儿后面我们会具体讲解怎么来做这个过程。</p>
<p><img src="Image00440.jpg" alt></p>
<p>图15-21 某种建模形式</p>
<p>在有些资料里还会看到如图15-21所示的建模形式，也就是说一个神经元不仅对图形的像素是否有着色（二值化后的黑色）敏感，还对每个图形的深度（height）敏感。图形深度是RGB颜色的值的深度量化，即对在计算机颜色处理中常用的红色Red、绿色Green、蓝色Blue进行参数化处理。如果对颜色敏感，在刚刚的设计中，就需要对每个神经元建模，使其成为可以接受16×12×3个参数的神经元。如果不敏感，那就直接把图像二值化后再做训练。</p>
<p>第二种功能：参数共享。</p>
<p>上述例子里，即便不需要对颜色敏感，还是需要用训练来调整595262400个权值，而这个数量虽然比前面9万多亿的设计好很多但是仍然很多。那么这时就需要第二个强大的功能派上用场，那就是参数共享。</p>
<p>在刚刚设计的卷积层中，每个神经元都对应16×12个参数，一共2033×1525，即3100325个神经元，如果这3100325个神经元的192个参数都是相等的，那么实际上要训练的参数数目就变为192了。</p>
<p>有关权值共享的问题只是一个减少参数训练复杂度的技巧，或者只是方案之一，而非必要方案，是强行设置的一种权值方式。可以把这16×12个参数看成是提取特征的方式，该方式与图像捕捉的位置无关。大致原理：视觉神经在捕捉外界进来的光信号时是不加分别的，所有的神经对于映射到它上的信号（16×12个输入）是一视同仁地看待的（权值一样），不同的是这个阵列上的不同细胞映射的信号对象不一样，只映射它最邻近的对象。</p>
<p>更直观地，当从一个大尺寸图像中随机选取一小块，如16×12作为样本，并且从这个小块样本中学习到了一些特征，这时可以把从这个16×12样本中学习到的特征作为探测器，应用到这个图像的任意地方中。特别地，可以用从16×12样本中所学习到的特征与原本的大尺寸图像作卷积，从而从这个大尺寸图像上的任一位置获得一个不同特征的激活值。</p>
<p><img src="Image00441.jpg" alt></p>
<p>图15-22 一个3×3的卷积核在5×5的图像上做卷积</p>
<p>为了理解方便，在做示例时进行简化。如图15-22所示为一个3×3的卷积核在5×5的图像上做卷积的过程。每个卷积都是一种特征提取方式，就像一个筛子，将图像中符合条件（激活值越大越符合条件）的部分筛选出来，即用右侧的3×3小方块从上到下从左到右地套左侧的5×5的大方块。</p>
<p>分别得到9个矩阵，如图15-23所示。</p>
<p><img src="Image00442.jpg" alt></p>
<p>图15-23 得到9个矩阵</p>
<p>假设设计的卷积核也是一个矩阵，如图15-24所示。</p>
<p>这个卷积核输出的结果定义为用卷积结果矩阵和卷积核做内积的形式：</p>
<p><img src="Image00443.jpg" alt></p>
<p>其中v是一个卷积结果矩阵，w是卷积核，那么整个计算就变成了数方格的问题，也就是这9幅局部图对应的输出就变成了如图15-25所示的矩阵。</p>
<p><img src="Image00444.jpg" alt></p>
<p>图15-24 卷积核</p>
<p><img src="Image00445.jpg" alt></p>
<p>图15-25 对应的输出</p>
<p>到这里卷积层的工作就已经完成了，就是通过这种从左到右从上到下的平移来看平移过程中卷积核的范围内应该提取哪些特征值。这个过程似曾相识，在BP神经网络中也同样接触过，只是当时用的网络是全连接方式。</p>
<p>建议卷积层的神经元节点的接收输入有大部分重叠，这里重叠的部分达到了<img src="Image00446.jpg" alt><br>，即一个神经卷积的16列感知对象中有15列被其旁边临近的神经元所感知，12行感知对象中有11行被其旁边临近的神经元所感知，重叠比例大约为86%，也可以尝试其他重叠比例，这不是一个一成不变的值。</p>
<p>2.设计采样层</p>
<p>采样层也叫池层（Pooling），它的目的是将前面卷积层输出的特征提取值进行量化。</p>
<p>这种层的作用是继续对临近的输入点进行特征提取，算法设计也不固定，可以是求平均值，也可以是求最大值等，总之就是把在前面卷积层输出的矩阵继续进行特征化。</p>
<p><img src="Image00447.jpg" alt></p>
<p>图15-26 采样层</p>
<p>以上述卷积层作为输入，可以尝试做一个2×2单位的采样层，即用右侧的2×2小方块从上到下从左到右地套左侧的3×3的大方块，如图15-26所示。</p>
<p><img src="Image00448.jpg" alt></p>
<p>图15-27 结果矩阵</p>
<p>这样就可以得到如图15-27所示的矩阵。</p>
<p>每一个节点是求4个输入的平均值，这样根据采样层的输入视图求平均则分别得到输出：</p>
<p>（3+3+4+4）÷4=3.5</p>
<p>（3+3+4+3）÷4=3.25</p>
<p>（4+4+5+4）÷4=4.25</p>
<p>（4+3+4+3）÷4=3.5</p>
<p>这就是采样层输出的结果。</p>
<p>同样的方式可以应用于更大的图片处理，如2048×1536像素的图片。16×12的卷积核和3×3的卷积核原理完全相同。</p>
<p>在例子中，采样层的神经元节点的接收输入有部分重叠，这里用2×2的采样节点去做采样重叠的部分达到了1/2，也可以尝试不重叠或者以其他比例进行重叠。</p>
<p>3.完整网络设计</p>
<p>在了解了卷积层和采样层的工作方式后，要对完整的苹果图片进行识别，卷积网络可以设计为如图15-28所示的形式。</p>
<p><img src="Image00449.jpg" alt></p>
<p>图15-28 神经网络</p>
<p>卷积层C1： 可以设计2033×1525，即3100325个神经元节点。如同在前面讨论过的连接方法。</p>
<p>采样层S1： 可以采用不重叠的方式进行采样，设计使用输入参数为16×12的神经节点，那么需要128×128，即16384个节点。</p>
<p>卷积层C2： 这里可以再设计一层卷积层进行卷积处理，设计使用输入参数为16×16的神经节点，需要113×113，即12769个节点。</p>
<p>采样层S2： 可以继续采用不重叠的方式进行采样处理，可以尝试采用8×8的输入点数量，那么需要15×15个神经元节点。</p>
<p>输出层O：<br>最后的输出层采用全连接方式。由于前段的采样层S2输出为15×15，所以这一层应该采用15×15个节点，每个节点有15×15，即255个参数。这255个参数可以考虑用高位128位都标记成1，低位127位都标记成0来表示苹果图片的分类。</p>
<p>4.训练</p>
<p>可以准备1000张甚至更多的苹果图片，都进行向量化。然后像在前面识别图片那样来做训练即可。</p>
<p>但是这个过程可能会非常漫长，因为节点数量和权值实在是太多了。</p>
<p>这个方法在PC上处理2048×1536像素的图片是有学术意义的，但是以目前个人PC的处理水平要想商用是很困难的。何况这个例子中做的还仅仅是二值化后的苹果图形的识别而不是RGB深度3通道下的识别，可想而知这个训练过程更为漫长。</p>
<p>一般来说，越复杂的图形图像训练需要的样本越多。2012年6月Google最神秘的部门Google X通过1.6万片处理器，模拟10亿多条连接，构建了一个庞大的系统，用于模拟人类的大脑神经网络。借助“谷歌大脑”，无须接受人类的任何培训和指令，就可以利用内在算法从海量数据中自动提取信息，学会如何识别猫。这是人类有史以来在机器学习（也有人说这叫深度学习）中所做的最有益的尝试之一。面对从YouTube视频中找到的1000万张数字照片，这个“谷歌大脑”能从中自主学习猫的长相。然后就可以自己对其他给它的图片进行判断——“这张是猫。这张不是猫。”这种自学成才的能力别说在当时，就是在现在也颇为惊人。</p>
<p>这种对图像、影像、声音做自动识别的能力，通常称为深度学习。</p>
<h2 id="15-7-深度学习"><a href="#15-7-深度学习" class="headerlink" title="15.7 深度学习"></a>15.7 深度学习</h2><p>深度学习（Deep Learning），我们在这里不打算讲太多，只是提一下我对这个名词的理解吧。</p>
<p>深度学习不是一种算法，而是一种新的境界，说它是新的领域也不合适，因为确实也不是新领域，研究的问题还是老问题。</p>
<p>现在研究深度学习的人很多，深度学习相关的框架体系也有很多，如深度神经网络（Deep Neuron Networks，DNN）、卷积神经网络、深度信念网络（Deep Belief Networks，DBN）和递归神经网络（Recurrent Neural Network，RNN）等。说到深度学习，就得问问它的反面“浅度学习”说的是什么。</p>
<p>相对现在提出的深度学习的概念不同，过去研究的“浅度学习”更多是针对结构化数据的学习和研究，还有半结构化数据的学习和研究，模型简单、结构固定、量化容易，规模小。这些用过去的统计、概率、一般机器学习基本方法都能得到比较好的解决方案。</p>
<p>但是随着计算机计算能力的发展，网络规模的逐渐扩大，存储能力的不断提高，过去时间复杂度非常高的不可行的方案，慢慢也就变得可行了，原来认为解决问题不能成行的思路，慢慢也就变得能够成行了。早在20世纪七八十年代各国的高级实验室就都在研究能不能做大规模的模式识别，不管是用大规模的神经网络去做图片的机器学习，还是做手写识别，这些尝试都是做过的。关键不是当时的数学家建模能力有问题，而是硬件不够强大。</p>
<p>这些深度学习的共同点基本上就是网络输入层节点多，网络层次深，支持的分类种类多而复杂。大部分还是围绕着神经网络或者基于神经网络的思路来发展的，而且在前几节都有过相应的接触。既然趋势是支持分类种类多而复杂的学习方向，那么以有限向无限的方向前进，以离散向连续的方向前进的发展就随着客观条件的成熟变得越来越现实。所以也看到在边界划分比较清晰的声音识别、图像识别、棋牌博弈等领域深度学习的效果会格外好，甚至有时候让人叹为观止，觉得真正的人工智能可能离人们越来越近。</p>
<p>就在成书前不久发生了几件在深度学习方面比较震撼的事情。</p>
<p>事件一：一杆进洞的高尔夫机器人。</p>
<p>TNW中文站2016年2月11日报道，一台名为LDRIC的高尔夫机器人完成了一杆进洞。而1997年，传奇高尔夫选手“老虎”伍兹也曾在这里完成同样的一击。据称LDRIC是“发射定向机器人智能电路”的英文缩写，同时也是对伍兹的名字“埃尔德里克（Eldrick）”的致敬。</p>
<p>事情二：围棋AI的胜利。</p>
<p>英国《自然》杂志2016年1月28日封面文章称，美国谷歌公司旗下的人工智能软件“阿尔法围棋”（AlphaGo）打败了欧洲围棋冠军樊麾（专业二段）。而在不久的将来，AlphaGo将在韩国首尔与韩国围棋选手李世石九段一决高下，李世石是近10年来获得世界第一头衔最多的棋手，谷歌为此提供了100万美元作为奖金。</p>
<p>在笔者截稿前也就是北京时间2016年3月15日下午17时10分左右，李世石九段苦斗5盘，最终以1比4负于谷歌的人工智能机器人AlphaGo。这不仅令赛前断言李世石将以5比0大胜人工智能的人们大跌眼镜，而且也大大提振了人们对于人工智能发展的信心。</p>
<p>这两个事件可以说是让人非常兴奋的，同时也让很多人感到紧张，现在人工智能的发展让人们对机器人越来越敬畏。</p>
<p>确实，这些原来在人工智能领域让科学家感到力不从心的问题现在越来越让人感觉轻松。除了数学科学的发展以外，硬件本身的制造能力与水平的提高，制造成本的下降，也让越来越多的复杂问题变得简单。然而，这些问题如果仔细观察可以发现，仍然是一些边界划定明确的问题，不管是打高尔夫球的机器人，还是下围棋的机器人。它们处理的问题对比人所处理的问题来说，依旧非常单纯，模型维度单纯，游戏规则也单纯，和过去处理的问题来对比只是数据量比较大而已，没有太多新鲜的东西。在人们明确告诉计算机应该怎么去计算，什么步骤，什么规则，而遗留的问题仅仅是计算量的情况下，计算机完全可以通过分布式计算方式来进行弥补，并逼近或超过人类的水平。但是对于那些人类现在都不清楚究竟应该怎样告诉计算机建模，应该用怎么样的步骤来计算的场合，计算机是没有办法去理解问题本身的奥妙的，就更别说像人类这么丰富协调地感知和对应反馈了。</p>
<p>应该说，现在的深度学习水平和真正的理想的人工智能相差还是太悬殊——尽管它的发展确实很迅速，但是还是初级、割裂而且片面的。真正要达到像人这样智能的生命体的水平，恐怕只有让计算机的计算单元在数量级上与人的大脑基本相当，甚至要超过大脑几个数量级。所以担心机器人造反的事情，等到这个前提实现再开始也不算晚。</p>
<h2 id="15-8-小结"><a href="#15-8-小结" class="headerlink" title="15.8 小结"></a>15.8 小结</h2><p>在学习了回归、朴素贝叶斯、决策树、支持向量机，以及本章讨论的人工神经网络以后，可以发现，这些算法的思路都有一个共同点，说到底都是研究多维向量空间分类的问题，都是根据众多的v（a，b，c，d，…）这样的训练样本到某一个或几个分类映射的关系，判断新的给定样本的分类归属问题。</p>
<p>每种算法都有自己优势，也都有自己的局限性。这就好比一个很大的工具箱，里面有电锯、钢锯、线锯等各种锯子，它们都是锯子，都是为了最终把一段原木变成一件精美的家具，但是每种工具都有自己擅长的场合，还有一些自己不擅长的场合。人们要做的事情就是掌握每种工具的优缺点，所谓“尺有所短寸有所长”，在不同的场合选用不同的工具，并注意同时规避不同工具的问题，这样就能达到事半功倍的效果。</p>
<h1 id="第16章-大数据框架简介"><a href="#第16章-大数据框架简介" class="headerlink" title="第16章 大数据框架简介"></a>第16章 大数据框架简介</h1><h2 id="16-1-著名的大数据框架"><a href="#16-1-著名的大数据框架" class="headerlink" title="16.1 著名的大数据框架"></a>16.1 著名的大数据框架</h2><p>框架（Frame）是一个在计算机领域常用的词汇。</p>
<p>熟悉PHP、Java或者Python的读者对这个词肯定非常熟悉，Laravel、Spring、Django等都是著名的语言开发框架。</p>
<p>每一个框架中都封装了大量的工具类，它的最大作用就是帮助使用者节省创建工程的时间，把大家共通性的需求和问题做以实现。这样创建工程时，程序员就只需要进行类似搭积木式的工作就能够把需求完成。这就是框架的作用。</p>
<p>大数据框架与此类同，就是用来简化数据科学家和数据开发工程师编程难度的一种工具。</p>
<p>按照最早最朴素的方式理解大数据的含义：大数据解决的就是单机无法处理的数据。无论存储数据还是计算数据都变得困难无比，大数据框架的出现就是为了解决这个问题，让使用者可以像使用本地主机一样使用多个计算机的处理器，像使用一个本地磁盘一样使用一个大规模的存储集群。这样可以让科学家和工程师把精力集中在自己的业务上，在需要并行计算时调用大数据框架的应用程序接口（API），这就是大数据框架的作用。</p>
<p>生活中也有很多框架的例子：如去银行办理业务，需要填写不同的表格，这些表格就是框架化的典型例子；如发送通知邮件，会有通知邮件的通用格式，只需要填写时间、地点、人物、事件……这些框架的共通特点就是，只需要关注多态化的事务，把这些个性化的信息当做参数，共通的部分已经做成了框架。</p>
<p>大数据也包含很多种类的框架，一般分成两类，即大数据计算框架和大数据存储框架。</p>
<p>大数据计算框架又可以按照执行方式分成两类：一类是执行一次就结束的、对计算时间要求不高的离线计算框架，另一种是对处理时间有严格要求的实时计算框架。</p>
<p>离线计算多用于模型的训练和数据的预处理，最经典的就是Hadoop的MapReduce方式了；而实时计算框架是要求立即返回计算结果的，快速响应请求，如Storm、Spark Streaming等框架，多用于简单的累加计数和基于训练好的模型进行分类等操作。</p>
<p>无论是离线计算还是实时计算，都需要持久地保存大量的清洗过的数据和计算结果，这就需要大数据存储框架来解决了。</p>
<p>经典的Hadoop HDFS就具备了动态扩容以及冗余化存储（存储多份数据）的能力。这样既能保证数据源增大时用户仍然可以像操作本地磁盘一样操作HDFS，又可以保证计算结果的安全性，它是在大数据存储中最主流的解决办法之一。</p>
<p>除了计算和存储，在完整的处理过程中，会加入一些NoSQL存储和一些小工具来提升用户的使用体验，毕竟世界并不是全部由结构化数据组成的。在大数据计算中要缓存一些中间结果或者进行快速的批量写入操作，那么我们会在计算和存储之间加入NoSQL存储引擎来存储需要的结果。再配合一些对传统SQL优化的工具，使SQL适用于体积更大的数据，就完成了大数据框架的大部分流程了。下面具体介绍这些常用的框架。</p>
<h2 id="16-2-Hadoop框架"><a href="#16-2-Hadoop框架" class="headerlink" title="16.2 Hadoop框架"></a>16.2 Hadoop框架</h2><p>将Hadoop称作框架其实并不准确，更多人喜欢称Hadoop为生态圈，因为它除了有计算和存储功能外还提供了相当多的组件，来完成大数据方方面面的工作。Hadoop生态圈的组件非常多，图16-1所示为Hadoop 1.0环境中的生态圈组成，爬虫工具、集群化存储、工作流、数据流、交互式脚本、NoSQL数据库、数据仓库、数据挖掘框架，几乎是应有尽有。</p>
<p><img src="Image00450.jpg" alt></p>
<p>图16-1 Hadoop生态圈</p>
<p>现在在生产环境中，通常使用Hadoop 2.0环境。通常说的Hadoop只是其中最核心的框架，主要分为以下4个部分。</p>
<p>（1）Hadoop Common：这是Hadoop的核心功能，是对其他的Hadoop模块做支撑的，里面包含了大量的对底层文件、网络的访问，对数据类型的支持，以及对象的序列化、反序列化的操作支持等。</p>
<p>（2）Hadoop Distributed File System（HDFSTM<br>）：Hadoop分布式文件系统，也就是上面提到的HDFS，它用于存储大量的数据。</p>
<p>（3）Hadoop YARN：一个任务调度和资源管理的框架。</p>
<p>（4）Hadoop MapReduce：基于YARN的并行大数据处理组件。</p>
<p>请注意Hadoop 1.0和Hadoop 2.0的区别，如图16-2所示。Hadoop 1.0环境的MapReduce是直接运行的，Hadoop 2.0环境的MapReduce依赖于YARN框架，在YARN框架启动后，MapReduce在需要运行的时候把任务提交给YARN框架，让YARN框架来分配资源择机运行，这是两者最大的区别。</p>
<p><img src="Image00451.jpg" alt></p>
<p>图16-2 Hadoop 1.0和Hadoop 2.0</p>
<p>一般把Hadoop Common、HDFS、YARN、MapReduce这四部分统称为Hadoop框架，而在Hadoop生态环境中还有进行SQL化管理HDFS的Hive组件，支持OLTP业务的NoSQL分布式数据库HBase组件，进行图形界面管理的Ambari组件等，Hadoop生态圈会增加越来越多的软件，提高软件的便利性。</p>
<h3 id="16-2-1-MapReduce原理"><a href="#16-2-1-MapReduce原理" class="headerlink" title="16.2.1 MapReduce原理"></a>16.2.1 MapReduce原理</h3><p>之前介绍了Hadoop生态圈和Hadoop框架，相信读者对Hadoop是什么有了一定的了解，那么在Hadoop框架中最著名的就是MapReduce组件，它的处理逻辑来源于谷歌的旧三驾马车之一——MapReduce <a href="#ch1_back">[1]</a><br>。MapReduce是解决问题并行任务的一种模型，将一个可拆解的任务分散到多个计算节点进行计算，最后合并计算结果。</p>
<p>例如，现在需要解决一个问题：尽可能以比较快的速度统计一个图书馆在书架上一共陈列了多少本书（图16-3）。</p>
<p><img src="Image00452.jpg" alt></p>
<p>图16-3 统计图书馆的书（见彩插）</p>
<p>一种方法是，找一个在数数方面有超高本领的人，由他一个人来完成；另一种方法是，雇用一大批资质平庸的负责统计图书数量的人和一个负责分配任务的人，由分配任务的人负责划分区域，确保每个人都分到一部分要统计的书架，不重不漏。然后对所有的人下发开始统计的指令，统计图书的人将自己负责的区域统计完成记录到纸上，所有统计图书的人上交统计结果后，负责分配任务的人将所有人的统计结果进行累加，得到图书统计的结果。如果中途有人因为一些意外原因发生计数终止，那么就再派一个人前去重新完成他未完成的工作任务。</p>
<p>不难想象，如果方法得当，后一种方法要比前一种方法靠谱一些。</p>
<p>这个有超高本领的人是不是容易被找到，他一个人会不会有失误，他的薪水要求是不是太高，这些问题的可控性会变得非常不好。而资质平庸的人通常在市场供应方面不会让我们那么担心，只要统计的方法论和调度方式没有问题，不仅这种方式的风险更小，而且成本更低，速度更快，MapReduce就是这样一种并行机制。</p>
<p>下面从辩证的角度来看这种机制的优点和缺点。</p>
<p>优点如下。</p>
<p>（1）隐藏大量技术细节。开发人员不需要关注容灾管理、负载均衡和并行计算实现的代码部分，只需要调用相关的API，设置参数即可。</p>
<p>（2）可伸缩性好。在Map阶段，可以实现每增加一台服务器就将计算能力接入到集群里，而且能实现在集群运行时添加计算节点（一般用在线扩容这个技术名词来描述这一特点）。</p>
<p>缺点如下。</p>
<p>（1）实时性差。和磁盘交互频繁，中间要多次将计算结果保存到磁盘，使实时计算能力大打折扣。</p>
<p>（2）编程习惯需要适应。需要将在学习《数据结构》或者算法理论中学习到的算法实现方式转换成为MapReduce方式，编程时需要特意构建这种程序逻辑。而且这种方式的局限性导致并非所有的问题都适合用MapReduce方式进行解决。</p>
<p>虽然有不少缺点，但是Hadoop仍然是目前离线计算的利器，下面介绍如何部署一套Hadoop以及用Hadoop来做单词统计。</p>
<p><a href="#ch1">[1]</a><br>谷歌的旧三驾马车是GFS、MapReduce、BigTable，新三驾马车是Caffeine、Pregel、Dremel。</p>
<h3 id="16-2-2-安装Hadoop"><a href="#16-2-2-安装Hadoop" class="headerlink" title="16.2.2 安装Hadoop"></a>16.2.2 安装Hadoop</h3><p>本节主要介绍在CentOS 7单机环境下的Hadoop搭建过程。</p>
<p>1.准备Hadoop需要的软件</p>
<p>（1）安装Java软件包。CentOS 7发行版本默认会安装Java运行环境，可以使用which命令来确认Java是否安装，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  which java/usr/bin/java</span><br></pre></td></tr></table></figure>
<p>也可以自行下载和安装自己需要的Java版本，建议Java版本高于7.0。</p>
<p>这里使用从官方网站下载的最新版本的Java开发包，下载地址：<a href="http://www.oracle.com/technetwork/cn/java/javase/downloads/index.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/cn/java/javase/downloads/index.html</a><br>。</p>
<p>将下载好的安装包安装到指定位置，这里以保存到/opt目录下为例，操作命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ tar zxf java-jdk-7.0.tgz </span><br><span class="line">$ mv java-jdk-7* /opt/java</span><br></pre></td></tr></table></figure>
<p>设置环境变量“JAVA_HOME”，这个变量用来指定Java程序的工作目录。在/etc/bashrc目录下添加Java安装目录，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java</span><br></pre></td></tr></table></figure>
<p>（2）设置SSH通过秘钥方式访问。Hadoop多个节点之间通信会采用SSH秘钥认证方式，为避免每次通信都需要用户输入密码，这里需要生成一对SSH秘钥，生成秘钥使用如下命令：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen –t rsa</span><br><span class="line">$ cat ～/.ssh/id_rsa.pub &gt;&gt; ～/.ssh/authorized_keys</span><br><span class="line">$ chmod 600 ～/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

</details>

<p>（3）下载Hadoop软件包。访问<a href="http://hadoop.apache.org/releases.html#Download" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html#Download</a><br>，这里提供了多个版本的Hadoop软件包下载，建议读者测试和开发时使用最新的稳定版本（Stable），这里以2.6.4版本为例，下载位置如图16-4所示。</p>
<p><img src="Image00453.jpg" alt></p>
<p>图16-4 下载位置</p>
<p>下载完成后将Hadoop解压缩到/opt目录，并设置环境变量指向Hadoop的安装目录。命令如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar zxf hadoop-2.6.4.tar.gz –C /opt</span><br><span class="line">$ echo &apos;export HADOOP_HOME=/opt/hadoop-2.6.4&apos; &gt;&gt; /etc/bashrc</span><br><span class="line">$ source /etc/bashrc</span><br><span class="line">$ cd $HADOOP_HOME   #进入Hadoop安装目录可以看到编译好的文件</span><br></pre></td></tr></table></figure>

</details>

<p>2.修改配置文件并启动服务</p>
<p>（1）修改Hadoop HDFS配置文件。接下来需要设置Hadoop的配置文件，这种单机运行的模式也称作伪分布模式，和集群模式略有区别。这里使用伪分布模式进行部署。修改如下配置文件。</p>
<p>①$HADOOP_HOME/etc/hadoop/core-site.xml。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs：//localhost：9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

</details>

<p>②$HADOOP_HOME/etc/hadoop/hdfs-site.xml。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

</details>

<p>（2）启动Hadoop HDFS服务。</p>
<p>①首次启动格式化存储空间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd $HADOOP_HOME</span><br><span class="line">$ bin/hdfs namenode –format</span><br></pre></td></tr></table></figure>
<p>②启动NameNode和DataNode进程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cd $HADOOP_HOME$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>③验证端口是否启动。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ netstat –ntpl | grep 9000</span><br></pre></td></tr></table></figure>
<p>（3）修改Hadoop YARN配置文件。</p>
<p>①$HADOOP_HOME/etc/hadoop/mapred-site.xml。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

</details>

<p>②$HADOOP_HOME/etc/hadoop/yarn-site.xml。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

</details>

<p>（4）启动Hadoop YARN服务。</p>
<p>①启动命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd $HADOOP_HOME</span><br><span class="line">$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>②验证端口是否启动。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ netstat –ntpl | grep 8088</span><br></pre></td></tr></table></figure>
<p>伪分布方式配置起来非常简单，多用于开发环境部署，接下来就对伪分布环境进行测试。</p>
<h3 id="16-2-3-经典的WordCount"><a href="#16-2-3-经典的WordCount" class="headerlink" title="16.2.3 经典的WordCount"></a>16.2.3 经典的WordCount</h3><p>提到大数据的计算能力测试，就一定会有最经典的单词统计，因为它可以充分发挥各个节点的计算能力，由于Hadoop采用Java开发，这里提供官方的WordCount源代码。</p>
<p>1.源代码</p>
<p>（1）WordCount.java文件源代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException；</span><br><span class="line">import java.util.StringTokenizer；</span><br><span class="line">import org.apache.hadoop.conf.Configuration；</span><br><span class="line">import org.apache.hadoop.fs.Path；</span><br><span class="line">import org.apache.hadoop.io.IntWritable；</span><br><span class="line">import org.apache.hadoop.io.Text；</span><br><span class="line">import org.apache.hadoop.mapreduce.Job；</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper；</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer；</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat；</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat；</span><br><span class="line">public class WordCount &#123;</span><br><span class="line">    public static class TokenizerMapper</span><br><span class="line">        extends Mapper&lt;Object， Text， Text， IntWritable&gt;&#123;</span><br><span class="line">    private final static IntWritable one = new IntWritable（1）；</span><br><span class="line">    private Text word = new Text（）；</span><br><span class="line">    public void map（Object key， Text value， Context context</span><br><span class="line">                    ）throws IOException， InterruptedException &#123;</span><br><span class="line">        StringTokenizer itr = new StringTokenizer（value.toString（））；</span><br><span class="line">        while（itr.hasMoreTokens（））&#123;</span><br><span class="line">            word.set（itr.nextToken（））；</span><br><span class="line">            context.write（word， one）；</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  public static class IntSumReducer</span><br><span class="line">    extends Reducer&lt;Text，IntWritable，Text，IntWritable&gt; &#123;</span><br><span class="line">    private IntWritable result = new IntWritable（）；</span><br><span class="line">    public void reduce（Text key， Iterable&lt;IntWritable&gt; values，</span><br><span class="line">                       Context context</span><br><span class="line">                       ）throws IOException， InterruptedException &#123;</span><br><span class="line">      int sum = 0；</span><br><span class="line">      for（IntWritable val ： values）&#123;</span><br><span class="line">        sum += val.get（）；</span><br><span class="line">        &#125;</span><br><span class="line">        result.set（sum）；</span><br><span class="line">        context.write（key， result）；</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">public static void main（String[] args）throws Exception &#123;</span><br><span class="line">    Configuration conf = new Configuration（）；</span><br><span class="line">    Job job = Job.getInstance（conf， &quot;word count&quot;）；</span><br><span class="line">    job.setJarByClass（WordCount.class）；</span><br><span class="line">    job.setMapperClass（TokenizerMapper.class）；</span><br><span class="line">    job.setCombinerClass（IntSumReducer.class）；</span><br><span class="line">    job.setReducerClass（IntSumReducer.class）；</span><br><span class="line">    job.setOutputKeyClass（Text.class）；</span><br><span class="line">    job.setOutputValueClass（IntWritable.class）；</span><br><span class="line">    FileInputFormat.addInputPath（job， new Path（args[0]））；</span><br><span class="line">    FileOutputFormat.setOutputPath（job， new Path（args[1]））；</span><br><span class="line">    System.exit（job.waitForCompletion（true） 0 ： 1）；</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>

<p>（2）确保/etc/bashrc包含以下环境变量。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java</span><br><span class="line">export PATH=$JAVA_HOME/bin：$PATH</span><br><span class="line">export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar</span><br></pre></td></tr></table></figure>

</details>

<p>（3）编译代码。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop com.sun.tools.javac.Main WordCount.java </span><br><span class="line">$ jar cf wc.jar WordCount*.class</span><br></pre></td></tr></table></figure>

</details>

<p>（4）准备要统计的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cd $HADOOP_HOME</span><br></pre></td></tr></table></figure>
<p>①创建测试文件。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &quot;Hello World Bye World&quot; &gt; file01</span><br><span class="line">$ echo &quot;Hello Hadoop GoodBye Hadoop&quot; &gt; file02</span><br></pre></td></tr></table></figure>

</details>

<p>②将测试文本放入HDFS的/input目录。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs  dfs –mkdir /input</span><br><span class="line">$ bin/hdfs dfs –put file0* /input</span><br></pre></td></tr></table></figure>

</details>

<p>（5）开始统计。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop jar wc.jar WordCount /input /output</span><br></pre></td></tr></table></figure>

</details>

<p>在终端上能看到Map和Reduce的进度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs –cat /output/part-r-00000</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Bye 1 </span><br><span class="line">Goodbye 1 </span><br><span class="line">Hadoop 2 </span><br><span class="line">Hello 2 </span><br><span class="line">World 2</span><br></pre></td></tr></table></figure>
<p>在计算过程中，所看到的Map（）和Reduce（）的进度就是将统计的所有内容映射到指定的内存块中，如果Hadoop使用集群环境，这些计算会分布到不同的主机上实现并行处理。</p>
<p>2.原理解释</p>
<p>MapReduce的过程非常复杂，高手们通常喜欢直接翻看官方的文档和源代码。</p>
<p>作为入门手册，这里只对MapReduce的过程中发生的逻辑性的操作进行解释，读者明白原理了就知道怎么用了。</p>
<p>一次完整的MapReduce计算处理过程从大的环节上分为两个部分，即Map和Reduce。完整的流程是一个从前到后的处理流，如图16-5所示。</p>
<p><img src="Image00454.jpg" alt></p>
<p>图16-5 处理流程</p>
<p>Map的处理过程如下。</p>
<p>第一步：把输入文件读进来。</p>
<p>第二步：输出构造一个Key-Value文件。</p>
<p>这一步很重要，因为大部分逻辑都是在Reduce中完成的，所以在Map的部分实际要完成对Reduce操作内容的迎合性构造，让Reduce能够处理以Key-<br>Value对形成的文件内容。</p>
<p>以刚刚的WordCount为例：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public void map（Object key， Text value， Context context</span><br><span class="line">                    ）throws IOException， InterruptedException &#123;</span><br><span class="line">    StringTokenizer itr = new StringTokenizer（value.toString（））；</span><br><span class="line">    while（itr.hasMoreTokens（））&#123;</span><br><span class="line">        word.set（itr.nextToken（））；</span><br><span class="line">        context.write（word， one）；</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>

<p>这一段程序做了一件事，就是把输入的文件用空格进行了切割，然后把切割完毕后得到的每个词都输出成</p>
<p>词one</p>
<p>的形式。</p>
<p>构造的输入文件是file01和file02：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hello World Bye World</span><br><span class="line">Hello Hadoop GoodBye Hadoop</span><br></pre></td></tr></table></figure>
<p>所以中间结果部分输出的文件实际上变成了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hello one</span><br><span class="line">World one</span><br><span class="line">Bye one</span><br><span class="line">World one</span><br><span class="line">Hello one</span><br><span class="line">Hadoop one</span><br><span class="line">GoodBye one</span><br><span class="line">Hadoop one</span><br></pre></td></tr></table></figure>
<p>为了让Reduce可以分段处理，还做了一个排序。这里需要强调的是，排序实际上不是这么做的，这里这么写完全是为了容易理解。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Bye one</span><br><span class="line">GoodBye one</span><br><span class="line">Hadoop one</span><br><span class="line">Hadoop one</span><br><span class="line">Hello one</span><br><span class="line">Hello one</span><br><span class="line">World one</span><br><span class="line">World one</span><br></pre></td></tr></table></figure>
<p>Reduce的处理过程如下。</p>
<p>第一步：把文件读进来。</p>
<p>第二步：对一个Key的文本部分进行处理。</p>
<p>Reduce最核心的代码如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public void reduce（Text key， Iterable&lt;IntWritable&gt; values，</span><br><span class="line">                   Context context</span><br><span class="line">                   ）throws IOException， InterruptedException &#123;</span><br><span class="line">    int sum = 0；</span><br><span class="line">    for（IntWritable val ： values）&#123;</span><br><span class="line">        sum += val.get（）；</span><br><span class="line">    &#125;</span><br><span class="line">    result.set（sum）；</span><br><span class="line">    context.write（key， result）；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</details>

<p>这部分代码在做一个循环，只是不是遍历整个TEMP文件，而是只能看到其中一个Key所覆盖的部分，不同的Key会被分给不同的Reduce程序实例。</p>
<p>也就是说，在刚刚这个例子中，会有5段Reduce程序实例被启动，它们会被框架分配到不同的节点（如果有）分别处理。</p>
<p>Part1：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Bye one</span><br></pre></td></tr></table></figure>
<p>Part2：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GoodBye one</span><br></pre></td></tr></table></figure>
<p>Part3：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hadoop oneHadoop one</span><br></pre></td></tr></table></figure>
<p>Part4：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello oneHello one</span><br></pre></td></tr></table></figure>
<p>Part5：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">World oneWorld one</span><br></pre></td></tr></table></figure>
<p>这样5段输入文件分别产生的输出结果如下。</p>
<p>Part1：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Bye 1</span><br></pre></td></tr></table></figure>
<p>Part2：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GoodBye 1</span><br></pre></td></tr></table></figure>
<p>Part3：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2</span><br></pre></td></tr></table></figure>
<p>Part4：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello 2</span><br></pre></td></tr></table></figure>
<p>Part5：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">World 2</span><br></pre></td></tr></table></figure>
<p>最后一步就是进行合并，输出结果文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Bye 1 Goodbye 1 Hadoop 2 Hello 2 World 2</span><br></pre></td></tr></table></figure>
<p>整个过程就像在图书馆里数图书数量一样，多个人一起做，最后做一个合并。Map和Reduce在一次运行的过程中都可能会有多个实例出现，每个实例处理一部分数据，通过彼此协同完成整个海量数据的计算操作。</p>
<p>WordCount是MapReduce入门最经典的例子。MapReduce所能处理的事物也远比这个例子复杂，甚至可以出现MapReduce之后紧跟一个MapReduce，类似用管道进行处理的方式。有兴趣的读者可以在网上寻找更多关于MapReduce示例。</p>
<h2 id="16-4-分布式列存储框架"><a href="#16-4-分布式列存储框架" class="headerlink" title="16.4 分布式列存储框架"></a>16.4 分布式列存储框架</h2><p>在生产生活中会接触到很多信息数据，而大部分信息是由非结构化数据组成的，特别是在大数据处理过程中尤为明显。这些数据都无法用关系型数据库的思维方式建模，因此在大数据的处理过程中，就出现了很多非结构化数据处理需求。</p>
<p>为了存储便利，易于理解，在完整的大数据体系中采用了很多NoSQL（Not Only SQL的缩写）数据库。基于NoSQL理念设计的最著名的系统是Google的BigTable和滑铁卢大学开发的HBase。Facebook将Google BigTable和Amazon Dynamo的完全分布式架构集于一身，开发了Cassandra，后于2010年正式成为了Apache基金会项目。Cassandra和HBase的出现不但使NoSQL的处理速度得到了前所未有的提升，而且它们能够组成集群化也为Hadoop、Spark等需要大容量数据快速写入的业务场景提供了非常有用的工具。著名的甲骨文公司Oracle也有自己的NoSQL产品，叫做NoSQL Database。</p>
<p>1.Cassandra简介</p>
<p>Cassandra的名字来源于希腊神话的一位女先知，因此该项目的Logo是一只明亮的眼睛。最突出的特点是它的可扩展性，给集群添加新节点时，可以直接指向新的主机，不必重启任何进程和改变任何查询，是非常便利的自动热扩展机制。这是一个非常好的特性。</p>
<p>此外，Cassandra还有一个优良的特性，即支持SQL语言。这也让广大的SQL爱好者觉得非常亲切，大大降低了学习的门槛。</p>
<p>2.HBase简介</p>
<p>HBase采用Java语言开发，和Cassandra一样，同样借鉴了Google的BigTable模型，主要为Hadoop生态圈提供了列存储服务。</p>
<p>HBase使用HFile对文件进行列式存储，HFile存储以列族为单位。在读取时如果只需用行键作为索引要扫描一个列族的内容，那么其他不相干的列族即便存在和索引条件对应的逻辑关系也不会被扫描到，这种方式在一定程度上避免了IO拥塞，也更加适合这种小尺寸的OLTP操作。</p>
<p>但是和Cassandra一样，HBase也不支持事务操作。</p>
<p>3.Cassandra和HBase对比</p>
<p>（1）Cassandra部署更简单。Cassandra只有一种角色，而HBase除了Region Server外还需要ZooKeeper来同步集群状态。</p>
<p>（2）数据一致性是否可配置。Cassandra的数据一致性是可以配置的，可以更改为最终一致性，而HBase是强一致性的。</p>
<p>（3）负载均衡算法不同。Cassandra通过一致性哈希来决定数据存储的位置，而HBase靠Master节点管理数据的分配，将过热的节点上的Region动态分配给负载较低的节点。因此Cassandra的平均性能会优于HBase，但是Hbase有Master节点，热数据的负载更均衡。</p>
<p>（4）单点问题。正是由于Hbase存在Master节点，因此会存在单点问题。</p>
<h2 id="16-5-PrestoDB——神奇的CLI"><a href="#16-5-PrestoDB——神奇的CLI" class="headerlink" title="16.5 PrestoDB——神奇的CLI"></a>16.5 PrestoDB——神奇的CLI</h2><h3 id="16-5-1-Presto为什么那么快"><a href="#16-5-1-Presto为什么那么快" class="headerlink" title="16.5.1 Presto为什么那么快"></a>16.5.1 Presto为什么那么快</h3><p>有这么一句话“天下武功，唯快不破”，而Presto就会给人们这样一种体验。</p>
<p>Presto诞生于Facebook，由Facebook内部工程师和开源社区工程师共同进行维护，它就是一种超快的SQL查询工具，它诞生的主要用途就是作为Hive和Pig的替代产品，快速地完成海量的数据查询工作。</p>
<p>Presto不但能够解析SQL还能支持多种数据源，如HDFS、Cassandra、MySQL等。但是Presto并不是传统的数据库，还不能支持在线事务处理。它更像是一个分布式查询中间件，采用了分布式查询引擎，同时读取多个数据源中的大数据集。它的集群结构如图16-9所示。</p>
<p><img src="Image00458.jpg" alt></p>
<p>图16-9 Presto的集群结构</p>
<p>Presto分为CLI、Coordinator、Worker3个部分，其中服务器部分的Coordinator可以看作是调度各个Worker的调度器，而且它还承担着让客户端CLI角色连接的功能。它将大量的计算分配到Worker来执行，保证Presto能够快速地进行SQL查询。</p>
<h3 id="16-5-2-安装Presto"><a href="#16-5-2-安装Presto" class="headerlink" title="16.5.2 安装Presto"></a>16.5.2 安装Presto</h3><p>1.下载</p>
<p>访问以下链接来下载最新版本的Presto安装包，也可以定期访问官方文档，来获取最新版本：<a href="https://prestodb.io/docs/current/installation/deployment.html" target="_blank" rel="noopener">https://prestodb.io/docs/current/installation/deployment.html</a>。</p>
<p>将下载的安装包解压缩，创建一个etc目录，命令如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar zxf presto-server-0.138.tar.gz –C /opt</span><br><span class="line">$ mv /opt/presto-server-0.* /opt/presto</span><br><span class="line">$ cd /opt/presto</span><br><span class="line">$ mkdir etc</span><br></pre></td></tr></table></figure>

</details>

<p>2.配置Presto支持Hive</p>
<p>Presto需要配置以下4类信息来完成基本设置，需要将配置文件放入etc目录。</p>
<p>（1）节点属性：每个节点的名称和数据存储位置。</p>
<p>（2）JVM参数：运行时的内存参数。</p>
<p>（3）服务器属性：Presto阶段运行时的角色信息。</p>
<p>（4）Catalog属性：连接指定数据源的信息。</p>
<p>节点属性配置文件etc/node.properties内容如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 集群名称</span><br><span class="line">node.environment=production</span><br><span class="line"># 集群的唯一标示，避免同网络环境多套Presto集群互相干扰</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-ffffffffffff</span><br><span class="line"># 数据存储的目录</span><br><span class="line">node.data-dir=/var/presto/data</span><br></pre></td></tr></table></figure>

</details>

<p>JVM参数文件etc/jvm.config内容如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xmx16G</span><br><span class="line">-XX：+UseG1GC</span><br><span class="line">-XX：G1HeapRegionSize=32M</span><br><span class="line">-XX：+UseGCOverheadLimit</span><br><span class="line">-XX：+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX：+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX：OnOutOfMemoryError=kill -9 %p</span><br></pre></td></tr></table></figure>

</details>

<p>Presto会将运行的JVM虚拟机参数设置为此内容。</p>
<p>服务器属性配置文件etc/config.properties包含服务器角色信息，如果只作为Coordinator，应该至少包含以下信息：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">coordinator=true</span><br><span class="line">node-scheduler.include-coordinator=false</span><br><span class="line">http-server.http.port=8080</span><br><span class="line">query.max-memory=50GB</span><br><span class="line">query.max-memory-per-node=1GB</span><br><span class="line">discovery-server.enabled=true</span><br><span class="line">discovery.uri=http：//example.net：8080</span><br></pre></td></tr></table></figure>

</details>

<p>如果只作为Worker，应该至少包含如下信息：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">coordinator=false</span><br><span class="line">http-server.http.port=8080</span><br><span class="line">query.max-memory=50GB</span><br><span class="line">query.max-memory-per-node=1GB</span><br><span class="line">discovery.uri=http：//example.net：8080</span><br></pre></td></tr></table></figure>

</details>

<p>如果只是测试Presto的功能，可以将一台机器既作为Coordinator，也作为Worker，应该包含以下信息：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">coordinator=true</span><br><span class="line">node-scheduler.include-coordinator=true</span><br><span class="line">http-server.http.port=8080</span><br><span class="line">query.max-memory=5GB</span><br><span class="line">query.max-memory-per-node=1GB</span><br><span class="line">discovery-server.enabled=true</span><br><span class="line">discovery.uri=http：//example.net：8080</span><br></pre></td></tr></table></figure>

</details>

<p>coordinator选项用来指定Presto实例是否作为一个Coordinator来接收客户端的信息。</p>
<p>node-scheduler.include-coordinator只有在Presto实例作为Coordinator角色时才有用处，用来进行任务调度。</p>
<p>http-server.http.port用来指定服务的端口，Presto使用HTTP协议进行通信。</p>
<p>discovery-server.enabled表示Presto通过自动发现机制来找到集群中所有的节点。</p>
<p>discovery.uri通常指向Presto实例的Coordinator角色，Coordinator角色通过内嵌的自动发现服务接收Worker角色的注册。</p>
<p>Catalog属性通过在etc/catalog下创建属性文件来完成属性的注册。Catalog通过connectors访问数据源。例如，创建一个etc/catalog/hive.properties文件来访问Hive数据源，配置信息如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">connector.name=hive-hadoop2</span><br><span class="line">hive.metastore.uri=thrift：//example.net：9083</span><br></pre></td></tr></table></figure>

</details>

<p>针对不同的Hadoop版本，Presto有对应的Hive connector，这里支持以下4种。</p>
<p>（1）hive-hadoop1：Apache Hadoop 1.x。</p>
<p>（2）hive-hadoop2：Apache Hadoop 2.x。</p>
<p>（3）hive-cdh4：Cloudera CDH 4。</p>
<p>（4）hive-cdh5：Cloudera CDH 5。</p>
<p>完成以上的基本配置，Presto就可以正式启动了。</p>
<p>3.启动Presto</p>
<p>可以使用bin/launcher来启动Presto实例，为了便于调试，Presto提供了两种启动方式，前台启动命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/launcher run</span><br></pre></td></tr></table></figure>
<p>可以将日志和查询结果输出到当前终端。也可以在后台启动，避免因关闭终端导致程序退出，后台启动命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/launcher start</span><br></pre></td></tr></table></figure>
<p>更多的命令选项可以通过help参数来获取，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/launcher –help</span><br></pre></td></tr></table></figure>
<p>4.命令行接口</p>
<p>为了连接到指定的Presto实例，需要使用Presto CLI功能进行连接，可以访问以下地址下载Presto CLI：<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.138/presto-cli-0.138-executable.jar" target="_blank" rel="noopener">https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.138/presto-cli-0.138-executable.jar</a>。</p>
<p>这是一个可执行的jar文件，下载后重命名为presto，赋予可执行权限后即可运行，相关命令如下：</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mv presto-cli-0.138-executable.jar presto</span><br><span class="line">$ chmod +x presto</span><br><span class="line">$ ./presto --server localhost：80 --catalog hive  --schema default</span><br></pre></td></tr></table></figure>

</details>

<p>（1）server参数指定Presto的Coordinator的IP地址和端口，这里使用了本机的IP地址和端口。</p>
<p>（2）catalog参数为Presto集群定义catalog名称。</p>
<p>（3）schema参数指定默认访问的数据库名称。</p>
<p>连接成功后，可以执行查询SQL语句来享受Presto带来的飞一样的速度。</p>
<p>如果读者有兴趣还可以参考这篇博文：<a href="http://7737197.blog.51cto.com/7727197/1727186" target="_blank" rel="noopener">http://7737197.blog.51cto.com/7727197/1727186</a>，是关于部署Presto on Cassandra的，就是把Cassandra作为Presto的数据源。</p>
<h2 id="16-6-小结"><a href="#16-6-小结" class="headerlink" title="16.6 小结"></a>16.6 小结</h2><p>Hadoop、Spark、PrestoDB等大数据框架有着非常好的稳定性、扩展性、高可用性等优势，在企业应用中有着非常好的前景。</p>
<p>本章介绍的大数据框架基本都是分布式数据处理的框架，优势是处理单机不方便处理的数据存储、数据统计、数据排序的操作。但是对于迭代性较强的机器学习来说，刚刚介绍的这些大数据框架会有不适用的地方，有不少算法也不适合迁移到其上来进行操作。</p>
<p>建议使用以下两种办法。</p>
<p>办法一： 使用抽样方法提取少量数据，把学习或分析挖掘的内容放在一台计算机上进行计算和处理。</p>
<p>办法二： 使用分布式的深度学习框架来处理极大规模的机器学习数据，如Caffe，Caffe的最新版已经支持分布式GPU在CNN网络训练了。</p>
<h1 id="第17章-系统架构和调优"><a href="#第17章-系统架构和调优" class="headerlink" title="第17章 系统架构和调优"></a>第17章 系统架构和调优</h1><p>在多年做架构师的过程中，有不少朋友和我探讨过系统调优的思路，也有一些年轻的同事会来请教，有的人提出的调优问题非常具体，而有时候有的人会非常泛泛地提出一个问题“系统怎么调优”。</p>
<p>通常具体的问题因为场景确定，所以相对比较好回答，而泛泛的问题通常不容易回答。笔者对调优的思路进行了总结跟大家进行分享与讨论。不管你用的是Linux、Windows还是一些小型机用的UNIX系统亦或其他系统，优化这个话题几乎是永远避不开的。</p>
<p>永远没有最快只有更快，人类对效率的苛求是贪婪的，当然这也是人类技术进步的重要原动力之一。</p>
<p>优化究竟应该怎么做呢？优化有没有一些原则或者判断标准？答案是肯定的。</p>
<p>首先必须明确一个问题，即优化是在优化什么？</p>
<p>优化是有着对象和目标的，如果抛开对象和目标来谈，优化几乎是一个没头没尾的伪命题。回想一下，在什么情况下我们会将“优化”一词脱口而出？很多读者估计也有类似的体会，那就是，在对系统目前的状况不满意或者不满足的时候。而这些情况通常包括两个大的方面：一个是时间，一个是空间。我们无非是对这两种情况中的一种或者两种的当前状况不满，几乎找不到第三种东西来衡量。而用来支持这两个方面的因素，如CPU、内存、磁盘、网络，这些因素称为资源。</p>
<h2 id="17-1-速度——资源的配置"><a href="#17-1-速度——资源的配置" class="headerlink" title="17.1 速度——资源的配置"></a>17.1 速度——资源的配置</h2><p>对速度的不满在调优的场景里占大多数，速度的不满一般来说也是分两种：</p>
<p>一种是对一个“体型”较大的任务执行的时间过长不满；</p>
<p>一种是对一个“体型”较小的任务的响应速度过长不满。</p>
<p>这两种不满看上去都是一样的速度问题，但是思路不完全一样。一般来说，在做这类优化之前先要做一件事，就是判断一下究竟是资源不足，还是资源分配不合理。</p>
<p>常见的场景可能有以下这些：例如，一个进程在服务器上运行，但是速度确实比期望的慢，而CPU和磁盘的带宽却大量闲置，这种情况下很显然是资源配置不合理。因为资源不是不够，而是由于线程调度，或者算法，或者其他一些原因没有被利用上。这种情况下估计你去申请购买新机器，如果老板花的确实是自己的钱的话十有八九是不会给你批的，相信我。</p>
<h3 id="17-1-1-思路一：逻辑层面的优化"><a href="#17-1-1-思路一：逻辑层面的优化" class="headerlink" title="17.1.1 思路一：逻辑层面的优化"></a>17.1.1 思路一：逻辑层面的优化</h3><p>在服务器上跑的程序，尤其是Batch通常是彼此之间独立的。这种情况下，其实是可以考虑让它们同时来执行，充分利用CPU和内存的资源。但是也要注意，要确认这种变化给磁盘带来的IO增加不会让它成为系统的瓶颈<br>。这就是进程级别的并发。</p>
<p>还有的时候一个进程可以分解为多个独立作业和一个合并操作，那么这种情况下通常可以尝试着多启动几个进程或者线程，让每个进程或者线程处理整个作业的一部分，最后结束的时候做一个作业结果的“合并”操作，提高并行化，提高资源利用率。这种应用比较典型的就是Hadoop环境中的MapReduce程序，实际是在很多节点各启动若干个Map进程和Reduce进程，让它们在不同节点上操作，分摊IO和CPU的资源压力。在单台服务器上也有类似的操作，如一个MySQL服务器进程在接受一个SQL请求时，这个SQL不论请求多少个表，不论它有多少个不相干的子查询，不论写得有多“优美”，它都只能在一个CPU内核上一步一步地走下去。所以，如果采用MySQL环境做关联分析，就只能把一个SQL中的两个独立子查询放到两个或者更多的线程（进程）里去做请求，再用一个监控线程（进程）观察结果，最后做连接查询。有必要的话可以使用Memory内存视图的Hash索引进行速度优化。</p>
<h3 id="17-1-2-思路二：容器层面的优化"><a href="#17-1-2-思路二：容器层面的优化" class="headerlink" title="17.1.2 思路二：容器层面的优化"></a>17.1.2 思路二：容器层面的优化</h3><p>当一台或多台服务器上有很多进程，但是资源占用普遍比较低时，还可以考虑使用容器层面的优化。</p>
<p>可以使用KVM或者Docker这样的容器把服务器资源划分成多个虚拟的服务器资源。这种情况下，原本在多个服务器的少量负载经过迁移会合起来加载在一个服务器上，而节省出来的服务器资源可以用来做其他的服务，在硬件的成本上会有一大笔节省。</p>
<p>现在的阿里云、腾讯云、金山云、亚马逊云等云产品服务商就是大规模使用了虚拟化技术，从而使得运维成本大为降低。</p>
<p>虽然容器层面的优化对于直接减少程序运行的时间作用较为间接，但在庞大的系统内提高硬件整体的使用效率还是非常有好处的。</p>
<h3 id="17-1-3-思路三：存储结构层面的优化"><a href="#17-1-3-思路三：存储结构层面的优化" class="headerlink" title="17.1.3 思路三：存储结构层面的优化"></a>17.1.3 思路三：存储结构层面的优化</h3><p>目前在服务器普遍配置了RAID10磁盘阵列以后，磁盘IO在硬件层面进一步并行化的余地越来越小了。那么还有没有其他的办法可以对IO层面进行优化呢？有的。</p>
<p>例如，为了缓存一些数据做迭代运算，磁盘发生非常频繁的读写，每次几个GB（量比较小，至少内存能够承载），但是一次处理可能要读写数百次，这样会大量占用磁盘IO。这种情况下，不妨尝试在内存中虚拟或者划分出一个独立的空间，以供做IO使用。这样把CPU和磁盘之间的IO转化成为CPU和内存之间的IO，这种效率的提升可能是数千倍的。</p>
<p>另外，磁盘在做IO的过程中，是不是扫描了一些本可以不扫描的磁盘块？解决这种问题有很多成熟的办法。</p>
<p>在数据仓库里使用列式存储，从本质来讲也是用这种方法来规避没必要扫描的数据块被扫描。表分区（Table Partitioning）、索引（Index），这两种技术同样是为了解决数据查找中没必要扫描的数据块被访问而带来的IO效率下降问题。</p>
<p>资源分配不合理的情况比较好解决，就是找出在系统里CPU、内存、磁盘、网络中，哪些资源被大量闲置，如果利用起来能否提高并行性，基本就是这样一种思路。</p>
<h3 id="17-1-4-思路四：环节层面的优化"><a href="#17-1-4-思路四：环节层面的优化" class="headerlink" title="17.1.4 思路四：环节层面的优化"></a>17.1.4 思路四：环节层面的优化</h3><p>环节层面的优化是一个边缘化的问题，因为这个层面上的优化通常会涉及硬件资源以外的一些问题——换句话说，这一类问题在计算机的CPU、内存、磁盘、网络层面考虑可能还是不能解决的。</p>
<p>1.虚拟机惹的祸</p>
<p>笔者几年前在某500强企业的IT解决方案中心做顾问的时候，曾经遇到过一个案例。</p>
<p>这个案例从技术层面来说就是一个ERP系统页面请求速度太慢的问题，大概需要2到3秒时间才能把一个页面的数据完全加载完毕。不管2到3秒这个速度是不是够快，是不是比平时访问电商网页或者在线论坛的速度快，对于一个对效率要求很苛刻的500强企业来说，这是一件不能容忍的事情。况且，页面所访问的服务器也是DELL提供的很新的技术方案，48个CPU内核，192GB内存。服务器在公司内网，内网带宽又基本都是1Gbps的光纤到楼层，楼层内部又都是100Mbps的以太网。所以从这个层面上看这种页面的延迟没有道理，而其他项目组在配置基本相当的服务器环境和终端环境，打开页面时间都是1秒以内。</p>
<p>但是很快就发现了问题所在，这个ERP系统的页面是使用Silverlight制作的。Silverlight是微软出品的一种跨服务器跨平台的插件，主要目的是解决浏览器上的流媒体和交互丰富性的问题，基本可以认为是Adobe Flash的替代者。然而这种技术框架有一个天生的问题就是慢，因为它调用的是微软.NET虚拟机的资源，而虚拟机本身的运行机制就是一种多层间接调用的架构，指令不是直接下达到CPU上，而是经过虚拟机，由虚拟机调度线程再发送到CPU。在一次HTTP请求的过程中，有几百个指令会以这种方式传递给CPU，延迟是显而易见的。</p>
<p>最后为了赢得这1秒多的时间，不得不推翻了整个项目的架构部分，采用HTML4+CSS2的方式。立竿见影，延迟瞬间就压缩到1秒以内了。但是代价是牺牲了一些交互上的丰富性和美观性，那个时候HTML5和CSS3还不成熟，还不能作为成熟的技术方案，所以说现在用HTML5和CSS3的程序员们真是赶上好时候了。</p>
<p>2.CDN是个好东西</p>
<p>除了刚刚这个例子以外，平时也能见到很多从环节层面进行优化的例子。最常见的就是CDN技术。</p>
<p>CDN技术是一种近几年非常火的技术，全称是Content Delivery Network，内容分发网络。CDN应该说是一套完整的网络加速解决方案，包括分布式存储、负载均衡、网络请求的重定向和内容管理等多个技术环节部分。对于用户在网页上请求图片加载慢，或者文件下载慢等非本地带宽过小带来的数据下载问题能有很好的改善作用。例如，当一个网站使用了CDN技术对网页资源进行加速的策略开启后，这些资源就会通过CDN提供商的分发策略分发到很多的缓存服务器上去。当用户进行该网站的访问时，这些资源引用的地址会自动指向这个离得最“近”，访问最快的缓存服务器节点上去，这样就能使资源下载加速了。</p>
<p>互联网是一个非常复杂的东西，不仅是拓扑结构复杂，其中不同的交换设备有着不同的交换策略，是一个分布式的自协调的连通系统。不同运营商之间也会由于技术性的或者非技术性的问题引发跨网（跨运营商）的带宽变窄问题。为了解决这种问题，我们不仅仅会用到CDN技术，还需要使用一种叫BGP双线/多线机房的技术来进行网络加速。</p>
<p>BGP（Border Gateway Protocol，边界网关协议）是一种在TCP协议上运行的自治系统之间动态交换路由信息的路由协议。启用BGP技术的机房一般称作BGP机房，服务器租用商或提供商通过技术的手段，实现不同运营商能共同访问一个IP，并且不同运营商之间都能以最快的速度访问这个IP地址。把服务器放在BGP机房给用户带来的好处就是，在BGP机房基本可以不考虑不同的用户跨网访问服务器会因运营商网络不同而产生的“带宽歧视”问题。</p>
<h3 id="17-1-5-资源不足"><a href="#17-1-5-资源不足" class="headerlink" title="17.1.5 资源不足"></a>17.1.5 资源不足</h3><p>资源不足的情况通常比较麻烦，因为如果观察到服务器上的CPU、磁盘IO、网络IO都非常繁忙，要想办法先排除是业务逻辑上设计的疏漏导致的不合理或者意外的资源请求太多，还是“真的”资源不够。如果是由于一些疏漏导致的资源请求过于集中，那么通过debug或者优化业务逻辑，还是能够获解的。但是如果不是这些问题，那就是资源确实比客观真实的需求少了。典型的例子就是，在保存日志的情况下，业务要求无损永久存档，但是即便在启用压缩且不留冗余的情况下，还是很快把磁盘填满，那就是典型的磁盘资源不足了。</p>
<p>总之，还是要先用一些办法确定资源分配究竟是不足还是不合理，再用“低成本”的资源换取“高成本”的资源。</p>
<h2 id="17-2-稳定——资源的可用"><a href="#17-2-稳定——资源的可用" class="headerlink" title="17.2 稳定——资源的可用"></a>17.2 稳定——资源的可用</h2><p>稳定性是压倒一切的。在服务器程序开发方面，有以下共识。</p>
<p>（1）服务器快比服务器慢要好。（客户体验会更好。）</p>
<p>（2）服务器慢要比服务器宕机好。（客户体验不好，好歹还在提供服务，还有流量进来。）</p>
<p>（3）服务器宕机要比服务器损坏了好。（已经是损失了，损失小点吧，尤其是不要损坏数据。）</p>
<p>如果在服务器反应慢和服务器宕机这两种事件中一定要做一个选择，一般都会选择前者。可是如果服务器的资源不足怎么办？请求过多，请求过于频繁，服务器进行过规划但是还是全部资源都用光，怎么办？考虑租用云服务试试看。</p>
<h3 id="17-2-1-借助云服务"><a href="#17-2-1-借助云服务" class="headerlink" title="17.2.1 借助云服务"></a>17.2.1 借助云服务</h3><p>如果资源占用是有比较规律的（周期性的），而且峰值过高，这种情况下是比较适合租用云服务的。在峰值到来时使用云服务；在峰值过去后，低谷时退订云服务。</p>
<p>在使用云服务时要先评估一下，对比一下使用云服务的成本。对比的时候要对比两种方案的完整成本，云服务的成本就是云平台的收费，自建系统的成本包括服务器购买、服务器维护、服务器折旧、带宽租赁等，看看哪个对于自己来说更划算。</p>
<h3 id="17-2-2-锁分散"><a href="#17-2-2-锁分散" class="headerlink" title="17.2.2 锁分散"></a>17.2.2 锁分散</h3><p>做过数据库开发的人对锁（Lock）一定不陌生。</p>
<p>一般的锁都是指互斥锁，也就是说系统里存在这样一种资源，A用户在占用的同时，B用户是不能占用的。这种情形在日常生活中也是随处可见，如餐厅里的座椅，一个用户在使用时其他用户就不能使用，直到这个用户离开，座椅不再使用了才能再被其他人占用。</p>
<p>在计算机系统里也有这种情况，当一个SQL的事务正在做多行的数据更新时，通常会锁住这些行，其他的线程要想读取这些行都要等待，直到这个事务释放行锁。在一个系统中如果有大量的锁资源也会出现一种奇怪的现象，就是看上去似乎CPU和磁盘IO都还远没有达到上限，但是每个事务请求却慢得让人无法忍受。</p>
<p>锁是一个在资源上保证独立性的工具，没有它很多事情是做不了的。</p>
<p>例如每年至少要骂3次的12306（五一长假、十一长假、春节长假），12306多少有点冤枉。本来火车的运载能力有限，票数有限，这是事实。火车上的座位甚至是站位和刚刚说的餐厅的情况是一样的，一个人占了另一个人就没办法使用，这些占与不占的情况不用等上车之后去看，在订票时就显示出来了，所以可以想象，在抢票时12306的后台有几十万把大大小小的锁时开时合，让大家按照锁开合的指示等待锁资源的释放，当锁释放以后，如有人买了票但是没有付款票被重新释放，这个时候我们就又能够获得这把锁，由我们锁住这张票，然后开始付款的过程。而这个过程中，别的任何一个人都没办法获得我锁住的这张票。</p>
<p>淘宝的交易系统与12306有着很大的不同。12306上的信息完全是一对一的互斥资源，必须在全国范围内的全局进行锁互斥和争抢。而淘宝的每个店铺都是在独立提供自己的货品，甚至货品的表示只是一个数字，而非具体的每个不可替换的货品。那么这些货品可以被分解到几乎任意多的小集群小系统中去对外提供服务，再辅以CDN缓存技术降低网络延迟，用户体验自然要比12306高几个档次。这并非是说12306技术水平比淘宝差太多，而是他们做的压根就不是一样的事情。</p>
<h3 id="17-2-3-排队"><a href="#17-2-3-排队" class="headerlink" title="17.2.3 排队"></a>17.2.3 排队</h3><p>如果硬件资源真的很紧张，既没办法做锁分散，也没办法通过租用云服务来解决。还有一个方法来保证在服务器不死机的情况下对外提供服务，那就是让用户排队。</p>
<p>排队这种事情司空见惯，尤其是在吃饭的时候。如果在吃午餐或者晚餐的时候到各大商场的招牌餐厅去看看就知道了，人气旺的店铺外面是有取号机的，没有取号机的店铺也会有两个漂亮妹子招呼拿号排队。银行也一样，银行是比较典型的“硬件资源不足”但又要提供服务的场景，只能让用户等。</p>
<p>在网络服务层面也有排队现象吗？有的，只是有的体验做得好，有的体验做得差。</p>
<p>排队也是有比较成熟的方案的，通常来说可以考虑在服务器和浏览器两侧进行配合。核心思路如下。</p>
<p>（1）能够在客户端（浏览器）挡住的访问坚决不要放到服务器上来。</p>
<p>例如，可以用JavaScript代码来进行“封堵”，至少做一个比较好的倒计时提示来告诉访问者前面还有多少人，或者要倒计时等多久才能排队等到位。</p>
<p>（2）能够分散到多个服务器上进行排队提示的内容坚决不要集中到一台服务器上来。这个原则和负载均衡的原则是没有区别的，我们不希望大家都一窝蜂到窗口来问“还差多少人到我”。能够用大广告牌说明的问题就写在大广告牌上，大家自己看。所以在服务器上也可以考虑用类似的方式进行分散性的广播，而不要集中到一台服务器去做排队询问。</p>
<h3 id="17-2-4-谨防“雪崩”"><a href="#17-2-4-谨防“雪崩”" class="headerlink" title="17.2.4 谨防“雪崩”"></a>17.2.4 谨防“雪崩”</h3><p>在不少介绍调优和架构的资料里会提到这样一个词汇“雪崩”。“雪崩”是一种自然现象，也是一种灾害，通常发生在常年积雪的山区（图17-1 <a href="#ch1_back">[1]</a> ）。</p>
<p><img src="Image00459.jpg" alt></p>
<p>图17-1 雪崩</p>
<p>由于声波震动或者地壳活动等原因，导致原来覆盖在山上的积雪产生一些内应力上的变化，使得它开始像泥石流那样具备一定的流动性，而后小规模的流动性逐渐引发大规模的流动性形成灾害。也就是说，一个几万吨重甚至几十万吨重的雪盖形成的重力势能转化为动能的破坏性可能最开始只是一只鸟在山顶上落了一下，或者是一个人大吼了一声。</p>
<p>在系统架构中提到的雪崩和这个现象看上去很类似，就是由于一台服务器或者一台服务器中的某个模块发生故障进而引发连锁反应，最后导致大量的服务器或者软件模块无法正常工作，这种现象也叫做“急剧变坏”现象。例如，常用的负载均衡型的集群里就会有类似的现象发生。</p>
<p>假如有一个PHP的集群，10个节点，前端的路由器用Round Robin（等权轮询）算法为后面10台PHP转发HTTP请求，当每台服务器都达到CPU占用80%的负载时，其实压力已经接近极限了。此时如果有一台PHP服务器突然停止响应，根据负载均衡协议，这个节点会被暂时移出整个负载均衡集群，那么新进来的负载就会被压在另外9台服务器上，粗略计算一下，其他9台服务器的负载大约会上升到89%，进而有更大概率引发其他服务器的崩溃，而崩溃的服务器再被移出负载均衡集群……后面的事情大家想想都能知道，服务器集群崩溃得越来越快，直到整个集群完全垮掉。这种由一个点的故障引起整个系统崩溃的现象就叫做雪崩现象。</p>
<p>要想防止雪崩需要做好以下几件事情。</p>
<p>（1）对服务器负载的估算。对服务器的负载应该有一个比较合理的估算。来自前端的HTTP请求或者其他形式的压力都可以通过软件来模拟进行压力测试，测试一台服务器在CPU达到60%左右时的负载数量。</p>
<p>（2）线上测试估算服务器数量。与其说线上测试，不如说按需购买。在把服务器上的服务释放之后，会有用户正常的负载需求从互联网上流进来，这个流量是有着周期规律波动性的，如在一天内有规律的波动性，在一周内有规律的波动性。这些峰值和低谷出现在什么时间，负载分别为多少，可以通过对单台服务器的CPU以及网络连接监控来捕捉到。进而可以推算出峰值和低谷各自需要的服务器数量——在安排资源时要多准备一些。</p>
<p>例如，峰值时如果计算出需要2台服务器，每台服务器60%的负载，其实这时服务器集群是没有真正“冗余”的，因为一旦其中的一台服务器由于故障停止响应立刻会引发雪崩，所以这时应该是在刚刚的基础上加一台服务器更为保险，即一共3台服务器，每台服务器40%的CPU负载。如果计算出需要4台服务器，每台服务器60%的负载，则如果一台服务器发生故障，其余3台服务器的CPU负载会被压力推升到80%，但是应该远没有第一种情况危险。</p>
<p>请注意，在负载均衡的集群中防止雪崩是一个资源和风险平衡的过程。选好这个平衡点就能在保证不发生雪崩的情况下资源投入最少。</p>
<p><a href="#ch1">[1]</a> 图片来自百度图库。</p>
<h2 id="17-3-小结"><a href="#17-3-小结" class="headerlink" title="17.3 小结"></a>17.3 小结</h2><p>系统架构这个课题是一个辩证使用技术和方法论保证服务性价比的事情。任何技术、任何方法都有其特点和局限性，只有融会贯通地使用才能在架构优选中获得更好的思路和解决方案。</p>
<h1 id="第18章-数据解读与数据的价值"><a href="#第18章-数据解读与数据的价值" class="headerlink" title="第18章 数据解读与数据的价值"></a>第18章 数据解读与数据的价值</h1><p>本章是杂谈性的内容，是基本脱离技术内容以外的一些延展性的话题讨论。阅读起来应该会更有一定的发散性，也会更觉得轻松一些。</p>
<p>在前几章里已经介绍过关于数字化运营的一些基本知识了，只要做好下面几步，就拥有了最基本的运营条件。</p>
<p>（1）数据收集。</p>
<p>（2）数据存储。</p>
<p>（3）数据结构化建模。</p>
<p>（4）指标体系。</p>
<p>不需要太多高深的知识，只需要认真把这每个环节都做好，一个公司的数字化运营就会自然传承迭代并帮助公司运营自如。</p>
<h2 id="18-1-运营指标"><a href="#18-1-运营指标" class="headerlink" title="18.1 运营指标"></a>18.1 运营指标</h2><p>指标作为独立的一节知识在第5章已经进行过讨论了。</p>
<p>运营指标和普通的指标有什么区别呢？从名字上来看，运营指标应该是指标的一个子集，也就是说有部分指标是可以作为运营指标来用的，有一部分则不适合。</p>
<p>运营指标，顾名思义，是为运营直接服务的指标。不同行业的运营指标，无论是关注的内容还是关注的周期其实是不一样的。</p>
<p>在日常生产工作中，利用运营指标能做的事情主要是纵向和横向的对比。</p>
<p>纵向，就是一个部门或者一个人，对不同时段的同一指标进行对比，以判断其运行状态是好转还是恶化，是进步还是退步；如一个部门的月生产产值，同比增长1000万元，环比增长200万元。这就是纵向比较。</p>
<p>横向，就是指部门和部门之间，个人和个人之间，尤其是那些职能相同的部门和职能相同的人之间的对比，能够看出工作效率的差异和盈亏的多寡。例如，张三的月销售额为100万元，李四为30万元，王五为50万元。通过对比可以得到清晰的工作能力量化排名，这就是典型的横向比较。</p>
<p>指标体系的建立和对比对于企业内部营造一种积极进取的气氛是非常重要的。</p>
<h3 id="18-1-1-互联网类型公司常用指标"><a href="#18-1-1-互联网类型公司常用指标" class="headerlink" title="18.1.1 互联网类型公司常用指标"></a>18.1.1 互联网类型公司常用指标</h3><p>下面举几个互联网类型的公司最常用的指标例子。</p>
<p>1.搜索类产品</p>
<p>搜索类产品大多是一些搜索引擎网站，有的网站搜索的对象比较宽泛，有的则是垂直类别的，如只供搜索各种新闻，或者只供搜索各种图片等。搜索类产品提供的服务核心是一个或几个关键词到一些链接的映射，通过这个映射可以迅速用关键词跳转到目标页面。盈利当然就是靠那些希望购买这种关键词对自己页面有高命中率的商家了。所以自然而然就有点击量、转化率等这些他们比较关心的指标了。</p>
<p>常用指标有以下几种。</p>
<p>日点击数（Page Views）：每天网站点击数量。</p>
<p>每月独立访问量（Monthly Uniques）：在没有用户体系的搜索类产品里一般是指独立IP的访问量，1个IP访问1次和10次都算1个独立访问。</p>
<p>点击付费链接的用户百分率（Percentage of Users that Click a Paid Link）：也就是“转化率”，表示列在用户面前付费链接里有多少比例的链接会让用户有兴趣点击。一般搜索类网站这个值都比较低的，有1%左右算是比较正常。</p>
<p>每次点击收入（Revenue per Click）：平均每次点击的收入。</p>
<p>2.游戏类产品</p>
<p>游戏类产品也有相应的一些指标。</p>
<p>每日/月活跃用户（Daily/Monthly Activated Users）：也就是俗称的“日活”和“月活”。一般来说，游戏的繁荣程度很大因素来自于活跃玩家的数量。日活/月活指标不只在游戏类产品中才会被人用到，很多软件产品也都会对这个值进行统计。只是游戏是一个用户互动性很强的产品，它的繁荣程度与这个值关联太密切，所以这个指标才被看得很重要。</p>
<p>付费用户转化率（Conversion Rate to Paying User）：付费用户与注册用户的占比。</p>
<p>平均每用户收入（Average Revenue per User，ARPU）：指的是以月或者年为单位的从每个用户身上平均收入多少。</p>
<h3 id="18-1-2-注意事项"><a href="#18-1-2-注意事项" class="headerlink" title="18.1.2 注意事项"></a>18.1.2 注意事项</h3><p>先说一个悲观的观点，也是很多数字化运营中数字解读者容易陷入的误区——指标能解释一切。但是事实却不是这样——运营指标能看到现象，甚至可以说对现象的感知及其灵敏度，但是它永远不能自我解释原因。</p>
<p>不仅是这样，有经验的运营人员可以通过自己的经验和行业知识对原因做试探性的推断和解释，但是仍然不能通过指标来直接并准确解释原因。它只是一个仪表，只是一个和温度计、体检报告并无二致的结果告知性的数字化表现，而非原因解释性的。</p>
<p>这就好比人们感觉不舒服，去医院做一个身体检查，检查结果是胃炎，那胃炎应该怎么吃药是有成熟的解决办法的，不管是老中医开的汤药，还是西医开的抗生素，开了就去吃，吃了就能好。但是至于是怎么得的胃炎？是暴饮暴食导致的，是饮酒过度引起的，是长期吃刺激性食物引起的，还是由其他原因引起的细菌性感染？这个在报告里未必能看出来，还需要进行进一步排查。</p>
<p>在驾驶汽车的过程中，看到油箱表下降较快也只是看到一个现象，当它下降到红线位置时就是提醒我们应该去加油了。至于为什么下降快，是因为刚刚爬坡，油门给得过大，还是油箱有渗漏？仪表盘是没办法做解读的，只能根据进一步的各环节检修进行排查。</p>
<p>那汽车仪表盘的场景有没有“优化”的手段呢？如果想把这个排查的时间过程缩短，是不是可以在这些环节各自再设立一个指标作为众多仪表的一员，如“平均坡度仪”——记录刚刚过去的2小时里爬坡的角度平均为多少；“平均油门”——记录刚刚过去的2小时里油门给得多大，对比一下看比平时大还是小。如果这两个指标都正常，那就说明十有八九是油箱有渗漏了，也就不需要排查其他环节，直接检查油箱和油路。飞机驾驶舱的仪表盘非常多，你以为是飞机制造商喜欢耍酷还是飞行员自己有密集偏好症吗？绝对不是。就是为了把这个空重将近280吨的大家伙每个关键环节的实时状态都展现在飞行员的面前，来保证飞行的安全进行。图18-1所示为空客A380的驾驶舱仪表盘，够复杂吧？密集恐惧症患者请绕行。</p>
<p><img src="Image00460.jpg" alt></p>
<p>图18-1 空客A380的驾驶舱仪表盘</p>
<p>在驾车的例子里，“优化”是加引号的，其实原因也很简单，就是说说“优化”是不是真的够优。这个事情究竟是不是值得为此专门放两个仪表在车上，毕竟它是有成本的，而且仪表盘上又多了两个示数。这也是在指标化运营中一直面临的一个矛盾，指标的含义、指标的数量设置要合理，要让这个指标的维护和解读的成本与它的作用和收益相称。这一点请读者一定要注意。</p>
<h2 id="18-2-AB测试"><a href="#18-2-AB测试" class="headerlink" title="18.2 AB测试"></a>18.2 AB测试</h2><p>AB测试在很多互联网产品中都很常用，甚至有很多老牌的软件企业也从这种方式中汲取经验。</p>
<p>AB测试指的是什么呢？</p>
<p>在我看来，AB测试是一种评价体系的核心思想。大致的工作流程如下，当不知道一种产品的A方案好还是B方案好时，或者两种设计完全不同的产品A和B的市场反应如何时，会考虑找两组用户来进行测试。</p>
<p>如设置两个对比组，A组100人，B组100人，给A组产品的A方案，给B组产品的B方案，然后观测各种反应指标。最后得出一种相对客观的比对结论。这就是AB测试的整体思路。</p>
<p>AB测试虽然对于互联网产品是一种舶来品——在很多传统行业里早就已经开始使用了，而且近几年也逐渐应用更为广泛。</p>
<p>传统行业里有哪些地方用了AB测试吗？有的。</p>
<p>例如，药品的临床测试，有很多新药，要测试其是否真的有效，或者其药效是否比其他药的药效好，通常采用的方法叫做“随机对照试验”（Randomized Controlled Trial，RCT），也就是将病患分为两组，然后一组给药一组不给药；如果是对比两种药物，那就是一组给待测药一组给另一种对比药物，在疗程结束后对比治愈率。为了避免人为情绪化因素以及个别样例的特殊反应对测试结果的影响，又进化出一种叫做“大样本随机双盲试验”的办法，算是对“随机对照试验”的进一步科学化的诠释。</p>
<p>两者的不同点在于：</p>
<p>第一，大样本，样本量加大稀释个别样例特殊反应对统计结果的影响；</p>
<p>第二，双盲，就是让病患和医生都对药品和分发对象事先不知晓，让所有人都在这个被他人安排好的测试旅程中一步一步进行试验，直到最后再去对比测试的结果。这样就避免了在人与人接触的过程中由于主观情绪掺杂在交谈里引起的一些难以把握的因素。例如，医生如果主观上对这种药不看好，或者主观上认为这种药效果不错，在交谈的过程中或多或少会有情绪上对病患的暗示，那这种暗示对于治疗配合程度的影响会干扰测试结果。假如治疗结果好，就说不清究竟是药品真的很管用，还是其中有更多“安慰剂”<br><a href="#ch1_back">[1]</a> 的成分。</p>
<p>除此之外，国内在推行一些制度或者管理办法时也都有“试点企业”、“经济特区”的一些局部区别性的制度，目的就是为了看这种AB测试的对比效果，效果好了就推广，效果不好就停掉再试其他方法。</p>
<p><a href="#ch1">[1]</a><br>安慰剂：Placebo，具有一定的作用，对有心理因素参与控制的自主神经系统功能如血压、心率、胃分泌、呕吐、性功能等的影响较大。它的心理影响效应对病症的缓解在临床上已经得到了相当程度认可。</p>
<h3 id="18-2-1-网页测试"><a href="#18-2-1-网页测试" class="headerlink" title="18.2.1 网页测试"></a>18.2.1 网页测试</h3><p>在互联网产品的开发过程中，AB测试的使用也是非常广泛的，尤其是在拿不准用户喜好的时候。</p>
<p>在一个网页（界面）上线时，再好的经验也没办法判断究竟这一次发布结果如何，在有两个以上选择时也会面临这种问题。那就不如都交给市场，让市场的反应说了算。</p>
<p>例如，要做一个网站（网页），不确定用户对哪一个更喜欢，那就在试运营或者公测时让访问的用户被随机分配到一种方案上，如果有两种备选方案，那就是A方案和B方案。让他从头到尾都使用这一种方案风格对网站（网页）进行访问。记得把他访问的路径记录下来。某一个人个案性的访问路径、访问时间、点击数可能不能说明什么，这必须看宏观统计。在测试一个网站的过程中要至少选择数百人进行随机分派，有条件也可以多选一些。</p>
<p>但是为了保证测试不是由于提供的内容导致用户的好恶不同而只是由于表现形式不同的，那就要保证对A、B两组人只提供外形（样式）差异的网站。因为除了网站外形的不同以外，网站提供的内容、访问产生的延迟等都会对网站对用户的吸引程度有影响，这个因素要尽可能排除在外。</p>
<p>在做完一轮测试以后，周期是自己设定的，可以是3天，也可以是7天或其他的天数，如果基本能够保证每个用户从第一次看到网站入口开始就被分成A、B两组中的一组而且从一而终，那就可以看看以下指标。以每种方案为观察单位。</p>
<p>（1）用户平均一次访问的页数。</p>
<p>（2）用户平均每页逗留时间的长度。</p>
<p>（3）用户再访问比例。</p>
<p>（4）N天留存率（回访率）。</p>
<p>通过这些比较大概就能得出方案孰优孰劣的结论了。也可能两者差不多，也可能都不太理想但是其中一个略好一些，但是这时已经能够做出选择了。</p>
<h3 id="18-2-2-方案测试"><a href="#18-2-2-方案测试" class="headerlink" title="18.2.2 方案测试"></a>18.2.2 方案测试</h3><p>如果不是一个网站，而是某一个产品的方案呢？还能这么简单地去做AB测试吗？其实也是可以的，因为AB测试不是一个具体的测试工具而是一种测试的思想。那我们再来看一个例子。</p>
<p>我们以一个右下角分辨率为300×200像素的升窗广告位产品为例。如果你是这个广告位产品的产品经理，你需要考虑这个广告位如何安排版面的问题，至少有以下两种选择。</p>
<p>其一，可以推送一幅大的广告。占满整个小广告屏，然后让里面的4个广告位进行2秒为单位的滚动。如图18-2 <a href="#ch1_back">[1]</a><br>所示为一个6幅图自动切换的样例，能够看到下面有6个切换按钮，在画面滚动切换时，如果想跳转到其他编号的广告画面则单击相应按钮进行切换。4幅广告也是同理，只是切换按钮只有4个。</p>
<p><img src="Image00461.jpg" alt></p>
<p>图18-2 6幅图自动切换</p>
<p>其二，可以推送4幅小的广告。让这4幅小的广告拼起来占满整个小广告屏。如图18-3 <a href="#ch2_back">[2]</a><br>所示为一个9幅图的样例，版式为3×3，也有4幅图的，即版式为2×2。</p>
<p><img src="Image00462.jpg" alt></p>
<p>图18-3 9幅图同时显示</p>
<p>在用户触发了这次广告推送的事件以后，广告后台就要做出反应，可以用随机的方法进行1：1的推送，即让这两种展示方式在每次请求中的几率均等，都是50%。一般来说，这基本也能够保证足够的随机性了。</p>
<p>之后就可以观察究竟由哪一个带来的广告点击转化率高，是第一种方式容易诱导用户点击广告位还是第二种方式容易诱导用户点击广告位，这通过用点击数除以推送数就能够得出来，甚至用不了一天就出结果，极容易验证。只要有了结果，就可以考虑在全局使用这种方案了，因为已经有了足够的且确实的理由。</p>
<p>在做互联网产品时千万不要犯经验主义的错误，经验永远是局限的，唯一不变的东西就是变化本身。像这种广告，在没有测试之前不能武断地认为一定是哪个更好，因为两种方式也确实各有优缺点。</p>
<p>第一种大广告好处是图片清晰，内容可以更丰富；不好的因素是滚动，滚动就意味着不能一目了然地看清所有的信息。第二种广告的好处是一目了然；不好的因素是可用的分辨率变小，文字和图片的展示都比第一种更为有限。哪种转化率高只能通过比对结果的数据来说话了。</p>
<p>扩展一下这个话题。如果想把这个例子做到极致，还可以尝试对每次弹出的信息做分类。区分一下素材和题材，在相同的素材和题材下面去对比看哪一种转化率高。如最后方案AB测试对比结果可能如表18-1所示。</p>
<p>表18-1 AB测试对比结果</p>
<p><img src="Image00463.jpg" alt></p>
<p>如果能够得到这样一个表格，那么在不同的广告投放方案被触发时是可以采用不同的排版策略的，这样会比“一刀切”的排版方式提供更大的全局转化率。</p>
<p>其他维度上的对比可以再想其他方式，总之，有了AB测试，基本所有这种难以琢磨的偏好把握都有了量化对比的手段。</p>
<p><a href="#ch1">[1]</a> 图片来源于京东商城截图。</p>
<p><a href="#ch2">[2]</a> 图片来源于淘宝网截图。</p>
<h3 id="18-2-3-灰度发布"><a href="#18-2-3-灰度发布" class="headerlink" title="18.2.3 灰度发布"></a>18.2.3 灰度发布</h3><p>在游戏新版本的发布环节中有一个名词叫“灰度发布”。这个词很形象，因为它表示的就是一个“黑白混杂”的情况。</p>
<p>那“黑”和“白”分别指什么呢？其实我们可以认为“黑”就是旧版本，“白”是新版本，在两个版本进行更迭时就是一个从“黑”到“白”的过程。</p>
<p>一般一个游戏的客户端从启动就开始检测是否有新版本可以更新，如果有，它就会启动更新模块开始下载，并把这些文件覆盖到客户端游戏的程序中去。不得不承认，即便在技术积累非常好的公司里，在全网范围内做更新都是一件很有风险的事情，这个风险很大程度上已经不是技术层面的风险了——即便在封测阶段、内测阶段、体服公测阶段（在体验服务器上进行的半公开测试）的测试都能通过，不死机、不闪退、不卡顿，其实也并不能保证在全网更新后游戏论坛会被吐槽的人民群众刷屏。毕竟游戏版本中的对错不是以技术标准来衡量的，更多的是玩家情绪的反应，而情绪这个东西又太复杂，在这些测试的阶段也不一定能测出好的效果。怎么办？“灰度发布”应运而生。</p>
<p>用策略文件进行控制，可以仅对全网环境中的部分用户——可以是5%，可以是10%，也可以再多一些（但是这些数量级比封测和体服的数量级还是大多了）进行更新，看看他们的反应。这些反应有的会直接体现在当天的DAU（日活跃用户）上，也可能体现在下面接连几天的DAU上，也可能体现在其他指标上，也有可能会更直接地体现在官网论坛上或者客服的电话里。</p>
<p>每次“灰度发布”都是一个决策的实验而已，而接下来就是两种选择。一种，反响良好，继续更新到全网范围内。另一种，反响不理想，把已经升级的客户端回滚到前一个稳定版本——不能变好起码也要保持现状。</p>
<p>“灰度发布”在游戏里用得多只是因为游戏的版本更迭比较频繁，仅此而已，并非它只适合于游戏软件。其他任何的可以通过互联网进行分发的软件产品都可以采用这种思路，甚至是云端用网页来实现的软件也能用这种方式进行试探性的用户反馈测试。思路就是这样，简单吧？</p>
<p>不要小看这个简单的东西，用得好会让产品每次都能顺利爬台阶，一步一步走向正确的方向，这比求助任何行业专家都要成本低而且反馈灵敏。</p>
<h3 id="18-2-4-注意事项"><a href="#18-2-4-注意事项" class="headerlink" title="18.2.4 注意事项"></a>18.2.4 注意事项</h3><p>AB测试虽然好用，但是也需要注意技巧，尤其是它的局限性。请务必注意！</p>
<p>AB测试测试的是两种不同的方案，虽然能够比较出哪一种效果更好，然而方案的相异点越多，越无法定位造成影响的原因。</p>
<p>1.量化比较对象</p>
<p>在对比的过程中尽可能去量化比较的对象。例如，在网站外形的比较中，字体大小的磅数，显示窗体大小的尺寸，每页的行数，如果想进行研究把他们作为对比的对象的话，这些值是要量化的。AB测试有可能会进行多轮，多轮之间的结果对比要形成一定的结论性的东西，也就是要试出一个经验值（Magic Number）或者一个知识。至少下一次再做同类的事情不需要从头开始试起，而如果要试，也就是试一下有没有比这个已知的最好值更好的值。这对于“创新即生命”的互联网产品是极有意义的。你能接受所有的运营人员每天都在用“大一点”，“稍微有点小”，“不够快”这种感性的说法来在彼此之间传递信息吗？如果不能，那就尽量做到量化吧。</p>
<p>2.单一化</p>
<p>两个网站方案，色调不同，文字大小不同，布局也不同，每一页的条目数量不同，即便最后确实能比较出来有一种风格更容易被人喜欢。但是，究竟是由哪一种或几种因素“引发”了这种偏好的表现不得而知。</p>
<p>如果一定要得到对应的解释，应在每次方案比较时把方案之间不同的地方压缩到最少，如只有一个方面不同，其他的都相同。通常这样比较出来的结果针对性会非常强，对形成自己完善的产品运营和演进体系是有好处的。如果担心要验证的方面太多会让验证周期加长，则可以同时开启多个AB测试的对照组，每个对照组进行独立的单一属性的对比，这样也能够在一定程度上缩短测试的周期。</p>
<p>3.强隔离</p>
<p>AB测试还要注意一个问题，也就是测试的环境应该是一种强隔离的环境，因为测试对象内部与外界如果联系过多会直接导致测试的失败或者根本无法进行。</p>
<p>世界上很多国家都是实行夏时制的，如澳大利亚、俄罗斯以及欧盟各国。</p>
<p>夏时制指的是一种为节约能源而人为规定地方时间的制度，在这一制度实行期间所采用的统一时间称为“夏令时间”——对应的非制度期间叫做“冬令时间”。一般在天亮得早的夏季人为将时间提前一小时，如把表从9：00拨到8：00，然后在5个月后夏令时结束的时候再把表从8：00拨回到9：00而且是全国人民都这么做，据说这样可以使人早起早睡，减少照明量，以充分利用光照资源，从而节约照明用电。</p>
<p>我国在1935年到1979年间间断地实行过若干次夏时制，最近的一次是1986年到1991年，每年的4月到9月这5个月时间实行夏时制。最后还是由于认为这种制度得不偿失而取消掉——不管怎么说，在这段时间里，要保证所有的计时器时间都同步变化这一个小时，学校上课、火车载客、医院就诊，机构的时钟要变，人的时钟也要变。最要命的是不少人要为这1个小时花一两周来倒时差。所以权衡利弊，在1991年以后我国再也没有做过夏时制的调整。</p>
<p>按说这种全国性的新制度政策应该就像经济特区或者试点城市一样做一个试点性的测试，但是就是真的有人想做恐怕实行者也会说“不管是城里还是乡下这东西都不会玩”。这些试点城市和外界的一切联系都是要靠时间来进行同步的，尤其是在国内这么频繁互动的环境，这个地区和其他地区的交通时刻表要做一个1小时的差值变换，电视节目转播要做1小时的差值变换，恐怕连打个电话都要将这边是几点那边是几点反复强调，这些同样是巨大的成本。</p>
<p>4.其他不良后果</p>
<p>如果要对产品用户做大礼包赠送这种活动，可是不知道送什么细节内容组合让用户更有黏着性或更满意。这种情况也是可以考虑使用AB测试的。</p>
<p>准备两种不同内容的礼包，然后让用户自己选，记得做好登记工作以及事后持续不断的数据反馈工作，这样较为妥帖。</p>
<p>切忌1：不允许选择。</p>
<p>如果不让用户自己选择，而是进行随机性的派发，那么很可能会让用户收到自己不满意的礼包而其实明明有另外一种礼包可能更适合他却没有派送给他。这种情况如果被用户知道，轻则背地里吐槽说运营人员脑子进水，重则会引发用户集体性拂袖而去。反正哪一种都不是原先派发礼包的目的。而且在大量的自主选择的过程中也能看出一定的情趣取向。</p>
<p>切忌2：价值悬殊。</p>
<p>如果所做的两种对比礼包内容价值相差悬殊，尤其是非自主性选择的情况下，也会引起用户对公司厚此薄彼策略的猜测，如果公关部门不能很好地处理可能有引发一些群体事件。</p>
<h2 id="18-3-数据可视化"><a href="#18-3-数据可视化" class="headerlink" title="18.3 数据可视化"></a>18.3 数据可视化</h2><p>数据可视化是一个一直以来都被数据运营人员重视的问题。做好数据可视化，关键有以下两点。</p>
<p>我在大学毕业毕业不久的时候，由于对职场工作感觉心里没底，曾经问过一个世界500强的大型化妆品跨国公司的大中华区总经理——好在他是我表姐夫，不收咨询费。我问他，“工作怎么去把控，报表怎么做比较好？”他只说了几句话，让我当时也是茅塞顿开，虽然当时是确实“不明觉厉”的感觉，但今天回想起来感觉这真的是精华中的精华。</p>
<p>第一，一切工作尽量目标化和数字化。这个其实说的就是指标运营的问题，就算是公司没有指标，自己对自己有指标的要求也会让人有激励自己进步的能力。</p>
<p>第二，陈述简洁化。能用图的不要用表格，能用表格的不要用条目，能用条目的不要用段落。这个其实说的是可视化的问题。</p>
<h3 id="18-3-1-图表"><a href="#18-3-1-图表" class="headerlink" title="18.3.1 图表"></a>18.3.1 图表</h3><p>在运营中对可视化的重视不是因为我们要赶时髦，喜欢喊口号，而是人自身对外界信息认知本身就有的敏感而有的迟钝。很多东西与生俱来，要让说教使得每个人克服这种迟钝的难度很大，不如从人认知特点的角度进行弥补，这才是可视化被重视的根源——也就是以人为本。</p>
<p>信息越少，人的注意力越容易集中；反之，信息越多，人的注意力越不容易集中。 这才是在报表上应该着力注意的问题。</p>
<p>人们对图的敏感度是比较高的，对形状和颜色的敏感度也比较高。所以在数据可视化中多采用柱状图、折线图、散点图、饼图、雷达图、热力图……</p>
<p>柱状图如图18-4 <a href="#ch1_back">[1]</a> 所示。</p>
<p>折线图如图18-5 <a href="#ch2_back">[2]</a> 所示。</p>
<p>柱状图和折线图主要是用来观察变化趋势的。</p>
<p>散点图如图18-6 <a href="#ch3_back">[3]</a> 所示。</p>
<p><img src="Image00464.jpg" alt></p>
<p>图18-4 柱状图</p>
<p><img src="Image00465.jpg" alt></p>
<p>图18-5 拆线图</p>
<p><img src="Image00466.jpg" alt></p>
<p>图18-6 散点图</p>
<p>散点图是用来观察分布密度的。示例中的散点图看上去像是做回归用的，而平常多见的散点图通常是没有用直线去穿点的过程的。</p>
<p>饼图如图18-7 <a href="#ch4_back">[4]</a> 所示。</p>
<p><img src="Image00467.jpg" alt></p>
<p>图18-7 饼图</p>
<p>饼图是用来观察比例关系的。</p>
<p>热力图如图18-8 <a href="#ch5_back">[5]</a> 所示。</p>
<p><img src="Image00468.jpg" alt></p>
<p>图18-8 热力图</p>
<p>热力图是用来观察连续维度下的“热度”变化的，是一种特殊的散点图。</p>
<p>雷达图如图18-9所示。</p>
<p><img src="Image00469.jpg" alt></p>
<p>图18-9 雷达图</p>
<p>雷达图是用来表示单个对象多维度（属性）分布均衡度的。</p>
<p>还有很多更为丰富的图表内容，也有一些是这些基本图形图表的变种，但是能够掌握这些就已经能够在平时的数据挖掘和运营中解决绝大部分问题了。</p>
<p><a href="#ch1">[1]</a> 引用自echarts.baidu.com 网站示例。</p>
<p><a href="#ch2">[2]</a> 引用自echarts.baidu.com 网站示例。</p>
<p><a href="#ch3">[3]</a> 引用自echarts.baidu.com 网站示例。</p>
<p><a href="#ch4">[4]</a> 引用自echarts.baidu.com 网站示例。</p>
<p><a href="#ch5">[5]</a> 引用自echarts.baidu.com 网站示例。</p>
<h3 id="18-3-2-表格"><a href="#18-3-2-表格" class="headerlink" title="18.3.2 表格"></a>18.3.2 表格</h3><p>有的数据内容维度多，超过2维的向量数据已不能在A4纸上打印，而超过3维的向量数据在计算机显示器上也很难表示出来。这种高维度的数据不方便用图形表示，所以只能退而求其次用表格来表示。</p>
<p>表格的好处是内容丰富而且行列规整、横平竖直，是一种非常好的结构化数据标本。然而即便是这样领导们也是不喜欢看表格的，数字天然就没有颜色和形状对人的视觉刺激感好。好在一般都是数据科学家面对大量的多维数据分析，分析成低维度数据交给决策层时已经可以用视觉冲击良好的图表来表现了，例如，图18-10 <a href="#ch1_back">[1]</a> 所示的二维表格。</p>
<p><img src="Image00470.jpg" alt></p>
<p>图18-10 二维表格</p>
<p>最不推荐的就是用条目来做描述，如果用，也是用于在一个图形或者一个表格之后做补充说明。</p>
<p>段落化的陈述最不提倡，因为人们从这种非结构化的陈述中提取信息是时间成本最高的。</p>
<p><a href="#ch1">[1]</a> 图片来源于百度图库。</p>
<h2 id="18-4-多维度——大数据的灵魂"><a href="#18-4-多维度——大数据的灵魂" class="headerlink" title="18.4 多维度——大数据的灵魂"></a>18.4 多维度——大数据的灵魂</h2><h3 id="18-4-1-多大算大"><a href="#18-4-1-多大算大" class="headerlink" title="18.4.1 多大算大"></a>18.4.1 多大算大</h3><p>“大数据”这个词本身具有莫大的误导性，再加上行业里动辄宣传和鼓吹Google、Facebook的机房有多大，存量有多大，就更容易让人们认为，必须数据量超级大才算大数据入了门，才算“大”数据。认为数据很多才算大才有价值，就好比饭越多越好吃，音乐声音越大才越好听一样经不起推敲。</p>
<p>做大数据的目的究竟是什么呢？尤其是作为商业用途来说，无非是为了多赚钱或者多省钱，不论是直接的还是间接的。一旦脱离开这些，谁来买单，谁来背成本？大数据价值就成了伪命题。</p>
<p>Google和Facebook这样的公司做大规模系统的目的也不是为了炫富，而是他们确实数据量膨胀到一定程度了，不得不使用一些平时应用场景里不多见的技术，所以“这些技术一定是大数据的必备条件”就自然变成了误导人们的信息。作为挑战尖端科技和中国这种人口基数的互联网公司的客观需求，研究超大规模架构集群技术是一个方向而且绝对正确，但不建议中小型公司邯郸学步。</p>
<p>中小型公司需要大数据吗？答案是肯定的，不仅需要，而且非常需要。中小型公司要用大数据做什么？这种需求多少年来一直没有变过，还是刚刚说的要么多赚钱要么多省钱。多赚钱多省钱的途径在数据运营中最常见的就是指标管理，再有就是诸如财务分析、人力成本分析、工作效率及成果分析等。这些东西在日常生产生活中占了绝大多数的数据应用场景。对这些对象研究明白了就已经能解决大部分运营问题了。</p>
<p>如果还想做得深入一些怎么办？再把参考维度的数据增多就可以了，如刚刚这些数据指标是否和气候变化有关？是否和地理位置有关？是否和大气污染程度有关？是否跟当前热播的电视剧有关？是否跟短时间内网上的一个热词有关？是否跟交通状况有关？是否跟人们使用的上网设备有关等。这些数据的引入不需要做得非常多，只要相互结合有效且丰富适度，就可以挖掘。甚至指标自身前后是否彼此有影响规律，也是一个值得研究的课题。</p>
<h3 id="18-4-2-大数据网络"><a href="#18-4-2-大数据网络" class="headerlink" title="18.4.2 大数据网络"></a>18.4.2 大数据网络</h3><p>大数据很重要，尤其是从国家战略的高度来看这个问题就更是如此。国家要做决策需要很多事实作为依据，尤其需要大量的数据作为参考。数据的获取有很多途径，而成本最高的也许莫过于派出专门的调查小组去做访谈和总结了。这种方式缺点显而易见——时间周期长，人工成本高，协调工作难度大。其实根本没有必要派出很多调查小组去调查能从大数据网络中得到的信息，正所谓“一叶知秋”<br><a href="#ch1_back">[1]</a> 。</p>
<p>从广泛部署的物联网来看，很多以前不可想象的问题到现在已经逐步得到了解决。</p>
<p>想知道一个地区繁荣程度如何？最简单的就看用电和用水就可以了，带有远程抄表功能的智能电表、水表可以进行实时数据归集，每天都能看到数字的变化。做做同比、环比就知道这个地区的电器和人口活跃程度有没有变化，不管是常驻的还是流动的，而且还能区分是民用水电的增加还是工业水电的增加。</p>
<p>想知道一个地区交通状况如何？看看那些高速公路和国道、省道上的测速摄像头记录就能知道。什么时间，车流通过多少，车速如何，就能够从一定程度反映出交通的情况，是人迹寥寥还是车水马龙，是风驰电掣还是拥堵不堪。如果做路段改造，成果也能从这些数据里直接得到体现，而且都是立竿见影。</p>
<p>想知道一个地区的水文信息、气候信息，也没问题，传感网络可以遍及到每个人迹罕至的地方，不管是江河湖海还是沙漠沼泽，无人值守的传感器可以为人们代劳数据收集和上报的工作。</p>
<p>总而言之，这些原本需要大量花费人力物力才能进行收集和统计的数据，做起来已经越来越轻松——在这背后是大数据中心的功劳。这样的工作量在大数据时代到来之前是根本没办法想象的。</p>
<p>一个一个独立的大数据中心能够在某一个分领域或者一个地区做到数据收集，数据存储，数据分析，但这还不是数据的终极价值。因为人类社会客观上来说是广泛联系的，这种联系既然存在于人与人、人与物、物与物之间，那必定能够体现于广泛联系的数据之中。因此，一个孤立的大数据中心建得再好也是无法从割裂的数据去诠释全局的数据全貌的。如果想要了解这种广泛的联系，那必定会产生广泛的数据交换的需求，这就是大数据网络——由大数据中心彼此连接形成的数据交换网络。</p>
<p><a href="#ch1">[1]</a> 《淮南子·说山训》：见一叶落而知岁之将暮。</p>
<h3 id="18-4-3-去中心化才能活跃"><a href="#18-4-3-去中心化才能活跃" class="headerlink" title="18.4.3 去中心化才能活跃"></a>18.4.3 去中心化才能活跃</h3><p>大数据中心之间之所以会形成网络，不是因为有人以强制手段命令它成为这种样子，同样是自然形成的——以自然形成的东西其背后的力量就是自然（这里的自然不是说的山川湖泊花鸟鱼虫的这种保护自然环境的自然）的力量，自然的力量远比人类的力量澎湃与不可逆转。</p>
<p>人类在世界各地的大陆和岛屿上广泛分散存在而不是集中存在，即便是集中聚居在城市之中，城市的规模也是有限的，要想在更大规模内形成交互和协作，城市和城市之间就要建立联系。容易让肉眼看到的东西如高速公路、电话线缆、光纤网络、飞机航线等，这些都是联系的方式。城市本身是拥有膨胀和坍缩的自然动力的，城市的边界扩展和萎缩说到实质还是由于资源，也就是人口和支持人口的一切生存必需物质是否支持其膨胀。当城市膨胀到一定程度时必然会引发极多的“大城市病”，如就医难、上学难、出行难……说来说去还都是资源分配不足的问题，这些问题势必对城市扩张形成阻碍。</p>
<p>大数据产业的发展会遇到完全一样的问题，数据资源的集中必然会带来交换速度加快、备份方便、计算方便等多种便利，而同时也会提高大数据中心建设的难度。人工成本、电力成本、管理成本、灾备硬件成本，这些都在给资本输入行业抬高门槛。而这种牵一发而动全身的庞大结构也让每一次技术演进、技术决策、方案调整如履薄冰。资源越集中对于资源调度的难度其实就越大，这个和人类本身的认知和掌控能力有关。</p>
<p>大数据产业里任何一个环节的发展思路都应该以去中心化为原则，而不应直奔“大而全”的目标。这个原理就像做这样的选择一样：是在中国建立一个大城市还是几百个分散的小城市，是建立一个特别大的超市还是若干个分散的小超市，是建一个特别大的办事处还是分散的小办事处。哪种更有好的操作性，哪种更容易帮助资源得到合理分配，哪种方式就应该是首选。</p>
<p>去掉中心的本身不是目的，去掉中心是为了发展更为迅速。为此同样还要让各个分散的大数据中心以及各个生产环节建立连接，尤其是帮助数据在逻辑层面建立连接，让它们的维度扩展和丰富起来。</p>
<h3 id="18-4-4-数据会过剩吗"><a href="#18-4-4-数据会过剩吗" class="headerlink" title="18.4.4 数据会过剩吗"></a>18.4.4 数据会过剩吗</h3><p>数据作为一种产品，它的生产会过剩吗？先说说商品为什么会过剩的。</p>
<p>从书本上学到的商品过剩的概念也好，还是在平时的生活中碰到的商品过多而产生的大甩卖也好，是一种供大于求的现象。换句话说，生产了大量的超过市场消化能力的产品，就是过剩。</p>
<p>过剩很可怕。粮食生产过剩，粮贱伤农，农民苦不堪言；成衣生产过剩，也就只能低价甩卖，甚至是没有最低只有更低——我国在20世纪80年代初到90年代中后期在全国各地持续的“限产压锭”的过程中已经吃过了纺织品生产过剩的苦头。大量的工人下岗、转产，纺织厂也是并转关停。</p>
<p>想想看，数据会不会有一天生产过剩，以至于大家都廉价去售卖数据甚至不要钱让别人来用自己的数据呢？</p>
<p>即便是在有知识产权保护的前提下，有一点也是可以肯定的，那就是同质的数据是有可能“过剩”的。如果大家都售卖的是完全一样的数据，连质量也完全一样，那么这种数据必然会产生供过于求的情况——两个完全一样的物品（服务）那就只能比谁更便宜。这种情况下，数据生产商其实就退变成中介了，而且还不是高档的中介，这不是大数据产业核心的意义所在。</p>
<p>但是数据的生产有一个天然的上限，可以说出这个上限在哪里，但是怎么也不可能摸到它，这个上限就是“世界乃至宇宙发生所有行为综合的客观描述”。——不知道你同意不同意。</p>
<p>世界上的万事万物都是运动的，每一刻，每一个行为都是可以进行量化的客观记录的，这些记录将形成一个无穷大的数据样本空间，连维度都是无穷大的，大到无法记录全貌。现在这一刻虽然意识到大数据世界的重要，但是仅仅是在整个大数据空间的奇点上，真正的大数据市场还远远没有开始，还有大量的数据根本没有记录甚至有很多有价值的数据还没被人们认识到是数据。</p>
<p>这些还没被认识到的数据同样会受到供需的影响，缓缓地、逐步地纳入人类的数据研究视线，这只是时间和性价比的问题而已。所以从这个角度来说，数据的生产会过剩吗？应该说，不会，而且永远不会。</p>
<h2 id="18-5-数据变现的场景"><a href="#18-5-数据变现的场景" class="headerlink" title="18.5 数据变现的场景"></a>18.5 数据变现的场景</h2><p>最后来说说数据变现的问题。</p>
<p>我们的信息本身就是一种财产，我们的手机号、年龄、身份证号、职业、年收入水平、购房与否、购车与否、婚否、育否……这些信息已经很丰富了。这些信息已经能够让那些商家愿意花一大笔钱来购买，然后按图索骥，找到“高需求用户”，推销各种“高需求产品”。这些商家自然是获得了好处，但是人们的隐私却被冒犯了。原因无非是某些机构在登记了人们的信息以后转手卖给了这些商家，这是一种表现最为赤裸的数据变现。</p>
<p>想想看，商家买这些数据的目的是什么呢？应该是为了帮助他改进业务。为什么能够帮助他改进业务呢？因为他一旦有了这些数据就能不需要大海捞针式地再去逐个给所有手机号打电话了，节约时间，节约人力成本，这才是他愿意为这些数据买单的最根本的原因。有些数据你可能白给他他都没兴趣，因为拿着没用，不能赚钱也不能省钱。</p>
<p>我们把例子举透彻一些吧。</p>
<p>如果这种骚扰电话使用大海捞针的方式，假设需要打10000通才能成交1笔，被骂9999次。那么在获取了这份名单之后，也许只需要拨打1000通就能成交1笔，被骂999次，产能提高90%。如果加之有人预先对用户做过画像，进一步进行了转化率的优化筛查，也许只需要拨打100通就能成交1笔，被骂99次。同样是成交1笔，产能提高了99%。换而言之，假设原来要雇佣100个人来打这10000通电话，现在只需要雇用1个人就够了。那99个人就是节约下来的人力，就是用蒸汽机驱动的纺纱机代替纺织女工的过程。</p>
<h3 id="18-5-1-数据价值的衡量的讨论"><a href="#18-5-1-数据价值的衡量的讨论" class="headerlink" title="18.5.1 数据价值的衡量的讨论"></a>18.5.1 数据价值的衡量的讨论</h3><p>1.交换价值</p>
<p>前面反复提到，大数据产业的价值在于“洞悉世界”——消除不确定，变不定为肯定。在这个消除不确定的过程中，大数据解放了大量的生产力，这是大数据产业从经济学和社会学层面体现出来的意义。</p>
<p>说到这里，可以考虑尝试着用公式性的方式描述一下数据的价值。</p>
<p>在《资本论》里有这样的论述，马克思说：“作为价值，一切商品都只是一定量的凝固的劳动时间 <a href="#ch1_back">[1]</a><br>。”随着市场经济的逐步繁荣，《资本论》的论述或许会有些单纯，或者有一定的局限性。那么就按照市场经济的观点来做一个调整性的解释——商品真实的成交价是一个更容易被人认可的价值体现。</p>
<p>数据如果要变现，自然有成交价，这个成交价就是数据自身的“交换价值”。还是用《资本论》里熟悉的名词来类比。一个人为了了解和消除不确定性购买数据，而为此付出金钱代价，这个成交价就是它的交换价值。即便数据一样，成交价也可能因人而异，这就像平时在市场上看到的现象那样，一样的工业品，如键盘、鼠标、U盘这些最常见的东西，即便是完全没有差异的鼠标，也会由于当时的供需造成价格的波动，那这个时刻的交换价值实际也是在波动的，这种现象同样存在于数据交易中。</p>
<p>2.作用价值</p>
<p>数据的获取为人们消除不确定性。那么通过其他途径消除这种不确定性的成本就是数据为人们替代下来的，就是数据发挥的作用，所以“通过其他途径消除这种不确定性的成本”应该被称为这片数据的作用价值——这更像是一种成本估算。如果没有数据会怎么样？自己亲自去做访谈了解，听新闻，查看各种其他方面的资料，这是要消耗大量成本的。但是，即便在没有任何途径消除这种不确定性的时候，作用价值应该也不会无穷大，因为根据自己经验、推测等手段，还是会对这个待了解的数据的“真实值”有一个“逼近”的过程。所以作用价值几乎永远是一个有限值。</p>
<p>3.使用价值</p>
<p>而用什么来类比数据的“使用价值”呢？可以用获取了这些数据后所消除的不确定性的大小来衡量，也就是用数据的信息熵来衡量。使用价值永远是小于等于作用价值的。</p>
<p>4.场景价值</p>
<p>而到了这一步，还需要有一种概念出现，就是“场景价值”——在相同的信息熵的情况下，在不同场景里数据的“场景价值”是不同的；甚至完全相同的数据在不同的场景下也是有不同的“场景价值”的。这个场景价值应该是信息在场景中带来的收益，这个收益就是两种状态的成本差异，就拿刚才的那个骚扰电话的场景来看，买一个普通的画像清单，能够减少90%的人工投入，那么减去的90%的人工成本用具体金钱衡量出来的数字就是场景价值；买一个优化过的画像清单，能够减少99%的人工投入，那么减去的99%的人工成本用具体金钱衡量出来的数字就是场景价值。</p>
<p>可以认为：如果要让数据交易成为一个产业，必须使得：</p>
<p>场景价值＞作用价值＞使用价值＞交换价值</p>
<p>如果数据的场景价值大于交换价值，数据的买卖就会成交，这个价值之差越大成交的可能性越大，而这类数据就具有升值空间。</p>
<p>日常其他商品交易的场景里也有类似的场景，一个地方的房价现在是5000元一平方米，周边相对还比较荒凉，去购物，去看病，去上学，在这里生活附加的成本尤其是时间成本还相当高，所以这个5000元一平方米基本只有一个居住价值。但是根据规划，以后5年这里周边会陆续新建地铁、购物中心、中小学校、三甲医院。这些附加的成本会随着这些设施的建立逐渐降低，而转移到楼盘的价格上去，这个价格会一直上涨，上涨的上限应该是与其他地方楼盘提供的同质资源附加价值总和持平——包括居住价值在内的一切相关资源。这时候楼盘再上涨已经没有意义，即便喊价上涨，但是买家已经能够在其他比这个地方低价的地区花更少的钱买到同质的资源，那这个喊价十有八九不成交，就是所谓的有价无市。</p>
<p>数据的场景价值也是这样，由于数据里蕴含的信息对场景中消除不确定性有帮助，进而带来的收益就是数据的价值，那就相当于是一种投资。投资回报高就会吸引人，反之回报低就不吸引人。这个回报是数据中的信息带来的收益的价值，也就是回报和交换价值的差值，这个差值越大就说明投资收益率越高，越吸引人，反之就越小，越容易被资本抛弃。</p>
<p>5.示例分析</p>
<p>刚才有一个问题没说完，就是关于“使用价值”的问题。这应该是一个数据中蕴含信息所发挥价值的度量问题。</p>
<p>在平时生产生活中，人们的行为实际是已经计入了先验概率的。而使用数据中的信息这种行为，实际是在享受其信息量获得的增益。在平时的生产生活中，即便在没有购买任何数据的情况下，也会根据当前具有的认知、经验、推理、猜测，使用所有能用的手段来消灭不确定性。这个时候必然不是像无头苍蝇一样盲目去试，穷举地去试，在前面说的骚扰电话的例子里就是一个电话号码不落地逐个打过去的情况。而根据很多辅助性的信息或者其他的数据信息已经能够在购买或阅读一片数据之前拥有了一定的消除不确定性的能力，这个其实就是先验概率发挥的作用。而在购买或阅读了数据之后，进一步消除了不确定性，这其实是一个和概率有关的事情，很容易联想到信息量或熵的概念：</p>
<p><img src="Image00471.jpg" alt></p>
<p>在计算使用价值时可以考虑用以下公式：</p>
<p><img src="Image00472.jpg" alt></p>
<p>其中m是信源数量。</p>
<p>在m个信源是等概率的情况下，H（x）的最大值为1，也就是100%。可以认为这种情况下数据的使用发挥了巨大的价值；前面同样也计算过熵在先验概率分布极不平衡时是比较小的。信息熵在这种情况下取值在0～1之间。</p>
<p>不妨把数据使用价值定义为</p>
<p>使用价值=（作用价值-交换价值）×信息熵</p>
<p>从定性分析的角度来看，作用价值越大，交换价值越小，那么数据使用价值就越大；而在信息熵越大的情况下，这个数据的使用价值越不打折扣，反之数据的使用价值就比较小。如果作用价值和交换价值很贴近，那数据使用价值的空间就非常有限了，这种时候即便信息熵是1（最大值），这种数据发挥的作用也就越小，因为与穷举试错的成本快差不多了。</p>
<p>例如，张三准备在一个城市开一个小卖店，但是卖什么没拿准。他在多方了解下，想了以下4种方案。</p>
<p>方案1：在A街区B店面开一个煎饼店。</p>
<p>方案2：在C街区D店面开一个茶餐厅。</p>
<p>方案3：在C街区D店面开一个便利店。</p>
<p>方案4：开一个专营咖啡机和咖啡豆的网店。</p>
<p>这4种方案看起来几乎差不多，如果自己要去各个街区进行调查，包括网络调查人流量、客户性别比例、年龄比例、日均消费等数据，一个人花费的人工成本、交通、饮食都加在一起需要40000元。而有专业的公司拥有大数据咨询报告系统，可以提供完备的信息，而购买这种报告仅需5000元一份。那么张三完全可以花5000元来购买这份报告，这个5000元就是交换价值，40000元就是作用价值。这4个方案如果没有明显的彼此之间的差异，张三原本选择每个方案的概率都是1/4，那么按照前面的说法来计算，信息熵就是</p>
<p><img src="Image00473.jpg" alt></p>
<p>数据的使用价值就是：（40000-5000）×1=35000元。</p>
<p>再举一个例子。</p>
<p>假如有这么一个场景，某公司制定第二年的产品策略和销售计划，随后几个主管提出了3种不同的方案。而且3种方案听起来都比较有道理但是缺乏足够的数据依据，经过大家充分讨论，权衡利弊，在决策层的领导内进行了支持表决。同意选择方案1的有70%的人，同意选择方案2的有20%的人，同意选择方案3的有10%的人。可以粗略理解：其中有70%的人可能会选择方案1，20%的人可能会选择方案2，10%的人可能会选择方案3。这时候需要数据对决策做出支持。假设自己进行调查研究需要花费的总成本为100万，从拥有大数据咨询报告系统的公司购买了相应的数据，花费20万，做出了最终决策，选择了方案3。最终公司第二年销售收益为1000万。</p>
<p>在这个例子里面，20万为交换价值，作用价值应该是100万，场景价值是80万，数据的价值边界应该划在消除信息不确定的成本上，而不是间接收益。另外，可能比较有争议的地方是在选择了支持度最低的方案3上，表面上看上去，数据的购入支持了最不可能胜出的方案3，所以这次胜出很大程度应该归功于这一片数据的价值，应该用这个1000万做基数来进行衡量。我的看法是，这是对数据价值的夸大，也是不客观的一种表现。因为这1000万真的不是因为花了20万就直接得到的，而实际的过程是，仅仅花了20万，完成了一个100万的成本才能做的事情，在决策后进而通过一系列其他的努力和配合，最终达成了1000万的价值。边界的划分很重要。或许还有一个疑问，就是3种方案中，数据的帮助使得这个10%支持度的方案胜出，而非最大支持度的70%的方案胜出，所以有两种情况可能会被认为是不同的，那就是用户选择了70%支持度的方案1和10%支持度的方案3，这两种情况下数据的使用价值肯定是不一样的。但是，决策实际上是在了解了信息之后做出的，换句话说，如果真的花了100万的人工成本去做了这次同质的分析，应该会得到跟购买这次数据一样的结果，买数据花了20万。这里数据的价值只是两种获得途径的比较，也就是大数据市场边界的划分。</p>
<p>最后试着把事情做到极致——试算一下在刚才的骚扰电话的例子里，从10000条数据中锁定100条数据，带来的交换价值有多大呢？这个场景很特殊，因为打这10000通电话本身就是一个消除不确定性的过程，等于是消除不确定性和生产融为一体的，在这种情况下，数据的场景价值和作用价值几乎快等价了。</p>
<p>假设平均一次通话时间需要3分钟，包括接听、通话、挂机，包括成交和被骂的不同场景。那么一小时可以通话20通，一天8小时通话160通。10000通电话需要一个人62.5个工作日才能完成，大约3个人1个月的时间。假设话务员的人工成本为4000元一个月，那就需要12000元的成本，这个就是作用价值。用网上找到的一条报道来试着给数据定个售价，“信用卡客户信息遭泄：网购价2分一条，金卡持卡人信息可售5元。多位‘信息贩子’均表示，根据个人信息‘品质’的不同，价格也分为‘三六九等’，最新信用卡开户数据按照0.5元一条出售；二手数据可以便宜到0.35元每条；部分高端客户如金卡、白金卡持卡人信息每条售价则高达5元 <a href="#ch2_back">[2]</a><br>。”假设1条0.5元，在花费了50元购买了100条电话记录后，仅需要1个人0.6天的时间，也就是大约190元的成本，就能完成原来12000元成本才能做完的事情。由于这种场景下的作用价值巨大，使用价值和场景价值都巨大，所以在这个场景下数据交易才会大行其道，屡禁不止。</p>
<p>除了这些未来会很有市场的例子外，还有哪些是现阶段就可以开始发挥价值的数据呢？</p>
<p><a href="#ch1">[1]</a> 来源于《资本论》第一卷，第53 页。</p>
<p><a href="#ch2">[2]</a> 来源于《每日经济新闻》。</p>
<h3 id="18-5-2-场景1：征信数据"><a href="#18-5-2-场景1：征信数据" class="headerlink" title="18.5.2 场景1：征信数据"></a>18.5.2 场景1：征信数据</h3><p>征信数据这个东西其实还是很敏感的，很多人认为这个是隐私。</p>
<p>隐私应该说是一种文明进步的产物，但是也确实是一把及其锋利的双刃剑。先看这样一则报道。</p>
<p>“日前，稀里糊涂染上了艾滋病的河南小伙小新成了舆论关注的焦点。去年3月，他和女友小叶筹备婚事，两人在民政局办理婚姻登记当天，前往永城市妇幼保健院进行婚检。检查报告很快出来了，但医生单独叫住了女友小叶。小叶再次做完了检查后，小新得到医生的答复是，一切正常。婚检后小新也没再多想什么，就与妻子小叶同了房。一个月后，小新前往外地打工，然而6月初，小叶接到永城市疾控中心打来的电话，称她已经确诊为HIV阳性。丈夫小新这才知道，当时小叶被医生叫住是因为被查出疑似HIV阳性。不幸的是，小新随后也被查出感染了艾滋病毒。</p>
<p>令小新不解的是，3月份婚检时，小叶已经查出疑似感染艾滋病毒，为什么医院当时不把这个结果告知自己，导致悲剧无法挽回？在痛苦中挣扎了一段时间后，小新从法院拿到一份当初婚检时的检验报告，并向法院提起了诉讼，要求进行婚检的妇幼保健院承担责任。</p>
<p>事件成为舆论焦点后，永城市妇幼保健院承受了不小的压力，许多人都认为妇幼保健院向小新隐瞒了妻子感染艾滋病毒的情况才导致了悲剧的发生。而妇幼保健院对此的解释是，按照我国有关法律，医疗机构及其医务人员应当对患者的隐私保密。婚检机构认为，自己只有权利告诉检查者小叶本人，向小新坦白应该是小叶自己的责任 <a href="#ch1_back">[1]</a> 。”</p>
<p>这则报道的场景和征信几乎是完全一样。一方是一个人的隐私，另一方是对他人产生危害的可能性。这两者确实应该加以权衡，而不能一股脑地规定要彻底保护一切隐私，或者说所有的隐私必须完全公开。</p>
<p>征信问题是人类文明城市化到了一定程度必然引发的问题，这种问题在祖祖辈辈一直居住的农村几乎是没有悬念的。一个人家境如何，品行如何，在一个居住了几十年的只有一千多口人甚至更少的小山村里那几乎是尽人皆知，在这样一个小的社会环境里，人和人之间的距离非常近。早上发生的事情，可能中午就传得全村人都知道，彼此了解的程度如此之深，那所有小范围内的风险问题都能自行解决了——不确定程度很低。</p>
<p>但是城市截然不同，大家来自天南海北五湖四海，有很多人在高楼大厦里住了大半辈子可能一个单元的人都认不全。那么对于整个社会里的更为复杂的协作关系如何协调？这种新社会体系下的关系自然需要把征信数据进行合法化管理，让人们把它当成自己的终生名片来珍视和爱护。这种征信是城市人之间协作的保障，它的变现是人们为了付出一些金钱成本去消除人信誉的不确定性的一种变现方式。</p>
<p><a href="#ch1">[1]</a> 来源于《北京娱乐信报》。</p>
<h3 id="18-5-3-场景2：宏观数据"><a href="#18-5-3-场景2：宏观数据" class="headerlink" title="18.5.3 场景2：宏观数据"></a>18.5.3 场景2：宏观数据</h3><p>宏观数据比较简单，就是指统计性的数据。如一个地区有多少艾滋病人，是100还是50，这些数据对于很多做社会学和病理学研究的人是有意义和有价值的。况且这只是一个数字，没有说谁具体得了这种病，也就不涉及隐私，当然可以进行交易。</p>
<p>其他一切的宏观统计数据理论上说都是可以进行交易的，前提必须得到它的拥有人的授权。这也好理解，虽然卖汽车是合法的，但是总不能把别人家的汽车随便开来卖掉吧。</p>
<p>宏观数据的价值到此为止吗？远不止。</p>
<p>在互联网飓风席卷整个世界的时候，什么是生存的最终能力——创新。创新说白了就是大量的个体去试，大量的试错中有个别试对的活下来，这个就是成功的创新。以新产品市场研究为例。新产品市场研究靠什么？当然是数据。作为一个产品经理，或者一个对产品负责任的人，需要对未来出品的产品做一个评估。产品准备卖给谁？卖多少？在什么地区卖？借助什么渠道卖？毛利大概多少？产品的预期生存周期是多少？有办法消除这些疑问的不确定性吗？还是闭着眼去试？</p>
<p>举一个例子。如果想开一个店。会在哪里选址？会卖什么？怎么来做BP（Business Plan，商业计划）。需要了解的信息包括各街区的人流量信息；在某一街区人流信息中的年龄段比例；这些人在这些街区购买对应产品的平均预算；这些人是喜欢一个人来还是结伴而来；在哪些时间段人流量会比较大，大到什么程度。</p>
<p>这些信息会帮助把这个BP充实起来，当对这些信息有了比较充分的了解后可以更容易去做决定。</p>
<p>如果街区的人流量本身就不足，甚至大部分都是匆匆的过客，那可能开店本身都成为困难了。</p>
<p>这个地区的过客都是些什么人，是一些上下学回家的小朋友，年轻的上班族还是广场舞大妈们？这会帮助选择，是卖甜甜圈、奶茶还是商务人士喜欢的咖啡、下午茶，还是适合本地人口味的煎饼、早茶。</p>
<p>他们每天的预算决定了卖的单品的单价，是应该高档次高规格，还是应该更贴近大众更实惠。</p>
<p>一个人来还是结伴来是不一样的，尤其是结伴来的人是商务关系、情侣关系，还是男女闺蜜？这决定了出售的单品在差异化上应该迎合哪些人的习惯。是应该精致、浪漫，还是搞怪猎奇。</p>
<p>时间段对于人流的影响也间接影响到人力成本，是一个人能搞定这个事情，还是要两个人全职，还是在高峰段雇一个勤工俭学的学生帮忙打理。这也是影响预算的重要因素。</p>
<p>这些数据完全不用担心没有人卖，只要有需求，就会有人考虑卖；只要这种数据对消除不确定性有帮助，那就会有市场；只要数据的提供还有利可图，那就会吸引更多的人继续加入数据的生产与提供。</p>
<p>刚刚这些数据，如果用3000元能买到，会考虑购买吗？可以选择花3000元买下它，然后认真阅读和研究；也可以选择花20万元把店先开起来，开的过程中再摸索。究竟哪一种更划算，仁者见仁智者见智。</p>
<p>还有一些数据，它的宏观价值会更容易直接被人看到。例，某地区的某种疾病的意外事故、发病率、分布年龄段、治愈率等。这些宏观统计信息不涉及任何个人隐私内容，有价值吗？有！谁会对这些数据感兴趣？至少有两种行业会对这种数据感兴趣。一个是保险行业，一个是制药行业。保险公司购买这样的数据用来设计和调整险种，制药厂购买这些数据用来研究一些细分类的药品。这些信息能够极大地消除不确定性带来的成本并提高产品设计的针对性。</p>
<h3 id="18-5-4-场景3：画像数据"><a href="#18-5-4-场景3：画像数据" class="headerlink" title="18.5.4 场景3：画像数据"></a>18.5.4 场景3：画像数据</h3><p>画像数据在前面的章节中多次提到。画像数据指的就是一个人、一个终端或者任何一个物体的表述标签信息。这些标签信息，尤其是针对人的画像，虽然包含了大量的人的隐私或者关键性的描述信息，但是只要把握一点，这些数据里面不要轻易定位到人。只要不定位到人，那么这些画像信息再如何丰富也不能算作是侵犯隐私。这一点很重要，这已经给了人们变现的空间。</p>
<p>如通过一个MAC地址，可以请求到一些兴趣相关的标签。这个显然与个人隐私没什么关系。通过一个人的手机号，能够请求到一些关于他通话时长或者网络流量大小的数据。这个算隐私吗？只要这不是那种他不想让别人知道的数据，就不能算隐私。所以数据的开放和拥有者的授权体系应该是大数据交易和变现的制度性和技术性保障根源。</p>
<h2 id="18-6-小结"><a href="#18-6-小结" class="headerlink" title="18.6 小结"></a>18.6 小结</h2><p>数据的价值是一种不会枯竭的资源，它不断产生，不断被人挖掘，不会轻易到达生产量的上限，它能够解放生产力，从这个角度来说，它就像第一次工业革命时候的蒸汽机，第二次工业革命时候的电力一样有能量。</p>
<h1 id="附录A-VMware-Workstation的安装"><a href="#附录A-VMware-Workstation的安装" class="headerlink" title="附录A VMware Workstation的安装"></a>附录A VMware Workstation的安装</h1><h4 id="A-1-VMware简介"><a href="#A-1-VMware简介" class="headerlink" title="A.1 VMware简介"></a>A.1 VMware简介</h4><p>VMware是由VMware公司（VMware，Inc.）开发，业内非常著名的虚拟化软件。它使用简单，基于图形化管理，而且效率非常高。</p>
<p>它为用户提供了多个版本，包括：VMware工作站（VMware Workstation）、VMware服务器（VMware Server）、VMware ESX服务器（VMware ESX Server），以及云端版本等。</p>
<p>这里主要为读者提供VMware的桌面级产品VMware工作站版本，读者可以亲自动手实践，在不破坏Windows工作环境下，体验Linux操作系统的使用和在Linux操作系统环境下使用大数据热门的软件。</p>
<h4 id="A-2-安装前的准备工作"><a href="#A-2-安装前的准备工作" class="headerlink" title="A.2 安装前的准备工作"></a>A.2 安装前的准备工作</h4><p>1.硬件配置需求</p>
<p>在安装软件之前用户需要准备足够的磁盘空间和运行时所占用的内存，为了保证大数据软件流畅运行，至少保证如下硬件配置。</p>
<p>·CPU：Intel Core i3。</p>
<p>·内存：4.00GB。</p>
<p>·硬盘空间：30GB。</p>
<p>2.VMware下载方法</p>
<p>可以从VMware软件的官方网站下载最新版本的VMware。最新版本为VMware Workstation 12，下载地址如下：<a href="http://www.vmware.com/products/workstation/workstation-evaluation" target="_blank" rel="noopener">http://www.vmware.com/products/workstation/workstation-evaluation</a>。</p>
<p>如果读者使用的是Windows操作系统，可以单击Download Now按钮进行下载，网页内容如图A-1所示。</p>
<p><img src="Image00474.jpg" alt></p>
<p>图A-1 下载页面</p>
<p>VMware Workstation为商业软件，用户可以免费试用30天。</p>
<p>3.安装过程</p>
<p>下载完成后，正式进入安装环节，首先运行软件，进入友好的安装向导界面，如图A-2所示。</p>
<p>下一步，来到“最终用户许可协议”界面，同意后进入下一步，如图A-3所示。</p>
<p><img src="Image00475.jpg" alt></p>
<p>图A-2 安装向导界面</p>
<p><img src="Image00476.jpg" alt></p>
<p>图A-3 “最终用户许可协议”界面</p>
<p>继续选择安装位置，这里选择了默认安装位置，读者可以根据实际情况单击“更改”按钮选择磁盘空间较大的分区，继续单击“下一步”按钮，如图A-4所示。</p>
<p>进入准备安装界面，单击“安装”按钮后开始进行VMware的安装，如图A-5所示。</p>
<p>当安装进度条完成时，VMware软件即可安装成功，如图A-6所示。</p>
<p>安装成功后会提示“安装完成”，单击“完成”按钮即可完成VMware软件的安装，如图A-7所示。</p>
<p><img src="Image00477.jpg" alt></p>
<p>图A-4 “自定义安装”界面</p>
<p><img src="Image00478.jpg" alt></p>
<p>图A-5 准备安装界面</p>
<p><img src="Image00479.jpg" alt></p>
<p>图A-6 安装进度条</p>
<p><img src="Image00480.jpg" alt></p>
<p>图A-7 完成安装</p>
<h1 id="附录B-CentOS虚拟机的安装方法"><a href="#附录B-CentOS虚拟机的安装方法" class="headerlink" title="附录B CentOS虚拟机的安装方法"></a>附录B CentOS虚拟机的安装方法</h1><h4 id="B-1-下载光盘镜像"><a href="#B-1-下载光盘镜像" class="headerlink" title="B.1 下载光盘镜像"></a>B.1 下载光盘镜像</h4><p>访问CentOS官方网站：<a href="http://www.centos.org" target="_blank" rel="noopener">http://www.centos.org</a><br>，下载最新版本的CentOS系统安装光盘镜像。本示例中下载地址如下：<a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1511.iso" target="_blank" rel="noopener">http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1511.iso</a>。</p>
<h4 id="B-2-创建VMware虚拟机"><a href="#B-2-创建VMware虚拟机" class="headerlink" title="B.2 创建VMware虚拟机"></a>B.2 创建VMware虚拟机</h4><p>运行安装好的VMware虚拟机，在主界面单击“创建新虚拟机”按钮，如图B-1所示。</p>
<p>进入“新建虚拟机向导”界面。单击“浏览”按钮选择下载的CentOS 7安装镜像，如图B-2所示。</p>
<p><img src="Image00481.jpg" alt></p>
<p>图B-1 单击“创建新虚拟机”按钮</p>
<p><img src="Image00482.jpg" alt></p>
<p>图B-2 选择CentOS 7安装镜像</p>
<p>VMware会自动检测到ISO文件对应的操作系统类型，单击“下一步”按钮，进入虚拟机设置部分，为虚拟机起一个好识别的名称并设置保存位置，建议将虚拟机保存到空间充裕的分区，随着使用次数的增多，虚拟机占用磁盘空间会逐渐增大。这里将虚拟机安装到E盘下的目录，如图B-3所示。</p>
<p>单击“下一步”按钮，设置虚拟磁盘大小，需要注意的是，分配的磁盘空间不会立即被占用，随着存储的数据增加占用空间会逐渐增大。这里保持默认设置即可，如果需要增加磁盘空间，可以在安装系统后进行添加。单击“下一步”按钮，完成基本安装，如图B-4所示。</p>
<p><img src="Image00483.jpg" alt></p>
<p>图B-3 设置名称和位置</p>
<p><img src="Image00484.jpg" alt></p>
<p>图B-4 指定磁盘容量</p>
<p>进入安装的最后阶段：“自定义硬件界面”，通过这个界面可以定义虚拟机支持的硬件，这里保持默认设置即可，单击“完成”按钮后，开始进行CentOS操作系统的安装。</p>
<h4 id="B-3-安装CentOS-7操作系统"><a href="#B-3-安装CentOS-7操作系统" class="headerlink" title="B.3 安装CentOS 7操作系统"></a>B.3 安装CentOS 7操作系统</h4><p>CentOS 7安装光盘镜像引导后，会出现安装引导界面，在这个界面下可以进行ISO的光盘完整性检测等操作。安装新的CentOS系统，在这里选择Install CentOS 7选项即可，进入开始安装界面，如图B-5所示。</p>
<p>首先进入语言选择界面，这里在下拉列表框中选择“简体中文（中国）”选项，并单击“继续”按钮，如图B-6所示。</p>
<p>接下来进入安装设置部分，在“软件选择”选项根据虚拟机类型来定制软件包，如图B-7所示。</p>
<p>这里建议初次使用Linux的读者选择带有图形界面的软件包，单击“软件选择”按钮，在列表中选择“GNOME桌面”单选按钮后，单击“完成”按钮，如图B-8所示。</p>
<p><img src="Image00485.jpg" alt></p>
<p>图B-5 开始安装界面</p>
<p><img src="Image00486.jpg" alt></p>
<p>图B-6 语言选择界面</p>
<p><img src="Image00487.jpg" alt></p>
<p>图B-7 安装设置</p>
<p><img src="Image00488.jpg" alt></p>
<p>图B-8 “软件选择”界面</p>
<p>继续设置CentOS系统的安装位置，单击“安装位置”按钮，选择“自动配置分区”单选按钮，并单击“完成”按钮即可，由于Linux操作系统的分区方式较灵活，不推荐初次使用的用户自动分区，避免删除重要的系统文件。操作方法如图B-9所示。</p>
<p>完成设置后，“开始安装”按钮已经可以使用了，单击“开始安装”按钮进行新系统的安装，如图B-10所示。</p>
<p>安装进度条会提示安装的进展，在安装过程中，用户可以设置Linux管理员的密码，Linux操作系统的超级管理员叫做root，相当于Windows中的Administrator，单击“ROOT密码”按钮，设置一个不容易被遗忘的密码，在对CentOS操作系统进行管理时，会用到这个账号和密码。除了root用户，还需要为Linux添加一个非管理用户，用于日常使用，这里可以根据自己的习惯创建一个自己喜欢的用户名和密码，如图B-11所示。</p>
<p>设置完成后会提示“已经设置ROOT密码”，等待进度条完成系统即可成功安装。安装完成后单击“重启”按钮，就可以进行CentOS系统的使用了，如图B-12所示。</p>
<p><img src="Image00489.jpg" alt></p>
<p>图B-9 设置安装位置</p>
<p><img src="Image00490.jpg" alt></p>
<p>图B-10 单击“开始安装”按钮</p>
<p><img src="Image00491.jpg" alt></p>
<p>图B-11 配置账户</p>
<p><img src="Image00492.jpg" alt></p>
<p>图B-12 完成安装</p>
<h1 id="附录C-Python语言简介"><a href="#附录C-Python语言简介" class="headerlink" title="附录C Python语言简介"></a>附录C Python语言简介</h1><p>Python是一种面向对象的解释型计算机程序设计语言，由Guido van Rossum于1989年发明，第一个公开发行版发行于1991年。</p>
<p>Python是纯粹的自由软件，它的源代码和解释器CPython遵循GPL（GNU General Public License）协议。</p>
<p>Python和C语言不一样，它是一种脚本语言。C语言在写完源代码后是需要编译成二进制代码才能够执行的；Python则不需要，它在生产环境中出现仍旧是源代码的.py文件形式，在执行的瞬间才由Python解释器将源代码转换为字节码，然后再由Python解释器来执行这些字节码。</p>
<p>这种形式的好处是不需要考虑平台系统的问题，可以和Java语言一样“一次编写到处执行”。缺点也是显而易见的，就是每次进行字节码转换和字节码执行时没有直接执行二进制的效率高。好在对于执行效率苛刻的场合毕竟较少，另外，随着计算机硬件能力的提升，执行效率的矛盾也变得不明显了。</p>
<p>和其他计算机语言一样，Python语言也有自己的一套语法基础。有顺序、分支、循环、调用的程序组织结构，以及数字、字符串、列表、元组、集合等多种数据类型。</p>
<p>1.安装Python</p>
<p>安装Python的方法不止一种，这里只介绍使用yum安装Python的方式。</p>
<p>按照默认方式安装好CentOS 7操作系统后，Python已经被正确安装，可以执行如下命令查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost Desktop]# python -V</span><br><span class="line">Python 2.7.5</span><br></pre></td></tr></table></figure>
<p>2.Hello World</p>
<p>Python的Hello World与其他计算机语言没什么区别，而且更加简洁，可以直接在交互式编程环境中编写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print（&quot;Hello， Python！&quot;）；</span><br></pre></td></tr></table></figure>
<p>3.行与缩进</p>
<p>Python脚本文件和普通的文本文件没有太大区别，一般以.py作为后缀。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line"># 文件名：test.py</span><br><span class="line">if True：</span><br><span class="line">    print &quot;True&quot;</span><br><span class="line">else：</span><br><span class="line">    print &quot;False&quot;</span><br></pre></td></tr></table></figure>

</details>

<p>其中#为注释标记，如果在一行中使用#，那么#后的内容是不会被解释执行的。</p>
<p>下面的if和else是分支型语句，当if后的内容为True（真实）时，则执行if所辖的部分，否则执行else所辖的部分。</p>
<p>注意Python语言中是不用begin/end或{}来表示执行段落的起止的，这里的if和else需要左侧对齐，用缩进来表示段落所辖范围界限。</p>
<p>4.变量类型</p>
<p>Python语言中标准的数据类型有几种：Numbers（数字）、String（字符串）、List（列表）、Tuple（元组）、Dictionary（字典）。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line">counter = 100 # 赋值整型变量</span><br><span class="line">miles = 1000.0 # 浮点型</span><br><span class="line">name = &quot;John&quot; # 字符串</span><br><span class="line">print counter</span><br><span class="line">print miles</span><br><span class="line">print name</span><br></pre></td></tr></table></figure>

</details>

<p>这段代码演示了整数型数字、浮点型数字以及字符串类型的赋值和打印操作。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line">list = [ &apos;abcd&apos;， 786 ， 2.23， &apos;john&apos;， 70.2 ]</span><br><span class="line">tinylist = [123， &apos;john&apos;]</span><br><span class="line">print list # 输出完整列表</span><br><span class="line">print list[0] # 输出列表的第一个元素</span><br><span class="line">print list[1：3] # 输出第二个至第三个元素</span><br><span class="line">print list[2：] # 输出从第三个开始至列表末尾的所有元素</span><br><span class="line">print tinylist * 2 # 输出列表两次</span><br><span class="line">print list + tinylist # 打印组合的列表</span><br></pre></td></tr></table></figure>

</details>

<p>这段代码演示的是列表类型的操作，列表很像Java语言中的数组，只是列表允许不同类型的数据放在同一个列表中，而数组不可以——它只能要求所有的元素类型一致。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line">tuple =（ &apos;abcd&apos;， 786 ， 2.23， &apos;john&apos;， 70.2 ）</span><br><span class="line">tinytuple =（123， &apos;john&apos;）</span><br><span class="line">print tuple # 输出完整元组</span><br><span class="line">print tuple[0] # 输出元组的第一个元素</span><br><span class="line">print tuple[1：3] # 输出第二个至第三个元素</span><br><span class="line">print tuple[2：] # 输出从第三个开始至列表末尾的所有元素</span><br><span class="line">print tinytuple * 2 # 输出元组两次</span><br><span class="line">print tuple + tinytuple # 打印组合的元组</span><br></pre></td></tr></table></figure>

</details>

<p>这段代码演示的是元组类型的操作。操作方法和列表很像，但是Python语法不允许对元组中的元素进行二次赋值。它相当于只读类型的列表。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line">dict = &#123;&#125;</span><br><span class="line">dict[&apos;one&apos;] = &quot;This is one&quot;</span><br><span class="line">dict[2] = &quot;This is two&quot;</span><br><span class="line">tinydict = &#123;&apos;name&apos;： &apos;john&apos;，&apos;code&apos;：6734， &apos;dept&apos;： &apos;sales&apos;&#125;</span><br><span class="line">print dict[&apos;one&apos;] # 输出键为&apos;one&apos; 的值</span><br><span class="line">print dict[2] # 输出键为 2 的值</span><br><span class="line">print tinydict # 输出完整的字典</span><br><span class="line">print tinydict.keys（）# 输出所有键</span><br><span class="line">print tinydict.values（）# 输出所有值</span><br></pre></td></tr></table></figure>

</details>

<p>这段代码演示的是字典类型的操作。字典类型有些像Java中的HashMap，是通过主键Key来访问对应的Value值，而不是靠下标来访问。</p>
<p>5.循环语句</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line">count = 0</span><br><span class="line">while（count &lt; 9）：</span><br><span class="line">   print &apos;The count is：&apos;， count</span><br><span class="line">   count = count + 1</span><br><span class="line">print &quot;Good bye！&quot;</span><br></pre></td></tr></table></figure>

</details>

<p>这段代码演示的是while循环，while循环后面的条件表示在满足条件的时候执行while所辖的程序段。在这段程序中表示count&lt;9的情况下，执行下面的两行语句，不包括</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print &quot;Good bye！&quot;</span><br></pre></td></tr></table></figure>
<p>这一行。</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line">for num in range（10，20）：  # 迭代 10 到 20 之间的数字</span><br><span class="line">   for i in range（2，num）： # 根据因子迭代</span><br><span class="line">      if num%i == 0：      # 确定第一个因子</span><br><span class="line">         j=num/i          # 计算第二个因子</span><br><span class="line">         print &apos;%d 等于 %d * %d&apos; %（num，i，j）</span><br><span class="line">         break            # 跳出当前循环</span><br><span class="line">   else：                  # 循环的 else 部分</span><br><span class="line">      print num， &apos;是一个质数&apos;</span><br></pre></td></tr></table></figure>

</details>

<p>上面这段程序略显繁琐，但是内容仍然很简单。</p>
<p>这是循环的另一种写法——for循环，for循环也是一种循环，后面写出的是一个循环范围。这里是一个二重循环，也就是两个循环发生了嵌套——在一个循环的执行中有另一个循环。外层循环是让num在10和20之间做循环，内层循环是i在2和num之间做循环。</p>
<p>6.函数</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line"># 定义函数</span><br><span class="line">def printme（ str ）：</span><br><span class="line">   &quot;打印任何传入的字符串&quot;</span><br><span class="line">   print str；</span><br><span class="line">   return；</span><br><span class="line"># 调用函数</span><br><span class="line">printme（&quot;我要调用用户自定义函数！&quot;）；</span><br><span class="line">printme（&quot;再次调用同一函数&quot;）；</span><br></pre></td></tr></table></figure>

</details>

<p>函数是一种最小单位的代码段封装。关键字是def，def后面的printme是函数名，str是参数名称。这个函数的内容就是直接打印传入的变量值。</p>
<p>最后两句是对函数的调用。</p>
<p>7.模块</p>
<details>
  <summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#！/usr/bin/python</span><br><span class="line"># -*- coding： UTF-8 -*-</span><br><span class="line"># 导入模块</span><br><span class="line">import support</span><br><span class="line"># 现在可以调用模块里包含的函数了</span><br><span class="line">support.print_func（&quot;Zara&quot;）</span><br></pre></td></tr></table></figure>

</details>

<p>模块是一种大单位的代码段集合，例如，一个support.py的文件中有多个函数定义，其中一个叫做print_func函数。在不对support.py这个模块进行引用的时候是不能调用print_func函数的。上面这段代码中，import support是导入support.py模块，下面的support.print_func（”Zara”）是调用support中的print函数，并传入变量”Zara”作为参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from fib import Fibonacci</span><br></pre></td></tr></table></figure>
<p>这是导入模块的另一种写法，区别是它能够导入一个模块的一部分而非全部模块代码。示例中是指从fib这个模块中只导入Fibonacci这个函数。</p>
<p>8.小结</p>
<p>以上就是Python语言中所涉及的最基本的语法。而强大的Python所支持的其他内容读者如果有兴趣可以再找一些专门介绍Python的资料来学习，本书对Python基本语法的介绍到此为止。</p>
<p>在本书中所列举的示例代码中，所涉及的库有以下几个。</p>
<p>（1）NumPy。NumPy系统是Python的一种开源的数值计算扩展库。它提供了许多高级的数值编程工具，如矩阵数据类型、矢量处理，以及精密的运算库。专门用于严格的数字处理。多为大型金融公司使用。核心的科学计算组织，如Lawrence Livermore、NASA用其处理一些本来使用C++、FORTRAN或MATLAB等所做的任务。</p>
<p>（2）matplotlib。一个专业的绘图工具库，官方网址为<a href="http://matplotlib.org/" target="_blank" rel="noopener">http://matplotlib.org/</a> 。</p>
<p>它调用简单，使用非常方便，在配合Python进行数据挖掘和报表制作的过程中是一种利器。</p>
<p>（3）SciPy。SciPy是一款方便、易于使用、专为科学和工程设计的Python工具包。它提供的内容很丰富，文件输入输出、特殊函数、线性代数运算、快速傅里叶变换、统计与随机、微分和积分、图像处理等诸多封装内容。官方网址为<a href="http://www.scipy.org/" target="_blank" rel="noopener">http://www.scipy.org/</a><br>，读者如果有兴趣可以去了解更多的内容。</p>
<p>（4）Scikit-learn。Scikit-learn是最著名的Python机器学习库之一，在附录D中会做比较详细的介绍。</p>
<h1 id="附录D-Scikit-learn库简介"><a href="#附录D-Scikit-learn库简介" class="headerlink" title="附录D Scikit-learn库简介"></a>附录D Scikit-learn库简介</h1><p>Python的开源社区非常活跃，也有很多和Java等开源语言一样的框架或库体系，其中Scikit-<br>learn（简写成sklearn）是最著名的Python机器学习库之一。官方网址为<a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">http://scikit-learn.org/stable/</a> 。</p>
<p>sklearn基于BSD开源许可证，最早由David Cournapeau在2007年发起，目前也是由社区自愿者进行维护，经年累月，整个项目的内容已经相当丰富了，目前最新的稳定版是0.17版本。</p>
<p>用户手册内容也很友好，覆盖面很全，包括有监督的学习（分类）、无监督的学习（聚类）、模型选择与评价、数据集转换、数据集提取应用（数据示例下载）、大规模计算策略、计算效率七大部分。</p>
<p>本书中的机器学习算法大多使用sklearn库完成，主要涉及Supervised learning和Unsupervised learning两个部分，相信它也能帮助读者在生产生活中很大程度地提高生产效率。</p>
<p>在安装sklearn之前请确认Python已经安装。安装Python的方法见附录C。</p>
<p>如果发现Python软件未被正确安装，可以使用CentOS系统自带的包管理工具“yum”进行安装。安装方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost Desktop]# yum install –y python</span><br></pre></td></tr></table></figure>
<p>然后安装sklearn。sklearn的安装很简单，只要能够连接上互联网，直接使用pip安装即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install –U scikit-learn</span><br></pre></td></tr></table></figure>
<h1 id="附录E-FANN-for-Python安装"><a href="#附录E-FANN-for-Python安装" class="headerlink" title="附录E FANN for Python安装"></a>附录E FANN for Python安装</h1><p>前往FANN官方网站：<a href="http://leenissen.dk/fann/wp/download/" target="_blank" rel="noopener">http://leenissen.dk/fann/wp/download/</a>。单击Download按钮，进入下载页面，如图E-1所示。</p>
<p><img src="Image00493.jpg" alt></p>
<p>图E-1 下载页面</p>
<p>解压缩：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ unzip FANN-2.2.0-Source.zip</span><br></pre></td></tr></table></figure>
<p>编译：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cmake .$ make install</span><br></pre></td></tr></table></figure>
<h1 id="附录F-群众眼中的大数据"><a href="#附录F-群众眼中的大数据" class="headerlink" title="附录F 群众眼中的大数据"></a>附录F 群众眼中的大数据</h1><p>在本书成书的过程中，笔者试着采访过一些“群众”，让他们说说他们眼中的大数据一词是什么概念，得到了很多有趣的说法。</p>
<p>da shu ju hao chi ma？</p>
<p>——尹闪闪，幼儿班孩子</p>
<p>大数据并不一定是数据大，而是通过大数据分析挖掘、分析方法使得数据的价值大大放大。</p>
<p>可能很多人还会说大数据是炒作，我认为“存在即合理”，现在市场上做大数据的公司良莠不齐，也说明目前正处于大数据快速发展阶段，我们也逐步感受到大数据带来的便利，比如智慧城市项目、生物基因检测等。如果你处于公司的决策层，有数据做支撑你会更有底气。</p>
<p>——彭瑶，数据分析师</p>
<p>小县城林业工作数十年，退休在即，回顾一番，变化最大的应该就是案头厚厚的文档越来越少，服务群众的效率越来越高。林、人、企在数据的结合相比以前简单的分类，等着群众来办事的方式变成了能主动跑下去。</p>
<p>——谭波道，公务员</p>
<p>大数据嘛，数据量大、数据种类多样、有价值。比如之前我做过的保险的一些项目，他们得到购买保险的客户，根据数据分析，更好地推广他们的保险，把不同需求的保险推给不同的人。</p>
<p>——陆飞，男程序员</p>
<p>大数据：抽象出的真实世界。</p>
<p>——左妍，女程序员</p>
<p>学好大数据思维，掌握未来发展之道！</p>
<p>——向上，IT技术论坛创始人</p>
<p>大数据……开始就是认为和数字有关系，基本认为就是和数字打交道，听到最多的就是Python（也不知道是啥），虽然直至现在也不能深入了解大数据，但是自己觉得大数据要具有很强大的洞察力和逻辑分析能力，对此自己也充满好奇，计划在闲暇时间通过阅读书籍或和大师“白话”了解大数据（小白选手不要理）。</p>
<p>——王月，德语专业大学生</p>
<p>大数据就是在一堆繁杂的事物里面找规律找共性，它应该能够影响人类的生活……能让我们变成想象中的外星人吗？</p>
<p>——李娜，高级人力资源经理</p>
<p>大数据是真正的新能源。它将成为每一个现代企业的重要资产，并在人类商业社会演进的道路中扮演关键作用。</p>
<p>——李力，CEO</p>
<p>大家的回答是如此丰富，也从一个侧面说明，大数据就是一种见仁见智的艺术性的产业，它会在各个领域对各个行业的人们持续不断地产生深远的影响。</p>
<h1 id="写作花絮"><a href="#写作花絮" class="headerlink" title="写作花絮"></a>写作花絮</h1><p><img src="Image00494.jpg" alt></p>
<p><img src="Image00495.jpg" alt></p>
<p><img src="Image00496.jpg" alt></p>
<p><img src="Image00497.jpg" alt></p>
<p><img src="Image00498.jpg" alt></p>
<p><img src="Image00499.jpg" alt></p>
<p><img src="Image00500.jpg" alt></p>
<p><img src="Image00501.jpg" alt></p>
<p><img src="Image00502.jpg" alt></p>
<p><img src="Image00503.jpg" alt></p>
<p><img src="Image00504.jpg" alt></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Allen B.Downey.贝叶斯思维：统计建模的Python学习法[M].许杨毅，译.北京：人民邮电出版社，2015.</p>
<p>[2] 郑捷.机器学习：算法原理与编程实践[M].北京：电子工业出版社，2015.</p>
<p>[3] Jiawen Han，Micheline Kamber，Jian Pei.数据挖掘：概念与技术[M].北京：机械工业出版社，2007.</p>
<p>[4] 王家林.大数据Spark企业级实战[M].北京：电子工业出版社，2015.</p>
<p>[5] 涂子沛.数据之巅[M].北京：中信出版社，2014.</p>
<p>[6] Simon Haykin.神经网络与机器学习[M].申富饶，徐烨，郑俊，等译.北京：机械工业出版社，2009.</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      <div>    
       
       
      <ul class="post-copyright">
        <li class="post-copyright-author">
            <strong>本文作者：</strong>hac_lang
        </li>
        <li class="post-copyright-link">
          <strong>本文链接：</strong>
          <a href="/2016/12/20/book-《白话大数据与机器学习》/" title="book_《白话大数据与机器学习》">2016/12/20/book-《白话大数据与机器学习》/</a>
        </li>
        <li class="post-copyright-license">
          <strong>版权声明： </strong>
          许可协议，请勿用于商业，转载注明出处！
        </li>
      </ul>
      
      </div>
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/计算机/" rel="tag"># 计算机</a>
          
            <a href="/tags/人工智能/" rel="tag"># 人工智能</a>
          
            <a href="/tags/books/" rel="tag"># books</a>
          
            <a href="/tags/互联网/" rel="tag"># 互联网</a>
          
            <a href="/tags/更毕/" rel="tag"># 更毕</a>
          
            <a href="/tags/豆瓣7/" rel="tag"># 豆瓣7</a>
          
            <a href="/tags/AI/" rel="tag"># AI</a>
          
            <a href="/tags/科普/" rel="tag"># 科普</a>
          
            <a href="/tags/数据分析/" rel="tag"># 数据分析</a>
          
            <a href="/tags/大数据/" rel="tag"># 大数据</a>
          
            <a href="/tags/自评5/" rel="tag"># 自评5</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/11/11/book-《机器学习实战》-Peter-Harrington-李锐等译/" rel="next" title="book_《机器学习实战》_Peter Harrington_李锐等译">
                <i class="fa fa-chevron-left"></i> book_《机器学习实战》_Peter Harrington_李锐等译
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/12/25/book-《机器学习-周志华-人工智能入门》-西瓜书-2016-01-01/" rel="prev" title="book-《机器学习-周志华-人工智能入门》-西瓜书-2016-01-01">
                book-《机器学习-周志华-人工智能入门》-西瓜书-2016-01-01 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  
    <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="hac_lang">
  
  <p class="site-author-name" itemprop="name">hac_lang</p>
  <div class="site-description motion-element" itemprop="description">小白hac_lang的笔记，涉及内容包含但不限于<br>人工智能 &ensp; 信息安全 &ensp; 网络技术<br>软件工程 &ensp; 基因工程 &ensp; 嵌入式 &ensp; 天文物理</div>
</div>


  <nav class="site-state motion-element">
    
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    

    

    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>







  <div class="links-of-author motion-element">
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://github.com/HACLANG" title="GitHub &rarr; https://github.com/HACLANG" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://stackoverflow.com/yourname" title="StackOverflow &rarr; https://stackoverflow.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://gitter.im" title="Gitter &rarr; https://gitter.im" rel="noopener" target="_blank"><i class="fa fa-fw fa-github-alt"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.jianshu.com/u/442ddccf3f32" title="简书 &rarr; https://www.jianshu.com/u/442ddccf3f32" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="Quora &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://plus.google.com/yourname" title="Google &rarr; https://plus.google.com/yourname" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="mailto:haclang.org@gmail.com" title="E-Mail &rarr; mailto:haclang.org@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="skype:haclang?call|chat" title="Skype &rarr; skype:haclang?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://twitter.com/haclang2" title="Twitter &rarr; https://twitter.com/haclang2" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/hac.lang.1675" title="FaceBook &rarr; https://www.facebook.com/hac.lang.1675" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
    
  </div>








          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-text">前言</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要写这本书"><span class="nav-text">为什么要写这本书</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#本书特色"><span class="nav-text">本书特色</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#读者对象"><span class="nav-text">读者对象</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何阅读本书"><span class="nav-text">如何阅读本书</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-大数据产业"><span class="nav-text">第1章 大数据产业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-大数据产业现状"><span class="nav-text">1.1 大数据产业现状</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-对大数据产业的理解"><span class="nav-text">1.2 对大数据产业的理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-大数据人才"><span class="nav-text">1.3 大数据人才</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-供需失衡"><span class="nav-text">1.3.1 供需失衡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-人才方向"><span class="nav-text">1.3.2 人才方向</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-环节和工具"><span class="nav-text">1.3.3 环节和工具</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-门槛障碍"><span class="nav-text">1.3.4 门槛障碍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-小结"><span class="nav-text">1.4 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第2章-步入数据之门"><span class="nav-text">第2章 步入数据之门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-什么是数据"><span class="nav-text">2.1 什么是数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-什么是信息"><span class="nav-text">2.2 什么是信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-什么是算法"><span class="nav-text">2.3 什么是算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-统计、概率和数据挖掘"><span class="nav-text">2.4 统计、概率和数据挖掘</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-什么是商业智能"><span class="nav-text">2.5 什么是商业智能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-小结"><span class="nav-text">2.6 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第3章-排列组合与古典概型"><span class="nav-text">第3章 排列组合与古典概型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-排列组合的概念"><span class="nav-text">3.1 排列组合的概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-公平的决断——扔硬币"><span class="nav-text">3.1.1 公平的决断——扔硬币</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-非古典概型"><span class="nav-text">3.1.2 非古典概型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-排列组合的应用示例"><span class="nav-text">3.2 排列组合的应用示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-双色球彩票"><span class="nav-text">3.2.1 双色球彩票</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-购车摇号"><span class="nav-text">3.2.2 购车摇号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-德州扑克"><span class="nav-text">3.2.3 德州扑克</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-小结"><span class="nav-text">3.3 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第4章-统计与分布"><span class="nav-text">第4章 统计与分布</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-加和值、平均值和标准差"><span class="nav-text">4.1 加和值、平均值和标准差</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-加和值"><span class="nav-text">4.1.1 加和值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-平均值"><span class="nav-text">4.1.2 平均值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-标准差"><span class="nav-text">4.1.3 标准差</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-加权均值"><span class="nav-text">4.2 加权均值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-混合物定价"><span class="nav-text">4.2.1 混合物定价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-决策权衡"><span class="nav-text">4.2.2 决策权衡</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-众数、中位数"><span class="nav-text">4.3 众数、中位数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-1-众数"><span class="nav-text">4.3.1 众数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-2-中位数"><span class="nav-text">4.3.2 中位数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-欧氏距离"><span class="nav-text">4.4 欧氏距离</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-曼哈顿距离"><span class="nav-text">4.5 曼哈顿距离</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-同比和环比"><span class="nav-text">4.6 同比和环比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-抽样"><span class="nav-text">4.7 抽样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-8-高斯分布"><span class="nav-text">4.8 高斯分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-9-泊松分布"><span class="nav-text">4.9 泊松分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-10-伯努利分布"><span class="nav-text">4.10 伯努利分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-11-小结"><span class="nav-text">4.11 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第5章-指标"><span class="nav-text">第5章 指标</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-什么是指标"><span class="nav-text">5.1 什么是指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-指标化运营"><span class="nav-text">5.2 指标化运营</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-1-指标的选择"><span class="nav-text">5.2.1 指标的选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-指标体系的构建"><span class="nav-text">5.2.2 指标体系的构建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-小结"><span class="nav-text">5.3 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第6章-信息论"><span class="nav-text">第6章 信息论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-信息的定义"><span class="nav-text">6.1 信息的定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-信息量"><span class="nav-text">6.2 信息量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-1-信息量的计算"><span class="nav-text">6.2.1 信息量的计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-2-信息量的理解"><span class="nav-text">6.2.2 信息量的理解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-香农公式"><span class="nav-text">6.3 香农公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-熵"><span class="nav-text">6.4 熵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-1-热力熵"><span class="nav-text">6.4.1 热力熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-2-信息熵"><span class="nav-text">6.4.2 信息熵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-5-小结"><span class="nav-text">6.5 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第7章-多维向量空间"><span class="nav-text">第7章 多维向量空间</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-向量和维度"><span class="nav-text">7.1 向量和维度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-1-信息冗余"><span class="nav-text">7.1.1 信息冗余</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-2-维度"><span class="nav-text">7.1.2 维度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-矩阵和矩阵计算"><span class="nav-text">7.2 矩阵和矩阵计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-数据立方体"><span class="nav-text">7.3 数据立方体</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-4-上卷和下钻"><span class="nav-text">7.4 上卷和下钻</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-5-小结"><span class="nav-text">7.5 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第8章-回归"><span class="nav-text">第8章 回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1-线性回归"><span class="nav-text">8.1 线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-拟合"><span class="nav-text">8.2 拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-3-残差分析"><span class="nav-text">8.3 残差分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-4-过拟合"><span class="nav-text">8.4 过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-5-欠拟合"><span class="nav-text">8.5 欠拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-6-曲线拟合转化为线性拟合"><span class="nav-text">8.6 曲线拟合转化为线性拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-7-小结"><span class="nav-text">8.7 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第9章-聚类"><span class="nav-text">第9章 聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-1-K-Means算法"><span class="nav-text">9.1 K-Means算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2-有趣模式"><span class="nav-text">9.2 有趣模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-3-孤立点"><span class="nav-text">9.3 孤立点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-4-层次聚类"><span class="nav-text">9.4 层次聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-5-密度聚类"><span class="nav-text">9.5 密度聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-6-聚类评估"><span class="nav-text">9.6 聚类评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-1-聚类趋势"><span class="nav-text">9.6.1 聚类趋势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-2-簇数确定"><span class="nav-text">9.6.2 簇数确定</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-3-测定聚类质量"><span class="nav-text">9.6.3 测定聚类质量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-7-小结"><span class="nav-text">9.7 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第10章-分类"><span class="nav-text">第10章 分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#10-1-朴素贝叶斯"><span class="nav-text">10.1 朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-1-天气的预测"><span class="nav-text">10.1.1 天气的预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-2-疾病的预测"><span class="nav-text">10.1.2 疾病的预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-3-小结"><span class="nav-text">10.1.3 小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-2-决策树归纳"><span class="nav-text">10.2 决策树归纳</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-1-样本收集"><span class="nav-text">10.2.1 样本收集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-2-信息增益"><span class="nav-text">10.2.2 信息增益</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-3-连续型变量"><span class="nav-text">10.2.3 连续型变量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-3-随机森林"><span class="nav-text">10.3 随机森林</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-4-隐马尔可夫模型"><span class="nav-text">10.4 隐马尔可夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-4-1-维特比算法"><span class="nav-text">10.4.1 维特比算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-4-2-前向算法"><span class="nav-text">10.4.2 前向算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-5-支持向量机SVM"><span class="nav-text">10.5 支持向量机SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-1-年龄和好坏"><span class="nav-text">10.5.1 年龄和好坏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-2-“下刀”不容易"><span class="nav-text">10.5.2 “下刀”不容易</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-3-距离有多远"><span class="nav-text">10.5.3 距离有多远</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-4-N维度空间中的距离"><span class="nav-text">10.5.4 N维度空间中的距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-4-N维度空间中的距离-1"><span class="nav-text">10.5.4 N维度空间中的距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-6-分不开怎么办"><span class="nav-text">10.5.6 分不开怎么办</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-7-示例"><span class="nav-text">10.5.7 示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-8-小结"><span class="nav-text">10.5.8 小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-6-遗传算法"><span class="nav-text">10.6 遗传算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-6-1-进化过程"><span class="nav-text">10.6.1 进化过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-6-2-算法过程"><span class="nav-text">10.6.2 算法过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-6-3-背包问题"><span class="nav-text">10.6.3 背包问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-6-4-极大值问题"><span class="nav-text">10.6.4 极大值问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-7-小结"><span class="nav-text">10.7 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第11章-关联分析"><span class="nav-text">第11章 关联分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#11-1-频繁模式和Apriori算法"><span class="nav-text">11.1 频繁模式和Apriori算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-1-频繁模式"><span class="nav-text">11.1.1 频繁模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-2-支持度和置信度"><span class="nav-text">11.1.2 支持度和置信度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-3-经典的Apriori算法"><span class="nav-text">11.1.3 经典的Apriori算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-4-求出所有频繁模式"><span class="nav-text">11.1.4 求出所有频繁模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-2-关联分析与相关性分析"><span class="nav-text">11.2 关联分析与相关性分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-3-稀有模式和负模式"><span class="nav-text">11.3 稀有模式和负模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-4-小结"><span class="nav-text">11.4 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第12章-用户画像"><span class="nav-text">第12章 用户画像</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#12-1-标签"><span class="nav-text">12.1 标签</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-2-画像的方法"><span class="nav-text">12.2 画像的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-1-结构化标签"><span class="nav-text">12.2.1 结构化标签</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-2-非结构化标签"><span class="nav-text">12.2.2 非结构化标签</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-3-利用用户画像"><span class="nav-text">12.3 利用用户画像</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-1-割裂型用户画像"><span class="nav-text">12.3.1 割裂型用户画像</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-2-紧密型用户画像"><span class="nav-text">12.3.2 紧密型用户画像</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-3-到底“像不像”"><span class="nav-text">12.3.3 到底“像不像”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-4-小结"><span class="nav-text">12.4 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第13章-推荐算法"><span class="nav-text">第13章 推荐算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#13-1-推荐思路"><span class="nav-text">13.1 推荐思路</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-1-贝叶斯分类"><span class="nav-text">13.1.1 贝叶斯分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-2-利用搜索记录"><span class="nav-text">13.1.2 利用搜索记录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-2-User-based-CF"><span class="nav-text">13.2 User-based CF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-3-Item-based-CF"><span class="nav-text">13.3 Item-based CF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-4-优化问题"><span class="nav-text">13.4 优化问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-5-小结"><span class="nav-text">13.5 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第14章-文本挖掘"><span class="nav-text">第14章 文本挖掘</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#14-1-文本挖掘的领域"><span class="nav-text">14.1 文本挖掘的领域</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-2-文本分类"><span class="nav-text">14.2 文本分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-1-Rocchio算法"><span class="nav-text">14.2.1 Rocchio算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-2-朴素贝叶斯算法"><span class="nav-text">14.2.2 朴素贝叶斯算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-3-K-近邻算法"><span class="nav-text">14.2.3 K-近邻算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-4-支持向量机SVM算法"><span class="nav-text">14.2.4 支持向量机SVM算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-3-小结"><span class="nav-text">14.3 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第15章-人工神经网络"><span class="nav-text">第15章 人工神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#15-1-人的神经网络"><span class="nav-text">15.1 人的神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-1-1-神经网络结构"><span class="nav-text">15.1.1 神经网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-1-2-结构模拟"><span class="nav-text">15.1.2 结构模拟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-1-3-训练与工作"><span class="nav-text">15.1.3 训练与工作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-2-FANN库简介"><span class="nav-text">15.2 FANN库简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-3-常见的神经网络"><span class="nav-text">15.3 常见的神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-4-BP神经网络"><span class="nav-text">15.4 BP神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-4-1-结构和原理"><span class="nav-text">15.4.1 结构和原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-4-2-训练过程"><span class="nav-text">15.4.2 训练过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-4-3-过程解释"><span class="nav-text">15.4.3 过程解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-4-4-示例"><span class="nav-text">15.4.4 示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-5-玻尔兹曼机"><span class="nav-text">15.5 玻尔兹曼机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-5-1-退火模型"><span class="nav-text">15.5.1 退火模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-5-2-玻尔兹曼机"><span class="nav-text">15.5.2 玻尔兹曼机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-6-卷积神经网络"><span class="nav-text">15.6 卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#15-6-1-卷积"><span class="nav-text">15.6.1 卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-6-2-图像识别"><span class="nav-text">15.6.2 图像识别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-7-深度学习"><span class="nav-text">15.7 深度学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15-8-小结"><span class="nav-text">15.8 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第16章-大数据框架简介"><span class="nav-text">第16章 大数据框架简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#16-1-著名的大数据框架"><span class="nav-text">16.1 著名的大数据框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16-2-Hadoop框架"><span class="nav-text">16.2 Hadoop框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#16-2-1-MapReduce原理"><span class="nav-text">16.2.1 MapReduce原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-2-2-安装Hadoop"><span class="nav-text">16.2.2 安装Hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-2-3-经典的WordCount"><span class="nav-text">16.2.3 经典的WordCount</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16-4-分布式列存储框架"><span class="nav-text">16.4 分布式列存储框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16-5-PrestoDB——神奇的CLI"><span class="nav-text">16.5 PrestoDB——神奇的CLI</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#16-5-1-Presto为什么那么快"><span class="nav-text">16.5.1 Presto为什么那么快</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-5-2-安装Presto"><span class="nav-text">16.5.2 安装Presto</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16-6-小结"><span class="nav-text">16.6 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第17章-系统架构和调优"><span class="nav-text">第17章 系统架构和调优</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#17-1-速度——资源的配置"><span class="nav-text">17.1 速度——资源的配置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-1-思路一：逻辑层面的优化"><span class="nav-text">17.1.1 思路一：逻辑层面的优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-2-思路二：容器层面的优化"><span class="nav-text">17.1.2 思路二：容器层面的优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-3-思路三：存储结构层面的优化"><span class="nav-text">17.1.3 思路三：存储结构层面的优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-4-思路四：环节层面的优化"><span class="nav-text">17.1.4 思路四：环节层面的优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-1-5-资源不足"><span class="nav-text">17.1.5 资源不足</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#17-2-稳定——资源的可用"><span class="nav-text">17.2 稳定——资源的可用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#17-2-1-借助云服务"><span class="nav-text">17.2.1 借助云服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-2-2-锁分散"><span class="nav-text">17.2.2 锁分散</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-2-3-排队"><span class="nav-text">17.2.3 排队</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-2-4-谨防“雪崩”"><span class="nav-text">17.2.4 谨防“雪崩”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#17-3-小结"><span class="nav-text">17.3 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第18章-数据解读与数据的价值"><span class="nav-text">第18章 数据解读与数据的价值</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#18-1-运营指标"><span class="nav-text">18.1 运营指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#18-1-1-互联网类型公司常用指标"><span class="nav-text">18.1.1 互联网类型公司常用指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-1-2-注意事项"><span class="nav-text">18.1.2 注意事项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-2-AB测试"><span class="nav-text">18.2 AB测试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#18-2-1-网页测试"><span class="nav-text">18.2.1 网页测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-2-2-方案测试"><span class="nav-text">18.2.2 方案测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-2-3-灰度发布"><span class="nav-text">18.2.3 灰度发布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-2-4-注意事项"><span class="nav-text">18.2.4 注意事项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-3-数据可视化"><span class="nav-text">18.3 数据可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#18-3-1-图表"><span class="nav-text">18.3.1 图表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-3-2-表格"><span class="nav-text">18.3.2 表格</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-4-多维度——大数据的灵魂"><span class="nav-text">18.4 多维度——大数据的灵魂</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#18-4-1-多大算大"><span class="nav-text">18.4.1 多大算大</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-4-2-大数据网络"><span class="nav-text">18.4.2 大数据网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-4-3-去中心化才能活跃"><span class="nav-text">18.4.3 去中心化才能活跃</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-4-4-数据会过剩吗"><span class="nav-text">18.4.4 数据会过剩吗</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-5-数据变现的场景"><span class="nav-text">18.5 数据变现的场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#18-5-1-数据价值的衡量的讨论"><span class="nav-text">18.5.1 数据价值的衡量的讨论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-5-2-场景1：征信数据"><span class="nav-text">18.5.2 场景1：征信数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-5-3-场景2：宏观数据"><span class="nav-text">18.5.3 场景2：宏观数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-5-4-场景3：画像数据"><span class="nav-text">18.5.4 场景3：画像数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-6-小结"><span class="nav-text">18.6 小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录A-VMware-Workstation的安装"><span class="nav-text">附录A VMware Workstation的安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-1-VMware简介"><span class="nav-text">A.1 VMware简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-2-安装前的准备工作"><span class="nav-text">A.2 安装前的准备工作</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录B-CentOS虚拟机的安装方法"><span class="nav-text">附录B CentOS虚拟机的安装方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-1-下载光盘镜像"><span class="nav-text">B.1 下载光盘镜像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-2-创建VMware虚拟机"><span class="nav-text">B.2 创建VMware虚拟机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-3-安装CentOS-7操作系统"><span class="nav-text">B.3 安装CentOS 7操作系统</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录C-Python语言简介"><span class="nav-text">附录C Python语言简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录D-Scikit-learn库简介"><span class="nav-text">附录D Scikit-learn库简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录E-FANN-for-Python安装"><span class="nav-text">附录E FANN for Python安装</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录F-群众眼中的大数据"><span class="nav-text">附录F 群众眼中的大数据</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#写作花絮"><span class="nav-text">写作花絮</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-text">参考文献</span></a></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hac_lang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  





  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="true"></script>









  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  
  

  
  

  


  

  

  

  

  


  


  




  




  




  



<script>
// GET RESPONSIVE HEIGHT PASSED FROM IFRAME

window.addEventListener("message", function(e) {
  var data = e.data;
  if ((typeof data === 'string') && (data.indexOf('ciu_embed') > -1)) {
    var featureID = data.split(':')[1];
    var height = data.split(':')[2];
    $(`iframe[data-feature=${featureID}]`).height(parseInt(height) + 30);
  }
}, false);
</script>


  

  

  


  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":250,"height":500},"mobile":{"show":false,"scale":0.5},"react":{"opacity":0.7},"log":false,"tagMode":false});</script></body>
</html>
